## 压缩与部署

### 压缩综述

[深度学习助力数据压缩，一文读懂相关理论](https://mp.weixin.qq.com/s/YBJwLqqL7aVUTG0LaUbwxw)

#### layer dropout

[模型压缩实践系列之——layer dropout](https://mp.weixin.qq.com/s/K1R_thLJqegm6QDj2GA5ww)

### 剪枝相关

[2019年的最后一个月，这里有6种你必须要知道的最新剪枝技术](https://mp.weixin.qq.com/s/dABJbmPyEyKugdntHJqwsw)


#### slimmable networks

[深度学习模型剪枝：Slimmable Networks三部曲](https://mp.weixin.qq.com/s/Yiu3GNzzWtuX7aszyKKt5A)

#### TAS(NAS+剪枝)

[Network Pruning via Transformable Architecture Search](https://arxiv.org/pdf/1905.09717.pdf)

[https://github.com/D-X-Y/NAS-Projects](https://github.com/D-X-Y/NAS-Projects)

网络剪枝是深度学习的一个有趣的领域。其思路是分析神经网络的结构，并在其中找到“死角”和有用的参数。然后按照估计好的深度和宽度建立一种新架构，称为剪枝网络。然后，可以将来自原网络中的有用参数传输到新网络。这种方式对于深度卷积神经网络（CNN）特别有用，如果在嵌入式系统中进行部署，网络规模可能会变得很大且不切实际。在前一种情况下，网络剪枝可以减少超参数数量，降低CNN的计算成本。

本文实际上一开始就进行了大型网络的训练。然后通过传输体系结构搜索（TAS）提出了搜索小型网络的深度和宽度的建议。最后，使用知识提炼将大型网络中的知识转移到小型网络中。


### GDP

[GDP：Generalized Device Placement for Dataflow Graphs](https://arxiv.org/pdf/1910.01578.pdf)

大型神经网络的运行时间和可扩展性会受到部署设备的影响。随着神经网络架构和异构设备的复杂性增加，对于专家来说，寻找合适的部署设备尤其具有挑战性。现有的大部分自动设备部署方法是不可行的，因为部署需要很大的计算量，而且无法泛化到以前的图上。为了解决这些问题，研究者提出了一种高效的端到端方法。该方法基于一种可扩展的、在图神经网络上的序列注意力机制，并且可以迁移到新的图上。在不同的表征深度学习模型上，包括 Inception-v3、AmoebaNet、Transformer-XL 和 WaveNet，这种方法相比人工方法能够取得 16% 的提升，以及比之前的最好方法有 9.2% 的提升，在收敛速度上快了 15 倍。为了进一步减少计算消耗，研究者在一系列数据流图上预训练了一个策略网络，并使用 superposition 网络在每个单独的图上微调，在超过 50k 个节点的大型图上得到了 SOTA 性能表现，例如一个 8 层的 GNMT。

推荐：本文是谷歌大脑的一篇论文，通过图网络的方法帮助将模型部署在合适的设备上。推荐收到硬件设备限制，需要找到合适部署图的方法的读者参考。

[ICCV 2019 提前看 | 三篇论文，解读神经网络压缩](https://mp.weixin.qq.com/s/86A9kZkl_sQ1GrHMJ6NWpA)

### metapruning

[MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning](https://arxiv.org/abs/1903.10258)

旷视。近年来，有研究表明无论是否保存了原始网络的权值，剪枝网络都可以达到一个和原始网络相同的准确率。因此，通道剪枝的本质是逐层的通道数量，也就是网络结构。鉴于此项研究，Metapruning决定直接保留裁剪好的通道结构，区别于剪枝的裁剪哪些通道。

本文提出来一个Meta network，名为PruningNet，可以生成所有候选的剪枝网络的权重，并直接在验证集上评估，有效的搜索最佳结构。

### data-free student

[Data-Free Learning of Student Networks](https://arxiv.org/abs/1904.01186v1)

该篇论文是华为提出的一篇蒸馏方向的论文，其主要的创新点是提出的蒸馏过程**不需要原始训练数据的参与**。

### 样本相关性用于蒸馏

[Correlation Congruence for Knowledge Distillation](https://arxiv.org/abs/1904.01802)

这篇论文是由商汤提出的一篇蒸馏方向论文，其主要的亮点在于研究**样本之间的相关性**，利用这种相关性作为蒸馏的知识输出。

### pu learning+压缩

[视频 \| NeurIPS 2019分享：华为诺亚方舟提出基于少量数据的神经网络模型压缩技术](https://mp.weixin.qq.com/s/yAQxDASOg-w5NLi_dSyVsA)

[Positive-Unlabeled Compression on the Cloud](https://arxiv.org/pdf/1909.09757.pdf)

NeurIPS 2019，华为诺亚方舟。神经网络的小型化已经在 cnn 网络中取得了巨大的成功，并且能够在终端设备上例如手机、相机等进行落地应用。然而，由于隐私方面等限制，在实际进行神经网络的小型化时，可能只存在少量标记过的训练数据。如何在这种情况下压缩神经网络并取得好的结果，是急需解决的问题。本讲座首先介绍了半监督学习的正类与未标记学习（pu learning）方法，之后介绍了基于**pu learning**的少量数据下的**神经网络模型压缩**方法。

### 对抗训练+压缩

[Adversarially Trained Model Compression: When Robustness Meets Efﬁciency](https://papers.nips.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework)

快手nips2019，把各种压缩方法都集中到一起，并做一种联合优化，这和之前按照 Pipeline 的形式单独做压缩有很大的不同。与此同时，模型还能抵御对抗性攻击。

#### GSM

[Global Sparse Momentum SGD for Pruning Very Deep Neural Networks](https://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks)

快手nips2019，一般剪枝方法都需要大量调参以更好地保留模型性能，而全局稀疏动量 SGD 会端到端地学习到底哪些权重比较重要，重要的就少压缩一点，不重要的就多压缩一点。核心思想在于，我们给定一个压缩率，模型在训练中就能自己剪裁，并满足这个压缩率。


### 无监督量化

[ResNet压缩20倍，Facebook提出新型无监督模型压缩量化方法](https://mp.weixin.qq.com/s/eUfy_MhyD3mEa73j4m6evA)

[And the Bit Goes Down: Revisiting the Quantization of Neural Networks](https://arxiv.org/abs/1907.05686)

### Autocompress

[AAAI 2020 \| 滴滴&东北大学提出自动剪枝压缩算法框架，性能提升120倍](https://mp.weixin.qq.com/s/4UcjyNQLp7_BNT-LzuscCw)

[AutoCompress: An Automatic DNN Structured Pruning Framework forUltra-High Compression Rates](https://arxiv.org/abs/1907.03141)
