## DL基础研究

### DL背后的原理

[从2019 AI顶会最佳论文，看深度学习的理论基础](https://mp.weixin.qq.com/s/34k4UK0xZ9TUZIsq1-eHcg)

MIT 教授 Tomaso Poggio 曾在他的系列研究中 [1] 表示深度学习理论研究可以分为三大类：

+ 表征问题（Representation）：为什么深层网络比浅层网络的表达能力更好？
+ 最优化问题（Optimization）：为什么梯度下降能找到很好的极小值解，好的极小值有什么特点？
+ 泛化问题（Generalization）：为什么过参数化仍然能拥有比较好的泛化性，不过拟合？

对于表征问题，我们想要知道深度神经网络这种「复合函数」，它的表达能力到底怎么确定，它的复合机制又是什么样的。我们不再满足于「能拟合任意函数」这样的定性描述，我们希望知道是不是有一种方法能描述 50 层 ResNet、12 层 Transformer 的拟合能力，能不能清楚地了解它们的理论性质与过程。

有了表征能力，那也只是具备了拟合潜力，深度学习还需要找到一组足够好的极值点，这就是模型的最优解。不同神经网络的「最优化 Landscape」是什么样的、怎样才能找到这种高维复杂函数的优秀极值点、极值点的各种属性都需要完善的理论支持。

最后就是泛化了，深度模型泛化到未知样本的能力直接决定了它的价值。那么深度模型的泛化边界该怎样确定、什么样的极值点又有更好的泛化性能，很多重要的特性都等我们确定一套理论基准。


[英伟达工程师解读NeurIPS 2019最热趋势：贝叶斯深度学习、图神经网络、凸优化](https://mp.weixin.qq.com/s/lj5B81hQumfJGYkgSfNVTg)

neurips2019 杰出新方向论文奖颁给了Vaishnavh Nagarajan和J. Zico Kolter的《一致收敛理论可能无法解释深度学习中的泛化现象》(Uniform convergence may be unable to explain generalization in deep learning)，其论点是一致收敛理论本身并不能解释深度学习泛化的能力。随着数据集大小的增加，泛化差距(模型对可见和不可见数据的性能差距)的理论界限也会增加，而经验泛化差距则会减小。

Shirin Jalali等人的论文《高斯混合模型的高效深度学习》(Efficient Deep Learning of Gaussian mix Models)从这个问题引入：“通用逼近定理指出，任何正则函数都可以使用单个隐藏层神经网络进行逼近。深度是否能让它更具效率？”他们指出，在高斯混合模型的最佳贝叶斯分类的情况下，这样的函数可以用具有一个隐藏层的神经网络中的O(exp(n))节点来近似，而在两层网络中只有O(n)节点。

#### NTK

神经正切核(neural tangent kernel, NTK)是近年来研究神经网络优化与泛化的一个新方向。它出现在数个spotlight报告和我在NeuIPS与许多人的对话中。Arthur Jacot等人基于完全连通神经网络在无限宽度限制下等同于高斯过程这一众所周知的概念，在函数空间而非参数空间中研究了其训练动力学。他们证明了“在神经网络参数的梯度下降过程中，网络函数(将输入向量映射到输出向量)遵循函数的核函数梯度成本，关于一个新的核：NTK。”他们还表明，当有限层版本的NTK经过梯度下降训练时，其性能会收敛到无限宽度限制NTK，然后在训练期间保持不变。

NeurIPS上关于NTK的论文有：

[Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers](https://arxiv.org/abs/1811.04918)

[On the Inductive Bias of Neural Tangent Kernels](https://arxiv.org/abs/1905.12173)

但是，许多人认为NTK不能完全解释深度学习。神经网络接近NTK状态所需要的超参数设置——低学习率、大的初始化、无权值衰减——在实践中通常不用于训练神经网络。NTK的观点还指出，神经网络只会像kernel方法一样泛化，但从经验上看，它们可以更好地泛化。

Colin Wei等人的论文“Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel”从理论上证明了具有权值衰减的神经网络泛化效果要比NTK好得多，这表明研究 L2-regularized 神经网络可以更好的理解泛化。NeurIPS的以下论文也表明，传统的神经网络可以超越NTK：

[What Can ResNet Learn Efficiently, Going Beyond Kernels?](https://arxiv.org/abs/1905.10337)

[Limitations of Lazy Training of Two-layers Neural Network](https://arxiv.org/abs/1906.08899)

### 优化算法

#### 优化算法综述

[理论、算法两手抓，UIUC 助理教授孙若愚 60 页长文综述深度学习优化问题](https://mp.weixin.qq.com/s/-vD6OMcyJ_hQ3ms5RW20JA)

[Optimization for deep learning: theory and algorithms](https://arxiv.org/pdf/1912.08957.pdf)

#### 复合函数最优化

[Efﬁcient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent](https://papers.nips.cc/paper/8916-efficient-smooth-non-convex-stochastic-compositional-optimization-via-stochastic-recursive-gradient-descent)

nips2019快手，针对复合函数的最优化方法。这里复合指的是一个数学期望函数中复合了另一个数学期望，而常规的 ML 目标函数就只有最外面一个数学期望。这种最优化方法在风险管理或 RL 中非常有用，例如在 RL 中解贝尔曼方程，它本质上就是复合函数最优化问题。

#### adabound

AdaBound是一种优化程序，旨在提高不可见的数据的训练速度和性能，可用PyTorch实现。

AdaBound：一种基于PyTorch实现的优化器，训练速度堪比Adam，质量堪比SGD（ICLR 2019）

[Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://openreview.net/forum?id=Bkg3g2R9FX)

[https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound)

#### batchsize与学习率相关

《控制批大小和学习率以很好地泛化：理论和实证证据》(Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence)中，Fengxiang He 的团队在CIFAR数据集上使用SGD训练了1600个ResNet-110和VGG-19模型，发现这些模型的泛化能力与 batch size负相关，与学习率正相关，与批大小/学习率之比负相关。

#### backpack框架

[BackPACK: Packing more into backprop](https://arxiv.org/abs/1912.10985)

自动微分框架只在计算平均小批量（mini-batch）梯度时进行优化。但在理论上，小批量梯度方差或 Hessian 矩阵近似值等其他数量可以作为梯度实现高效的计算。研究人员对这些数量抱有极大的兴趣，但目前的深度学习软件不支持自动计算。此外，手动执行这些数量非常麻烦，效率低，生成代码的共享性也不高。这种情况阻碍了深度学习的进展，并且导致梯度下降及其变体的研究范围变窄。与此同时，这种情况还使得复现研究以及新提出需要这些数量的方法之间的比较更为复杂。因此，为了解决这个问题，来自图宾根大学的研究者在本文中提出一种基于 PyTorch 的高效框架 BackPACK，该框架可以扩展反向传播算法，进而从一阶和二阶导数中提取额外信息。研究者对深度神经网络上额外数量的计算进行了基准测试，并提供了一个测试最近几种曲率估算优化的示例应用，最终证实了 BackPACK 的性能。

#### Ranger

[可以丢掉SGD和Adam了，新的深度学习优化器Ranger：RAdam + LookAhead强强结合](https://mp.weixin.qq.com/s/htneyNQ779P1qzOafOY-Rw)

#### Shampoo

[二阶梯度优化新崛起，超越 Adam，Transformer 只需一半迭代量](https://mp.weixin.qq.com/s/uHrRBS3Ju9MAWbaukiGnOA)

[Second Order Optimization Made Practical](https://arxiv.org/abs/2002.09018)

摘要：目前，无论是从理论还是应用层面来说，机器学习中的优化都是以随机梯度下降等一阶梯度方法为主。囊括二阶梯度和/或二阶数据统计的二阶优化方法虽然理论基础更强，但受限于计算量、内存和通信花销等因素，二阶梯度优化方法的普及度不高。然而在谷歌大脑与普林斯顿大学等研究者的努力下，二阶梯度优化终于在实战大模型上展现出独特的优势。

研究者表示，为了缩短理论和实际优化效果之间的差距，该论文提出了一种二阶优化的概念性验证，并通过一系列重要的算法与数值计算提升，证明它在实际深度模型中能有非常大的提升。具体而言，在训练深度模型过程中，二阶梯度优化 Shampoo 能高效利用由多核 CPU 和多加速器单元组成的异构硬件架构。并且在大规模机器翻译、图像识别等领域实现了非常优越的性能，要比现有的顶尖一阶梯度下降方法还要好。

推荐：本文的亮点在于研究者提出了真正应用的二阶梯度最优化器，在实战大模型上展现出独特的优势。


### 激活函数

#### 激活函数汇总

[从ReLU到GELU，一文概览神经网络的激活函数](https://mp.weixin.qq.com/s/np_QPpaBS63CXzbWBiXq5Q)

#### GELU

[超越ReLU却鲜为人知，3年后被挖掘：BERT、GPT-2等都在用的激活函数](https://mp.weixin.qq.com/s/LEPalstOc15CX6fuqMRJ8Q)

[Gaussian Error Linear Units (GELUs)](https://arxiv.org/pdf/1606.08415.pdf)

#### Relu神经网络的记忆能力

[Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity](https://arxiv.org/abs/1810.07770)

Chulhee Yun等人发表“小型ReLU网络是强大的记忆器：对记忆能力的严格分析”，表明“具有Omega(sqrt(N))隐藏节点的3层ReLU网络可以完美地记忆具有N个点的大多数数据集。

### 梯度泄露

[梯度会泄漏训练数据？MIT新方法从梯度窃取训练数据只需几步](https://mp.weixin.qq.com/s/nz2JFp8Y7WD5UgOpCDagwQ)

[Deep Leakage from Gradients](https://arxiv.org/abs/1906.08935)

### 彩票假设

原始论文：[The lottery ticket hypothesis: Finding sparse, trainable neural networks](http://arxiv.org/abs/1803.03635)

[田渊栋从数学上证明ICLR最佳论文“彩票假设”，强化学习和NLP也适用](https://mp.weixin.qq.com/s/Q3n28uDk1UEi43bN61NF8w)

fb的博客：[https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks](https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks)

[One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers](https://arxiv.org/pdf/1906.02773.pdf)

NLP中彩票假设的应用：

[Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP](https://arxiv.org/pdf/1906.02768.pdf)

[Proving the Lottery Ticket Hypothesis: Pruning is All You Need](https://arxiv.org/pdf/2002.00585.pdf)

Frankle 和 Carbin 在 2018 年提出的彩票假说表明，一个随机初始化的网络包含一个小的子网络，这个子网络在进行单独地训练时，其性能能够与原始网络匹敌。在本文中，研究者证明了一个更有力的假说（正如 Ramanujan 等人在 2019 年所猜想的那样），即对于每个有界分布和每个带有有界权重的目标网络来说，一个具有随机权重的充分过参数化神经网络包含一个具有与目标网络几乎相同准确率的子网络，并且无需任何进一步的训练。

===>从根本上来说，剪枝随机初始化的神经网络与优化权重值一样重要。

[剪枝与学习权重同等重要，Lottery Ticket Hypothesis第一次被理论证明](https://mp.weixin.qq.com/s/3X48NLdTHYoYeqVS0-A1OQ)


### 知识蒸馏

[一文总览知识蒸馏概述](https://mp.weixin.qq.com/s/-krzT5svcRsGILCDms7-VQ)

### 生成模型

[AAAI 2020 论文解读：关于生成模型的那些事](https://mp.weixin.qq.com/s/b3vSKfHY12XtlIeps7gaNg)

[Probabilistic Graph Neural Network（PGNN）：Deep Generative Probabilistic Graph Neural Networks for Scene Graph Generation](https://grlearning.github.io/papers/135.pdf)

[Reinforcement Learning（RL）: Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning](https://pdfs.semanticscholar.org/826d/b2e5f340a90fc9672279f9e921b596aba4b7.pdf)

[Action Learning: MALA: Cross-Domain Dialogue Generation with Action Learning](https://arxiv.org/pdf/1912.08442.pdf)

### 双下降问题

[深度学习模型陷阱：哈佛大学与OpenAI首次发现“双下降现象”](https://mp.weixin.qq.com/s/RG2mVNAocf0hjXgMArK9NQ)

[Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/pdf/1912.02292.pdf)

### 一些疑难杂症

[如何发现「将死」的ReLu？可视化工具TensorBoard助你一臂之力](https://mp.weixin.qq.com/s/XttlCNKGvGZrD7OQZOQGnw)

### 度量学习

[深度度量学习中的损失函数](https://mp.weixin.qq.com/s/1Tqu8aLn4Jy6ED0Rk3iPHQ)

### 损失函数

#### L_DMI

[NeurIPS 2019 \| 一种对噪音标注鲁棒的基于信息论的损失函数](https://mp.weixin.qq.com/s/MtApYev80-xVEd70lLp_zw)

[L_DMI: An Information-theoretic Noise-robust Loss Function](https://arxiv.org/abs/1909.03388)

[https://github.com/Newbeeer/L_DMI](https://github.com/Newbeeer/L_DMI)

#### 损失函数的pattern

[Loss Landscape Sightseeing with Multi-Point Optimization](https://arxiv.org/abs/1910.03867)

[https://github.com/universome/loss-patterns](https://github.com/universome/loss-patterns)

### softmax优化

#### Mixtape

[CMU杨植麟等人再次瞄准softmax瓶颈，新方法Mixtape兼顾表达性和高效性](https://mp.weixin.qq.com/s/DJNWt3SpnjlwzQqUiLhdXQ)

[Mixtape: Breaking the Softmax Bottleneck Efficiently](https://papers.nips.cc/paper/9723-mixtape-breaking-the-softmax-bottleneck-efficiently.pdf)

### 自监督

[OpenAI科学家一文详解自监督学习](https://mp.weixin.qq.com/s/wtHrHFoT2E_HLHukPdJUig)

[https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html)

### 机器学习+博弈论

[当博弈论遇上机器学习：一文读懂相关理论](https://mp.weixin.qq.com/s/1t6WuTQpltMtP-SRF1rT4g)

### normalization相关

#### BN

[批归一化到底做了什么？DeepMind研究者进行了拆解](https://mp.weixin.qq.com/s/6jAKhdw93E22PPQyBEwBQw)

#### FRN

[超越BN和GN！谷歌提出新的归一化层：FRN](https://mp.weixin.qq.com/s/9EjTX-Al28HLV0k1FZPvIg)

[谷歌力作：神经网络训练中的Batch依赖性很烦？那就消了它！](https://mp.weixin.qq.com/s/2QUlIm8AmA9Bc_kpx7Dh6w)

[Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks](https://arxiv.org/abs/1911.09737)

### VAE

[变分推断（Variational Inference）最新进展简述](https://mp.weixin.qq.com/s/olwyTaOGCugt-thgZm_3Mg)

### 优化方法

[On Empirical Comparisons of Optimizers for Deep Learning](https://arxiv.org/pdf/1910.05446.pdf)

摘要：优化器选择是当前深度学习管道的重要步骤。在本文中，研究者展示了优化器比较对元参数调优协议的灵敏度。研究结果表明，在解释文献中由最近实证比较得出的排名时，元参数搜索空间可能是唯一最重要的因素。但是，当元参数搜索空间改变时，这些结果会相互矛盾。随着调优工作的不断增加，更一般的优化器性能表现不会比近似于它们的那些优化器差，但最近比较优化器的尝试要么假设这些包含关系没有实际相关性，要么通过破坏包含的方式限制元参数。研究者在实验中发现，优化器之间的包含关系实际上很重要，并且通常可以对优化器比较做出预测。具体来说，流行的自适应梯度方法的性能表现绝不会差于动量或梯度下降法。

推荐：如何选择优化器？本文从数学角度论证了不同优化器的特性，可作为模型构建中的参考资料。

### 调参

[你有哪些deep learning（rnn、cnn）调参的经验？](https://www.zhihu.com/question/41631631/)

### 表示学习

[窥一斑而知全豹，三篇论文遍历ICLR 2020新型表征方式](https://mp.weixin.qq.com/s/k_vTgZGoVD-pVBjEqI6sfw)


### 新的结构

#### NALU

[Measuring Arithmetic Extrapolation Performance](https://arxiv.org/abs/1910.01888)

摘要：神经算术逻辑单元（NALU）是一种神经网络层，可以学习精确的算术运算。NALU 的目标是能够进行完美的运算，这需要学习到精确的未知算术问题背后的底层逻辑。评价 NALU 性能是非常困难的，因为一个算术问题可能有许多种类的解法。因此，单实例的 MSE 被用于评价和比较模型之间的表现。然而，MSE 的大小并不能说明是否是一个正确的方法，也不能解释模型对初始化的敏感性。因此，研究者推出了一种「成功标准」，用来评价模型是否收敛。使用这种方法时，可以从很多初始化种子上总结成功率，并计算置信区间。通过使用这种方法总结 4800 个实验，研究者发现持续性的学习算术推导是具有挑战性的，特别是乘法。

推荐：尽管神经算术逻辑单元的出现说明了使用神经网络进行复杂运算推导是可行的，但是至今没有一种合适的评价神经网络是否能够成功收敛的标准。本文填补了这一遗憾，可供对本领域感兴趣的读者参考。

#### 权重无关

[https://weightagnostic.github.io/](https://weightagnostic.github.io/)

[Weight Agnostic Neural Networks](https://arxiv.org/abs/1906.04358)

[探索权重无关神经网络](https://mp.weixin.qq.com/s/g7o60Ypri0J1e65cZQspiA)

#### on-lstm

[Ordered Neurons: Integrating Tree Structures Into Recurrent Neural Networks](https://arxiv.org/abs/1810.09536)

[https://zhuanlan.zhihu.com/p/65609763](https://zhuanlan.zhihu.com/p/65609763)

### 其他相关理论

#### 因果关系

[贝叶斯网络之父Judea Pearl力荐、LeCun点赞，这篇长论文全面解读机器学习中的因果关系](https://mp.weixin.qq.com/s/E04x_tqWPaQ4CSWfGVbnTw)

[Causality for Machine Learning](https://arxiv.org/abs/1911.10500)

由 Judea Pearl 倡导的图形因果推理（graphical causal inference）源于 AI 研究，并且在很长一段时间内，它与机器学习领域几乎没有任何联系。在本文中，研究者探讨了图形因果推理与机器学习之间已建立以及应该建立哪些联系，并介绍了一些关键概念。本文认为，机器学习和 AI 领域的未解决难题在本质上与因果关系有关，并解释了因果关系领域如何理解这些难题。

### 能量模型

[ICLR 2020 \| 分类器其实是基于能量的模型？判别式分类器设计新思路](https://mp.weixin.qq.com/s/7qcuHvk9UoCvoJRFzQXuHQ)

[Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One](https://arxiv.org/abs/1912.03263)

### 贝叶斯深度学习

正如Emtiyaz Khan在他的受邀演讲《基于贝叶斯原理的深度学习》中所强调的那样，贝叶斯学习和深度学习是非常不同的。根据Khan的说法，深度学习使用“试错”(trial and error)的方法——看实验会把我们带向何方——而贝叶斯原理迫使你事先思考假设(先验)。

与常规的深度学习相比，贝叶斯深度学习主要有两个吸引人的点：不确定性估计和对小数据集的更好的泛化。在实际应用中，仅凭系统做出预测是不够的。知道每个预测的确定性很重要。在贝叶斯学习中，不确定性估计是一个内置特性。

传统的神经网络给出单点估计——使用一组权值在数据点上输出预测。另一方面，贝叶斯神经网络使用网络权值上的概率分布，并输出该分布中所有权值集的平均预测，其效果与许多神经网络上的平均预测相同。因此，贝叶斯神经网络是自然的集合体，它的作用类似于正则化，可以防止过拟合。

拥有数百万个参数的贝叶斯神经网络的训练在计算上仍然很昂贵。收敛到一个后验值可能需要数周时间，因此诸如变分推理之类的近似方法已经变得流行起来。Probabilistic Methods – Variational Inference类发表了10篇关于这种变分贝叶斯方法的论文。

[Importance Weighted Hierarchical Variational Inference](https://arxiv.org/abs/1905.03290)

[A Simple Baseline for Bayesian Uncertainty in Deep Learning](https://arxiv.org/abs/1902.02476)

[Practical Deep Learning with Bayesian Principles](https://arxiv.org/abs/1906.02506)

### 可解释性

[相信你的模型：初探机器学习可解释性研究进展](https://mp.weixin.qq.com/s/7ngrHNd4__MN3Wb5RMv6qQ)

[NeurIPS 2019：两种视角带你了解网络可解释性的研究和进展](https://mp.weixin.qq.com/s/oud7w6MNWPO8svEHZxD4ZA)

[Intrinsic dimension of data representations in deep neural networks](https://arxiv.org/pdf/1905.12784v1.pdf)

对于一个深度网络，网络通过多层神经层渐进的转换输入，这其中的几何解释应该是什么样的呢？本文的作者通过实验发现，以固有维度（ID：intrinsic dimensionality）为切入点，可以发现训练好的网络相比较未训练网络而言，其每层的固有维度数量级均小于每层单元数，而且 ID 的存在可以来衡量网络的泛化性能。

[This Looks Like That: Deep Learning for Interpretable Image Recognition](https://arxiv.org/pdf/1806.10574.pdf)

当人遇到图像判断的时候，总是会分解图片并解释分类的理由，而机器在判断的时候总是跟人的判断会有些差距。本文旨在缩小机器分类和人分类之间的差距，提出了一个 ProtoPNet，根据人判断的机理来分类图像。本文网络通过分解图像，得到不同的原型部分，通过组成这些信息最终得到正确的分类。


challenge sets，大部分是英文，中文比较少。构造方法：

+ 从已有数据集泛化：改下位词、同义词、反义词
+ 从已有数据集只抽出可用的部分
+ 使用模板建模具体语言特征
+ 对抗样本


### 子集选择

[AAAI 2020线上分享 \| 南京大学：一般约束下子集选择问题的高效演化算法](https://mp.weixin.qq.com/s/gl6HNZZoQcsHdhF2v_uriQ)

[An Efficient Evolutionary Algorithm for Subset Selection with General Cost Constraints](http://www.lamda.nju.edu.cn/qianc/aaai20-eamc-final.pdf)

子集选择问题旨在从 n 个元素中，选择满足约束 c 的一个子集，以最大化目标函数 f。它有很多应用，包括影响力最大化，传感器放置等等。针对这类问题，现有的代表性算法有广义贪心算法和 POMC。广义贪心算法耗时较短，但是受限于它的贪心行为，其找到的解质量往往一般；POMC 作为随机优化算法，可以使用更多的时间来找到质量更好的解，但是其缺乏多项式的运行时间保证。因此，我们提出一个高效的演化算法 EAMC。通过优化一个整合了 f 和 c 的代理函数，它可以在多项式时间内找到目前已知最好的近似解，并且其在多类问题上的试验也显示出比广义贪心算法更好的性能。

[AAAI 2020 \| 南京大学提出高效演化算法 EAMC：可更好解决子集选择问题](https://mp.weixin.qq.com/s/QDbWwT5ZP2MNF3NVHX_SzQ)

