下载本文pdf：[https://github.com/daiwk/collections/blob/master/pdfs/llm_aigc.pdf](https://github.com/daiwk/collections/blob/master/pdfs/llm_aigc.pdf)

各种学习相关代码

[https://github.com/daiwk/llms_new](https://github.com/daiwk/llms_new)


# 从word2v到Transformer

## LSTM

[超生动图解LSTM和GRU，一文读懂循环神经网络！](https://mp.weixin.qq.com/s/vVDAB2U7478yOXUT9ByjFw)


## fasttext&word2vec

注：w2v训练时的内积不是2个emb-in的内积，而是emb-in和emb-out的内积

[fasttext源码解析](https://my.oschina.net/u/3800567/blog/2877570)

+ ```Dictionary::readWord```：空格分割，一次读出来一个word
+ ```Dictionary::add```：每个word求个hash，加进词典时，id就是从0开始的序号，同时记录一下词频
+ ```Dictionary::threshold```：按词频排序，扔掉低频词
+ ```Dictionary::initNgrams```：每个词，加上前缀BOW（<）和后缀（>），然后先扔进这个词的subwords里，然后再调用``` Dictionary::computeSubwords```把这个词的ngrams也扔进它的subwords里

整个词表，是word数+bucket这么大，其中bucket表示可容纳的subwords和wordNgrams的数量，默认200w


[为什么Word2Vec训练中, 需要对负采样权重开3/4次幂？](https://zhuanlan.zhihu.com/p/144563199?utm_id=0)

[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)里提到

![](../assets/word2vec_3_4_sample.jpeg)

通过对权重开3/4次幂，可以提升低频词被抽到的概率。在保证高频词容易被抽到的大方向下，通过权重3/4次幂的方式，适当提升低频词、罕见词被抽到的概率。如果不这么做，低频词，罕见词很难被抽到，以至于不被更新到对应的Embedding。

## BPE/WordPiece分词

[【Subword】深入理解NLP Subword算法：BPE、WordPiece、ULM](https://mp.weixin.qq.com/s/U9F8G-OUCb9kunTk_tZYFw)


## Transformer原理

[从三大顶会论文看百变Self-Attention](https://mp.weixin.qq.com/s/R9FoceRsPB3ceqKpnYPvbQ)

[包学包会，这些动图和代码让你一次读懂「自注意力」](https://mp.weixin.qq.com/s/Z0--eLLiFwfSuMvnddKGPQ)

[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)

[从熵不变性看Attention的Scale操作](https://kexue.fm/archives/8823)

[Transformers Assemble（PART I）](https://mp.weixin.qq.com/s/NZM05zyUkldOwpNIbsOtDQ) 讲了3篇

[Transformers Assemble（PART II）](https://mp.weixin.qq.com/s/JdUVaQ3IyrflHvxIk5jYbQ) 又讲了三篇

### 为什么层次化softmax没人用了

[Transformer 结构中最后一层 softmax 为什么不再使用 层次化softmax了呢？](https://www.zhihu.com/question/310845030/answer/595573391?hb_wx_block=0&utm_source=wechat_session&utm_medium=social&utm_oi=632586637935251456)

主要还是计算资源的问题。

Mikolov发明word2vec的几个版本大概在13-14年前后。那个时候GPU非常少见，印象里面CMU的NLP组没有GPU，Stanford NLP lab只有6块K40。

大规模直接算softmax是在google的14年那篇seq2seq做MT的文章。为了快，把一个softmax并行在4块GPU上，每个GPU负责四分之一。那个年代，大多数NLP组全组都不会有4块GPU。

hierarchical softmax是softmax的近似，suboptimal的。当如今计算资源足够大的时候，当然包括时间和显存 (BERT 和 Elmo 都没有用hierarchical)，hierarchical softmax就逐渐退出了历史舞台。

### Transformer会不会规划未来

[Transformer本可以深谋远虑，但就是不做](https://mp.weixin.qq.com/s/1kolCWSsFAp4e9MGG089vQ)

[Do Language Models Plan for Future Tokens?](https://arxiv.org/pdf/2404.00859.pdf)

在训练期间的梯度既会为**当前token位置**的损失优化权重，也会为该序列**后面的token**进行优化，那么这二者会以怎样的比例分配资源？

+ 预缓存假设（pre-caching hypothesis）：在时间步$$t$$计算与**当前时间步的推理任务无关**但**可能对未来时间步$$t + \tau$$有用**的特征
+ 面包屑假设（breadcrumbs hypothesis）：与时间步$$t$$**最相关的特征**已经**等同**于将在**时间步$$t + \tau$$最有用**的特征。

设计了一种合成场景，其中**只能通过显式的预缓存完成任务**，即模型必须为下一token预先计算信息，**否则就无法在一次单向通过中准确计算出正确答案。**发现明显的证据说明transformer可以学习预缓存，即当必须预计算信息来最小化损失时，它们就会这样做。

但在真实语言数据上，语言模型并不会显著地准备用于未来的信息。相反，它们是计算对预测**下一个token有用的特征**——事实证明**这对未来的步骤也很有用**。


## Transformer的FLOPS和访存带宽

[https://zhuanlan.zhihu.com/p/624740065](https://zhuanlan.zhihu.com/p/624740065)

$$A$$的shape是$$m\times k$$，$$B$$的shape是$$k\times n$$，那么矩阵乘法$$AB$$需要$$m\times k\times n$$次的乘法，也需要同样多次的加法，所以FLOPS是$$2\times m\times k\times n$$

假设batchsize是$$b$$，序列长度$$s$$，原来的emb是$$d$$，即输入的是$$[b,s,d]$$，一般$$d=d_k=d_v=d_q$$，$$W_Q$$、$$W_K$$、$$W_V$$都是$$d_v\times d_v$$，对应的Q、K、V矩阵都是$$s\times d_v$$，有$$head\_num$$个头，每个头的维度$$per\_head\_d=\frac{d}{head\_num}$$

### attention的FLOPS

attention的公式：

XXX
\begin{aligned}
&Q=x W_Q, K=x W_K, V=x W_V\\
&x_{\text {out }}=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{h}}\right) \cdot V \cdot W_o+x
\end{aligned}
XXX


+ 计算3个Q、K、V：要算三次$$s\times d$$和$$d\times d_v$$的矩阵乘法，所以是：$$3\times 2\times b\times s\times d\times d_v$$
    + 输入：$$[b, s, d]$$和3个$$[b, d, d_v]$$
    + 输出：$$[b, s, d_v]$$，再把最后一维$$d_v$$拆成$$head\_num$$份，再把中间两维互换一下，得到$$[b, head\_num, s, per\_head\_d]$$
+ 计算Q和K的相似度：要算一次$$s\times d_v$$和$$d_v\times s$$的矩阵乘法，$$2\times b\times s^2\times d_k$$
    + 输入：$$[b, head\_num, s, per\_head\_d]$$和$$[b, head\_num, per\_head\_d, s]$$
    + 输出：$$[b, head\_num, s, s]$$
+ 把相似度用到V上：要算一次$$s\times s$$和$$s\times d_v$$的矩阵乘法，，$$2\times b\times s^2 \times d_v$$
    + 输入：$$[b, head\_num, s, s]$$和$$[b, head\_num, s, per\_head\_d]$$
    + 输出：$$[b, head\_num, s, per\_head\_d]$$
+ 最后过一个线性映射：要算一次$$s\times d_v$$的和$$d_v\times d_v$$的矩阵乘法，$$2\times b\times s\times d_v\times d_v$$
    + 输入：$$[b, s, d_v]$$和$$[d_v, d_v]$$
    + 输出：$$[b, s, d_v]$$

因为$$d_k=d_v=d_q=d$$，单纯计算attention总共就是$$8bsd^2 + 4bs^2d$$

### FFN的FLOPS

FFN的公式：

XXX
x=f_{\text {gelu }}\left(x_{\text {out }} W_1\right) W_2+x_{\text {out }}
XXX

在原始Transformer中，$$W_1$$的shape是$$[d,4d]$$，$$W_2$$的shape是$$[4d,d]$$，

+ 第一个线性层：$$2\times b\times s\times\ d\times 4d=8\times b\times s\times\ d^2$$
    + 输入：$$[b, s, d]$$和$$[d,4d]$$
    + 输出：$$[b, s, 4d]$$
+ 第二个线性层：$$2\times b\times s\times\ 4d\times d=8\times b\times s\times\ d^2$$
    + 输入：$$[b, s, 4d]$$和$$[4d,d]$$
    + 输出：$$[b, s, d]$$

所以一层Transformer，即attention+FFN的计算量为$$(8bsd^2 + 4bs^2d)+16bsd^2=24bsd^2+4bs^2d$$

有两点需要注意的：

+ 对NLP任务来讲，一般$$d$$是个比较固定的值，如512，而$$s$$变大，效果会更好，所以一般是$$s>d$$，所以复杂度取决于$$s$$的大小。
+ 但有些模型的初始设置不是这样的，例如GPT3的175B模型里，$$s=2048,d=12288$$，当然，算力够的话也可以把$$s$$变大

自己感觉：既然是$$24bsd^2+4bs^2d$$，其实就是$$6sd^2$$和$$s^2d$$的大小，即$$6d$$和$$s$$的大小，如果$$6d>s$$，则$$d^2$$起主导，反之$$s^2$$起主导

### DIN的FLOPS

特殊地，对于推荐中的DIN那种，看当前item和历史s个item的相关性，即q的序列长度只有1，不考虑多头，而这其实也是decoder预测下一个词时过一层Transformer的复杂度

已经有3个序列长度为$$s-1$$的QKV的cache，要算第$$s$$个词和这$$s-1$$个词的attention

+ 计算第$$s$$个词的3个Q、K、V：要算三次$1\times d$和$$d\times d_v$$的矩阵乘法，所以是：$$3\times 2\times b\times 1\times d\times d_v$$
    + 输入：$$[b, 1, d]$$和3个$$[b, d, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 计算Q和K的相似度：要算一次$$1\times d_v$$和$$d_v\times s$$的矩阵乘法，$$2\times b\times 1\times d_k\times s$$【这里的K是历史$$s-1$$长度的序列拼上当前词，当然对DIN来讲要去掉当前词，这里先忽略这个】
    + 输入：$$[b, 1, d_v]$$和$$[b, d_v, s]$$
    + 输出：$$[b, 1, s]$$
+ 把相似度用到V上：要算一次$$1\times s$$和$$s\times d_v$$的矩阵乘法，，$$2\times b\times 1 \times d_v \times s$$【同样地，这里的V是历史$$s-1$$长度的序列拼上当前词，当然对DIN来讲要去掉当前词，这里先忽略这个】
    + 输入：$$[b, 1, s]$$和$$[b, s, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 最后过一个线性映射：要算一次$$1\times d_v$$的和$$d_v\times d_v$$的矩阵乘法，$$2\times b\times 1\times d_v\times d_v$$
    + 输入：$$[b, 1, d_v]$$和$$[d_v, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 第一个线性层：$$2\times b\times 1\times\ d\times 4d=8\times b\times 1\times\ d^2$$
    + 输入：$$[b, 1, d]$$和$$[d,4d]$$
    + 输出：$$[b, 1, 4d]$$
+ 第二个线性层：$$2\times b\times 1\times\ 4d\times d=8\times b\times 1\times\ d^2$$
    + 输入：$$[b, 1, 4d]$$和$$[4d,d]$$
    + 输出：$$[b, 1, d]$$

总共是$$6bd^2+2bds+2bds+2bd^2+8bd^2+8bd^2=24bd^2+4bds$$

### Transformer的访存

GPU架构的介绍参考[https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)和[https://www.zhihu.com/question/319355296/answer/2193938981](https://www.zhihu.com/question/319355296/answer/2193938981)，GPU对比CPU如下：

+ 任务模式
    + CPU由专为顺序串行处理而优化的几个核心组成
    + GPU则拥有一个由数以千计的更小、更高效的核心（专为同时处理多重任务而设计）组成的大规模并行计算架构。同时CPU相当的一部分时间在执行外设的中断、进程的切换等任务，而GPU有更多的时间并行计算。
+ 功能定位
    + CPU不但要承担计算任务还有承担逻辑控制等任务。
    + GPU在渲染画面时需要同时渲染数以百万记的顶点或三角形，故GPU的设计是可以充分支持并行计算。
+ 系统集成
    + GPU作为一类外插设备，在尺寸、功率、散热、兼容性等方面的限制远远小于CPU，这样可以让GPU有较大的显存和带宽。

以A100为例，整体架构如下

![ga100-full-gpu-128-sms](../assets/ga100-full-gpu-128-sms.png)

1. PCIE层：通过PCIE接口以外设的方式集成到服务器上。
2. 中间一坨绿色的部分是GPU的计算核心**SM(Streaming Multiprocessor)**，在A100中，一个SM有64个用于计算的Core，共108个SM(图里是GA100,有128个SM)，故共6192个Core。
3. 中间蓝色部分是L2缓存
4. NVLink：**多个GPU间进行通信**的组件，会优化GPU间的通信，提升传输效率。
5. 两侧的HBM2是显存，目前的A100的显存有两种40G and 80G

A100的SM如图所示，

![a100-sm](../assets/a100-sm.png)

GPU的显存分成两部分：

+ Global Memory：整体架构图中的**两侧HBM2**部分，例如A100 80G就有80G的global memory，**2TB/s带宽**，访问速度比较慢
+ Shared Memory：SM图中**浅蓝色**的L1 Data Cache，例如A100中每个SM中有**192KB**，访问速度比较快

从图中可见，A100的**FP16**有**312T的FLOPS**

![A100-tensorcore-performance](../assets/A100-tensorcore-performance.png)

以矩阵乘法为例，$$[M, K] \times [K, N] -> [M,N]$$，

+ 计算时间：$$2MKN/FLOPS$$
+ 访存时间为：$$(MN+MK+KN)/memory\_bandwidth$$，因为首先要**读取**$$MK$$和$$NK$$这两个矩阵，然后结果还要**写入**$$MN$$这个矩阵里。假设是fp16，占2bytes，那就还要乘以2

假设$$b=1,s=4096,d=d_k=2048$$，以计算Q和K的相似度为例，对比一下训练和预测时的**计算耗时**和**访存耗时**

+ **训练时**：$$M=4096,N=2048,K=4096$$==>**计算是瓶颈**
    + FLOPS：$$2\times b\times s^2\times d_k=2\times 1\times 4096^2\times 2048=68719476736$$
    + 计算耗时：$$FLOPS/max\_FLOPS=68719476736/(312\times 10^{12})=0.00022s=220\times 10^{-6}s=220us$$
    + 访存耗时：$$(MN+MK+KN)/memory\_bandwidth=2\times (4096\times 2048+4096\times 4096+4096\times 2048)/(2\times 10^{12})=3.35544\times 10^{-5}s=33.544\times 10^{-6}=33.5544us$$

+ **预测时**：$$M=1,N=2048,K=4096$$==>**访存是瓶颈**
    + FLOPS：$$2\times b\times 1\times d_k\times s=2\times 1\times 2048\times 4096=16777216$$ 
    + 计算耗时：$$FLOPS/max\_FLOPS=16777216/(312\times 10^{12})=5.38\times 10^{-8}s=0.0538\times 10^{-6}s=0.0538us$$
    + 访存耗时：$$(MN+MK+KN)/memory\_bandwidth=2\times (1\times 2048+1\times 4096+4096\times 2048)/(2\times 10^{12})=8.3948\times 10^{-6}s=8.3948us$$

一些常见的名词：

+ H2D：host to device，从cpu拷贝到gpu
+ D2H：device to host，从gpu拷贝到cpu

## Transformer加速

### lightseq

&nbsp;

[LightSeq: A High Performance Inference Library for Transformers](https://arxiv.org/pdf/2010.13887.pdf)

[LightSeq2: Accelerated Training for Transformer-based Models on GPUs](https://arxiv.org/pdf/2110.05722.pdf)

[https://github.com/bytedance/lightseq](https://github.com/bytedance/lightseq)


# PLM：仅编码器/仅解码器/编码器+解码器

## 仅编码器的BERT

[BERT小学生级上手教程，从原理到上手全有图示，还能直接在线运行](https://mp.weixin.qq.com/s/ltVuXZ4nJh8Cb5X2UhB6tQ)

[BERT源码分析（PART I）](https://mp.weixin.qq.com/s/sSmTQ_cOLyAUV0aV0FkDvw)

[BERT源码分析（PART II）](https://mp.weixin.qq.com/s/1NDxWfBu_csu8qHV2tmmVQ)

[Dive into BERT：语言模型与知识](https://mp.weixin.qq.com/s/NjQtSKY85Np5IodRiKsrvg)

[关于BERT，面试官们都怎么问](https://mp.weixin.qq.com/s/c2PktKruzq_teXm3GAwe1Q)

主要讲了下面3篇：

[Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066)

[Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855)

[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document)


[A Primer in BERTology: What we know about how BERT works](https://arxiv.org/pdf/2002.12327.pdf)

摘要：目前，基于 Transformer 的模型已经广泛应用于自然语言处理中，但我们依然对这些模型的内部工作机制知之甚少。在本文中，来自麻省大学洛威尔分校的研究者对流行的 BERT 模型进行综述，并综合分析了 40 多项分析研究。他们还概览了对模型和训练机制提出的改进，然后描画了未来的研究方向。

[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)


[BERT系列文章汇总导读](https://mp.weixin.qq.com/s/oT2dtmfEQKyrpDTOrpzhWw)

[ALBERT、XLNet，NLP技术发展太快，如何才能跟得上节奏？](https://mp.weixin.qq.com/s/Toth-XKn2WKYkyDw6j5F3A)

[绝对干货！NLP预训练模型：从transformer到albert](https://mp.weixin.qq.com/s/Jgx9eHk9xiSOWEy0Ty3LoA)

[ALBERT一作蓝振忠：预训练模型应用已成熟，ChineseGLUE要对标GLUE基准](https://mp.weixin.qq.com/s/mvkFDy09BdKJC4Cja11PAA)

[有哪些令你印象深刻的魔改Transformer？](https://mp.weixin.qq.com/s/HS2tlT7t18cFytZVIsOXUg)

[BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！](https://mp.weixin.qq.com/s/jVSW0KDhaXuaIeOzoPmCJA)


### multi-head att 实现

输入原始的query(即from_tensor)之后, 把```[batch, from_seq, emb]```变成```[?, emb]```，其中```?=batch*from_seq```

```python
from_tensor_2d = reshape_to_matrix(from_tensor)

def reshape_to_matrix(input_tensor):
    """Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix)."""
    ndims = input_tensor.shape.ndims
    if ndims < 2:
        raise ValueError("Input tensor must have at least rank 2. Shape = %s" %
            (input_tensor.shape))
    if ndims == 2:
        return input_tensor
    width = input_tensor.shape[-1]
output_tensor = tf.reshape(input_tensor, [-1, width])
    return output_tensor
```

然后再接一个fc，把```[?, emb]```变成```[?, head_num * per_head]```，一般```head_num * per_head=emb```。

```python
query_layer = tf.layers.dense(
        from_tensor_2d,
        num_attention_heads * size_per_head,
        activation=query_act,
        name="query",
        kernel_initializer=create_initializer(initializer_range))
```

因为```?=batch*from_seq```，所以可以直接做如下变换

```python
query_layer = transpose_for_scores(query_layer, batch_size,
        num_attention_heads, from_seq_length,
        size_per_head)
```

实际就是把```?```拆开成```batch, from_seq```，整个变成```[batch, from_seq, head_num, per_head]```，然后做了个 transpose，把1和2互换了下，得到```[batch, head_num, from_seq, per_head]```

```python
def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
        seq_length, width):
    output_tensor = tf.reshape(
            input_tensor, [batch_size, seq_length, num_attention_heads, width])

    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
    return output_tensor
```

然后key也做完全一样的操作(不过处理的是to_tensor，如果是self-attention，那to_tensor=from_tensor), 得到```[batch, head_num, to_seq, per_head]```：

```python
to_tensor_2d = reshape_to_matrix(to_tensor)
key_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=key_act,
        name="key",
        kernel_initializer=create_initializer(initializer_range))

key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,
        to_seq_length, size_per_head)
```

然后就算$$QK^T$$了，注意这里对key取了转置，也就是```[batch, head_num, from_seq, per_head]```乘以```[batch, head_num, per_head, to_seq]```，得到的结果是```[batch, head_num, from_seq, to_seq]```：

```python
attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
attention_scores = tf.multiply(attention_scores,
    1.0 / math.sqrt(float(size_per_head)))

if attention_mask is not None:
    # `attention_mask` = [B, 1, F, T]
    attention_mask = tf.expand_dims(attention_mask, axis=[1])

    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
    # masked positions, this operation will create a tensor which is 0.0 for
    # positions we want to attend and -10000.0 for masked positions.
    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0

    # Since we are adding it to the raw scores before the softmax, this is
    # effectively the same as removing these entirely.
    attention_scores += adder
attention_probs = tf.nn.softmax(attention_scores)
attention_probs = dropout(attention_probs, attention_probs_dropout_prob)
```

然后看下value的操作：

```python
value_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=value_act,
        name="value",
        kernel_initializer=create_initializer(initializer_range))

# `value_layer` = [batch, to_seq, head_num, per_head]
value_layer = tf.reshape(
            value_layer,
            [batch_size, to_seq_length, num_attention_heads, size_per_head])

# `value_layer` = [batch, head_num, to_seq, per_head]
value_layer = tf.transpose(value_layer, [0, 2, 1, 3])

# `context_layer` = [batch, head_num, from_seq, per_head]
context_layer = tf.matmul(attention_probs, value_layer)

# `context_layer` = [batch, from_seq, head_num, per_head]
context_layer = tf.transpose(context_layer, [0, 2, 1, 3])
```

再确认一点，$$softmax(QK^T)$$是```[batch, head_num, from_seq, to_seq]```，而$$V$$是```[batch, head_num, to_seq, per_head]```，所以context_layer是```[batch, head_num, from_seq, per_head]```

最后，再搞一下，变回```[batch, from_seq, head_num * per_head]```：

```python
if do_return_2d_tensor:
# `context_layer` = [B*F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size * from_seq_length, num_attention_heads * size_per_head])
else:
# `context_layer` = [B, F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size, from_seq_length, num_attention_heads * size_per_head])
```

如上过程是$$Concat(head_1, ..., head_h)$$，其中$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$。包装在了函数```attention_layer```之中，我们注意到原文还有一个大小为$$hd_v \times d_{model}$$的$$W^O$$，也就是大小为$$d_{model}\times d_{model}$$，再看看源码。。也就是说，正常的bert里，```attention_heads```就只有一个元素，然后接了个```hidden_size```的fc，而前面的代码里也提到了```hidden_size```正好就是$$d_{model}$$，所以这就是$$W^O$$。

```python
attention_heads = []
with tf.variable_scope("self"):
    attention_head = attention_layer(xxxxx)
    attention_heads.append(attention_head)
    attention_output = None
    if len(attention_heads) == 1:
        attention_output = attention_heads[0]
    else:
        # In the case where we have other sequences, we just concatenate
        # them to the self-attention head before the projection.
        attention_output = tf.concat(attention_heads, axis=-1)
    # Run a linear projection of `hidden_size` then add a residual
    # with `layer_input`.
    with tf.variable_scope("output"):
        attention_output = tf.layers.dense(
            attention_output,
            hidden_size,
            kernel_initializer=create_initializer(initializer_range))
        attention_output = dropout(attention_output, hidden_dropout_prob)
        attention_output = layer_norm(attention_output + layer_input)

```

关于 mask，可以看看这个[https://juejin.im/post/5b9f1af0e51d450e425eb32d](https://juejin.im/post/5b9f1af0e51d450e425eb32d)

摘抄一下：

什么是padding mask呢？回想一下，我们的每个批次输入序列长度是不一样的！也就是说，我们要对输入序列进行对齐！具体来说，就是给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。
具体的做法是，把这些位置的值加上一个非常大的负数(可以是负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！

而sequence mask是为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。
那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1，下三角的值权威0，对角线也是0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。

### masked-language-model的实现

[https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L240](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L240)


如下，其中```hidden_size```就是是$$d_{model}$$：

```python
def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
                         label_ids, label_weights):
  """Get loss and log probs for the masked LM."""
  input_tensor = gather_indexes(input_tensor, positions)

  with tf.variable_scope("cls/predictions"):
    # We apply one more non-linear transformation before the output layer.
    # This matrix is not used after pre-training.
    with tf.variable_scope("transform"):
      input_tensor = tf.layers.dense(
          input_tensor,
          units=bert_config.hidden_size,
          activation=modeling.get_activation(bert_config.hidden_act),
          kernel_initializer=modeling.create_initializer(
              bert_config.initializer_range))
      input_tensor = modeling.layer_norm(input_tensor)

    # The output weights are the same as the input embeddings, but there is
    # an output-only bias for each token.
    output_bias = tf.get_variable(
        "output_bias",
        shape=[bert_config.vocab_size],
        initializer=tf.zeros_initializer())
    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    label_ids = tf.reshape(label_ids, [-1])
    label_weights = tf.reshape(label_weights, [-1])

    one_hot_labels = tf.one_hot(
        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)

    # The `positions` tensor might be zero-padded (if the sequence is too
    # short to have the maximum number of predictions). The `label_weights`
    # tensor has a value of 1.0 for every real prediction and 0.0 for the
    # padding predictions.
    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])
    numerator = tf.reduce_sum(label_weights * per_example_loss)
    denominator = tf.reduce_sum(label_weights) + 1e-5
    loss = numerator / denominator

  return (loss, per_example_loss, log_probs)
```

其中的gather如下：

```python
def gather_indexes(sequence_tensor, positions):
  """Gathers the vectors at the specific positions over a minibatch."""
  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
  batch_size = sequence_shape[0]
  seq_length = sequence_shape[1]
  width = sequence_shape[2]

  flat_offsets = tf.reshape(
      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
  flat_positions = tf.reshape(positions + flat_offsets, [-1])
  flat_sequence_tensor = tf.reshape(sequence_tensor,
                                    [batch_size * seq_length, width])
  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
  return output_tensor
```

注意调用时传的是如下参数

```python
    (masked_lm_loss,
     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
         bert_config, model.get_sequence_output(), model.get_embedding_table(),
         masked_lm_positions, masked_lm_ids, masked_lm_weights)
```

### BERT的可解释性

[ACL 2019 \| 理解 BERT 每一层都学到了什么](https://mp.weixin.qq.com/s/w2Cwo--GTKp5o8YKRtbl7g)

[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document)

探索BERT深层次的表征学习是一个非常有必要的事情，一是这可以帮助我们更加清晰地认识BERT的局限性，从而改进BERT或者搞清楚它的应用范围；二是这有助于探索BERT的可解释性

## 更复杂的BERT

[站在BERT肩膀上的NLP新秀们（PART III）](https://mp.weixin.qq.com/s/CxcyX5V9kBQDW8A4g0uGNA)

[BERT时代与后时代的NLP](https://mp.weixin.qq.com/s/U_pYc5roODcs_VENDoTbiQ)

[美团BERT的探索和实践](https://mp.weixin.qq.com/s/qfluRDWfL40E5Lrp5BdhFw)

[Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展](https://mp.weixin.qq.com/s/dF3PtiISVXadbgaG1rCjnA)

### 中文BERT

#### WWM

[哈工大讯飞联合实验室发布基于全词覆盖的中文BERT预训练模型](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650794872&idx=1&sn=dccd856283bdd4edcdad08cf75506697&chksm=8f477e93b830f7850e6c0ffe684264f704c6fcc4e126a6300b5ae33916aa676a279206e1e4ce&mpshare=1&scene=1&srcid=0701DAFsQt28gF1hGzH4llaM&pass_ticket=8wChBZeeRNV5mWLFKMXfVyWjwTb94XookbbSJiYpmEClqUrpybiGPpfilXkL5UQN#rd)

[https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)

论文：[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)

#### ERNIE

参考[中文任务全面超越BERT：百度正式发布NLP预训练模型ERNIE](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650758722&idx=1&sn=6742b0f86982890d78cb3ec3be9865b3&scene=0#wechat_redirect)

[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf)

使用entity-level masking和phrase-level masking两种mask方法

输入的每个样本由5个 ';' 分隔的字段组成，数据格式：

+ token_ids
+ sentence_type_ids：两句话，第一句都是0，第二句都是1
+ position_ids
+ seg_labels：分词边界信息: 0表示词首、1表示非词首、-1为占位符, 其对应的词为 CLS 或者 SEP；
+ next_sentence_label

例如：

```shell
1 1048 492 1333 1361 1051 326 2508 5 1803 1827 98 164 133 2777 2696 983 121 4 19 9 634 551 844 85 14 2476 1895 33 13 983 121 23 7 1093 24 46 660 12043 2 1263 6 328 33 121 126 398 276 315 5 63 44 35 25 12043 2;0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55;-1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 -1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 -1;0
```

和bert在mask上的区别：

![ernie-bert-masking-diff](../assets/ernie-bert-masking-diff.png)

一个句子的不同level的mask方式：

![ernie-different-mask-level](../assets/ernie-different-mask-level.png)


[ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412v1.pdf)

![ernie-2.0-loss](../assets/ernie-2.0-loss.png)

### 跨语言

#### XLM

[Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)，XLM的主要思想还是来自于这篇文章，借用了BERT的框架最后成了XLM。本文提出了LASER（Language-Agnostic SEntence Representations）

XLM：facebook提出[Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)，加了language emb

+ 无监督的方法：只依赖单语种数据（monolingual data）
+ 有监督的方法：对平行语料使用新的跨语言loss

![xlm](../assets/xlm.png)


[Facebook最新语言模型XLM-R：多项任务刷新SOTA，超越单语BERT](https://mp.weixin.qq.com/s/6oK-gevKLWDwdOy4aI7U7g)

XLM-R

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

来自facebook。针对多种跨语言的传输任务，大规模地对多语言语言模型进行预训练可以显著提高性能。在使用超过 2TB 的已过滤 CommonCrawl 数据的基础上，研究者在 100 种语言上训练了基于 Transformer 的掩模语言模型。该模型被称为 XLM-R，在各种跨语言基准测试中，其性能显著优于多语言 BERT（mBERT），其中 XNLI 的平均准确度为+ 13.8％，MLQA 的平均 F1 得分为+ 12.3％，而 FQ 的平均 F1 得分为+ 2.1％ NER。XLM-R 在低资源语言上表现特别出色，与以前的 XLM 模型相比，斯瓦希里语（Swahili）的 XNLI 准确性提升了 11.8％，乌尔都语（Urdu）的准确性提升了 9.2％。研究者还对获得这些提升所需的关键因素进行了详细的实证评估，包括（1）积极转移和能力稀释；（2）大规模资源资源的高低性能之间的权衡。最后，他们首次展示了在不牺牲每种语言性能的情况下进行多语言建模的可能性。XLM-Ris 在 GLUE 和 XNLI 基准测试中具有强大的单语言模型，因此非常具有竞争力。

### 更长序列

#### transformer-xl

[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)

![vanilla-segment](../assets/vanilla-segment.png)

最简单的处理长文本方法：对长文本直接切成多个segment，每个segment独立过transformer，**segment间是没有信息流动的**

![transformer-xl](../assets/transformer-xl.png)

transformer-xl的处理方法：参考RNN的**隐藏记忆单元**，对**上一个segment**计算的隐藏状态序列进行**fixed和cached**，并在模型处理下一个新的segment时将其缓存为**可重用的扩展上下文**。

如图，第k+1层的第i个元素，用到了第k层的第i-1, i-2, ...,i - segment_len+1这几个元素，所以**对于最顶层的来讲，实际看到的窗口就更长了**。为此还提出了**相对位置编码**，即使用两个token的**相对距离**代替之前的绝对位置。

此外，[Stabilizing Transformers for Reinforcement Learning](https://arxiv.org/abs/1910.06764)说明了为什么标准的transformer架构很难在强化学习中优化。研究者同时提出了一种架构Gated Transformer-XL(GTrXL)，可以很好地提升transformer架构和变体的稳定性，并加速学习，可以超过LSTM，在多任务学习 DMLab-30 基准上达到 SOTA 的水平。

#### XLNet

[XLNet : 运行机制及和 Bert 的异同比较](https://mp.weixin.qq.com/s/VCCZOKJOhCEjxfnoLSuRKA)

[Transformer-XL与XLNet笔记](https://mp.weixin.qq.com/s/g7I_V5a3Puy9uK11A--Xqw)

[什么是XLNet中的双流自注意力](https://mp.weixin.qq.com/s/9QmhN4KfukCtAxzprKDbAQ)

[20项任务全面碾压BERT，CMU全新XLNet预训练模型屠榜（已开源）](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650764408&idx=1&sn=92196097be1a5f993ef02de0bac8128d&chksm=871ab006b06d3910ec88e57598d6c8b1a38dead073b3f417b793ba71ac4750ae2a8263537fa2&mpshare=1&scene=1&srcid=&pass_ticket=%2BD9Ask8qPVeDCkEHTF8NEBVBQX9YmDDkPy9VdMIfOYJ2VtpyHOOhIYdS3wUnvPjn#rd)

参考[拆解XLNet模型设计，回顾语言表征学习的思想演进](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650765039&idx=3&sn=5f21b702a06b2b3c12c1e5f327c0b744&chksm=871ab291b06d3b87a7b35ff2e69bbaa6863e7737510783f0d7913bc575e759a6662242e6b97a&scene=0&xtrack=1&pass_ticket=6OQo9SLhUprzhz9WVqt5LanZi%2Bu5pLbXWpLouCtQ6gkfTHAGY5Li3M%2BDR0n3drA2#rd)

[他们创造了横扫NLP的XLNet：专访CMU博士杨植麟](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650767073&idx=1&sn=3a1014852f0ba8caaee9fdfadd344503&chksm=871aba9fb06d33891731e38d7ca2010516a73cff72c1569c89962160d0fc15ef0b747f9ff7f5&scene=0&xtrack=1&pass_ticket=Kz97uXi0CH4ceADUC3ocCNkjZjy%2B0DTtVYOM7n%2FmWttTt5YKTC2DQT9lqCel7dDR#rd)

[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

代码：[https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)

+ 自回归（autoregressive, AR）：主要的论文有这几篇：[Semi-supervised sequence learning](https://arxiv.org/abs/1511.01432)、[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)、[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)。通过一个autoregressive的模型来估计文本语料库的概率分布。也就是给定一个文本序列$$\mathbf{x}=\left(x_{1}, \cdots, x_{T}\right)$$，AR将likelihood因式分解(factorize)成一个前向的乘积$$p(\mathbf{x})=\prod_{t=1}^{T} p\left(x_{t} | \mathbf{x}_{<t}\right)$$，或者是一个后向的乘积$$p(\mathbf{x})=\prod_{t=T}^{1} p\left(x_{t} | \mathbf{x}_{>t}\right)$$。由于 AR 语言模型仅被训练用于编码**单向(uni-directional)语境（前向或后向）**，因而在深度双向语境建模中效果不佳。而下游语言理解任务通常需要双向语境信息，导致 AR 语言建模无法实现有效预训练。
+ 自编码（autoencoding, AE）：相关的预训练模型不会进行明确的密度估计(explicit density estimation)，而是从残缺的(corrupted)输入中**重建原始数据**。例如bert，使用一定比例的```[MASK]```，然后预测被mask掉的是什么东西。由于目标并不是密度估计，所以在重建的时候，可以考虑双向的上下文信息。但存在如下两个问题：
    + **finetune**时的真实数据**缺少**预训练期间使用的```[MASK]```这些**mask信息**，这导致**预训练和微调效果的差异（pretrain-finetune discrepancy）**。
    + 输入中要预测的token是被mask掉的，所以无法像AR那样使用乘积rule来建立联合概率分布。也就是说，给定未mask的 token，BERT**假设预测的token**之间**彼此独立**，这其实是对自然语言中普遍存在的**高阶、长期依赖关系**的一种**过度简化**。

基于这些优缺点，提出了一种泛化的自回归预训练模型XLNet：

+ Permutation Language Model(PLM)：在自回归LM模式下，最大化所有可能的**因式分解顺序**的对数似然，学习双向语境信息；
    + 把原来的```[MASK]```这个token干掉了，转而用attention mask来搞
    + 引入了排列：即原来是1234，可以输入3241，这个时候改一下mask就行
+ 引入了Transformer-XL的主要思路：**相对位置编码**以及**分段RNN机制**，实践已经证明这两点对于长文档任务是很有帮助的

![](../assets/xlnet.png)

### 更多的任务

#### MT-DNN

[Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/abs/1901.11504)

### 其他改进

#### RoBERTa

参考[重回榜首的BERT改进版开源了，千块V100、160GB纯文本的大模型](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650766934&idx=2&sn=54c479dd8e8e69cd9617b9a1962443e1&chksm=871aba28b06d333e0d5dc64754b7280776ba5c17577831077dc5cd2caa4c2335beec3afd8d34&scene=0&xtrack=1&pass_ticket=zAXdHORK5tTx549e9RwAgNcm7bjJrH4ENwbbTYVrAZDqpsE%2Fu1hY63b%2FoRfnZQdM#rd)

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

+ 修改了一些超参
+ 删掉nsp任务
+ 更大的batchsize和学习率


[RoBERTa中文预训练模型，你离中文任务的「SOTA」只差个它](https://mp.weixin.qq.com/s/EKFa40rLQlnEVuu9V7GDmg)

[https://github.com/brightmart/roberta_zh](https://github.com/brightmart/roberta_zh)


#### DeBERTa

[Deberta: Decoding-enhanced bert with disentangled attention](https://arxiv.org/pdf/2006.03654.pdf)

+ disentangled attention mechanism：每个词使用2个向量，$$H$$编码内容，$$P$$编码相对位置，i和j间的attention如下：

XXX
\begin{array}{r}
\mathrm{A}_{\mathrm{i}, \mathrm{j}}=\left\{\mathrm{H}_{\mathrm{i}}, \mathrm{P}_{\mathrm{i} \mid \mathrm{j}}\right\} \times\left\{\mathrm{H}_{\mathrm{j}}, \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}\right\}^{\mathrm{T}} \\
=\mathrm{H}_{\mathrm{i}} \mathrm{H}_{\mathrm{j}}^{\mathrm{T}}+\mathrm{H}_{\mathrm{i}} \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}^{\mathrm{T}}+\mathrm{P}_{\mathrm{i} \mid \mathrm{j}} \mathrm{H}_{\mathrm{j}}^{\mathrm{T}}+\mathrm{P}_{\mathrm{i} \mid \mathrm{j}} \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}^{\mathrm{T}}
\end{array}
XXX

+ 预训练：使用enhanced mask decoder，把绝对位置信息引入解码层，来预估被mask掉的token
+ finetune：virtual adversarial training

#### ELECTRA

[2019最佳预训练模型：非暴力美学，1/4算力超越RoBERTa](https://mp.weixin.qq.com/s/_R-Bp5lLov-QIoPRl6fFMA)

[ELECTRA: 超越BERT, 19年最佳NLP预训练模型](https://mp.weixin.qq.com/s/fR5OrqxCv0udKdyh6CHOjA)

[ELECTRA: pre-training text encoders as discriminators rather than generators](https://openreview.net/pdf?id=r1xmh1btvb)

使用新的预训练task：RTD(replaced token detection)：

+ 使用一个**生成网络**采样出token，替换一些原有的token
+ 训练一个**判别模型**，预估一个token**是否是被替换的**

RTD比MLM**更sample-efficient**，因为RTD只做二分类，而MLM需要做全词表的softmax

#### Matryoshka

[俄罗斯套娃 (Matryoshka) 嵌入模型概述](https://mp.weixin.qq.com/s/H3LWIs4hBQ-b_XLxICw7Dg)

[Matryoshka representation learning](https://arxiv.org/pdf/2205.13147.pdf)

俄罗斯套娃嵌入模型旨在将**更重要的信息存储在早期的维度中**，将不太重要的信息存储在后面的维度中。俄罗斯套娃嵌入模型的这一特点允许我们截断模型产生的原始 (大) 嵌入，同时仍保留足够的信息以在下游任务上表现良好。

![matryoshka-representation-learning](../assets/matryoshka-representation-learning.png)

[https://sbert.net/examples/training/matryoshka/README.html](https://sbert.net/examples/training/matryoshka/README.html)

训练时：

```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss

model = SentenceTransformer("microsoft/mpnet-base")

base_loss = CoSENTLoss(model=model)
loss = MatryoshkaLoss(
    model=model,
    loss=base_loss,
    matryoshka_dims=[768, 512, 256, 128, 64],
    matryoshka_weight=[1, 1, 1, 1, 1],
)

model.fit(
    train_objectives=[(train_dataset, loss)],
    ...,
)
```

预测时

```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

model = SentenceTransformer("tomaarsen/mpnet-base-nli-matryoshka")

matryoshka_dim = 64
embeddings = model.encode(
    [
        "The weather is so nice!",
        "It's so sunny outside!",
        "He drove to the stadium.",
    ]
)
embeddings = embeddings[..., :matryoshka_dim] # Shrink the embedding dimensions
print(embeddings.shape)
# => (3, 64)

# Similarity of the first sentence to the other two:
similarities = cos_sim(embeddings[0], embeddings[1:])
print(similarities)
# => tensor([[0.8910, 0.1337]])
```

注意，如果嵌入已经归一化，那么在截断后它们将不再归一化，因此你**可能需要重新归一化**。

```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
import torch.nn.functional as F

model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)

matryoshka_dim = 64
embeddings = model.encode(
    [
        "search_query: What is TSNE?",
        "search_document: t-distributed stochastic xxx.",
        "search_document: Amelia Mary Earhart was xxx.",
    ],
    convert_to_tensor=True,
)
# The Nomic team uses a custom architecture, 
# making them recommend Layer Normalization before truncation
embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))
embeddings[..., :matryoshka_dim] # Shrink the embedding dimensions

similarities = cos_sim(embeddings[0], embeddings[1:])
# => tensor([[0.7154, 0.4468]])
```

#### Piccolo2

[拿下SOTA！最强中文Embedding模型对标OpenAI，技术路线公开](https://mp.weixin.qq.com/s/G9izxyzyIHtGDb6kAYD-qg)

[Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](https://arxiv.org/pdf/2405.06932)

[https://huggingface.co/sensenova/piccolo-large-zh-v2](https://huggingface.co/sensenova/piccolo-large-zh-v2)

#### Conan-Embedding

[通过负样本挖掘炼出更强Embedding模型](https://mp.weixin.qq.com/s/z0jgPnvaPO6RTzgFzB8TFQ)

[Conan-embedding: General Text Embedding with More and Better Negative Samples](https://arxiv.org/pdf/2408.15710)

#### MEXMA

[MEXMA: Token-level objectives improve sentence representations](https://arxiv.org/abs/2409.12737)

[https://huggingface.co/facebook/MEXMA](https://huggingface.co/facebook/MEXMA)

[https://github.com/facebookresearch/mexma](https://github.com/facebookresearch/mexma)


## 更小的BERT

[BERT 瘦身之路：Distillation，Quantization，Pruning](https://mp.weixin.qq.com/s/ir3pLRtIaywsD94wf9npcA)

### albert

[刚刚，Google发布24个小型BERT模型，直接通过MLM损失进行预训练](https://mp.weixin.qq.com/s/s0ysFH4CRvsHY1Gp3b4DPQ)

[ALBERT：用于语言表征自监督学习的轻量级 BERT](https://mp.weixin.qq.com/s/0V-051qkTk9EYiuEWYE3jQ)

[谷歌ALBERT模型V2+中文版来了：之前刷新NLP各大基准，现在GitHub热榜第二](https://mp.weixin.qq.com/s/nusSlw28h4bOlw5hDsc-Iw)

[超小型BERT中文版横空出世！模型只有16M，训练速度提升10倍](https://mp.weixin.qq.com/s/eVlNpejrxdE4ctDTBM-fiA)

[https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh)


[预训练小模型也能拿下13项NLP任务，谷歌ALBERT三大改造登顶GLUE基准](https://mp.weixin.qq.com/s/kvSoDr0E_mvsc7lcLNKmgg)

ALBERT 模型在 GLUE、RACE 和 SQuAD 基准测试上都取得了新的 SOTA 效果，并且参数量还少于 BERT-large。

[ALBERT: a lite bert for self-supervised learning of language representations](https://openreview.net/pdf?id=H1eA7AEtvS)

通过对词嵌入矩阵进行因式分解，再为下游任务共享不同层的所有参数，这样可以大大降低 BERT 的参数量。

还提出了一种新型句间连贯性损失函数，它可以强迫模型学习句间的连贯性表达，从而有利于各种下游 NLP 任务。

ALBERT 通过两个参数削减技术克服了扩展预训练模型面临的主要障碍。

+ 对嵌入参数化进行因式分解：将大的嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。
+ 跨层参数共享：避免参数量随着网络深度的增加而增加。

两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。这些参数削减技术还可以充当某种形式的正则化，可以使训练更加稳定，而且有利于泛化。

为了进一步提升 ALBERT 的性能，还引入了一个自监督损失函数，用于句子级别的预测（SOP）。SOP 主要聚焦于**句间连贯**，用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。

albert_tiny：

input_ids先查word_embeddings(`\(V\times E=21118*128)`)，得到dim=128的表示，再查word_embeddings_2(`\(E\times M =128*312\)`)，得到dim=312的表示。

搞positionembedding时，并不用输入0 1 2...，只需要做一些slice的变换就行了

```python
    with tf.control_dependencies([assert_op]):
      full_position_embeddings = tf.get_variable(
          name=position_embedding_name,
          shape=[max_position_embeddings, width],
          initializer=create_initializer(initializer_range))
      # Since the position embedding table is a learned variable, we create it
      # using a (long) sequence length `max_position_embeddings`. The actual
      # sequence length might be shorter than this, for faster training of
      # tasks that do not have long sequences.
      #    
      # So `full_position_embeddings` is effectively an embedding table
      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
      # perform a slice.
      position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                     [seq_length, -1]) 
      num_dims = len(output.shape.as_list())

      # Only the last two dimensions are relevant (`seq_length` and `width`), so
      # we broadcast among the first dimensions, which is typically just
      # the batch size.
      position_broadcast_shape = [] 
      for _ in range(num_dims - 2):
        position_broadcast_shape.append(1)
      position_broadcast_shape.extend([seq_length, width])
      position_embeddings = tf.reshape(position_embeddings,
                                       position_broadcast_shape)
      output += position_embeddings
```

然后会通过create_attention_mask_from_input_mask把input_ids和input_mask搞一下，得到attention_mask去和attention做mask，主要是算loss啥的，把后面的mask掉不算

### distillbert

参考[小版BERT也能出奇迹：最火的预训练语言库探索小巧之路](https://mp.weixin.qq.com/s/a0d0b1jSm5HxHso9Lz8MSQ)

1.4w个stars。。

[https://huggingface.co/transformers](https://huggingface.co/transformers)

[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)


### tinybert

[TinyBERT：模型小7倍，速度快8倍，华中科大、华为出品](https://mp.weixin.qq.com/s/VL7TSHmZPKD-xGdOxNmnHw)

[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

提出了一个two-stage learning framework，在pre-training阶段和task-specific阶段都进行distill。

相比baseline，只有28% parameters和31%的inference时间

在glue上，7.5x小，infer上有9.4x快。

[哪吒”出世！华为开源中文版BERT模型](https://mp.weixin.qq.com/s/He6Xujoe5Ieo95Tshx7PnA)

[NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204)

[https://github.com/huawei-noah/Pretrained-Language-Model](https://github.com/huawei-noah/Pretrained-Language-Model)

[华为诺亚方舟开源哪吒、TinyBERT模型，可直接下载使用](https://mp.weixin.qq.com/s/cqYWllVCgWwGfAL-yX7Dww)


### reformer

[哈希革新Transformer：这篇ICLR高分论文让一块GPU处理64K长度序列](https://mp.weixin.qq.com/s/QklCVuukfElVDBFNxLXNKQ)

[Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB)

[https://github.com/google/trax/blob/master/trax/models/research/reformer.py](https://github.com/google/trax/blob/master/trax/models/research/reformer.py)

大型的 Transformer 往往可以在许多任务上实现 sota，但训练这些模型的成本很高，尤其是在序列较长的时候。在 ICLR 的入选论文中，我们发现了一篇由谷歌和伯克利研究者发表的优质论文。文章介绍了两种提高 Transformer 效率的技术，最终的 Reformer 模型和 Transformer 模型在性能上表现相似，并且在长序列中拥有更高的存储效率和更快的速度。论文最终获得了「8，8，6」的高分。在最开始，文章提出了将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度。此外，研究者使用可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数）。最终的 Reformer 模型和 Transformer 模型在性能上表现相同，同时在长序列中拥有更高的存储效率和更快的速度。

[大幅减少GPU显存占用：可逆残差网络(The Reversible Residual Network)](https://mp.weixin.qq.com/s/j6-x9ANF9b3Q1I_os_LJSw)

### LTD-bert

[内存用量1/20，速度加快80倍，腾讯QQ提出全新BERT蒸馏框架，未来将开源](https://mp.weixin.qq.com/s/W668zeWuNsBKV23cVR0zZQ)

### Q-bert

[AAAI 2020 \| 超低精度量化BERT，UC伯克利提出用二阶信息压缩神经网络](https://mp.weixin.qq.com/s/0qBlnsUqI2I-h-pFSgcQig)

[Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/pdf/1909.05840.pdf)

### Adabert

[推理速度提升29倍，参数少1/10，阿里提出AdaBERT压缩方法](https://mp.weixin.qq.com/s/mObuD4ijUCjnebYIrjvVdw)

[AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search](https://arxiv.org/pdf/2001.04246v1.pdf)


## 仅解码器的GPT 

[AI也能精彩表达：几种经典文本生成模型一览](https://mp.weixin.qq.com/s/GfP76I-BzzQcyLqQJoeXxw)

### GPT

2018年的[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。

![gpt1](../assets/gpt1.png)

微调阶段为每种下游任务专门设计：

+ 分类：输入一段文本，经过transformer，最后接一个Linear
+ entailment(蕴含)：输入2段文本，premise(假设)和hypothesis(假说)，经过transformer，最后接一个Linear
+ 相似度：输入2段文本a和b，a+b过transformer，b+a过transformer，再合起来接一个Linear
+ 多选题：输入context+答案1过transformer+Linear，答案2、答案3同样操作，将3个输出合在一起求softmax

### GPT2

2019年的[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模。

+ layernorm前移到每个sub-block之前
+ additional layernorm在最后的self-attention block才加上
+ 修改初始化方法，以考虑残差路径上的累积并缩放残差层的权重


[15亿参数最强通用NLP模型面世！Open AI GPT-2可信度高于所有小模型](https://mp.weixin.qq.com/s/nu2egJuG_yxIVfW9GfdlCw)

中文GPT2

[只需单击三次，让中文GPT-2为你生成定制故事](https://mp.weixin.qq.com/s/FpoSNNKZSQOE2diPvJDHog)

[https://github.com/imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)

[https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb](https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb)


[语言模型秒变API，一文了解如何部署DistilGPT-2](https://mp.weixin.qq.com/s/5B8bN2kplB4t1ctYJjN1zw)

huggingface的distill gpt-2：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

### nanogpt代码解读

简化版的gpt，
tiktoken：gpt2中使用的开源分词工具，比huggingface的tokenizer快得多

```python
import tiktoken
enc = tiktoken.get_encoding("gpt2")

# 字节对编码过程，我的输出是[31373, 995]
encoding_res = enc.encode("hello world")
print(encoding_res)

# 字节对解码过程，解码结果：hello world
raw_text = enc.decode(encoding_res)
print(raw_text)
```

类似的：[https://github.com/karpathy/llm.c](https://github.com/karpathy/llm.c)

[https://github.com/karpathy/build-nanogpt/tree/master](https://github.com/karpathy/build-nanogpt/tree/master)

fork了一个：[https://github.com/daiwk/build-nanogpt](https://github.com/daiwk/build-nanogpt)

GPT的整体结构：

```python
class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            # 这里有n层，每层是一个Block
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            # 最后一层需要加一个layer norm
            ln_f = nn.LayerNorm(config.n_embd),
        ))
        # 最后一层的linear
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # weight sharing scheme
        self.transformer.wte.weight = self.lm_head.weight

        # init params
        self.apply(self._init_weights)
```

```python
class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x):
        # 这里对原来的add&norm进行了改造，保留一个从始至终的+x的操作，让梯度能够直通输入
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

```python
class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)
        self.gelu    = nn.GELU(approximate='tanh')
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        return x
```

```python
class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1
        # regularization
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        # nh is "number of heads", hs is "head size", and C (number of channels) = nh * hs
        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
        # output projection
        y = self.c_proj(y)
        return y
```


## 编码器+解码器

### T5

[谷歌T5模型刷新GLUE榜单，110亿参数量，17项NLP任务新SOTA](https://mp.weixin.qq.com/s/YOMWNV5BMI9hbB6Nr_Qj8w)

[谷歌最新T5模型17项NLP任务霸榜SuperGLUE，110亿参数量！](https://mp.weixin.qq.com/s/rFT37D7p0MiS8XGZM35bYA)

[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)

将所有NLP任务都建模成text-to-text的生成任务，

mT5（[mt5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/pdf/2010.11934.pdf)）是T5的变种，基于新的Common Crawl的数据集（包括101种语言）上预训练

### MASS

[MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450v5)

bert只使用了Transformer的encoder部分，其下游任务也主要是适用于自然语言理解（NLU），对于类似文本摘要、机器翻译、对话应答生成等自然语言生成（NLG）任务显然是不太合适的。MASS 采用了编码器-解码器框架，并尝试在**给定部分句子**的情况下**修复整个句子**。MASS输入句子包含了一些连续的 Token，并且中间会带有一些**连续的Mask**，模型的任务是**预测出被Mask掉的词**是什么。相比 BERT 只有编码器，MASS 联合训练编码器与解码器，能获得更适合机器翻译的表征能力。

训练步骤主要分为两步：

+ Encoder： 输入为被随机mask掉连续部分token的句子，使用Transformer对其进行编码；这样处理的目的是可以使得encoder可以更好地捕获没有被mask掉词语信息用于后续decoder的预测；
+ Decoder： 输入为与encoder同样的句子，但是mask掉的正好和encoder相反，和翻译一样，使用attention机制去训练，但只预测encoder端被mask掉的词。该操作可以迫使decoder预测的时候更依赖于source端的输入而不是前面预测出的token，防止误差传递。


### BART

[多项NLP任务新SOTA，Facebook提出预训练模型BART​](https://mp.weixin.qq.com/s/1-EJ36-lY9YZSLBG5c2aaQ)

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)

BART是一个适用于seq2seq模型的去噪自编码器。预训练包括两个阶段：

+ 使用**任意噪声**函数**破坏文本**
+ 用seq2seq模型来**重建原始文本**


# LLM概述

PLM（pretrained language models），即bert等

## LLM简史

+ 2017年的[Learning to generate reviews and discovering sentiment](https://arxiv.org/pdf/1704.01444.pdf)尝试用rnn来实现智能系统
+ 2018年的gpt1：[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。确定对自然语言文本建模的基本原则为**预测下一个单词**。
+ 2019年的gpt2：[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模，**无需使用标记数据进行显式微调**。
    + 参考[The natural language decathlon: Multitask learning as question answering](https://arxiv.org/pdf/1806.08730.pdf)中**多任务求解的概率形式**： $$p(output|input,task)$$ 。
    + 提出“由于特定任务的有监督目标与无监督目标（语言建模）相同，只是在序列的子集上进行评估，因此，无监督目标的全局最小值也是有监督目标的全局最小值”，即每个NLP任务可以看作**世界文本子集的单词预测问题**，如果模型有足够能力来复原世界文本，无监督语言建模可以解决各种问题。
    + 仅无监督与监督微调的SOTA相比效果还是不太行。虽然GPT2模型规模相对较小，但如对话等任务在其基础上做微调还是能拿到很好的效果的，例如[DIALOGPT : Large-scale generative pre-training for conversational response generation](https://arxiv.org/pdf/1911.00536.pdf)、[End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2](https://aclanthology.org/2020.acl-main.54.pdf)
+ 2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning（这点在**3.3亿的BERT和15亿的gpt2中看不到**）。**预训练和ICL有相同的语言建模范式**：预训练预测给定上下文条件下的后续文本序列，ICL预测正确的任务解决方案，其可被格式化为给定任务描述和示范下的文本序列。
+ GPT-3的两种改进方法：
    + 使用代码数据训练：GPT-3主要问题是缺乏对复杂任务的推理能力，2021年openai提出了Codex（[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)），在github代码上微调的GPT。[A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more](https://arxiv.org/pdf/2112.15594.pdf)发现Codex能解决非常困难的编程问题，还能在数学问题上有显著提升。[Text and code embeddings by contrastive pre-training](https://arxiv.org/pdf/2201.10005.pdf)提出了训练文本和代码emb的对比学习，在线性探测分类、文本搜索、代码搜索等任务上有所提升。GPT-3.5就是在基于代码的GPT（code-davinci-002）的基础上开发的。
    + 与人类对齐：2017年openai就在[learning from human preference](https://openai.com/research/learning-from-human-preferences)的博客中提出了应用强化学习来学习由人类标的偏好比较，此后2021年7月openai发表了PPO。2020年GPT-2用RL进行微调，[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)也做了相似工作。2022年提出了RLHF的InstructGPT([Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf))，其中的**SFT就对应于常说的指令微调**。在openai的博客[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research)中提出了训练AI系统的3个有前途的方向：**使用人类反馈、协助人类评估、做对齐研究**。
+ 2022年的ChatGPT：用类似InstructGPT的方式进行训练，专门**对对话能力进行优化**，将人类生成的对话（**扮演用户和AI两个角色**）与InstructGPT数据集结合起来**以对话形式生成**。
+ 2023年的GPT-4：将文本输入扩展到**多模态信号**。此外，
    + 提升安全性：在RLHF训练中加入**额外的安全奖励信号**，采用多种干预策略如Anthropic提出的[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)提到的红队评估（read teaming）机制以减轻幻觉、隐私和过度依赖问题。
    + 改进的优化方法：使用**可预测扩展**（predictable scaling）的机制，使用模型训练期间的一小部分计算量**以预测最终性能**。
    + 迭代部署的工程方案：[Lessons learned on language model safety and misuse](https://openai.com/research/language-model-safety-and-misuse)，遵循5阶段的开发和部署生命周期来开发模型和产品。

## LLM列表（持续更新中）

+ 百亿：除了LLaMA（最大650亿）和NLLB（最大545亿），大多数在100亿-200亿之间，通常需要**数百甚至上千**个GPU或TPU。
+ 千亿：OPT、OPT-IML、BLOOM和BLOOMZ与GPT-3(175B)大致相同，GLM有1300亿，Galactica有1200亿，通常需要**数千**个GPU或者TPU。

| ckpt? | 模型 |发布时间 | 大小 | 预训练数据规模 | 硬件 | 训练时间  |
|---|---|---|---|---|---|---|
| Y | [T5](https://arxiv.org/pdf/1910.10683.pdf) | 2019.10| 11B |  1万亿tokens | 1024 TPU v3  |  - |
| N | [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) | 2020.05 | 175B |  3000万亿tokens | -  |  - |
| N | [GShard](https://arxiv.org/pdf/2006.16668.pdf) | 2020.06 | 600B |  1万亿tokens | 2048 TPU v3 | 4天 |
| Y | [mT5](https://arxiv.org/pdf/2010.11934.pdf) | 2020.10 | 13B |  1万亿tokens | -  |  - |
| Y | [PanGu-$$\alpha$$](https://arxiv.org/pdf/2104.12369.pdf) | 2021.04 | 13B |  1.1TB | 2048 Ascend 910 | - |
| Y | [CPM-2](https://arxiv.org/pdf/2106.10715.pdf) | 2021.06 | 198B |  2.6TB | - | - |
| N | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | 2021.07 | 12B |  1000万亿tokens | - | - |
| N | [ERNIE 3.0](https://arxiv.org/pdf/2107.02137.pdf) | 2021.07 | 10B |  3750亿tokens | 384 v100 | - |
| N | [Jurassic-1](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | 2021.08 | 178B | 3000亿tokens | 800 GPU | - |
| N | [HyperCLOVA](https://arxiv.org/pdf/2109.04650.pdf) | 2021.09 | 82B | 3000亿tokens | 1024 A100 | 13.4天 |
| N | [FLAN](https://arxiv.org/pdf/2109.01652.pdf) | 2021.09 | 137B | - | 128 TPU v3 | 60小时 |
| N | [Yuan 1.0](https://arxiv.org/pdf/2110.04725.pdf) | 2021.10 | 245B | 1800亿tokens | 2128 GPU | - |
| Y | [T0](https://arxiv.org/pdf/2211.01786.pdf) | 2021.10 | 11B | - | 512 TPU v3 | 27小时 |
| N | [Anthropic](https://arxiv.org/pdf/2112.00861.pdf) | 2021.12 | 52B | 4000亿tokens | - | - |
| N | [WebGPT](https://arxiv.org/pdf/2112.09332.pdf) | 2021.12 | 175B |  - | - | - |
| N | [Gopher](https://arxiv.org/pdf/2112.11446.pdf) | 2021.12 | 280B |  3000亿tokens | 4096 TPU v3 | 920小时 |
| N | [ERNIE 3.0 Titan](https://arxiv.org/pdf/2112.12731.pdf) | 2021.12 | 260B |  - | - | - |
| N | [GLaM](https://arxiv.org/pdf/2112.06905.pdf) | 2021.12 | 1200B | 2800亿tokens | 1024 TPU v4 | 574小时 |
| N | [LaMDA](https://arxiv.org/pdf/2201.08239.pdf) | 2022.01 | 137B |  7680亿tokens | 1024 TPU v3 | 57.5天 |
| N | [MT-NLG](https://arxiv.org/pdf/2201.11990.pdf) | 2022.01 | 530B | 2700亿tokens | 4480 80G A100 | - |
| N | [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf) | 2022.02 | 41B | 9670亿tokens | - | - |
| N | [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) | 2022.03 | 175B |  - | - | - |
| N | [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) | 2022.03 | 70B | 1.4万亿tokens | - | - |
| Y | [CodeGen](https://arxiv.org/pdf/2203.13474.pdf) | 2022.03 | 16B | 5770亿tokens | - | - |
| Y | [GPT-NeoX-20B](https://arxiv.org/pdf/2204.06745.pdf) | 2022.04 | 20B | 825GB | 96 40G A100 | - |
| Y | [Tk-Instruct](https://arxiv.org/pdf/2204.07705.pdf) | 2022.04 | 11B |  - | 256 TPU v3 | 4小时 |
| N | [PaLM](https://arxiv.org/pdf/2204.02311.pdf) | 2022.04 | 540B | 7800亿tokens | 6144 TPU v4 | - |
| Y | [UL2](https://arxiv.org/pdf/2205.05131.pdf) | 2022.05 | 20B |  825GB | 96 40G A100 | - |
| Y | [OPT](https://arxiv.org/pdf/2205.01068.pdf) | 2022.05 | 175B | 1800亿tokens | 992 80G A100 | - |
| Y | [NLLB](https://arxiv.org/pdf/2207.04672.pdf) | 2022.07 | 54.5B |  - | - | - |
| N | [AlexaTM](https://arxiv.org/pdf/2208.01448.pdf) | 2022.08 | 20B | 1.3万亿tokens | 128 A100 | 120天 |
| N | [Sparrow](https://arxiv.org/pdf/2209.14375.pdf) | 2022.09 | 70B | 64 TPU v3 | - | - |
| N | [WeLM](https://arxiv.org/pdf/2209.10372.pdf) | 2022.09 | 10B | 3000亿tokens | 128 A100 40G | 24天 |
| N | [U-PaLM](https://arxiv.org/pdf/2210.11399.pdf) | 2022.10 | 540B | - | 512 TPU v4 | 5天 |
| N | [Flan-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | 512 TPU v4 | 37小时 |
| N | [Flan-U-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | - | - |
| Y | [GLM](https://arxiv.org/pdf/2210.02414.pdf) | 2022.10 | 130B | 4000亿tokens | 768 40G A100 | 60天 |
| Y | [Flan-T5](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 11B |  - | - | - |
| Y | [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) | 2022.11 | 176B | 3660亿tokens | 384 80G A100 | 105天 |
| Y | [mT0](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 13B |  - | - | - |
| Y | [Galactica](https://arxiv.org/pdf/2211.09085.pdf) | 2022.11 | 120B | 1060亿tokens | - | - |
| Y | [BLOOMZ](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 176B |  - | - | - |
| Y | [OPT-IML](https://arxiv.org/pdf/2212.12017.pdf) | 2022.12 | 175B |  - | 128 40G A100 | - |
| Y | [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) | 2023.02 | 65B | 1.4万亿tokens | 2048 80G A100 | 21天 |
| N | [GPT-4](https://arxiv.org/pdf/2303.08774.pdf) | 2023.03 | - |  - | - | - |
| Y | [CodeGeeX](https://arxiv.org/pdf/2303.17568.pdf) | 2022.09 | 13B | 8500亿tokens | 1536 Ascend 910 | 60天 |
| N | [PanGU-$$\Sigma$$](https://arxiv.org/pdf/2303.10845.pdf) | 2023.03 | 1085B | 3290亿tokens | 512 Ascend 910 | 100天 |
| Y | [Pythia](https://arxiv.org/pdf/2304.01373.pdf) | 2023.04 | 12B | 3000亿tokens | 256 40G A100 | - |

可以直接把对应的md丢给gpt，叫它导出一个excel，然后就可以自定义排序或者画散点图看了


## LLM数据集

llm中文数据集：[https://juejin.cn/post/7238921093553438779](https://juejin.cn/post/7238921093553438779)

+ Books：
    + [BookCorpus](https://arxiv.org/pdf/1506.06724.pdf)：超过11000本电子书，用于GPT和GPT-2。
    + [Gutenberg](https://www.gutenberg.org/)：超过70000本文学作品，包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共领域，用于MT-NLG和LLaMA。
    + Books1和Books2：比BookCorpus大得多，但未公开，用于GPT-3。
+ CommonCrawl：最大的开源网络爬虫数据库之一，**百万亿字节**，有大量噪音和低质信息，需要过滤，有如下4个子集：
    + [C4](https://www.tensorflow.org/datasets/catalog/c4)：包括en（806G，训练T5、LaMDA、Gopher、UL2）、en.noclean（6T）、realnewslike（36G）、webtextlike（17G）、multilingual（38T，训练mT5）。
    + [CC-Stories](https://arxiv.org/pdf/1806.02847.pdf)：31G，内容以故事的形式展示
    + [CC-News](https://arxiv.org/pdf/1907.11692.pdf)：76G
    + [RealNews](https://arxiv.org/pdf/1905.12616.pdf)：120G
+ Reddit Links：Reddit上的帖子，高赞通常比较有用，可以拿来创建高质量数据集。
    + [WebText](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)：由Reddit上的高赞链接组成，未公开，对应的开源版是[OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)。
    + [Pushshift.io](https://arxiv.org/pdf/2001.08435.pdf)：实时更新的数据集，包括Reddit自创建以来的历史数据，有数据存储，也有实用工具，供用户搜索、总结和统计分析。
+ Wikipedia：大部分文章使用写作风格，并支持引用，英语版本用于大多数LLM，如GPT-3、LaMDA、LLaMA，还有多语言版。
+ Code：包括开源许可证的公共代码库（如github）和与代码相关的问答平台（如StackOverflow）,Google公开了[BigQuery](https://cloud.google.com/bigquery?hl=zh-cn)数据集，CodeGen用的BIGQUERY是其的一个子集。
+ 其他：
    + [The Pile](https://arxiv.org/pdf/2101.00027.pdf)有800G，包括书籍、网站、代码、科学论文和社交媒体平台，有22个子集，用于GPT-J(6B)、CodeGen(16B)、Megatron-Turing NLG（530B）。
    + [ROOTS](https://arxiv.org/pdf/2303.03915.pdf)由各种小数据集组成，共1.6T，包括59种语言（自然语言和编程语言），用于BLOOM。

## LLM开源库

+ transformers：huggingface的库
+ [deepspeed](https://github.com/microsoft/DeepSpeed)：微软的库，与pytorch兼容，训练了MT-NLG、BLOOM等模型，包括各种分布式训练优化技术，如**内存优化**（**ZeRO**、**梯度检查点**等）和**管道并行**。
+ megatron-lm：英伟达的库，同样包括各种分布式训练技术，包括**模型和数据并行**、**混合精度**训练和**FlashAttention**。（[Megatron-lm: Training multi-billion parameter language models using model parallelism](https://arxiv.org/pdf/1909.08053.pdf)、[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)和[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)）
+ [jax](https://github.com/google/jax)：google的库，允许用户在**带有硬件加速（GPU或TPU）**的情况下进行**数组的高效运算**，可以在**各种设备**高效计算，支持**自动微分**和**即时编译**等功能。
+ [colossal-AI](https://arxiv.org/pdf/2110.14883.pdf)：HPC-AI Tech的库，基于pytorch，可以使用[PatrickStar](Patrickstar: Parallel training of pre-trained models via a chunk-based memory management)提出的方法优化异构内存管理，分布了基于LLaMA的[ColossalChat](https://medium.com/pytorch/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+ [BMTrain](https://github.com/OpenBMB/BMTrain)：openBMB的库，强调代码简洁、低资源占用和高可用性
+ [FastMoE](Fastmoe: A fast mixture-of-expert training system)：专门用于MoE模型的训练库，基于pytorch，简化了将transformer转换为MoE模型的过程
+ [semantic-kernel](https://github.com/microsoft/semantic-kernel)：微软的开源库

![AI的4场景战役](../assets/4wars-in-aistck.png)

一些开源的小模型：[从零训练的 1B 以下小模型汇总](https://mp.weixin.qq.com/s/IWuMj6ywge2NAUhYmYQBLA)

## 一些综述

+ [Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media](https://github.com/daiwk/collections/blob/master/assets/LLM/foundation%20models%20NLP.pdf)
+ [大规模语言模型：从理论到实践](../assets/LLM/LLM-TAP.pdf)，[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf)邱锡鹏等
+ 人大的大模型综述：[https://github.com/RUCAIBox/LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)，[自己存了一份pdf](https://github.com/daiwk/collections/blob/master/assets/LLM/LLM_Survey_Chinese.pdf)，（**！！！本文大部分内容按这个来组织！！！**）
+ [Talking about large language models](https://arxiv.org/pdf/2212.03551.pdf)
+ [Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)，引用数2k+
+ [A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt](https://arxiv.org/pdf/2302.09419.pdf)，唐杰等
+ [Pre-Trained Models: Past, Present and Future](https://arxiv.org/pdf/2106.07139.pdf)
+ [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)
+ [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/pdf/2105.10311.pdf)
+ [A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)
+ [Towards reasoning in large language models: A survey](https://arxiv.org/pdf/2212.10403.pdf)
+ [Reasoning with language model prompting: A survey](https://arxiv.org/pdf/2212.09597.pdf)
+ [Dense Text Retrieval based on Pretrained Language Models: A Survey](https://arxiv.org/pdf/2211.14876.pdf)
+ [Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://zhuanlan.zhihu.com/p/395795968)
+ [如何高效部署大模型？CMU最新万字综述纵览LLM推理MLSys优化技术](https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ)：[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)
+ [一篇对大语言模型（LLMs）进行全面、深入分析的43页综述（Word2Vec作者出品）](https://mp.weixin.qq.com/s/5fbx0lM9V-Q7xYbeDauuHw)==>[Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)，[自己存了一份](https://github.com/daiwk/collections/blob/master/assets/LLM/Large%20Language%20Models-%20A%20Survey.pdf)
+ [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/pdf/2308.07107v3.pdf)
+ [awesome-LLM-resourses](https://github.com/WangRongsheng/awesome-LLM-resourses)

[大模型面试八股](https://zhuanlan.zhihu.com/p/643560888)

[大模型八股答案（一）——基础知识](https://zhuanlan.zhihu.com/p/643829565)

[大模型八股答案（二）——训练框架](https://zhuanlan.zhihu.com/p/643836163)


## 扩展法则(scaling law)

### openai的扩展法则

2020年,openai的[Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)通过拟合模型在不同数据大小（2000w到230亿个token）、不同的模型大小（7.68亿到15亿个**非嵌入参数**）的性能，提出了在**计算预算**$$c$$的条件下，$$L$$是用nats表示的交叉熵损失，模型性能与**模型规模**$$N$$、**数据集规模**$$D$$以及**训练计算量**$$C$$间存在如下幂律关系：

XXXL(N)=(\frac{N_c}{N})^{\alpha _N}, {\alpha}_N\sim 0.076,N_c\sim 8.8\times 10^{13}XXX

XXXL(D)=(\frac{D_c}{D})^{\alpha _D}, {\alpha}_D\sim 0.05,N_c\sim 5.4\times 10^{13}XXX

XXXL(C)=(\frac{C_c}{C})^{\alpha _C}, {\alpha}_C\sim 0.05,C_c\sim 3.1\times 10^{8}XXX

其中，$$N_c$$表示非嵌入参数数量，$$D_c$$表示训练token数量,$$C_c$$表示FP-days。

[Go Wider Instead of Deeper](https://arxiv.org/pdf/2107.11817)说了，transformer效果的提升**不在于计算量的变大**，而应该在于通过**提升模型的hidden dim**来增加模型参数量

### Chinchilla扩展法则

DeepMind在[Training compute-optimal large language models](https://arxiv.org/pdf/2203.15556.pdf)中提出了Chichilla扩展法则来指导LLM**最优计算量**的训练。通过变化更大范围的模型大小（7000w到160亿参数）和数据大小（50亿到5000亿个token）进行实验，拟合了如下的扩展法则：

XXX
L(N, D)=E+\frac{A}{N^\alpha}+\frac{B}{D^\beta}
XXX

其中$$E=1.69,A=406.4,B=410.7,\alpha = 0.34, \beta =0.28$$，通过在约束条件$$C\approx 6ND$$下优化损失$$L(N,D)$$，将计算预算最优地分配给模型大小和数据大小的方法：

XXX
N_{o p t}(C)=G\left(\frac{C}{6}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{6}\right)^b
XXX

其中$$a=\frac{\alpha}{\alpha+\beta}$$，$$b=\frac{\beta}{\alpha+\beta}$$，$$G$$是由$$A,B,\alpha,\beta$$计算出的扩展系数。

随着计算预算的增加，

+ openai的扩展法则更偏向于将更大预算分给**模型大小**，因为其对比各模型时使用了固定的训练数据量和学习率等超参，低估了数据量的作用。每增加10倍的计算量，应该让数据集大小增加为约1.8倍，模型参数量增加为约5.5倍。即**模型参数量更加的重要**。
+ Chinchilla扩展法则认为**模型大小和数据大小要同比例增加**，即$$a$$和$$b$$取值差不多。因为其在无视模型大小的前提下，发现设置与数据量差不多match的学习率能获得更好的loss。每增加10倍的计算量，应该让数据集大小增加为约3.16倍，模型参数量也增加为约3.16倍。即**数据集大小和模型参数量一样重要**。

然而，有一些能力（如涌现）无法根据扩展法则进行预测，只有当模型达到一定规模时才会出现。

![chinchilla-law](../assets/chinchilla-law.png)

飘红的就是常见的10B模型，大概要205B的token来训练，能达到**计算最优点**，当然**并不一定是loss最小的点**，这个可以参考llama3的现象

### scaling law的一些讨论

[Scaling Laws 又失灵了？谷歌新研究：扩散模型不是越大越好](https://mp.weixin.qq.com/s/ia9L6jr_lwowYHgLI1k_4g)

[Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://arxiv.org/pdf/2404.01367.pdf)

[腾讯混元、北大发现Scaling law「浪涌现象」，解决学习率调参难题](https://mp.weixin.qq.com/s/ff5_O0H5VQNkArKroJkEZQ)

[Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling](https://arxiv.org/pdf/2405.14578)

SIGIR24最佳论文：[Scaling Laws For Dense Retrieval](https://arxiv.org/pdf/2403.18684)

[中科大联合华为诺亚提出Entropy Law，揭秘大模型性能、数据压缩率以及训练损失关系](https://mp.weixin.qq.com/s/F4OFP1lzAGH4RSXcXBw7mw)

#### 词表的scaling law

&nbsp;

[NeurIPS 2024 | 大模型的词表大小，同样适用于Scaling Law](https://mp.weixin.qq.com/s/_DTvTMCtrW9WV3vELjU9jw)

[Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)

[https://github.com/sail-sg/scaling-with-vocab/](https://github.com/sail-sg/scaling-with-vocab/)

## 涌现能力

![llm-capabilities](../assets/llm-capabilities.png)

涌现能力：在小型模型中不存在而在大型模型中产生的能力，当规模达到一定程度时，性能显著提升，超出随机水平（参考
[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)）。与物理学中的**相变**现象类似（物质从一种相（状态）转变为另一种相的过程，通常伴随着能量的吸收或释放，并且涉及不同的物理性质，例如固体、液体和气体之间的转变）。

[普林斯顿DeepMind用数学证明：LLM不是随机鹦鹉！「规模越大能力越强」有理论根据](https://mp.weixin.qq.com/s/oYYuqbelBfCCSLW4Qo4POA)

[A Theory for Emergence of Complex Skills in Language Models](https://arxiv.org/abs/2307.15936)：


![涌现](../assets/emergent%20ability.png)

LLM的3种典型涌现能力及其对应代表模型：

### 上下文学习(in-context learning)

GPT-3（[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)）提出，只要提供一个自然语言指令和/或几个任务演示，语言模型就能通过完成输入文本的词序列的方式来为测试实例生成预期输出，不用额外的梯度更新。

+ ICL能力小模型不具备：1750亿的GPT-3有ICL能力，但GPT-1和GPT-2无此能力。
+ ICL能力取决于具体下游任务：130亿的GPT-3能在算术任务上有ICL，但1750亿的GPT-3在波斯语QA上无能为力。


### 指令遵循(instruction following)

使用**自然语言描述的混合多任务数据集进行微调（指令微调）**，LLM在**未见过的以指令形式描述的任务**上表现出色，具有更好的泛化能力。例如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)、[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)、[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)。

在[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的实验中，当模型大小达到680亿时，经过指定微调的LaMDA-PT开始在未见过的任务上显著优于未微调的模型，而80亿或更小的模型则没有这个现象。

在[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)的实验中，PaLM至少在620亿参数上才能在4个评估基准的各种任务上表现良好。

[精准0误差，输入价格打骨折！OpenAI官宣API支持结构化输出，JSON准确率100％](https://mp.weixin.qq.com/s/257SBcB2hr-xKPNYkUEErQ)


### 逐步推理(multi-step reasoning)

对于涉及多个推理步骤的复杂任务（如数学），可以使用**思维链（Chain-of-Thought, CoT）**提示策略（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)），让LLM通过**利用中间推理步骤的提示机制**来解决这类任务。

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)发现，CoT在模型大于600亿的PaLM和LaMBDA变体中能够提升在算术推理基准任务的效果，而当模型大于1000亿时，相比标准提示的优势更明显。

[How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

## LLM关键点

如何让LLM能够**通用**且**有能力**？

### 扩展

更大的模型、数据规模和更多的训练计算，但计算预算是有限的，可以用扩展法更高效地分配计算资源，如Chinchilla在**相同计算预算下增加训练token数**，优于更大模型规模的Gopher，同时需要数据清理。

### 训练

+ 分布式的训练框架：包括DeepSpeed（[Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters](https://dl.acm.org/doi/abs/10.1145/3394486.3406703)）和Megatron-LM（[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)和[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)）
+ 优化技巧：有助于提升训练稳定性和模型性能，如**重新开始以克服训练损失激增**（[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)）和**混合精度训练**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

### 能力引导

当LLM执行某些特定任务时，可能不会显式地展示出其通用求解器的能力，**设计合适的任务指令或具体的ICL策略**可以**激发**这种能力，例如

+ 通过**包含中间推理步骤的CoT提示**
+ 使用**自然语言表达的任务描述**，对LLM进行**指令微调**

### 对齐微调

由于预训练语料库包括高质量和低质量的数据，LLM可能生成有毒、偏见甚至有害的内容，要让LLM和人类价值观保持一致，如**有用性、诚实性和无害性**。RLHF相关工作如[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)能够产生高质量、无害的回答（例如拒绝回答侮辱性问题）。


### 工具操作

LLM本质是基于海量文本语料库进行文本生成训练的，对于不适合以文本形式表达的任务表现不佳（如数字计算），且其能力受限于预训练数据，无法获取最新信息。可以利用外部工具：

+ [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/pdf/2302.04761.pdf)能利用计算器进行准确计算
+ [Webgpt: Browser-assisted question-answering with human feed- back](https://arxiv.org/pdf/2112.09332.pdf)能利用搜索引擎检索未知信息

## 数据收集

### 数据获取

+ 通用文本数据：
    + 网页：例如CommonCrawl，同时需要过滤和处理以提高质量
    + 对话文本：公共对话数据如PushShift.io，对于在线社交媒体的对话数据，可以**转换成树形结构**，每句话与回应其的话相连。多方的对话树可以划分为预训练语料库中的多个子对话。过度引入对话数据可能会有潜在风险（[OPT: open pre-trained transformer language models](https://arxiv.org/pdf/2205.01068.pdf)）：陈述性指令和直接疑问句被错误地认为是对话的开始，导致指令的有效性下降。
    + 书籍：更正式的长文本，利于**学习语言知识**、**建模长期依赖关系**、**生成叙述性和连贯的文本**。
+ 专用文本数据：
    + 多语言文本：BLOOM的预训练语料中包括了46种语言，PaLM包含了122种
    + 科学文本：如arxiv论文、科学教材、数学 网页等，通常需要特定的标记化和预处理。
    + 代码：一是编程问答社区，二是开源代码仅为。对应长距离依赖和准确的执行逻辑，可能是复杂推理能力的来源。将推理任务格式化为代码形式还能帮LLM生成更准确的结果（如[Language models of code are few-shot commonsense learners](https://arxiv.org/pdf/2210.07128.pdf)和[Autoformalization with large language models](https://arxiv.org/pdf/2205.12615.pdf)）

### 数据预处理

+ 质量过滤：有一些基于分类器的方法，例如维基百科的数据为正样本，负采样其他数据训练二分类器，但这种方法会删除方言、口语和社会语言的高质量文本，可能导致有偏、减少多样性。还有启发式的方法，主要包括：
    + 基于语言的过滤：如果该llm主要用于某种语言，可以把其他语言删了
    + 基于度量的过滤：利用生成文本的评估度量（如**perplexity**）来检测和删除不自然的句子
    + 基于统计的过滤：如**标点符号分布**、**符号和单词比例**、**句子长度**等
    + 基于关键词的过滤：删除噪声或无用元素，如**HTML标签**、**超链接**、**模板**、**攻击性词语**等。
+ 去重：[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)中发现重复数据会降低多样性，可能导致训练不稳定。下面3个级的去重都很有用
    + 句子级：删掉包含重复单词和短语的句子，因为可能在语言建模中引入**重复模式**（[The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751.pdf)）(后面的章节会讲)
    + 文档级：通过文档间的表层特征（如n-gram或单词重合率）来删掉重复文档
    + 数据集级：训练集中删掉测试集可能出现的重复文本，防止训练集和评估集间的重叠
+ 隐私去除：删掉可识别个人信息（PII），如基于关键词（姓名、地址、电话号码）识别。另外，[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/pdf/2202.06539.pdf)发现LLM在隐私攻击下的脆弱性可能归因于预训练语料中存在**重复PII数据**。
+ 分词：可以直接利用已有分词器，也可以使用专门为预训练语料库设计的分词器，如SentencePiece，而且**BPE**(byte pair encoding)能**确保分词后的信息不会丢失**，但其中的如NFKC([Unicode normalization forms](https://unicode.org/reports/tr15/))的**归一化技术**可能会**降低分词的性能**。

### 预训练语料的重要性

+ 混合来源：不同领域和场景的数据能让LLM有更强大的泛化能力。需要**仔细设置数据分布**，Gopher对数据分布消融，发现增加书籍数据可以提升捕捉长期依赖的能力，增加c4数据集比例可以提升其在c4验证集上的效果，但单独训练过多的某个领域数据会影响LLM在其他领域的泛化能力。
+ 数据量：模型性能方面，**数据大小**也能看到与模型大小类似的**扩展法则**。LLaMA发现，用更多数据训练更长时间，较小的模型也能实现良好性能。
+ 数据质量：Gopher、GLaM和T5都发现，在清理后的数据上训练能提升llm效果。数据的重复可能导致『双下降现象』（[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)和[Deep double descent: Where bigger models and more data hurt](https://arxiv.org/pdf/1912.02292.pdf)），甚至会导致训练不稳定。此外，[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)还发现，**重复数据会降低LLM从上下文复制的能力**，从而影响**ICL中的泛化能力**。

注：**双下降**指的是随着模型复杂性的增加，可能**loss先下降，然后再升高，最后又下降**：
+ 当模型的复杂性低于数据的复杂性时，增加模型的复杂性可以帮助减少训练误差。
+ 当模型的复杂性超过数据的复杂性时，增加模型的复杂性反而可能导致训练误差增加。这是因为模型开始过拟合数据，捕获数据中的噪声而非实际的模式。
+ 当模型的复杂性远大于数据的复杂性时，训练误差可能再次开始减少。这是因为模型有足够的能力来对数据的噪声进行平滑，同时仍然能够捕获数据的实际模式。

# LLM模型架构

## MoE原理

[MoE模型的前世今生](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

[MoE 系列论文解读：Gshard、FastMoE、Tutel、MegaBlocks 等](https://mp.weixin.qq.com/s/T5eJZWGH3yRpK9bxmvhhTA)

[CMU开源GRIFFIN：一种新颖的无需训练的MoE方法，提高大模型的生成效率！](https://mp.weixin.qq.com/s/33ISRxfXp4Z7OCN1dBKGyA)

[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/pdf/2404.01365.pdf)

[From Sparse to Soft Mixtures of Experts](https://arxiv.org/pdf/2308.00951): softmoe

MoE的核心问题是一个**如何把token分配给哪个专家**的**离散优化**问题，有如下离散+稀疏的分配方法，都需要**辅助loss**来**平衡每个专家的负载**，以减少drop tokens：

+ 线性规划
+ 强化学习
+ 人为固定规则
+ 最优运输方法
+ 贪心topk token-choose-expert
+ 贪心topk expert-choose-token

![softmoe](../assets/softmoe.png)

在softmoe中，假设N个token，S个slot，E个expert

代码：

[https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97](https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97)和[https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128](https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128)

[算法、系统和应用，三个视角全面读懂混合专家（MoE）](https://mp.weixin.qq.com/s/3UEFwy87f8O4H1Tt4VVnig)


[A Survey on Mixture of Experts](https://arxiv.org/pdf/2407.06204)

![moe-gating](../assets/moe-gating.png)

[从ACL 2024录用论文看混合专家模型（MoE）最新研究进展](https://mp.weixin.qq.com/s/BCsRHvHnn3B8oOODjim-Kg)

### DeepSeekMOE

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066)

[https://github.com/deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)

动机：

+ 专家不够分化：以往的MoE模型专家数量很少，假如模型的知识涉及的领域很多，平均一个专家要包含很多领域的知识，即不够**专**
+ 专家有冗余：假设每个token只能选一个专家，又假设每个token都需要常识知识，结果就是不论选哪个专家，这个专家的参数里都有常识知识，因此有冗余了——最好的情况是有个专家专门负责提供常识知识，所有 token 都会用一下这个专家。

解法：

+ **增加专家数量**：专家是个FFN，假设两个矩阵是$$dh$$和$$hd$$，如果将它拆成2个专家，就是拆成2个$$dh/2$$和$$h/2*d$$，假设原来一个token选top-1专家，现在就是选top-2专家，拆前拆后的计算量和参数量没变，好处就是排列组合多样性更多了，选择也更灵活了。
+ **增设共享专家**：有的专家是必选的，除此以外，每个token按照自己的喜好，再来选top-k。比如有64个专家，那么第一个专家是所有token都要选的，除此以外，每个token还从剩下的63个里选择自己的top-1，其实就是top-2。

### Dynamic MoE

[Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/pdf/2403.07652)

[https://github.com/ZhenweiAn/Dynamic_MoE](https://github.com/ZhenweiAn/Dynamic_MoE)



### XMoE

是上面两种方法的并集

[XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection](https://arxiv.org/pdf/2403.18926)

[https://github.com/ysngki/XMoE](https://github.com/ysngki/XMoE)

### HyperMoE

[HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/pdf/2402.12656)

[https://github.com/Bumble666/Hyper_MoE](https://github.com/Bumble666/Hyper_MoE)

### Expert Pruning

[Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/pdf/2402.14800)

[https://github.com/Lucky-Lance/Expert_Sparsity](https://github.com/Lucky-Lance/Expert_Sparsity)

### MixLoRA

[Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/pdf/2402.15896)

### ESFT

[Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv.org/pdf/2407.01906)



### 多模态MoE

[混合专家更有主见了，能感知多模态分情况行事，Meta提出模态感知型专家混合](https://mp.weixin.qq.com/s/1FNqu0CwPmMFuDTMhli7WA)

[Chameleon: Mixed-modal early-fusion foundation models](https://arxiv.org/pdf/2405.09818)单一Transformer架构，可以根据下一个token的预测目标，对由离散图像和文本token组成的混合模态序列进行建模，从而在不同模态之间进行无缝推理和生成。然而对于Chameleon这样各种模态会在模型训练的早期混合起来的模型，想要拓展它的能力，需要投入大量算力。

[MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)使用路由式稀疏架构（routed sparse architecture）

### HMOE

(toread)

[优化传统MoE结构，腾讯混元团队提出专家差异化新思路](https://mp.weixin.qq.com/s/gOXGL_MReneAZCmO7lBIng)

[HMoE: Heterogeneous Mixture of Experts for Language Modeling](https://arxiv.org/pdf/2408.10681)

在 HMoE 中，**每个专家的大小不再相同**，从而赋予了每个专家不同的表达能力。这种差异化设计使得路由可以根据专家的实际能力动态分配不同难度的 token，有效解决了专家专业化程度不足的问题。 

### OLMoE

(toread)

[OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/pdf/2409.02060)

[https://huggingface.co/allenai/OLMoE-1B-7B-0924](https://huggingface.co/allenai/OLMoE-1B-7B-0924)

[https://github.com/allenai/OLMoE](https://github.com/allenai/OLMoE)

### 元象MOE

[中国最大开源MoE模型，255B参数无条件免费商用，元象发布](https://mp.weixin.qq.com/s/vX0gt2YfsbUbrCpvNBFHZw)

XVERSE-MoE-A36B，该模型总参数255B，激活参数36B，达到100B模型性能的「跨级」跃升。

[https://github.com/xverse-ai/XVERSE-MoE-A36B](https://github.com/xverse-ai/XVERSE-MoE-A36B)

### MoEUT

[Jurgen、曼宁等大佬新作：MoE重塑6年前的Universal Transformer，高效升级](https://mp.weixin.qq.com/s/CckHYrUpwCpft53-lZ4DwA)

[MoEUT: Mixture-of-Experts Universal Transformers](https://arxiv.org/pdf/2405.16039)

[https://github.com/robertcsordas/moeut](https://github.com/robertcsordas/moeut)

## 主流框架

+ **编码器-解码器架构(encoder-decoder)**：标准Transformer，如T5、BART，**只有少数LLLM还用这种结构**，如Flan-T5
+ **因果解码器架构(causual decoder)**：也叫**decoder-only**，**单向注意力掩码**，输入和输出token通过解码器以相同方式进行处理，以GPT系列为代表，现有大部分LLM都是这种架构，如OPT、BLOOM、Gopher等。
+ **前缀解码器架构(prefix decoder)**：修正因果解码器的掩码机制，使其能**对前缀token执行双向注意力**，并且**仅对生成的token执行单向注意力**（和encoder-decoder类似），即[Unified language model pre-training for natural language understanding and generation](https://arxiv.org/pdf/1905.03197.pdf)提出的uni-lm。[What language model architecture and pretraining objective works best for zero-shot generalization?](https://arxiv.org/pdf/2204.05832.pdf)建议不从头开始预训练，而是**继续训练因果编码器，然后将其转换成前缀编码器以加速收敛**。例如U-PaLM从PaLM演化而来，还有GLM-130B也是这种架构。

![uni-lm](../assets/uni-lm.png)

[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)

对于这3种架构，都可以用**MoE**进行扩展，每个输入的**一小部分神经网络权重**被**稀疏激活**，如[Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)和GLaM。[Unified scaling laws for routed language models](https://arxiv.org/pdf/2202.01169.pdf)发现，通过**增加专家数量或总参数大小**，性能会有显著改进。

### 讨论：为什么现在的LLM都是Decoder only的架构？

&nbsp;

[https://www.zhihu.com/question/588325646/answer/2940298964](https://www.zhihu.com/question/588325646/answer/2940298964)

+ **泛化性能强**：ICML 22的[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf).在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在**各种下游任务上zero-shot泛化性能最好**；另外，ACL23的[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/pdf/2212.10559.pdf)等工作表明，decoder-only模型相当于基于给出的几个示例**隐式地进行梯度下降**，对应的in-context learning泛化能力更强，
+ **秩**的讨论：[Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)的讨论，$$n\times d$$和$$d\times n$$相乘后（$$n\gg d$$）再加上softmax后，秩不超过$$d$$，而decoder-only中有一个下三角矩阵的mask，所以输入的是一个下三角矩阵，而下三角矩阵的行列式是对角线之积，且有softmax，对角线肯定大于0，所以是满秩的(行列式不为0-->矩阵经过变换后不会有一行或者一列全为0-->当前矩阵满秩)
+ 预训练**任务难度**更大：相比encoder-decoder，decoder-only架构里**每个位置能接触到的信息更少**，故难度更高，当模型大小和数据量够的时候，上限更高
+ 隐式学习了**位置信息**：[Transformer Language Models without Positional Encodings Still Learn Positional Information](https://aclanthology.org/2022.findings-emnlp.99.pdf)，encoder里对语序的区分能力较弱，需要结合position encoding，而causual attention隐式地具备了这种建模位置的能力。
+ **工程效率**：支持**复用kv-cache**，对多轮对话更友好，**『DIN的FLOPS』**一节里有讲

[盛名一时的BERT哪去了？这个问题的答案昭示了LLM范式的转变](https://mp.weixin.qq.com/s/fKeorQYwRlmmepuG1_aJlQ)

[What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising)

#### 去噪目标

&nbsp;

**去噪目标**指的是span corruption任务的任意变体，即填充(infilling)或填空(fill in the blank)。表达方式有很多，比如span长度、随机性、sentinel token等。

+ BERT类的模型中，大部分是**in-place**的去噪目标，例如对mask tokens的分类head，
+ T5的做法则是通过encoder-decoder或decoder-only模型来处理数据变换，即把masked token move to the back给模型预测。

去噪目标的效果很好，可以作为常规语言建模的**补充目标**，但不足以单独作为目标，因为去噪有两个缺点：

+ **更少的loss exposure**：在去噪目标中，只有**少量token会被mask和学习**，而常规语言建模则接近100%，使得**每个FLOP的样本效率非常低**
+ 比常规语言建模更不自然：以一种奇怪的方式重新设定输入输出格式，不太适合少样本学习

#### PrefixLM vs decoder-only

&nbsp;

注：归纳偏置(inductive bias)指模型在预测未遇到的输入时，做的一些假设的集合，例如最小描述长度（奥卡姆剃刀）指的就是当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。

对语言模型来说，双向注意力是一种有趣的**归纳偏置**，相比于较小规模的场景，双向注意力**在规模较大时可能就没那么重要了**，或者可能对不同的任务或模态有不同的影响，例如PaliGemma就用的prefixLM（[PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/pdf/2407.07726)）

![paligemma](../assets/paligemma.png)

PrefixLM也存在缓存问题，是这类架构的一个固有缺陷

#### encoder-decoder的优缺点

&nbsp;

+ 相比decoder-only的优势：endocer不受causal的限制，可以激进地试各种pooling/linear attention，因此可以offload一些**不那么重要的context到encoder中**，也可以**让encoder更小**，例如[Charformer](https://arxiv.org/pdf/2106.12672)
+ 相比prefixLM的缺点：输入和目标必须分配固定的预算，例如输入预算是1024token，那么encoder就**必须pad**到1024，而这可能会浪费大量计算。相反，在 PrefixLM 中，输入和目标可以直接concat起来，从而可以缓解这个问题


## 组件配置

### 标准化（norm）

&nbsp;

LN(layer norm)能缓解LLM训练不稳定的问题，其位置很重要。

![pre-ln](../assets/pre-ln.jpeg)

+ 前置LN：最初Transformer使用后置LN，但大多数LLM采用前置LN以实现更稳定的训练，尽管会有一些性能损失([On layer normalization in the transformer architecture](https://arxiv.org/pdf/2002.04745.pdf))。[Sandwich-LN](https://arxiv.org/pdf/2105.13290.pdf)在残差连接前添加额外的LN，虽然能避免数值爆炸，但有时会无法稳定LLM的训练，可能导致训练崩溃（[GLM-130B: an open bilingual pre-trained model](https://arxiv.org/pdf/2210.02414.pdf)）
+ [RMS Norm](https://arxiv.org/pdf/1910.07467.pdf)：训练和性能都不错，在Gopher和Chinchilla里使用
+ [Deep Norm](https://arxiv.org/pdf/2203.00555.pdf)：比LN有更好的训练稳定性，和后标准化一起用在GLM-130B里

![deep-norm](../assets/deep%20norm.png)

此外，**在emb后直接加额外的LN**能提升训练稳定性，但会导致**显著的性能下降**([What language model to train if you have one million GPU hours?](https://arxiv.org/pdf/2210.15424.pdf))，在后来的LLM中**被移除**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

[神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！](https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ)

[On the Nonlinearity of Layer Normalization](https://arxiv.org/pdf/2406.01255)

### 激活函数

&nbsp;

FFN中的激活函数：

+ [GeLU](https://arxiv.org/pdf/1606.08415.pdf)：大部分都是这个
+ [GLU(gated linear units)的变体](https://arxiv.org/pdf/2002.05202.pdf)：应用在PaLM和LaMDA等模型中，如SwiGLU和GeGLU有更好的效果，但在FFN中的参数量比GeLU要大50%


原始Transformer中

XXX\operatorname{FFN}\left(x, W_1, W_2, b_1, b_2\right)=\max \left(0, x W_1+b_1\right) W_2+b_2XXX

T5中把bias干掉了

XXX\operatorname{FFN}_{\operatorname{ReLU}}\left(x, W_1, W_2\right)=\max \left(x W_1, 0\right) W_2XXX

然后，$\operatorname{GELU}(x)=x \Phi(x)$，同时$\operatorname{Swish}_\beta(x)=x \sigma(\beta x)$，接下来

XXX\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes(x V+c)XXX
XXX\operatorname{Bilinear}(x, W, V, b, c)=(x W+b) \otimes(x V+c)XXX
XXX\operatorname{ReGLU}(x, W, V, b, c)=\max (0, x W+b) \otimes(x V+c)XXX
XXX\operatorname{GEGLU}(x, W, V, b, c)=\operatorname{GELU}(x W+b) \otimes(x V+c)XXX
XXX\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)XXX

对应起来就是

XXX\operatorname{FFN}_{\mathrm{GLU}}\left(x, W, V, W_2\right)=(\sigma(x W) \otimes x V) W_2XXX
XXX\operatorname{FFN}_{\text {Bilinear }}\left(x, W, V, W_2\right)=(x W \otimes x V) W_2XXX
XXX\operatorname{FFN}_{\operatorname{ReGLU}}\left(x, W, V, W_2\right)=(\max (0, x W) \otimes x V) W_2XXX
XXX\operatorname{FFN}_{\text {GEGLU }}\left(x, W, V, W_2\right)=(\operatorname{GELU}(x W) \otimes x V) W_2XXX
XXX\operatorname{FFN}_{\text {SwiGLU }}\left(x, W, V, W_2\right)=\left(\operatorname{Swish}_1(x W) \otimes x V\right) W_2XXX


### 位置编码

&nbsp;

Transformer的self-attention有转换不变性，故要位置编码以引入绝对或相对位置信息来建模序列。

+ 绝对位置编码：
    + 正弦函数：原始Transformer中使用
    + 可学习的位置编码：LLM中常用
+ 相对位置编码：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)提出，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的，根据**k和q之间的偏移量**生成emb
+ Alibi：[Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf)提出，使用**k和q之间距离的惩罚**来给注意力分数加bias，[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf)发现其有更好的**零样本泛化能力**和更强的**外推能力**，能够在**比训练序列更长的序列**上表现良好。
+ RoPE：[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)提出，**k和q之间的分数用相对位置信息计算**，利于建模长序列，在PaLM、LLaMA、GLM-130B中都有应用。
  

[Transformer升级之路：RoPE的底数设计原则](https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ)

[Base of RoPE Bounds Context Length](https://arxiv.org/pdf/2405.14591)

[Decoder-only的LLM为什么需要位置编码？](https://mp.weixin.qq.com/s/3sBYrKyEPP93nwigaOAAAA)

(toread)

### 注意力机制和Bias

+ 稀疏注意力：[Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf))，**计算复杂度更低**，GPT-3用了
+ FlashAttention：[Flashattention: Fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/pdf/2205.14135.pdf)，考虑显存访问
+ 其他attention：如[Random feature attention](https://arxiv.org/pdf/2103.02143.pdf)、[Big bird: Transformers for longer sequences](https://arxiv.org/pdf/2007.14062.pdf)
+ 移除bias：PaLM和Galactica中将bias删了，能够增加训练稳定性。


### 小结

#### 归一化位置

&nbsp;

sublayer表示FFN或self-attention模块

| 方法 | 公式 | 
|------|---------------|
| post Norm | $$\operatorname{Norm}(\mathbf{x}+\operatorname{Sulayerb}(\mathbf{x}))$$ |
| pre Norm | $$\mathbf{x}+\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x}))$$ |
| Sandwich Norm | $$\mathbf{x}+\operatorname{Norm}(\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x})))$$ |

#### 归一化方法

| 方法 | 公式 | 
|------|---------------|
|Layer Norm| $$\frac{\mathrm{x}-\mu}{\sqrt{\sigma}} \cdot \gamma+\beta, \quad \mu=\frac{1}{d} \sum_{i=1}^d x_i, \quad \sigma=\sqrt{\frac{1}{d} \sum_{i=1}^d(x_i-\mu)^2}$$ |
|RMSNorm| $$\frac{\mathrm{x}}{\operatorname{RMS}(\mathrm{x})} \cdot \gamma, \quad \operatorname{RMS}(\mathbf{x})=\sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}$$ |
|Deep Norm| $$LayerNorm (\alpha \cdot \mathbf{x}+\operatorname{Sublayer}(\mathbf{x}))$$ |

#### 激活函数

| 方法 | 公式 | 
|------|-----------------------|
|ReLU| $$\operatorname{ReLU}(\mathbf{x})=\max (\mathbf{x}, \mathbf{0})$$ |
| GeLU | $$\operatorname{GeLU}(\mathbf{x})=0.5 \mathrm{x} \otimes[1+\operatorname{erf}(\mathbf{x} / \sqrt{2})], \quad \operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$$ |
|Swish | $$\operatorname{Swish}(\mathbf{x})=\mathbf{x} \otimes \operatorname{sigmoid}(\mathbf{x})$$ |
|SwiGLU|$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$ |
|GeGLU|$$\operatorname{GeGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{GeLU}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$|

#### 位置嵌入

+ $$A_{ij}$$：**q和k之间**的**注意力分数**
+ $$r_{i-j}$$：基于**q和k之间偏移**的可学习标量
+ $$\mathbf{R}_{\theta, i-j}$$：旋转角度为$$t\cdot \theta$$的旋转矩阵

| 方法 | 公式 | 
|------|--------------|
|绝对位置编码| $$\mathbf{x}_i=\mathbf{x}_i+\mathbf{p}_i$$ |
|相对位置编码|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T+r_{i-j}$$|
|RoPE|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T$$|
|Alibi|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T-m(i-j)$$|


## 预训练任务

### 语言建模

&nbsp;

语言建模是**仅解码器LLM**的常见目标，给定token序列$$\mathbf{x}=\left\{x_1, \ldots, x_n\right\}$$，旨在基于序列中前面的token，自回归地预估目标token：

XXX
\mathcal{L}_{L M}(\mathbf{x})=\sum_{i=1}^n \log P\left(x_i \mid x_{<i}\right)
XXX


对应到代码里：

```python
hidden_states = outputs[0]
logits = self.lm_head(hidden_states)

# gemma系列模型会做soft-cap
cap = self.config.logits_soft_cap
logits = nn.functional.tanh(logits / cap) * cap

logits = logits.float()
loss = None
if labels is not None:
    # Shift so that tokens < n predict n
    ## 假设句子长度为4，词表有3个词，
    ## logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3], [0.6, 0.1, 0.3] ]
    ## labels: [0, 2, 1, 0]
    ## shift_logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3]
    shift_logits = logits[..., :-1, :].contiguous()
    ## shift_labels: [2, 1, 0]
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    # [bs, seq_len - 1, vocab_size] 变为 [(bs * (seq_len - 1)), vocab_size]
    shift_logits = shift_logits.view(-1, self.config.vocab_size)
    # [bs, seq_len - 1] 变为 [(bs * (seq_len - 1))]
    shift_labels = shift_labels.view(-1)
    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels)
```

**前缀解码器**架构使用的是前缀语言建模任务，其loss**不涉及对前缀内token的预测**，故预训练时**涉及的序列中token较少**，故当预训练token数相同时，前缀语言模型的**性能往往略低**于传统语言模型任务。

另外，自回归的loss：

+ 训练时：是可以并行的，因为每个位置的label是已知的，可以并行算，
+ 预测时：是串行的，因为得预测完了第t个词，才能去预测第t+1个词。

### 去噪自编码

&nbsp;

DAE是BERT待模型的常见任务，即MLM（masked language model），输入$$\mathbf{x}_{\backslash \tilde{\mathbf{x}}}$$是一些**有随机替换区间的损坏文本**，目标是恢复被替换的token $$\tilde{\mathbf{x}}$$：

XXX
\mathcal{L}_{D A E}(\mathbf{x})=\log P\left(\tilde{\mathbf{x}} \mid \mathbf{x}_{\backslash \tilde{\mathbf{x}}}\right)
XXX

在T5和GLM-130B中使用，**自回归地恢复替换区间**。

### 其他任务

multi-token prediction，一次预估未来的k个词

[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)

![multi-token-prediction](../assets/multi-token-prediction.png)


# 新的模型结构

## 长上下文的问题

[长序列（Long Context）大模型笔记](https://mp.weixin.qq.com/s/nUX3MbKbyxw6b6mhgoL-Mw?poc_token=HK37E2ejmhfX-g6qnVG3pIxFJXZEVk-vPpwriYAj)

[LLM长上下文的问题](https://mp.weixin.qq.com/s/5e5HtxJrxNuhsVxrKsL2gA)

以中文为例，大部分模型每个token对应的中文字数都>1.5个字，所以200k的token就对应30w字的上下文

对长文本的几个要求：

+ 在文本比较长的时候，还能保证通顺，**ppl要足够低**
+ 能attention到前面提过的细节，**不能自我矛盾**

注意：如果训练时是2k长度的语料，而推理设定8k窗口，那么PPL会急剧上升，因为

+ RoPE不能很好地处理没有训练过的**位置编码**
+ 推理时**注意力机制**所处理的token数量远超训练时的数量，导致注意力机制的崩坏

### 两阶段训练方式

+ 直接输入连续长文本（如书籍）
+ 多个中等文本拼接，再通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。

如果简单地增加长度，例如从4k变到32k，长度增加8倍，为了加速计算需要缓存中间结果（如QK的结果是$$s^2$$的空间复杂度），所以显存会扩大$$8^2=64$$倍。一般的做法是2阶段：

+ 第一阶段：用2k或者4k训练一个基础模型，让模型学好**文本内容**和**短位置关系**
+ 第二阶段：用**比第一阶段小的数据量优化**模型在**长上下文的效果**，具体做法见下节

### 针对位置编码的插值类方法

#### 线性插值

&nbsp;

[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf)

#### NTK-Aware Interpolation

&nbsp;

#### NTK-by-parts

&nbsp;

#### Dynamically NTK Scaled RoPE

&nbsp;

### 针对attention score的缩放方法

#### YaRN

&nbsp;

#### logn

&nbsp;

### lossless long context

[专访月之暗面杨植麟：lossless long context is everything](https://mp.weixin.qq.com/s/UMY0qZsCGh87KnW4wjfvoA)

## 外推问题

[语言模型窗口外推技术综述](https://mp.weixin.qq.com/s/5CqrlUZHrciMAFmeV2oYdw)

### longRoPE

[LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens](https://mp.weixin.qq.com/s/4ryyv59ofNOD--RCSdqktQ)

### CoPE

[解决Transformer根本缺陷，CoPE论文爆火：所有大模型都能获得巨大改进](https://mp.weixin.qq.com/s/JxB6JU6MxO3709mkg7penw)

[Contextual Position Encoding: Learning to Count What’s Important](https://arxiv.org/pdf/2405.18719)

### DAPE

[NeurIPS 2024 | Transformer长度外推，全新位置编码DAPE大幅提升模型性能](https://mp.weixin.qq.com/s/-7YsAMYYO92nItRJbqSrpw)

[DAPE: Data-Adaptive Positional Encoding for Length Extrapolation](https://arxiv.org/abs/2405.14722)

[https://github.com/chuanyang-Zheng/DAPE](https://github.com/chuanyang-Zheng/DAPE)


## retrieval head

[Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/pdf/2404.15574)


## SSM

[挑战Transformer的Mamba是什么来头？作者博士论文理清SSM进化路径](https://mp.weixin.qq.com/s/oXSwnL0sD96nnnqJyko7UA)

[Pretraining Without Attention](https://arxiv.org/pdf/2212.10544.pdf) 

[预训练无需注意力，扩展到4096个token不成问题，与BERT相当](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864837&idx=2&sn=c57f0b8a7daf7d45093448c8ff5df3fc&chksm=84e538bbb392b1ad857f3ad2c2d6d6dbc9562ff492a7cda336dc07abe4d3912c19fa4c30d365&scene=21#wechat_redirect)

[Diffusion Models Without Attention](https://arxiv.org/pdf/2311.18257.pdf)

[丢掉注意力的扩散模型：Mamba带火的SSM被苹果、康奈尔盯上了](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650900014&idx=4&sn=315ca75ba700a93598b5c581ad2fb735&chksm=84e44250b393cb4619e7c24b9f6affe88e4c71ebb97d0c43b5301edd0a11154af6322f68cfd4&scene=21#wechat_redirect)

[一文看懂Mamba，Transformer最强竞争者](https://mp.weixin.qq.com/s/gwE_OVWigV71Qiup7F_9Cg)

[Long Range Language Modeling via Gated State Spaces](https://arxiv.org/abs/2206.13947) 认为 Transformer 和 SSM 完全可以互补。

选择性状态空间模型(selective state space model)是Mamba论文作者Albert Gu此前主导研发的S4架构(Structured State Spaces for Sequence Modeling)的一个简单泛化。

[https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf](https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf)

序列模型在训练和推理时的侧重点：

+ 训练时：在整个list上计算loss，需要优化forward的耗时
+ 推理时：一次输入一个时间步，需要高效地顺序处理

### SSM原理

输入$$u(t) \in \mathbb{R}$$，输出是$$y(t) \in \mathbb{R}$$，引入**状态**$$x(t) \in \mathbb{R}^N$$，目标是学习映射$$u(t) \mapsto y(t)$$，

XXX
\begin{aligned}
x^{\prime}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t)
\end{aligned}
XXX

结合[维基百科](https://en.wikipedia.org/wiki/State-space_representation)的图，方便理解

![ssm](../assets/ssm.png)

各变量含义如下（看起来和RNN很像）：

+ $$\boldsymbol{A}\in \mathbb{R}^{N\times N}$$：状态矩阵
+ $$\boldsymbol{B}\in \mathbb{R}^{N\times N}$$：输入矩阵
+ $$\boldsymbol{C}\in \mathbb{R}^{N\times N}$$：输出矩阵
+ $$\boldsymbol{D}\in \mathbb{R}^{N\times N}$$：feedforward矩阵或者feedthrough矩阵
+ $$x^{\prime}(t):=\frac{d}{d t} \mathbf{x}(t)$$是$$x(t)$$关于$$t$$的导数，所以图里有一个**积分**$$\int$$操作把它变回$$x(t)$$

假设序列长度为$$L$$，那么

+ 时间复杂度：
    + SSM：$$O(N^2L)$$，因为$$\boldsymbol{A} x(t)$$是$$N\times N$$和$$N\times 1$$的矩阵乘法，复杂度是$$N^2$$
    + RNN：$$O(N^2L)$$，和SSM类似
    + CNN：$$O(kLM^2)$$，假设$$k$$个卷积核，每个卷积核$$M\times M$$，一般$$M \ll N$$
+ 空间复杂度：
    + SSM：$$O(N^2+NL)$$，因为中间状态$$x(t) \in \mathbb{R}^N$$是需要**额外存储**的
    + RNN：$$O(N^2)$$，所有时间步共享权重，不需要像SSM专门为每个时间步存一个状态$$x(t)$$
    + CNN：$$kM^2$$

所以传统SSM的时空复杂度都很高，作者提出了S4(structured state space model)

====>hippo的假设会让模型更关注宽波，对于很窄的毛刺信号效果不好，所以对大海捞针任务效果并不好

## Mamba

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)

[https://huggingface.co/state-spaces](https://huggingface.co/state-spaces)

[https://github.com/havenhq/mamba-chat](https://github.com/havenhq/mamba-chat)

[MODELING SEQUENCES WITH STRUCTURED STATE SPACES](https://stacks.stanford.edu/file/druid:mb976vf9362)

## Mamba2

[再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升](https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g)

[Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/pdf/2405.21060)

[https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)

## Mamba+Transformer

[Mamba真比Transformer更优吗？Mamba原作者：两个都要！混合架构才是最优解](https://mp.weixin.qq.com/s/omImpaiddmSJ968bCZ8qmw)

[An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887)

## Block-State Transformer

[Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)

## Jamba

[Mamba做大做强！混合Transformer，打败Transformer](https://mp.weixin.qq.com/s/Lde0G_zysfrGcRUX5nOf8g)

[https://huggingface.co/ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1)

让mamba做短程头，让transformer做长程头

## Hawk & Griffin

[RNN效率媲美Transformer，谷歌新架构两连发：同等规模强于Mamba](https://mp.weixin.qq.com/s/RtAZiEzjRWgqQw3yu3lvcg)

[Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/pdf/2402.19427)

### 简介

global attention在infer阶段的效率和序列长度成二次关系，而且序列长度与KV cache呈线性增长关系。[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)提出的multi-query attention(MQA)可以通过一个**constant factor**来**减小cache size**，部分缓解这个问题，但cache还是与序列长度线性相关。

循环模型能够将整个序列压缩到一个**fixed-size的hidden state**，并通过迭代进行更新。但要想取代transformer，rnn不仅需要在效果上可比，还要相似的硬件效率，相关工作如下：

+ [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396)
+ [Long range language modeling via gated state spaces](https://arxiv.org/pdf/2206.13947)提出了GSS block
+ [Simplified state space layers for sequence modeling](https://arxiv.org/pdf/2208.04933)
+ [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ [Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)
+ [Hyena hierarchy: Towards larger convolutional language models](https://arxiv.org/pdf/2302.10866)
+ [Mamba: Linear-time sequence modeling with selective state spaces](https://arxiv.org/pdf/2312.00752)

本文提出了新的**RG-LRU层**，一种新的gated linear recurrent layer，并基于此提出了将MLP和RG-LRU结合的Hawk，还有将MLP和RG-LRU与local attention混合的Griffin。

![griffin-hawk](../assets/griffin-hawk.png)

+ 对于最多超过7B的模型，Hawk和Griffin发现了**held-out loss**和**训练FLOPS**间的**power law scaling**（上图左边）
+ Hawk-3B比Mamba-3B在下游任务上要好，而训练的token数只有mamba的一半；Griffin-7B和14B与Llama-2效果相当，而训练数据只有其1/7
+ 训练在TPU-v3上完成，用JAX中的Pallas实现了RG-LRU(Real-Gated Linear Recurrent Unit)层的内核([https://jax.readthedocs.io/en/latest/pallas/index.html](https://jax.readthedocs.io/en/latest/pallas/index.html))，减小内存transfer
+ infer阶段比MQA的**吞吐高很多**（上图右边），而且对长序列有**更低的latency**
+ 训练时在长序列上比transformer更好，而且能够高效地学习**复制和检索**的任务。但如果没有finetune，直接对比pretrain的效果，transformer会更好

### 网络结构

包括三大类组件：

+ residual block: 
+ MLP block：
+ temporal-mixing block


![griffin-arch](../assets/griffin-arch.png)

#### Residual Block

&nbsp;

受pre-norm transformer的启发，用的RMSNorm

#### MLP block

&nbsp;

参考[Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083)（类似star两个W直接element-wise product），采用了gated MLP，即如下两个linear的结果（输入维度$$D$$，输出维度都是$$MD,M=3$$）进行element-wise product(类似GeGLU（[Glu variants improve transformer]((https://arxiv.org/pdf/2002.05202.pdf))）)，再过一个linear

+ 直接过一个linear，但没有激活
+ linear后加一个GeLU激活（实际用的```nn.functional.gelu(input, approximate="tanh")```）

看recurrentGemma-2b的[config](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json)，发现总共有26个block，一个block依次包括：

+ rg-lru或者MQA，每个block里的选择方式是(rg,rg,att,rg,rg,att,rg,rg,att,...)
+ 再过一个gated MLP

```python
  "_block_types": [
    "recurrent",
    "recurrent",
    "attention"
  ],
  "num_hidden_layers": 26,
  "attention_window_size": 2048,

```

#### Temporal-mixing block

&nbsp;

##### global Multi-Query Attention(MQA)

&nbsp;

[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)中为了加速推理，采用了MQA的方法，本文固定了head的维度为$$D_{head}=128$$，head的个数$$H$$也固定，且$$HD_{head}=D$$，所以model的维度$$D$$需要是128的倍数。没用绝对位置编码，用了RoPE。

##### local(sliding-window) MQA

&nbsp;

[Longformer: The long-document transformer](https://arxiv.org/pdf/2004.05150)提出了可以用local attention，即滑动窗口attention。让每个位置只和前面的固定个tokens去算attentioin，可以

+ 降低计算的FLOPS
+ 让KV cache的size的上界变成了**window的size**，从而不是序列长度的二次关系

##### RG-LRU

&nbsp;

类似GSS block，也类似Mamba，输入$$D$$，分别过一个linear得到两个分支，均是$$D_{RNN}$$：

+ 分支1：过GeLU激活，同上，实际用的```nn.functional.gelu(input, approximate="tanh")```
+ 分支2：
  + 参考[Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)的H3模型里的Shift-SSM，过一个Conv1D，其temporal filter dim是4
  + Conv1D的参数只有$$4D_{RNN}$$，因此再过一个RG—LRU模块

两个分支的输出element-wise product一下，再过一个linear得到$$d$$。

conv1d参考torch官方[doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)

+ 输入：$$\left(N, C_{i n}, L_{i n}\right)$$
+ 输出：$$\left(N, C_{\text {out }}, L_{\text {out }}\right)$$

其中，

XXX
L_{\text {out }}=\left\lfloor\frac{L_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel\_size }-1)-1}{\text { stride }}+1\right\rfloor
XXX

当```groups==in_channels and out_channels=K*in_channels```，也叫depthwise convolution，K是depthwise乘子。即**每个输入通道都有自己的卷积核，并且只作用于该通道**。

```python
        ## conv1d_width = 4, 
        ## 对图像卷积来讲，padding就是让输出的shape不小于输入shape
        ## 输入是hidden_size=2560，而lru_width也是2560
        self.conv_1d = nn.Conv1d(
                    config.lru_width, # in_channels
                    config.lru_width, # out_channels
                    kernel_size=config.conv1d_width,
                    groups=config.lru_width,
                    padding=config.conv1d_width - 1,
                )
        
        ## 输入[bs, seq_len, hidden_size]
        x_branch = self.linear_x(input_states)
        ## 变成[bs, hidden_size, seq_len]
        x_branch = x_branch.transpose(1, 2)
        if use_cache:
            if cache_position.shape[0] != 1:  # prefill
                self.conv1d_state = nn.functional.pad(x_branch, 
                    (self.conv1d_width - x_branch.shape[-1] - 1, 0))
                x_branch = self.conv_1d(x_branch)[..., :seq_len]
            else:  # decoding
                conv_state = torch.cat((self.conv1d_state, x_branch), -1)
                x_branch = torch.sum(conv_state * self.conv_1d.weight[:, 0, :], dim=-1) 
                    + self.conv_1d.bias
                x_branch = x_branch.unsqueeze(-1)
                self.conv1d_state = conv_state[:, :, 1:]
        else:
            # 前面维度不变，最后一维截断到seq_len
            x_branch = self.conv_1d(x_branch)[..., :seq_len]
```


参考[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)提出的LRU(Linear Recurrent Unit)，并参考传统LSTM和GRU引入了gate：

XXX
\begin{aligned}
r_t & =\sigma\left(W_a x_t+b_a\right), \quad \text { recurrence gate } \\
i_t & =\sigma\left(W_x x_t+b_x\right), \quad \text { input gate } \\
a_t & =a^{c r_t}, \\
h_t & =a_t \odot h_{t-1}+\sqrt{1-a_t^2} \odot\left(i_t \odot x_t\right) .
\end{aligned}
XXX

其中，recurrent weight $$a=\sigma(\Lambda)$$是一个对角矩阵，$$\Lambda$$是一个可学习的参数。$$c$$是一个常数8，为了**计算稳定**，在log-space计算$$a^{c r_t}$$，即先算出$$\log a_t$$，再取exp。

XXX
\log a_t=\log a^{c r_t}=\log \sigma(\Lambda)^{c r_t}=-\operatorname{csoftplus}(\Lambda) \odot r_t 
XXX

(这个公式可能有点问题，感觉应该是$$-\operatorname{csoftplus}(-\Lambda) \odot r_t$$，不过$$\Lambda$$是一个可学习的nn.Parameter，其实这个错误无所谓吧)

```python
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.block_width = config.lru_width // self.num_attention_heads

        self.recurrent_param = nn.Parameter(torch.empty([config.lru_width]))
        self.input_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.input_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))

        self.recurrent_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.recurrent_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))
        self.recurrent_states = None

    def forward(xxx):
        ## reshape成适合多头的情况
        reshape_act = activations.reshape(batch_size * seq_len, self.num_attention_heads, self.block_width)
        ## (num_attention_heads, batch_size * seq_len, block_width)
        reshape_act = reshape_act.permute(1, 0, 2)

        ## 批量矩阵乘法（baddbmm），在reshape_act和self.input_gate_weight之间进行，
        ## 并加上偏置self.input_gate_bias。这一步计算输入门的原始值。
        res = torch.baddbmm(self.input_gate_bias[:, None, :], reshape_act, self.input_gate_weight)
        input_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        ## 类似input_gate
        res = torch.baddbmm(self.recurrent_gate_bias[:, None, :], reshape_act, self.recurrent_gate_weight)
        recurrent_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        # Compute the parameter `A` of the recurrence.
        # 上面的公式
        log_recurrent_gate = -8.0 * recurrent_gate * nn.functional.softplus(self.recurrent_param)
        recurrent_gate = torch.exp(log_recurrent_gate)
```

还有如下几个特点：

+ $$W_a$$和$$W_x$$用LeCun init初始化
+ 初始化$$\Lambda$$，使得在训练开始时，$$a^c$$均匀分布在0.9和0.999之间，类似[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ 不像大部分SSM（例如[Hippo: Recurrent memory with optimal polynomial projections](https://arxiv.org/pdf/2008.07669)）基于orthogonal polynomials（正交多项式）理论进行初始化。
+ 也不像[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)在底层的连续系统上进行离散化定义
+ 不像原始的LRU用了复数，虽然[ On the universality of linear recurrences followed by nonlinear projections](https://arxiv.org/pdf/2307.11888v1)说复数的表达能力更强，但在实践中对语言模型并没有什么用（[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)）

##### gate特点

&nbsp;

先复习一下LSTM：

![lstm-peephole-lstm](../assets/lstm-peephole-lstm.png)

GRU：

+ 重置门（reset gate）：如果重置门关闭，会**忽略掉历史信息**，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的**输入门和遗忘门合并**，用于控制**历史信息对当前时刻隐层输出的影响**。如果更新门接近1，会把历史信息传递下去。

![gru](../assets/gru.png)

+ 两个gate只和$$x_t$$有关，**和$$h_{t-1}$$无关**。
+ input gate $$i_t$$和LSTM类似，直接对输入$$x_t$$进行filter或者scale down。
+ recurrent gate $$r_t$$和之前的gate机制不同：
    + mamba里的selection机制和GRU的**update gate**类似，在**之前的状态**和**当前输入$$x_t$$**之间进行插值（interpolate），功能类似LSTM的**forget gate**，能够**reset状态，并遗忘之前的信息**
    + 本文的recurrent gate则类似于在LRU的更新和之前的隐藏状态之间进行插值，能够**有效地丢弃输入**，并且**保持之前历史里的所有信息**。使得模型在处理不相关或重复输入（**uniformative inputs**）时，更能达到**超指数的记忆能力**，以更有效地保留有用信息(因为这个gate**和$$h_{t-1}$$无关**)。

## SAMBA

[长文本模型近期研究工作梳理](https://mp.weixin.qq.com/s/5u2w08twsJ6CgHZ2FpN8JA)

[SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/pdf/2406.07522)

## Mamba in llama

(toread)

[Mamba作者新作：将Llama3蒸馏成混合线性 RNN](https://mp.weixin.qq.com/s/jZGcBxhVUfb-Qv1UkqYK4A)

[The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/pdf/2408.15237)

## feedback attention memory

[TransformerFAM: Feedback attention is working memory](https://arxiv.org/pdf/2404.09173.pdf)

## infini-attention

[Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143.pdf)

[【重磅】谷歌重塑Transformer：无限记忆力，无限长输入，LLM基础研究重大突破](https://mp.weixin.qq.com/s/bV2b9uJ4GFQPhhggHT3VIA)

将**压缩记忆**整合进标准的点积注意力机制，并在单个Transformer块内同时实现了**掩码局部注意力**和**长期线性注意力机制**

![infini-attention](../assets/infini-attention.png)

与transformer-xl对比：

![infini-attention-vs-transformer-xl](../assets/infini-attention-vs-transformer-xl.png)

[https://github.com/mustafaaljadery/gemma-2B-10M](https://github.com/mustafaaljadery/gemma-2B-10M)

[https://github.com/dingo-actual/infini-transformer](https://github.com/dingo-actual/infini-transformer)

## MEGA

[Mega: Moving average equipped gated attention](https://arxiv.org/pdf/2209.10655.pdf)

[https://github.com/facebookresearch/mega](https://github.com/facebookresearch/mega)

### 标准的self-attention公式

&nbsp;

对于序列长度=$$n$$的输入$$\boldsymbol{X}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\right\} \in \mathbb{R}^{n \times d}$$，输出$$\boldsymbol{Y}=\left\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_n\right\} \in \mathbb{R}^{n \times d}$$

XXX
\boldsymbol{Y}=\operatorname{Attn}(\boldsymbol{X})=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \boldsymbol{V}
XXX

XXX
\boldsymbol{Q}=\boldsymbol{X} W_q+b_q,\boldsymbol{K}=\boldsymbol{X} W_k+b_k,\boldsymbol{V}=\boldsymbol{X} W_v+b_v
XXX

其中，$$\text { Attn : } \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$$，$$W_q, W_k, W_v \in \mathbb{R}^{d \times d}$$，$$b_q, b_k, b_v \in \mathbb{R}^d$$，而且有两种定义方式：

+ $$f(\cdot)=f_{\text {softmax }}(\cdot)$$，同时$$\tau(\boldsymbol{X})=\sqrt{d}$$，这是经典操作[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
+ $$f(\cdot)=f_{\mathrm{relu}^2}(\cdot)$$即$$relu(x)^2$$，同时$$\tau(\boldsymbol{X})=n$$，这是[Primer: Searching for efficient transformers for language modeling](https://arxiv.org/pdf/2109.08668.pdf)和[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)提出的

![gau](../assets/gau.png)

对于$$h$$个attention heads，计算$$\boldsymbol{A}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \in \mathbb{R}^{n \times n}$$需要的时间和空间复杂度都是$$O(hn^2)$$

### EMA

&nbsp;

对于输出的序列$$\boldsymbol{Y}$$采用Exponential Moving Average(指数滑动平均)如下

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha}) \odot \mathbf{y}_{t-1}
XXX

其中，$$\boldsymbol{\alpha} \in(0,1)^d$$表示权重衰减，$$\odot$$是element-wise product

$$\boldsymbol{\alpha}$$越大，$$1-\boldsymbol{\alpha}$$越小，对历史的衰减也越快：

![ema](../assets/ema.png)

EMA的计算看成$$n$$个**独立的卷积**，可以通过**FFT(快速傅立叶变换)**来加速计算。具体。。。再看看

**Damped EMA**：由于输入的x是d维向量，可以引入一个因子$$\boldsymbol{\delta} \in(0,1)^d$$让EMA更鲁棒：

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha} \odot \boldsymbol{\delta}) \odot \mathbf{y}_{t-1}
XXX

### MEGA

&nbsp;

EMA可以看成是一种**与位置相关的归纳偏置（inductive bias）**，即假设当前位置与之前位置满足滑动平均的关系，而attention矩阵的计算其实并没有考虑位置信息，所以可以把二者结合一下。

![mega](../assets/mega.png)

其中的multi-dimensional damped EMA大致流程如下：

+ **先变成h维**：先把$$\boldsymbol{X} \in \mathbb{R}^{n \times d}$$通过矩阵$$\beta$$映射成$$\boldsymbol{U}\in \mathbb{R}^{n \times h}$$
+ **计算EMA**：然后通过EMA得到$$\boldsymbol{h}\in \mathbb{R}^{n \times h}$$（具体$$\mathbf{h}_t^{(j)}=\boldsymbol{\alpha}_j \odot \mathbf{u}_t^{(j)}+\left(1-\boldsymbol{\alpha}_j \odot \boldsymbol{\delta}_j\right) \odot \mathbf{h}_{t-1}^{(j)}$$）
+ **再变回d维**：再通过一个矩阵$$\eta$$变回$$\boldsymbol{Y}\in \mathbb{R}^{n \times h}$$

然后看整个流程：

+ 先计算EMA

XXX
\begin{aligned}
\boldsymbol{X}^{\prime} & =\operatorname{EMA}(\boldsymbol{X}) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Z} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_z+b_z\right) & & \in \mathbb{R}^{n \times z}
\end{aligned}
XXX

+ 再基于EMA的结果计算QK，基于原始的X计算V：

XXX
\begin{array}{ll}
\boldsymbol{Q}=\boldsymbol{\kappa}_q \odot \boldsymbol{Z}+\boldsymbol{\mu}_q & \in \mathbb{R}^{n \times z} \\
\boldsymbol{K}=\boldsymbol{\kappa}_k \odot \boldsymbol{Z}+\boldsymbol{\mu}_k & \in \mathbb{R}^{n \times z} \\
\boldsymbol{V}=\phi_{\text {silu }}\left(\boldsymbol{X} W_v+b_v\right) & \in \mathbb{R}^{n \times v}
\end{array}
XXX

+ 计算带位置bias的attention：

XXX
\boldsymbol{O}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}+\boldsymbol{b}_{\mathrm{rel}}\right) \boldsymbol{V} \quad \in \mathbb{R}^{n \times v}
XXX

+ 通过reset gate $$\boldsymbol{\gamma}$$和update gate $$\boldsymbol{\varphi}$$计算输出

XXX
\begin{aligned}
\boldsymbol{\gamma} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_\gamma+b_\gamma\right) & & \in \mathbb{R}^{n \times v} \\
\boldsymbol{\varphi} & =\phi_{\text {sigmoid }}\left(\boldsymbol{X}^{\prime} W_{\varphi}+b_{\varphi}\right) & & \in \mathbb{R}^{n \times d} \\
\hat{\boldsymbol{H}} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_h+(\boldsymbol{\gamma} \boldsymbol{O}) U_h+b_h\right) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Y}&=\boldsymbol{\varphi} \odot \hat{\boldsymbol{H}}+(1-\boldsymbol{\varphi}) \odot \boldsymbol{X} \quad & & \in \mathbb{R}^{n \times d}
\end{aligned}
XXX


这里把attention里的softmax改成了如下的laplace函数：

XXX
f_{\text {laplace }}(x ; \mu, \sigma)=0.5 \times\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma \sqrt{2}}\right)\right]
XXX

其中，$$\operatorname{erf}(x)=\frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} \mathrm{~d} t=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \mathrm{~d} t$$是[误差函数](https://zh.wikipedia.org/wiki/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0)，

为了让laplace逼近$$f_{\text {relu }^2}$$，对于$$x=\sqrt{2}$$这个点，求解如下方程

XXX
\begin{aligned}
& f_{\text {relu2 }}(\sqrt{2})=f_{\text {laplace }}(\sqrt{2}) \\
& f_{\text {relu2 }}^{\prime}(\sqrt{2})=f_{\text {laplace }}^{\prime}(\sqrt{2})
\end{aligned}
XXX

可以得到$$\mu=\sqrt{1 / 2}$$，$$\sigma=\sqrt{1 / 4 \pi}$$，对应的曲线和准确率如下：

![laplace-vs-relu2](../assets/laplace-vs-relu2.png)

### mega-chunk

![mega-chunk](../assets/mega-chunk.png)

将序列切分成长度固定为$$c$$的$$k=n/c$$个chunk，对**每个chunk独立计算**上面的attention，这样复杂度就变成了$$O(kc^2)=O(nc)$$，由于有EMA，所以这样做还是能够保持一定程度的长距离依赖。

## MEGALODON

[Meta无限长文本大模型来了：参数仅7B，已开源](https://mp.weixin.qq.com/s/VML5hExo5iPsyEavxzIZSA)

[革命新架构掀翻Transformer！无限上下文处理，2万亿token碾压Llama 2](https://mp.weixin.qq.com/s/xgP9P51gjqJ93FYSWfPeaA)

[MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

[https://github.com/XuezheMax/megalodon](https://github.com/XuezheMax/megalodon)

[Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

基于MEGA进行改进，能够同时实现

+ 高效训练（减少通信和计算量）
+ 高效推理（保持恒定的KV缓存）


## MOD

[Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/pdf/2404.02258.pdf)


## multi-head moe

[微软让MoE长出多个头，大幅提升专家激活率](https://mp.weixin.qq.com/s/ZCRyb63M2DL4hOQh7uxxaw)

[Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)

[https://github.com/yushuiwx/MH-MoE](https://github.com/yushuiwx/MH-MoE)

## Lory

[150B token从头训练，普林斯顿Meta发布完全可微MoE架构Lory](https://mp.weixin.qq.com/s/UKIXGJTFzSeSZvoTe_c9CQ)

[Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](https://arxiv.org/pdf/2405.03133)

## Perciever

[Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf)

## Aaren

[Bengio等人新作：注意力可被视为RNN，新模型媲美Transformer，但超级省内存](https://mp.weixin.qq.com/s/mRt2A1n1CmO7uqzuLQHxkw)

[Attention as an RNN](https://arxiv.org/pdf/2405.13956)

### many-to-one RNN

对query向量$$q$$的attention可以看成是一个函数，对输入的$$N$$个token $$x_{1: N}$$通过他们的key和value $$\left\{\left(k_i, v_i\right)\right\}_{i=1}^N$$ 变换成输出 $$\text { Attention }\left(q, k_{1: N}, v_{1: N}\right)$$，假设$$\bar{s}_i=\operatorname{dot}\left(q, k_i\right)$$，那么输出就是

XXX
o_N=\sum_{i=1}^N \operatorname{softmax}(s)_i v_i=\frac{\sum_{i=1}^N \exp \left(s_i\right) v_i}{\sum_{i=1}^N \exp \left(s_i\right)}=\frac{\hat{a}_N}{\hat{c}_N}
XXX

可以发现，分子和分母其实都可以写成递推形式$$\hat{a}_k=\hat{a}_{k-1}+\exp \left(s_k\right) v_k$$和$$\hat{c}_k=\hat{c}_{k-1}+\exp \left(s_k\right)$$，由于直接这么做可能会产生很大或者很小的值（例如算exp），所以可以通过如下方式进行缓解：计录到第k步的最大值$$m_k=\max _{i \in\{1, \ldots, k\}} s_i$$，然后减掉它，即$$a_k=\sum_{i=1}^k \exp \left(s_i-m_k\right) v_i$$和$$c_k=\sum_{i=1}^k \exp \left(s_i-m_k\right)$$，这样就可以改写为如下形式：

XXX
\begin{aligned}
a_k & =a_{k-1} \exp \left(m_{k-1}-m_k\right)+v_k \exp \left(s_k-m_k\right) \\
c_k & =c_{k-1} \exp \left(m_{k-1}-m_k\right)+\exp \left(s_k-m_k\right) \\
m_k & =\max \left(m_{k-1}, s_k\right)
\end{aligned}
XXX

其中第一行其实就是$$s_i-m_k=s_i-m_k+m_{k-1}-m_k$$，然后exp一下，$$exp(s_i-m_k)=exp(s_i-m_k)exp(m_{k-1}-m_k)$$，总结成如下图：

![attention-rnn-cell](../assets/attention-rnn-cell.png)

所以Attention的RNN cell就是输入$$\left(a_{k-1}, c_{k-1}, m_{k-1}, q\right)$$，输出$$\left(a_k, c_k, m_k, q\right)$$，初始的状态是$$\left(a_0, c_0, m_0, q\right)=(0,0,0,q)$$

然后就可以将之前的attention看成如下几类many-to-one的RNN了：

+ 传统attention只计算最后的一个输出
+ self-attention使用**输入token**作为初始状态
+ Perceiver的cross-attention使用**依赖input的隐变量**作为初始状态

![many-to-one-rnn](../assets/many-to-one-rnn.png)

对于新来的token而言：

+ 传统的RNN一般是流式地输入数据，因此只需要O(1)的内存和计算就行了
+ Transformer需要把这个新token当成一个初始状态加进来，所以需要把之前时间步的再重新算一次，需要O(N)的计算
+ Perceiver中的隐变量是依赖输入的，而且这个新token会改变value，因此初始状态也会变，所以需要从头算一遍，需要O(NL)的计算，L是隐变量个数（可以参考代码[perceiver_torch](https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py)和如下gpt4o的回答）

![perceiver](../assets/perceiver.png)

### many-to-many RNN

要计算$$\left\{o_i=\operatorname{Attention}\left(q, x_{1: i}\right)\right\}_{i=1}^N$$，通过如下的并行前缀扫描算法，对于$$N$$个序列数据的$$N$$个前缀，通过关联运算符$$\oplus$$并行计算，能够高效地通过$$\left\{x_k\right\}_{k=1}^N$$计算$$\left\{\bigoplus_{i=1}^k x_i\right\}_{k=1}^N$$

![parallel-prefix-scan](../assets/parallel-prefix-scan.png)

由于$$\operatorname{Attention}\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)=o_k=\frac{a_k}{c_k}$$，为了计算$$\left\{\text { Attention }\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)\right\}_{k=1}^N$$，只需要先按这个并行扫描算法计算$$\left\{a_k\right\}_{k=1}^N$$、$$\left\{c_k\right\}_{k=1}^N$$和$$\left\{m_k\right\}_{k=1}^N$$，再把$$a_k$$和$$c_k$$结合起来就行。

接来来定义三元组$$\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right)$$，其中

+ $$A$$：一些下标的集合
+ $$\mathrm{m}_A=\max _{i \in A} s_i$$
+ $$\mathrm{u}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right)$$
+ $$\mathrm{w}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right) v_i$$

所以并行扫描算法的输入是$$\left\{\left(\mathrm{m}_{\{i\}}, \mathrm{u}_{\{i\}}, \mathrm{W}_{\{i\}}\right)\right\}_{i=1}^N=\left\{\left(s_i, 1, v_i\right)\right\}_{i=1}^N$$，再来定义操作$$\oplus$$：

XXX
\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right) \oplus\left(\mathrm{m}_B, \mathrm{u}_B, \mathrm{w}_B\right)=\left(\mathrm{m}_{A \cup B}, \mathrm{u}_{A \cup B}, \mathrm{w}_{A \cup B}\right)
XXX

其中，

+ $$\mathrm{m}_{A \cup B}=\max \left(\mathrm{m}_A, \mathrm{~m}_B\right)$$
+ $$\mathrm{u}_{A \cup B}=\mathrm{u}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{u}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$
+ $$\mathrm{w} _{A \cup B}=\mathrm{w}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{w}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$

这个并行扫描算法最终输出下式，即$$\left\{\left(m_k, c_k, a_k\right)\right\}_{k=1}^N$$：

XXX
\left\{\left(\mathrm{m}_{\{1, \ldots, k\}}, \mathrm{u}_{\{1, \ldots, k\}}, \mathrm{w}_{\{1, \ldots, k\}}\right)\right\}_{k=1}^N=\left\{\left(m_k, \sum_{i=1}^k \exp \left(s_i-m_k\right), \sum_{i=1}^k \exp \left(s_i-m_k\right) v_i\right)\right\}_{k=1}^N
XXX


![many-to-many-rnn](../assets/many-to-many-rnn.png)

### Aaren

Aaren(attention as a recurrent newral network)的结构如下：

XXX
\begin{aligned}
h_1^{(0)}, \ldots, h_N^{(0)} & \leftarrow x_1, \ldots, x_N \\
{\left[h_1^{(j+1)}, \ldots, h_N^{(j+1)}\right] } & \leftarrow \operatorname{Aaren}\left(q^{(j)},\left[h_1^{(j)}, \ldots, h_N^{(j)}\right]\right)
\end{aligned}
XXX

transformer的query是输入的token，而Aaren的query token $$q$$是在训练的过程中通过bp学习的。迭代地计算$$y_k$$只需要常数级的计算，因为它依赖$$h_{k-1}$$和$$x_k$$。

transformer：

+ 使用kv cache时需要线性的内存
+ 需要保存所有之前的tokens，包括在中间层的那些

aaren：

+ 只需要常数级的内存
+ 不需要保存之前的所有tokens

![stacking-aaren](../assets/stacking-aaren.png)

## Matmul-free

[从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗](https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag)

## H2O attention

[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/pdf/2306.14048)

+ sparsity for small cache size：kv cache很稀疏，只要5%效果就可以了
+ Heavy-Hitters for low miss rate：一小部分的token贡献了大部分attention score
+ 贪心法选择需要干掉的token：假设总共需要保留k个token，对于第i个token，加进来后，遍历集合里的所有token，看干掉哪一个对attention score影响最小，就把它干掉

## TransNAR

[拯救Transformer推理能力！DeepMind新研究TransNAR：给模型嵌入「算法推理大脑」](https://mp.weixin.qq.com/s/YPICpkYHAC7zTLC_0M_XkQ)

[Transformers meet Neural Algorithmic Reasoners](https://arxiv.org/pdf/2406.09308)

基于：[Neural Algorithmic Reasoning](https://arxiv.org/pdf/2105.02761)

## softmax数学原理

[通向概率分布之路：盘点Softmax及其替代品](https://mp.weixin.qq.com/s/tA9kJqD279dHnivzPkvbjg)

把softmax的分母记为$$Z(x)$$，其对数是max的一个光滑近似：

XXX
\begin{array}{r}
\log Z(\boldsymbol{x})=\log \sum_{j=1}^n e^{x_j}=\operatorname{logsumexp}(\boldsymbol{x}) \\
\lim _{\tau \rightarrow 0^{+}} \tau \operatorname{logsumexp}(\boldsymbol{x} / \tau)=\max (\boldsymbol{x})
\end{array}
XXX

当$$\tau$$取1时，可以得到$$\operatorname{logsumexp}(\boldsymbol{x}) \approx \max (\boldsymbol{x})$$

## StreamingLLM

[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453)

[深度解析streamingLLM之无限长文本生成能力](https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&mid=2247484158&idx=1&sn=666bd5fed13815b5765124646206bac5&chksm=cea54ae8f9d2c3fea1f3d5dfc94fc3d48abbd02a1278584bfd76ff8fe52ce59ce8966ced9cd5&token=562866785&lang=zh_CN#rd)

attention sink：输入给LLM推理开头的几个intial tokens是非常特殊的，就像水龙头一样，出水那一瞬间吸引了人们大量的attention。而且intial tokens与生成token的绝对距离距离和语义信息都不重要，重要的是这第一个或者前面几个token。

softmax需要所有位置的值的总和为1，因此必须给某些位置权重，即使这个位置对输出没有任何贡献，可能导致在backward的过程中产生错误的权重更新，而这个错误在后续的过程中很难被纠正。因此，模型倾向于将不必要的注意力值转嫁给特定的token。其实在[https://www.evanmiller.org/attention-is-off-by-one.html](https://www.evanmiller.org/attention-is-off-by-one.html)就提出了，要给softmax的分母+1：$$(\operatorname{softmax_1}(\mathrm{x}))_i=\frac{\exp \left(x_i\right)}{\sum_j \exp \left(x_j\right)+1}$$

当每个token都趋向负无穷时，softmax的极限是1/k（k是token数），也就是会让每个token的概率都是1/k

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}(\operatorname{softmax}(x))_i=\frac{1}{k}>0
XXX

而$$$$softmax_1$$$$因为分母有个1（即$$exp(0)=1$$），相当于在最前面引入了一个或者多个的无意义的global_token。同样当每个token都趋向负无穷时，因为分母已经有一个1了，所以可以让其他正常token都趋于0

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}\left(\operatorname{softmax}_1(x)\right)_i=0
XXX


[https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

集成到trt-llm里了：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)

+ trtllm-build时，加上```--streamingllm enable```
+ infer时，加上```--sink_token_length```设置sink的token数，加上```--max_attention_window_size```设置sliding_window

## 是否还要RAG?

[谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强](https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg)

## memory^3

[鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆](https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw)

[Memory3 : Language Modeling with Explicit Memory](https://arxiv.org/pdf/2407.01178)

## TSLLM

[没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练](https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg)

[AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training](https://arxiv.org/pdf/2309.17179)

[https://github.com/waterhorse1/LLM_Tree_Search](https://github.com/waterhorse1/LLM_Tree_Search)

## PEER

[单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE](https://mp.weixin.qq.com/s/-oocmPNRT5ddvNwIvYxiQA)

[MoE也有Scaling Law，「百万专家」利用率近100%！DeepMind华人挑战MoE极限](https://mp.weixin.qq.com/s/tBe9DZvzB6NB8HhLYP31CQ)

[Mixture of A Million Experts](https://arxiv.org/pdf/2407.04153)

## TTT

[彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态](https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ)

[Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620)

[https://github.com/test-time-training/ttt-lm-jax](https://github.com/test-time-training/ttt-lm-jax)

[https://github.com/test-time-training/ttt-lm-pytorch](https://github.com/test-time-training/ttt-lm-pytorch)

![ttt](../assets/ttt.png)

## Axiomatic Training

[6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码](https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ)

[公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4](https://mp.weixin.qq.com/s/Yera281WG8RNAOrRUYjH6g)

[Teaching Transformers Causal Reasoning through Axiomatic Training](https://arxiv.org/pdf/2407.07612v1)

训练大模型新范式——**公理框架（Axiomatic Framework）**，作者从头开始训练了6700万参数的模型，仅使用了简单的因果链作为训练数据。在推断复杂图表中的因果关系时，**67M模型的表现超越了十亿级参数LLM**，甚至可以与GPT-4相媲美。

## PNN

[AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键](https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw)

[Training of Physical Neural Networks](https://arxiv.org/pdf/2406.03372)

## JRT

[小技巧大功效，「仅阅读两次提示」让循环语言模型超越Transformer++](https://mp.weixin.qq.com/s/zdPlK4IHeEiW0ikmQMPJUA)，注：这里的transformer++指的就是llama2的架构

[Just read twice: closing the recall gap for recurrent language models](https://arxiv.org/pdf/2407.05483)

[https://github.com/HazyResearch/prefix-linear-attention](https://github.com/HazyResearch/prefix-linear-attention)

![jrt](../assets/jrt.png)

### order的重要性

&nbsp;

线性attention的recurrent state很小，存储空间有限，往往很难选择要存储哪些state。

如果集合A和集合B有交集，且A元素比B多，先出现集合A的时候，需要存储整个A，而先出现集合B的时候，则只需要存储集合B

### JRT-Prompt

&nbsp;

上下文学习任务以$$(\mathcal{C}, \mathcal{Q}, \mathcal{Y})$$作为输入，$$\mathcal{C}$$为一些上下文来源（如文档或代码存储库），$$\mathcal{Q}$$为给定上下文时对模型的一些问题或请求，$$\mathcal{Y}$$为答案。

+ 使用自回归的标准上下文学习模型$$\mathcal{A}$$，输入$$\mathcal{C}$$和$$\mathcal{Q}$$，并根据正确的完成情况$$Y$$来评估生成的输出$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q})$$。
+ JRT-PROMPT：在提示模型输出答案之前会在上下文中重复提示中的信息（如问题和文档），例如$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q}, \mathcal{C}, \mathcal{Q})$$。在上下文第二次出现时，模型根据完整的上下文来决定存储哪些信息。

示例：

```shell
# 原来的prompt
## input: 
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年

# JRT的prompt
## input：
百度成立于哪一年？百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年
```

### base的linear transformer

&nbsp;

通过$$\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{\tilde{d}}$$，使得$$\phi\left(\boldsymbol{q}_i\right)^{\top} \phi\left(\boldsymbol{k}_j\right) \approx \exp \left(\boldsymbol{q}_i^{\top} \boldsymbol{k}_j / \sqrt{d}\right) .$$

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i\left(\phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j\right)}{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i \phi\left(\boldsymbol{k}_j\right)}
XXX

先计算k和v的乘法，时间和空间复杂度是$$\mathcal{O}(N d \tilde{d})$$，而softmax attention是$$O\left(N^2 d\right)$$

infer阶段包括两个phases：

+ prefill：并行处理prompt，得到两个state：
    + KV-state：$$\boldsymbol{s}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j$$
    + K-state：$$\boldsymbol{z}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top}$$
+ decoding：计算如下3步，其中$$\boldsymbol{s}_i \in \mathbb{R}^{d \times \tilde{d}}$$，$$\boldsymbol{z}_i \in \mathbb{R}^{\tilde{d}}$$，一个decode step有$$O(1)$$的时间和空间复杂度，而softmax attention加上kv-caching有$$O(N)$$
    + $$\boldsymbol{s}_i=\boldsymbol{s}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top} \boldsymbol{v}_i$$
    + $$\boldsymbol{z}_i=\boldsymbol{z}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top}$$
    + $$\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{s}_i}{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{z}_i}$$

### JRT-RNN架构

&nbsp;

PLA（Prefix Linear Attention）受Prefix-LM启发，主要有2个特点：

+ prefix-LM在encoder和decoder的projection是共享的，而JRT-RNN的encoder用$$\boldsymbol{k}_e, \boldsymbol{v}_e$$，decoder用$$\boldsymbol{k}_d, \boldsymbol{v}_d$$
+ 编码器使用了non-causal的线性注意力，而解码器使用标准causal线性注意力。

prefill阶段，并行地对长度为$$l$$的prompt进行如下计算，如果长度$$l< M$$，进行left-pad到$$M$$长度

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}\right)}{\phi\left(\boldsymbol{v} q_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top}\right)}
XXX

初始化如下：

XXX
\boldsymbol{s}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}\right),\ \boldsymbol{z}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top}\right)
XXX

对于decoding阶段，输出$$y_i, i>M$$，和base的linear transformer一样，不需要修改

训练loss是ntp和mlm的混合，假设序列长度是$$N$$，前$$M$$个token算MLM，后面的$$N-M$$个token算NTP：

XXX
\mathcal{L}=\frac{w_1 \mathcal{L}_{\mathrm{NTP}}+w_2 \mathcal{L}_{\mathrm{MLM}}}{w_1+w_2}
XXX

### 效率提升

虽然Linear attention比softmax attention要快，但其实不如精心优化的softmax attention（如flash attention），[Simple linear attention language models balance the recall-throughput tradeof](https://arxiv.org/pdf/2402.18668)实现了一个io-aware的kernel（[https://github.com/HazyResearch/based/](https://github.com/HazyResearch/based/)），在prefill阶段精细地partitioning & storing暛的matrix-valued recurrent state across warp-registers，PLA参考这个实现了自己的加速版。

## Falcon Mamba

[非Transformer架构站起来了！首个纯无注意力大模型，超越开源巨头Llama 3.1](https://mp.weixin.qq.com/s/ET9gghK4asEr5ObuW2padw)

[Welcome FalconMamba: The first strong attention-free 7B model](https://huggingface.co/blog/falconmamba)

[https://huggingface.co/tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b)

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py)

核心是```FalconMambaMixer```这个类

无需增加内存存储，就可以处理任意长度的序列，并且能够在单个 24GB A10 GPU 上运行。

训练数据有5500GT ，主要由RefinedWeb数据集组成，并添加了来自公共源的高质量技术数据、代码数据和数学数据。

采用**多阶段训练策略**进行训练，上下文长度从2048增加到了8192。此外，受到**课程学习**概念的启发，整个训练阶段精心选择了混合数据，充分考虑了数据的**多样性和复杂性**。在最后的训练阶段，使用了一小部分高质量精选数据（即来自 Fineweb-edu 的样本），以进一步提升性能。

大部分训练是在**256个H100 80GB GPU**上完成的，采用了 3D 并行（TP=1、PP=1、DP=256）与 ZeRO 相结合的策略。

用adamW+WSD（预热 - 稳定 - 衰减）学习率schedule，在前50 GT的训练过程中，batch大小从b_min=128增加到了b_max=2048

## LM-steer

ACL 2024

[LM-Steer: Word Embeddings Are Steers for Language Models](https://arxiv.org/pdf/2305.12798)

[https://github.com/Glaciohound/LM-Steer](https://github.com/Glaciohound/LM-Steer)

发现词向量空间上的线性变换空间等价于对语言模型生成样式的调节，并以此设计了名为 LM-Steers 的语言模型调控方法。我们发现词向量的这种调节作用普遍存在于各种尺寸的语言模型中。它只需要学习原始模型 0.2% 的参数就可以引导各种风格。在语言模型去毒化和生成情感控制等任务上，LM-Steers 可以实现与最先进的受控生成方法相当或更好的性能，同时保持更好的生成质量平衡。学习到的 LM-Steer 还可以充当文本风格的解读器：它可以解释各种文本样式与词向量哪些维度相关，并且可以用于寻找最具代表性的文本片段。 LM-Steer 可通过显式计算来在不同语言模型之间转移，而不需要额外训练。我们还可以简单地通过缩放 LM-Steer 来实现风格的连续控制，或者实现多种生成控制的组合。

## razerattention

减少kv-cache的大小

[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://www.arxiv.org/pdf/2407.15891)

受anthropic的induction head([In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html))启发:

+ short head：对长文没有任何响应
+ long head(induction head)：对长文能直接找到对应位置，并且对那附近的词有很强的信号

## MLP-Mixer

[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)

## sigmoid attention

[Sigmoid注意力一样强，苹果开始重新审视注意力机制](https://mp.weixin.qq.com/s/4DvgsqkyNcj6HBrAHTygCA)

[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/pdf/2409.04431)

[https://github.com/apple/ml-sigmoid-attention](https://github.com/apple/ml-sigmoid-attention)

证明了从理论上讲，与 softmax 注意力相比，具有sigmoid注意力的Transformer是**通用函数逼近器**，并且受益于**改进的正则化**。

## lstm+transformer

[LSTM+Transformer王炸创新，荣登Nature](https://mp.weixin.qq.com/s/OTwbZEy-v93-fzvh1Z4m2Q)

## AnyGraph

[港大黄超团队推出AnyGraph, 首次揭秘图大模型的Scaling Law](https://mp.weixin.qq.com/s/iqQi4ZP7FdpHnMxZByOyoQ)

[AnyGraph: Graph Foundation Model in the Wild](https://arxiv.org/pdf/2408.10700)

[https://github.com/HKUDS/AnyGraph](https://github.com/HKUDS/AnyGraph)


## 液态神经网络

[给机器人装上「虫脑」？非Transformer液态神经网络终于来了！MIT CSAIL负责人创业成果](https://mp.weixin.qq.com/s/oowid3yCpFNTALCvgdSwXg)

Liquid Foundation Models（LFM），1B、3B和40B LFM在各个规模上均能实现SOTA性能，同时保持更小的内存占用和更高效的推理。2020年就有了[Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439)

## 差分transformer

[这篇论文非常火！差分Transformer竟能消除注意力噪声，犹如降噪耳机](https://mp.weixin.qq.com/s/hG_S85HkyAkTFAI2iQjl6g)

[Differential Transformer](https://arxiv.org/pdf/2410.05258)

Transformer往往会过度关注不相关的上下文，即注意力噪声（attention noise），而差分Transformer则能放大对答案范围的注意力并消除噪音，从而增强上下文建模的能力。

## SparseLLM

[NeurIPS 2024｜SparseLLM：突破性全局剪枝技术，大语言模型稀疏化革命](https://mp.weixin.qq.com/s/mfdkUFXCsB50iqKgxv7hQQ)

[SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)

[https://github.com/BaiTheBest/SparseLLM](https://github.com/BaiTheBest/SparseLLM)

## minLSTM+minGRU

[图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?](https://mp.weixin.qq.com/s/ueid-TAw-9OjtKFKA5lqSw)

[Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201v1)

## MixCon

[北大林宙辰团队全新混合序列建模架构MixCon：性能远超Mamba](https://mp.weixin.qq.com/s/FfCYq1Bx6d7NARKvDPwz2Q)

[MixCon: A Hybrid Architecture for Efficient and Adaptive Sequence Modeling](https://zhouchenlin.github.io/Publications/2024-ECAI-MixCon.pdf)


# 训练框架

mfu(Model Flops Utilization)模型算力利用率是分布式训练效率的优化目标。

一个模型定义好之后，前向和反向的**计算量**就是固定（不考虑动态图的话）的，除以**每个step的latency**就是mfu。以nanoGPT中的代码为例：

```python
def estimate_mfu(self, fwdbwd_per_iter, dt):
    """ estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
    # first estimate the number of flops we do per iteration.
    # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
    N = self.get_num_params()
    cfg = self.config
    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
    flops_per_token = 6*N + 12*L*H*Q*T
    flops_per_fwdbwd = flops_per_token * T # 计算量（T是序列长度）
    flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter # 每个step的flops * 每一次更新梯度要多少个step
    # express our flops throughput as ratio of A100 bfloat16 peak flops
    flops_achieved = flops_per_iter * (1.0/dt) # per second 一轮的计算量/一轮的耗时
    flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
    mfu = flops_achieved / flops_promised
    return mfu

##...
def xxx():
    # timing and logging
    t1 = time.time()
    dt = t1 - t0 ## 一次gradient_accumulation_steps后 更新梯度
    t0 = t1
    if iter_num % log_interval == 0 and master_process:
        # get loss as float. note: this is a CPU-GPU sync point
        # scale up to undo the division above, approximating the true total loss
        # (exact would have been a sum)
        lossf = loss.item() * gradient_accumulation_steps
        if local_iter_num >= 5: # let the training loop settle a bit
            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu
        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%")
    iter_num += 1
    local_iter_num += 1

```

一般分布式训练参数量越多->卡数越多->通信占比越高->MFU越低，所以要优化通信效率。

## 优化设置

+ batchsize：通常用比较大的batchsize，提高训练**稳定性**和**吞吐量**。GPT-3和PaLM在训练时动态增加batshzie，最终达到百万级别，batchsize**从3.2w逐渐增加到320w个token**。
+ 优化器：
    + Adam和[AdamW](https://arxiv.org/pdf/1711.05101v2.pdf)：基于**一阶梯度优化的低阶矩自适应估计**，用于GPT-3等，超参$$\beta_1=0.9, \beta_2=0.95, \epsilon=10^{-8}$$。
    + [Adafactor](https://arxiv.org/pdf/1804.04235.pdf)：在训练过程中节省显存，用于PaLM、T5等，超参$$\beta_1=0.9, \beta_2=1.0-k^{-0.8}$$
+ 学习率：
    + 预热（warm-up）：在训练的**初始0.1%到0.5%**的steps中，用**线性预热策略**逐渐增加学习率到最大值（$$5 \times 10^{-5}$$到$$1 \times 10^{-4}$$之间，GPT-3是$$6 \times 10^{-5}$$）
    + 衰减（decay）：后续steps中**余弦衰减**，逐渐降低到**最大值的约10%**，直到收敛
+ 稳定训练：
    + 权重衰减和梯度裁剪：**权重衰减率**设为0.1，**梯度裁剪阈值**设为1.0
    + 梯度检查点：容易出现loss突增，PaLM和OPT从**发生突增前的一个ckpt重新开始训练**，并**跳过可能有问题的数据**
    + 缩减emb梯度：GLM发现emb的异常梯度通常会导致loss突增，故**缩减emb梯度**以缓解




## 混合精度训练

&nbsp;

### FP16

&nbsp;

[Mixed precision training](https://arxiv.org/pdf/1710.03740.pdf)提出了用16位float（FP16）训练，减少**内存使用和通信开销**。A100等GPU具有的**FP16计算单元**是**FP32的两倍**，故FP16的计算效率能进一步提高。

![mixed-precision-training](../assets/mixed-precision-training.png)

+ **推理（预测）**：所有参数都是fp16，相对fp32，存储变成一半，速度提升1倍。
+ **训练**：参数和梯度用**fp32存储**，但是在**计算前**会**转成fp16**，**计算后**再**转回fp32**。主要为了**防止溢出**，loss要乘一个scale，然后在fp16的梯度要除以scale。

以adam优化器为例，对于每1个参数来说，fp16的训练占用20bytes显存，包括（详见：[https://zhuanlan.zhihu.com/p/519264636](https://zhuanlan.zhihu.com/p/519264636)）

+ fp16的参数：2bytes
+ fp16的梯度：2bytes
+ 优化器状态（optimizer state）：16bytes
    + fp32参数（4bytes）
    + fp32梯度（4bytes）([zero论文](https://arxiv.org/pdf/1910.02054v2.pdf)里提到的，用于reduce之类操作时需要的fp32内存，以1.5B的gpt2为例，需要6GB内存，倒推回来，就需要6/1.5byte=4byte)
    + fp32 variance【历史梯度平方和】（4bytes）
    + fp32 momentum【历史梯度滑动平均】（4bytes）

而在预测时只要存一个fp16的参数(2bytes)就行，所以**预测的显存是训练的1/10**，对应1.3B参数量的gpt2-xl，训练要占用$$20B\times 1.3\times 10^9=26GB$$，预测只要2.6GB

### BF16

&nbsp;

FP16可能导致**计算精度的损失**从而影响模型性能，BLOOM里用**BF16**(brain floating point)比FP16**分配更多指数位**和**更少的有效位**，在准确性方面更好

[https://blog.csdn.net/orangerfun/article/details/133106913](https://blog.csdn.net/orangerfun/article/details/133106913)

![fp16-bf16](../assets/fp16-bf16.png)

bf16的指数位和fp32一样多

## 可扩展的训练

需要**提高训练吞吐量**和**加载更大模型到显存中**

### 3D并行

&nbsp;

如下三种并行（数据并行、流水线并行、张量并行）的组合

#### 数据并行（Data Parallelism）

&nbsp;

将**模型参数和优化器状态复制**到多个GPU上，每个GPU只处理分给它的数据，不同GPU算出的梯度进行**聚合**得到batch的梯度，再更新**所有GPU上的模型**。高度可扩展，增加GPU数就能提高训练吞吐。

torch的ddp

```python
from torch.nn.parallel import DistributedDataParallel as DDP

ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    # this process will do logging, checkpointing etc.
    master_process = ddp_rank == 0
    seed_offset = ddp_rank # each process gets a different seed

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])
```

可以一起搞的技巧——**梯度累积**，当显存不够跑较大的batchsize时，训练效果可能会很差，可以先跑**多个mini-batch的前向和反向**，把梯度累积起来，再**更新一次参数**，在数学上等价于**跑一个较大的batchsize**。

```python
# forward backward update, with optional gradient accumulation to simulate larger batch size
# and using the GradScaler if data type is float16
for micro_step in range(gradient_accumulation_steps):
    if ddp:
        # in DDP training we only need to sync gradients at the last micro step.
        # 最后一个micro step才要sync梯度
        model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
    with ctx:
        logits, loss = model(X, Y)
    loss.backward() # 只是计算梯度，并不真的更新
optimizer.step()
optimizer.zero_grad(set_to_none=True)
```

也可以用torch的no_sync()：

```python
ddp = torch.nn.parallel.DistributedDataParallel(model, pg)
with ddp.no_sync():
    for input in inputs:
        ddp(input).backward()  # no synchronization, accumulate grads
ddp(another_input).backward()  # synchronize grads
```

#### 流水线并行（Pipeline Parallelism）

&nbsp;

将LLM的**不同层**分配到多个GPU上，一般Transformer模型中会**将连续的层加载到同一GPU上**，以减少在GPU间传输已计算的隐层状态或梯度的成本。简单的实现会导致**GPU利用率降低**，因为每个GPU要**等前一个完成计算**，导致不必要的**气泡开销**，如下方法可以提高流水线效率：

+ GPipe：[Gpipe: Efficient training of giant neural networks using pipeline parallelism](https://arxiv.org/pdf/1811.06965.pdf)
+ PipeDream：[PipeDream: Fast and Efficient Pipeline Parallel DNN Training](https://arxiv.org/pdf/1806.03377.pdf)，填充多个数据batch+异步梯度更新？看下paper先。。。

##### 1) GPipe

&nbsp;

![gpipe](../assets/gpipe.png)

Gpipe主要思想：

+ 图a：把模型不同layers顺序放在4张卡上，0->3卡流水线前向计算loss，3->0再反向计算gradients
+ 图b：从时间顺序上看，**每张卡有3/4时间是空闲的**，GPU利用率非常低
+ 图c：配合**梯度累积**，多个mini-batch可以同时跑在流水线里面，每张卡则有3/(3+4)的时间空闲（Bubble）

流水线并行的问题是中间有**Bubble**。当卡数$$K$$，梯度累积次数$$M$$，则$$Bubble=(K-1)/(K-1+M)$$

GPT里用[Weight Tying](https://paperswithcode.com/method/weight-tying)提升效果，输入和输出共享vocab embedding

##### 2) 重计算

&nbsp;

即gradient checkpointing，重计算(recomputation)是对于pipeline parallelism非常重要的一个优化，最开始在[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/pdf/1604.06174.pdf)一文中提到，在**flash attention**中也用了。

因为要做pipeline+梯度累积，前向过程中的**激活值**要保存，以留给反向过程使用，保存很多份的激活值对显存造成了很大压力。recomputation(也叫**checkpointing**)用时间来换空间（反向的时候进行一次激活值的重计算），可以缓解显存问题。

pytorch的[实现](https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py)。大致逻辑是包了一个```autograd.Function```，前向时保存一些inputs/rng_state(RNG state是Random Number Generator state的缩写，**随机数生成器的状态**。在深度学习和其他计算任务中，随机数生成器用于初始化参数、决定正则化技术如dropout的行为，以及在训练过程中选择样本等。RNG状态是指随机数生成器当前的内部状态，它可以用来在需要时重现或恢复特定的随机数序列，确保实验或模型训练的可重复性)，反向时重新计算

深度更深的时候，一般效果更好

cpu offload：层数很深的时候，可能可以把一些计算挪到cpu上去，再搞回gpu

#### 张量并行（Tensor Parallelism）

&nbsp;

[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)

**分解LLM的张量（参数矩阵）**，例如矩阵乘法$$Y=X A$$，$$A$$可以按列分成两个子矩阵$$A_1$$和$$A_2$$，从而改为$$Y=\left[X A_1, X A_2\right]$$，将$$A_1$$和$$A_2$$**放到不同GPU上**，然后就可能通过跨GPU通信将两个GPU的结果merge。

+ Megatron-LM：能扩展到更高维度的张量
+ Colossal-AI：
    + 为更高维度的张量实现了张量并行，[An efficient 2d method for training super-large deep learning models](https://arxiv.org/pdf/2104.05343.pdf)、[Tesseract: Parallelize the tensor parallelism efficiently](https://arxiv.org/pdf/2105.14500.pdf)和[Maximizing Parallelism in Distributed Training for Huge Neural Networks](https://arxiv.org/pdf/2105.14450.pdf)
    + 特别针对序列数据提出**序列并行**([Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120.pdf))，详见下一节

参考[https://zhuanlan.zhihu.com/p/622036840](https://zhuanlan.zhihu.com/p/622036840)

![tensor parallelism](../assets/tensor-parallelism.png)

原始矩阵乘法是```[m,k], [k, n] -> [m, n]```，有如下两种矩阵分解的等效：

+ **列并行（column parallelism）**：第一个矩阵不变，第二个矩阵**竖着劈成两半**，即$$B=[B_1, B_2]$$
    + ```[m,k], [k, n/2] -> [m, n/2]```
    + ```concat([m, n/2], [m, n/2]) -> [m, n]```
+ **行并行（row parallelism）**：两个矩阵都横着劈成两半，即$$A=\left[\begin{array}{l}A_1 \\A_2\end{array}\right],B=\left[\begin{array}{l}B_1 \\B_2\end{array}\right]$$。从2推广到k，其实就是**split-k算法**，把两个矩阵都分成k个小块，两两相乘后，最后reduce_sum一下。因为每个线程计算的矩阵更小了，开销小，可以通过加大线程数来提升并行效率。
    + ```[m, k/2], [k/2, n] -> [m, n]```
    + ```elemwise_add([m, n], [m, n]) -> [m, n]```

**行并行**还可以扩展到**推荐**里，假设user有k/2维，item也是k/2维，concat在一起，然后过一个k*d的mlp，即```[1,k] * [k, d] -->[1,d]```，那么可以按行并行的方法，拆成2个```[1, k/2]```和```[k/2,d]```相乘，再相加。这样item侧的```[k/2,d]```可以把全库缓存过来，在线实时算user，排序时把对应item向量抽出来，和user加起来就行

![megatron-transformer](../assets/megatron-transformer.png)

megatron对transformer进行了如下优化：

+ MLP第一个nn按**列分割**，第二个nn按**行分割**，中间省了一次通信
+ Attention按照head来分割(类似**列分割**)，后面接的nn按**行分割**，中间也省了一次通信

图里面的通信算子

+ $$f$$是前向identity，反向all-reduce
+ $$g$$是前向all-reduce，反向identity

综合来看，一层transformer layer如下

![megatron-transformer-1layer](../assets/megatron-transformer-1layer.png)

具体的计算量可以参考[https://colossalai.org/docs/features/1D_tensor_parallel/#introduction](https://colossalai.org/docs/features/1D_tensor_parallel/#introduction)：

![clossal-ai-efficiency](../assets/clossal-ai-efficiency.png)

### ZeRO

&nbsp;

[ZeRO: Memory Optimization Towards Training A Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v2.pdf)

fp16那一节中，optimizer state的显存占用，在**前向**和**反向**的时候都**不用**，只有最后**optimizer step**的时候才用。

===>zero的思想：把optimizer state**分shard存在不同的卡上**，只在**最后gather时才用**。

ZeRO（Zero Redundancy Optimizer）在DeepSpeed库中提出，解决**数据并行**中的**内存冗余**问题。数据并行其实并不需要每个GPU都存整个模型、梯度和优化器参数，ZeRO在每个GPU仅保存部分数据，当需要其余数据时从其他GPU检索。3种解决方案：

+ 优化器状态分区：zero1，对显存最大开销的部分进行shard
+ 梯度分区：zero2
+ 参数分区：zero3

前两种方案不会增加通信开销，第三种方案增加约50%通信开销，但能节省和gpu数成比例的内存。

![zero](../assets/zero.png)


详见官方博客：[ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)

```python
import deepspeed
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--local_rank", type=int, default=0)
deepspeed.add_config_arguments(parser)
args = parser.parse_args()

model, optimizer, _, _ = deepspeed.initialize(args=args,
                                              model=model,
                                              model_parameters=model.parameters())
X, Y = get_batch('train')
logits, loss = model(X, Y)
model.backward(loss)
model.step()
```

需要指定deepspeed的配置：

```python
{
  "train_batch_size": 64,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 6e-4,
      "weight_decay": 1e-2,
      "betas": [0.9, 0.95]
    }zer
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
        "warmup_min_lr": 6e-5,
        "warmup_max_lr": 6e-4,
        "warmup_num_steps": 2000
    }
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 1
  }
}
```

启动：

```shell
deepspeed --num_gpus=8 train.py --deepspeed_config xx.json
```

facebook的开源库**FSDP(full sharded data parallel)**([Fairscale: A general purpose modular pytorch library for high performance and large scale training](https://github.com/facebookresearch/fairscale))里基于pytorch实现了类似ZeRO的技术。

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
model = FSDP(model)  #, sharding_strategy=ShardingStrategy.SHARD_GRAD_OP)
```

还有一些paper也能降低内存，如

[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)

[Training deep nets with sublinear memory cost](https://arxiv.org/pdf/1604.06174.pdf)

### 序列并行

&nbsp;

**序列并行**([Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120.pdf))，可以进一步分解Transformer的注意力操作。

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)这个也是

对比TP：

![tensor-parallel](../assets/tensor-parallel.png)

SP：

![sequence-parallel](../assets/sequence-parallel.png)

### 综合对比各种并行

&nbsp;

几个缩写：params（p）/gradients(g)/optimizer states(os)/activation(a)

| 并行方法 | 显存效率 | 计算效率 | 限制 | 
|---|---|---|---|
| DP（数据并行） | p/g/os都复制在每张卡上，显存效率很低| 计算和通信可以overlap，如果都在一个minipod内扩展性很好；梯度累积可以提高计算效率| batchsize不能太大，否则模型效果有损；batchsize/dp不能太小，不然打不满tensorcore|
| ZeRO（解决DP的显存冗余） |zero1/2/3把os/g/p分别shard到每张卡上，显存效率很高| 需要做prefetch来减少通信对计算效率的影响| 同DP |
| PP（流水线并行） | 切分p，提高显存效率；a需要存多次，降低显存效率| 通信次数最少，只发生在多层之间的切分点，但是有Bubble| 每个Stage之间需要负载均衡，对模型结构和卡数有限制|
| TP（张量并行） | p/g/os/a被shard在每张卡上，显存效率也很高；有些层如layernorm是复制的，可以用sequence parallel优化| 梯度不需要同步，提高计算效率；每层插入了4次通信，而且是跟计算有依赖的，会降低计算效率；每层的计算量进行了切分，也会降低计算效率| 一般是单机内8卡使用nvlink时用TP |

把神经网络看成是输入$$X$$和权重$$W$$的矩阵乘法$$XW$$，那么，**DP和PP其实是对$$X$$的拆分**，而**TP则是对$$W$$的拆分**

整体对比可以看

![megatron-results](../assets/megatron-results.png)

一般这么整合：

+ 把机器分成N组，**不同组之间用DP**
+ 一组机器有M台机器，**不同机器之间用PP**
+ 一台机器有K张卡，**不同卡之间用TP**

## 编译优化

pytorch的TorchDynamo

[https://pytorch.org/docs/stable/torch.compiler_deepdive.html](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)

最简单的用法```torch.compile()```

![TorchDynamo](../assets/TorchDynamo.png)

## flash attention

[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)

FlashAttention其实是对$$softmax(QK^T)V$$的一种加速实现。

一般的实现：需要先 用矩阵$$C$$存$$QK^T$$的结果，然后对$$C$$按行做softmax得到新的$$C$$，再用$$C$$乘以$$V$$得到最后结果。

FlashAttention通过一些特殊技巧，不需要算出$$C$$这个临时变量，通过分块计算，**让临时变量总是可以放在cache里**，从而

+ 减少Global Memory的大小
+ 加速attenttion的计算，因为读cache比访问Global Memory快多了。

## flexattention

[新PyTorch API：几行代码实现不同注意力变体，兼具FlashAttention性能和PyTorch灵活性](https://mp.weixin.qq.com/s/8uoZZf4hNSLQYLKFr95jTw?poc_token=HAynt2ajgl8xoDMpMWq0t8R-ubMokh17VRDwmacE)

## 训练稳定性

学习率+batchsize的一些经验：

[https://zhuanlan.zhihu.com/p/64864995](https://zhuanlan.zhihu.com/p/64864995)

另外，swiglu会让act_norm变大，得加一些方式让模型能训动，一种方法是把weight_decay调小：

+ loss会变高==>[https://poe.com/s/Cv6lODy94INz1ozQKJYJ](https://poe.com/s/Cv6lODy94INz1ozQKJYJ)，正则项变大，L+norm中的L相对影响变小了，所以会变大，即为了防止过拟合，但可能eval的时候会更好
+ act_norm会降低20-30%，因为正则项更重要了，会让权重更接近0

层数加深时的稳定性调优：[Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://arxiv.org/pdf/2403.09635)

## 数据/训练策略

### 综述

[A Survey on Data Selection for Language Models](https://arxiv.org/pdf/2402.16827)

### 高质量数据

[Llama架构比不上GPT2？神奇token提升10倍记忆？](https://mp.weixin.qq.com/s/TMkn6yMTUrrGhxCQnd7_2g)

[Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/pdf/2404.05405.pdf)

制造**人工合成数据**，通过控制数据中知识的数量和类型，来严格调控数据中的**知识比特数 (bits)**。使用不同大小和架构的 LLM 在人工合成数据上进行训练，并给出数学定理，来精确计算训练好的模型从数据中**学到了多少比特的知识**。有如下几个发现：

+ 如果**训练时间充足**，不论使用何种模型架构，模型的**存储效率**均可以达到**2bit/param**（即平均每个模型参数可以存储2比特的信息）。而且发现transformer中的知识**并非主要存储在MLP层**，因为即便移除所有MLP层，模型仍能达到 2bit/param 的存储效率。
+ 如果**训练时间不充足**，GPT2模型能比LlaMA/Mistral存储超过30%的知识，主要是因为**GatedMLP(MoE)**会导致**训练不稳定**，因此对同样的知识，需要**更长的训练时间**。
+ **压缩/量化**的影响：将训练好的模型从float32/16压缩到int8，对知识的存储**毫无影响**。LLM可以达到“**信息论极限**”的**1/4**——因为int8只有8比特，但平均每个参数可以存储2比特的知识。
+ **高质量数据**的影响：如果我们的预训练数据中，有1/8来自高质量知识库（如百度百科），7/8来自低质量数据（如common crawl或论坛对话，甚至是完全随机的垃圾数据），会发现：
    + 即使对高质量数据的训练时间保持一致，**低质量数据的存在本身**可能会让模型对**高质量知识的存储量下降20倍**，即便将高质量数据的训练时间延长 3倍，知识储量仍会降低3倍
    + 解法：只需给所有的预训练数据**加上自己的网站域名token即可**，模型的知识存储量可以立即回升10倍，模型**不需要任何先验知识**来识别哪些网站上的知识是金子，而可以在预训练过程中，自动发现高质量知识的网站，并**自动为这些高质量数据腾出存储空间**。

### DSIR

[Data Selection for Language Models via Importance Resampling](https://arxiv.org/pdf/2302.03169)

[https://github.com/p-lambda/dsir](https://github.com/p-lambda/dsir)

大致思路：

输入语料样本$$z_i$$

+ 学习语料的特征分布（target data是$$\hat{p}$$，raw data是$$\hat{q}$$）
+ 利用语料样本衡量样本间的重要性权重$$w_i=\frac{\hat{p}\left(z_i\right)}{\hat{q}\left(z_i\right)}$$
+ 根据权重进行无放回的采样，兼顾多样性和分布一致性

### DoReMi

[DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/pdf/2305.10429) NeurIPS2023的spotlight

[https://github.com/sangmichaelxie/doremi](https://github.com/sangmichaelxie/doremi)

![doremi](../assets/doremi.png)

+ 使用初始的reference domain weights来训练一个小的reference model $$p_{ref}$$，可以用简单的方式合并，例如用样本量加权
+ 用reference mnodel来指导一个小的proxy model的训练，proxy model使用group DRO（group distributionally robust optimization）来得到domain weights，即让最大的loss gap最小化
XXX
\min _\theta \max _{\alpha \in \Delta^k} L(\theta, \alpha):=\sum_{i=1}^k \alpha_i \cdot\left[\frac{1}{\sum_{x \in D_i}|x|} \sum_{x \in D_i} \ell_\theta(x)-\ell_{\mathrm{ref}}(x)\right]
XXX
  + domain weights: $$\alpha$$
  + proxy model参数：$$\theta$$
  + domain语料：$$D$$
  + $$\ell_\theta(x)=-\log p_\theta(x)$$
+ 使用tune完的domain weights来训练大模型
+ 可以重复这个过程，用新的domain weights重新训练ref_model，迭代下去


### dataset decomposition

[Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum](https://arxiv.org/pdf/2405.13226)

LLM的训练语料大都是相同长度的sequence，一般是多篇文档concat到一起，然后再切分成等长的sequence，有可能一个sequence里有不同的毫不相关的文档，这样算attention就不太适合了。方法

+ 将数据划分为多个桶，每个桶中的序列长度均为$$2^i$$，保证每个序列仅来自同一个文档
+ 训练时可以给定一个固定的batch_len，即直接从某个桶里采样，也可以用特定长度策略，从多个桶里采样，组合为特定长度

![dataset-decomposition](../assets/dataset-decomposition.png)

### RHO-1

[RHO-1: Not All Tokens Are What You Need](https://arxiv.org/pdf/2404.07965)

[https://github.com/microsoft/rho](https://github.com/microsoft/rho)

![rho-1](../assets/rho-1.png)

把语料中的无用token从loss里删了

![slm](../assets/slm.png)

+ 训练ref_model，挑选少量高质量语料，建模语料的整体loss分布情况
+ 拿ref_model在整个预训练语料上计算每个token的ppl
+ 训练LLM，只关注得分比较高的tokens

### infinit lr

[Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/pdf/2403.08763)

linear warmup and cosine decay schedule：

+ linear warmup阶段：前$$T_{warmup}$$步线性增加学习率，即直到时间步$$t_{\text {ann }}=T_{\text {warmup }}$$，学习率设置为$$\eta_t=\eta_{\max } \cdot \frac{t}{T_{\text {warmup }}}$$
+ annealing阶段：对于接下来的$$T_{ann}$$个时间步，改为cosine annealing方式，即对于时间步$$t_{e n d}=T_{a n n}+t_{a n n}$$：
XXX
\eta_t=\eta_{\min }+\frac{\left(\eta_{\max }-\eta_{\min }\right)}{2} \cdot\left(\cos \left(\pi \cdot \frac{t-t_{a n n}}{t_{\text {end }}-t_{\text {ann }}}\right)+1\right)
XXX

infinit lr decay：

+ linear warmup阶段：同上
+ cool down阶段：学习率逐渐decay到一个常量$$\eta_{\text {const }}$$
+ 常数阶段：学习率保持为这个常数，该阶段结束时的ckpt可以用于新数据集的继续pretrain
+ annealing阶段：逐渐减小到最小值

XXX
\eta_t=\left\{\begin{array}{lll}
\eta_{\text {max }} \cdot \frac{t}{T_{\text {warmup }}} & t \in\left[0, t_{c d}\right] & \text { (warm-up) } \\
f_{c d}(t) & t \in\left(t_{c d}, t_{\text {const }}\right] & \text { (cooldown) } \\
\eta_{\text {const }} & t \in\left(t_{\text {const }}, t_{\text {ann }}\right] & \text { (constant) } \\
\eta_{\text {const }} \cdot\left(\frac{\eta_{\min }}{\eta_{\text {const }}}\right)^{\frac{t-t_{\text {ann }}}{t_{\text {end }}-t_{\text {ann }}}} & t \in\left(t_{\text {ann }}, t_{\text {end }}\right] & \text { (annealing) }
\end{array}\right.
XXX


### JEST

[DeepMind新方法：训练时间减少13倍，算力降低90%](https://mp.weixin.qq.com/s/8rkE6Rp2yw31gw0XhFcZXg)

[Data curation via joint example selection further accelerates multimodal learning](https://arxiv.org/pdf/2406.17711)

现有的大规模预训练数据筛选方法速度慢、成本高，并且没有考虑到批次组成或训练过程中数据相关性的变化，这限制了多模态学习中的效率提升。因此，DeepMind团队研究了**联合选择数据批次**而非单个样本是否能够加速多模态学习。

+ 挑选好的数据批次比单独挑选数据点更为有效
+ 在线模型近似可用于更高效地过滤数据
+ 可以引导小型高质量数据集以利用更大的非精选数据集

JEST能够在仅使用10%的FLOP预算的情况下超越之前的最先进水平。


## 硬件的可能影响

[https://zhuanlan.zhihu.com/p/701623664?utm_psn=1784500156948938753](https://zhuanlan.zhihu.com/p/701623664?utm_psn=1784500156948938753)

大模型训练（如InternLM-7B）实践中，曾经遇到过在A100集群上表现正常的代码和数据，迁移到A800集群却出现了模型准确度下降和梯度范数爆炸的问题。经过调查，我们发现这与**A800和A100 GPU的NVLink带宽差异**有关。通过在两个集群上使用nanoGPT模型进行的对照实验，我们确认了精度差异的原因在于**NCCL的Ring all-reduce算法实现**。进一步实验表明，设置环境变量NCCL_ALGO=Tree或使用gloo作为backend可以解决精度对齐问题。最终，我们提出了一个解决方案：**在A800集群上设置NCCL_ALGO=Tree**，强制使用Tree算法进行all-reduce操作，从而避免了Ring算法带来的精度问题，使得A800集群的模型能够正常收敛，并且与A100集群的训练精度对齐。


## fastpersist

[DeepSpeed 最新力作：大模型CKPT速度提升116倍](https://mp.weixin.qq.com/s/Gc-rMSMqWzycpn2Ye8lnIQ)

[FastPersist: Accelerating Model Checkpointing in Deep Learning](https://arxiv.org/pdf/2406.13768)


## pathways

[Pathways: Asynchronous Distributed Dataflow for ML](https://arxiv.org/pdf/2203.12533.pdf)

下载了，[pdf](../assets/LLM/pathways.pdf)

这个回答分析得不错
[https://www.zhihu.com/question/524596983/answer/2420225275](https://www.zhihu.com/question/524596983/answer/2420225275)

Google的大规模稀疏模型设计

[DESIGNING EFFECTIVE SPARSE EXPERT MODELS](https://arxiv.org/pdf/2202.08906.pdf)

代码：[https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)

## megatron-lm

[https://zhuanlan.zhihu.com/p/646406772](https://zhuanlan.zhihu.com/p/646406772)

## deepspeed

[https://zhuanlan.zhihu.com/p/343570325](https://zhuanlan.zhihu.com/p/343570325)


## ray-llm

[https://github.com/ray-project/ray/releases/tag/ray-2.4.0](https://github.com/ray-project/ray/releases/tag/ray-2.4.0)


## Google的几大LLM加速工具

### maxdiffusion

[https://github.com/google/maxdiffusion](https://github.com/google/maxdiffusion)

### JetStream

[https://github.com/google/JetStream](https://github.com/google/JetStream)

### maxtext

[https://github.com/google/maxtext](https://github.com/google/maxtext)

## Fire-Flyer AI-HPC

[用60%成本干80%的事，DeepSeek分享沉淀多年的高性能深度学习架构](https://mp.weixin.qq.com/s/-OeGYiN15vzwv0INPfIU5w)

[Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning](https://arxiv.org/pdf/2408.14158)


# 推理框架

## 量化

[ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/pdf/2303.08302.pdf)和[Compression of generative pre- trained language models via quantization](https://arxiv.org/pdf/2203.10705.pdf)

+ int8量化：[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf)
+ int4量化：GLM中用了


## PPL.LLM

[高性能 LLM 推理框架的设计与实现](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

[https://github.com/openppl-public/ppl.llm.serving](https://github.com/openppl-public/ppl.llm.serving)


## vLLM

[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

[Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180.pdf)

## 并行推理方法

[Efficiently scaling transformer inference](https://arxiv.org/pdf/2211.05102.pdf)

[3万字详细解析清华大学最新综述工作：大模型高效推理综述](https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg)

[万字综述大模型高效推理：无问芯穹与清华、上交最新联合研究全面解析大模型推理优化](https://mp.weixin.qq.com/s/7LKfamTnCyFih6_grf9m3A)

[A Survey on Efficient Inference for Large Language Models](https://arxiv.org/pdf/2404.14294)

[LLM后端推理引擎性能大比拼](https://mp.weixin.qq.com/s/dPd84P_VdKog8v2IcHDOrQ)

## LayerSkip

[LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/pdf/2404.16710)

[https://github.com/facebookresearch/LayerSkip](https://github.com/facebookresearch/LayerSkip)

## speculative-decoding

[Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318)

## medusa

decoder的并行化： [https://zhuanlan.zhihu.com/p/368592551](https://zhuanlan.zhihu.com/p/368592551)

[https://sites.google.com/view/medusa-llm](https://sites.google.com/view/medusa-llm)

用了tree-attention

[https://github.com/FasterDecoding/Medusa](https://github.com/FasterDecoding/Medusa)

## CLLM

[3倍生成速度还降内存成本，超越Medusa2的高效解码框架终于来了](https://mp.weixin.qq.com/s/Aw_bjXIQFdOJvN22UvW9UA)

[CLLMs：Consistency Large Language Models](https://arxiv.org/pdf/2403.00835)

## fasterTransformer/TensorRTLLM

[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

[https://github.com/NVIDIA/TensorRT-LLM/](https://github.com/NVIDIA/TensorRT-LLM/)

remove padding的逻辑如下，把整个batch的数据变成一行数据，加上offset标注是哪一条样本的

![effective_transformer](../assets/effective_transformer.png)

直接有这么个脚本：

[https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/convert_checkpoint.py](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/convert_checkpoint.py)

[https://github.com/daiwk/TensorRT-LLM/blob/main/tensorrt_llm/models/recurrentgemma/model.py](https://github.com/daiwk/TensorRT-LLM/blob/main/tensorrt_llm/models/recurrentgemma/model.py)

关于plugin：

+ 自己加plugins：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton)
+ 已有plugins：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp/tensorrt_llm/plugins](https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp/tensorrt_llm/plugins)

一些参数设置：

[https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml](https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml)

```--multiple_profiles``` enables multiple TensorRT optimization profiles in the built engines, it will benefits the performance especially when GEMM plugin is disabled, because more optimization profiles help TensorRT have more chances to select better kernels. However, this feature will increase the engine build time.


## huggingface/text-generation-inference

[https://huggingface.co/docs/text-generation-inference/index](https://huggingface.co/docs/text-generation-inference/index)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/causal_lm.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/causal_lm.py)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/seq2seq_lm.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/seq2seq_lm.py)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/mamba.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/mamba.py)


## block transformer

[KAIST-AI | 提出Block Transformer架构，大幅提升推理速度和内存效率，20倍增益！](https://mp.weixin.qq.com/s/H9qETDRwR9Q_3fG-XkBeeQ)

[Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/pdf/2406.02657)

[https://github.com/itsnamgyu/block-transformer](https://github.com/itsnamgyu/block-transformer)

## MoonCake

[月之暗面kimi底层推理系统方案揭秘](https://mp.weixin.qq.com/s/4SBRZKAjqcS2MkvnFPey_g)

[Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving](https://github.com/kvcache-ai/Mooncake/blob/main/Mooncake-v1.pdf)

[https://github.com/kvcache-ai/Mooncake/tree/main](https://github.com/kvcache-ai/Mooncake/tree/main)


## MInference(microsoft)

[MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/pdf/2407.02490)

![minference](../assets/minference.png)

## Sarathi-Serve(microsoft)

[OSDI 2024系列-低延迟大模型推理服务1](https://mp.weixin.qq.com/s/lfIyLnR2l5KBeuvPzeLBnQ)

[Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/pdf/2403.02310)

## SGLang(uc berkerley)

[贾扬清点赞：3K star量的SGLang上新，加速Llama 405B推理秒杀vLLM、TensorRT-LLM](https://mp.weixin.qq.com/s/FYwguU3USf12Wb5HXaHH3A)

[吞吐量提升5倍，联合设计后端系统和前端语言的LLM接口来了](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650904838&idx=4&sn=81a9f09f54f1b89b98fdc3d6be11ce51&chksm=84e45d78b393d46e38fed5d7e1952c9f465488b6953b63206a077f9d886c1104028b06c987c4&scene=21#wechat_redirect)

[https://github.com/sgl-project/sglang/](https://github.com/sgl-project/sglang/)

[SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/pdf/2312.07104)

[三个程序员奋战三天重写推理堆栈，Grok-2 mini直接提速两倍，马斯克亲发贺电](https://mp.weixin.qq.com/s/prC4R1Jjhc7r6mMXv_ZNcw)

## LazyLLM

[苹果让大模型学会偷懒：更快吐出第一个token，准确度还保住了](https://mp.weixin.qq.com/s/mAbiJEKd2zzNmt1pmGfmnQ)

[LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](https://arxiv.org/pdf/2407.14057)

## 各框架对比

[最佳LLM推理引擎？TensorRT vs vLLM vs LMDeploy vs MLC-LLM](https://mp.weixin.qq.com/s/AnqaukudFukLYSi55w9r2Q)

# 算子加速

## Triton

[天下苦英伟达久矣！PyTorch官方免CUDA加速推理，Triton时代要来？](https://mp.weixin.qq.com/s/6gkPA-xc7GsltM1Ywui_XQ)

## Mirage

[告别CUDA无需Triton！Mirage零门槛生成PyTorch算子，人均GPU编程大师？](https://mp.weixin.qq.com/s/M3WFt17QErAt46VuqkFjFQ)


# 典型LLM简介

llm榜单：

[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

![llm-families](../assets/llm-families.png)


## GPT系列

### GPT3

&nbsp;

2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning。

### CODEX

&nbsp;

[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)

能够解析自然语言，并生成代码。CODEX是gpt3在github上收集的代码语料上进行finetune得到的，并且在微软的copilot中使用。

### WebGPT

&nbsp;

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[https://openai.com/blog/webgpt/](https://openai.com/blog/webgpt/)

为了回答开放性问题，使用基于文本的浏览器对gpt3进行finetune，包括如下3个步骤：

+ 学习使用人类示范（demonstration）数据来模仿人类的浏览行为
+ 学习一个reward函数来预测人类偏好
+ 用强化学习和拒绝采样来优化reward函数


注：[重要性采样和拒绝采样](https://zhuanlan.zhihu.com/p/664233442)

+ **重要性采样**的关键是**降低方差**：因为相同的样本量，用$$\pi(x)$$分布采样得到的结果方差较大(或者是$$\pi(x)$$不好采样)，而用$$p(x)$$采样的样本得到的结果方差较小，用来估计原分布$$\pi(x)$$
+ **拒绝采样**：引入易于采样的分布$$Q(x)$$，然后从中随机地筛掉某些样本(根据**接受概率**接受或者拒绝样本)，使得剩下的样本服从分布$$P(x)$$

拒绝采样的步骤：

+ 从辅助分布$$Q(x)$$中采样得到样本$$x_i$$
+ 计算接受概率$$A = P(x_i) / (M \times Q(x_i))$$，其中$$M$$是一个常数，满足$$P(x) \leq M \times Q(x)$$对于所有$$x$$成立
+ 以概率$$A$$接受样本$$x_i$$，即生成一个随机数$$u$$，如果$$u \leq A$$，则接受样本$$x_i$$；否则拒绝样本$$x_i$$。

重复上述步骤，直到获得足够数量的样本。


### InstructGPT

&nbsp;

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

sft+rm+rl，在最小性能降低的情况下，提升了生成结果的真实性，并降低了毒害性

### ChatGPT&GPT-4

&nbsp;

2022.11.30推出了ChatGPT，基于GPT3.5，即InstructGPT的兄弟

2023.3推出了GPT-4，多模态LLM，能输入图像和文本


## LLaMA系列

### LLaMA

&nbsp;

2023年2月发布，[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)，开源的LLaMA-13B比 GPT3 175B在很多任务上都更好

参考代码：
[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

[https://github.com/meta-llama/llama](https://github.com/meta-llama/llama)

之前的工作考虑的是在训练预算有限的前提下，如何提升模型性能（2022年deepmind的[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)的Chinchilla）,llama考虑在预测时的预算。例如chinchilla是一个10b的模型在200b的token上训练，但其实一个7b的模型当用了1T的token后，性能仍在提升。LLama-13b比gpt3在大多数benchmark上好，但size只有1/10，在一个GPU上就能跑。

llama只用公开数据训练，而Chinchilla、PaLM、GPT-3都有自己的未公开数据集。其他的OPT、GPT-NeoX、BLOOM、GLM虽然也只用公开数据集，但打不过PaLM-62B或者Chinchilla

#### 预训练数据

&nbsp;

+ English CommonCrawl(67%)：使用CCNet pipeline，去重、用fasttext把非英文的页面删了，用n-gram把低质内容删了。此外，还训了一个线性模型，对页面进行分类：作为维基百科的引用 vs 随机采样的页面，最后把不属于引用这个类别的页面删了
+ C4(15%)：与CCNet类似，主要区别在质量过滤是基于启发式的规则，如标点符号的存在，或者词数和句子数
+ github(4.5%)：使用Google BigQuery里的公开github数据集，只用Apache、BSD和MIT证书的。低质判断是启发式规则，如字母数字占比、行的长度等，用正则删掉head等样式，最终以文件粒度进行去重。
+ wikipedia(4.5%)：2022年6-8月的数据，包括20种语言
+ Gutenberg and Books3(4.5%)：两个书籍数据集，对有90%以上内容重复的书籍做去重。
+ Arxiv(2.5%)：拿原始的tex文件，删掉first section之前的东西，还有一些注释、宏
+ Stack Exchange(2%)：高质量的问答网站，按答案的分数排序

![llama_data](../assets/llama_data.png)

tokenizer：BPE，使用sentencepiece的实现。将所有numbers切成单个数字，回退到字节去处理未知的utf8字符（fallback to bytes to decompose unknown UTF-8 characters）

总共有1.4T的token，对大部分训练数据，每个token在训练时只用了一次，除了维基和book大概用了两次。

附：gpt4说：当我们说"一个token只训练一次"，我们其实是在说在一个epoch（一个完整遍历训练集的过程）中，我们只遍历一次完整的数据集。如果一个特定的token在数据集中出现多次，那么在一个epoch中，这个token就会被用来训练模型多次。

![llama](../assets/llama_params.png)


![一些大模型](../assets/LLM/WechatIMG322.jpg)


#### 网络结构

&nbsp;

+ SwiGLU激活函数(PaLM)：取代ReLU，[Glu variants improve trans- former](https://arxiv.org/abs/2002.05202)，把PaLM里的$$4d$$改了$$2/34d$$

说白了就是输入$$x$$，SwiGLU激活完是$$swish(w_1(x)) * w_3(x)$$，其中swish又叫silu，是$$f(x)=x \cdot sigmoid(x)$$

然后再过一个$$w_2$$，得到$$w_2(swish(w_1(x)) * w_3(x))$$就是最终的ffn输出

以下是transformers里的实现：[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act] ## 默认是silu，即swish

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) 
                for i in range(self.config.pretraining_tp)], dim=-1)

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

这个是llama官方代码的实现：[https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345](https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345)

```python
class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
        ffn_dim_multiplier: Optional[float],
    ):
        """
        Initialize the FeedForward module.

        Args:
            dim (int): Input dimension.
            hidden_dim (int): Hidden dimension of the feedforward layer.
            multiple_of (int): Value to ensure hidden dimension is 
                a multiple of this value.
            ffn_dim_multiplier (float, optional): 
                Custom multiplier for hidden dimension. Defaults to None.

        Attributes:
            w1 (ColumnParallelLinear): Linear transformation for the first layer.
            w2 (RowParallelLinear): Linear transformation for the second layer.
            w3 (ColumnParallelLinear): Linear transformation for the third layer.

        """
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if ffn_dim_multiplier is not None:
            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```


+ Rotary embeddings(GPTNeo)：删掉原来的绝对位置编码，加上rotary positional embedding(RoPE)，网络的每一层都加，参考[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)
+ pre-normalization(gpt3)：提升训练**稳定性**，对每个子层的输入做norm，而非输出。此外，使用的是RMSNorm函数([Root mean square layer normalization](https://arxiv.org/abs/1910.07467))取代标准的layer-norm
    + layernorm计算单个样本在单层中所有激活的均值和标准差，并使用这些统计数据来归一化该层的激活。
    + RMSnorm只计算激活的平方根均值（RMS），而不是标准差。这样做的一个好处是计算上更简单，因为它省去了计算均值的步骤，只关注激活的规模（scale）而非其准确的分布。$$\operatorname{RMSNorm}\left(x_i\right)=\frac{x_i}{\sqrt{\frac{1}{H} \sum_{j=1}^H x_j^2}+\epsilon}$$，其中$$H$$是该层的神经元个数，而且也不用求均值

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        Initialize the RMSNorm normalization layer.

        Args:
            dim (int): The dimension of the input tensor.
            eps (float, optional): A small value added to the denominator 
                for numerical stability. Default is 1e-6.

        Attributes:
            eps (float): A small value added to the denominator 
                for numerical stability.
            weight (nn.Parameter): Learnable scaling parameter.

        """
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        Apply the RMSNorm normalization to the input tensor.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The normalized tensor.

        """
        # rsqrt(x)= 1/ sqrt(x)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        Forward pass through the RMSNorm layer.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor after applying RMSNorm.

        """
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

优化器：AdamW，cosine学习率schedule，最终学习率是最大学习率的10%。0.1的weight decay和1.0的gradient cliping，使用2000steps的warmup

#### 训练加速

&nbsp;

+ 对causal multi-head attention加速：实现在[http://github.com/facebookresearch/xformers](http://github.com/facebookresearch/xformers)中，降低内存使用和运行时间，参考[self-attention does not need $$o(n^2)$$ memory](https://arxiv.org/pdf/2112.05682.pdf)，以及[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)。思想是
    + 不存储attention weights
    + 不计算被mask的key/query得分
+ 减少xxx：


### LLaMA2

&nbsp;

2023年7月，[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

+ 基于公开数据集预自监督地训练一个llama-2
+ llama-2-chat模型:
    + sft后得到初始版本
    + 使用RLHF迭代地更新（拒绝采样+ppo） 

![llama-2-chat](../assets/llama-2-chat.png)

[https://zhuanlan.zhihu.com/p/636784644](https://zhuanlan.zhihu.com/p/636784644)

使用了GQA（grouped query attention）(参考[Gqa: Training generalized multi-query transformer models from multi-head checkpoints](https://arxiv.org/pdf/2305.13245.pdf))，在注意力机制中对**K/V进行参数共享**的方案，可以在**推理**过程中**减小KV缓存**。

![gqa](../assets/gqa.png)


### LLaMA3

#### 原始LLaMa3

&nbsp;

2024年4月

[开源大模型Llama 3王者归来！最大底牌4000亿参数，性能直逼GPT-4](https://mp.weixin.qq.com/s/KCyL8WTzXutPQ_k0Vl9Vwg)

[Llama 3超大杯有何惊喜？Meta会一直开源吗？当初为何笃信元宇宙？扎克伯格新访谈回应一切](https://mp.weixin.qq.com/s/e2n4ttcT8raDU877t53GPQ)

[Llama 3细节公布！AI产品总监站台讲解：Llama系列超庞大生态系统](https://mp.weixin.qq.com/s/iDAlop_LNv9evZtfPMPyUg)

[OpenAI 前创始成员、特斯拉自动驾驶前负责人 Andrej Karpathy 发表 Meta Llama 3 笔记](https://mp.weixin.qq.com/s/701PSyi954QHz_mt6Ddn-Q)

[Karpathy称赞，从零实现LLaMa3项目爆火，半天1.5k star](https://mp.weixin.qq.com/s/1poG0tEjmym1456mmR66nQ)

[Karpathy点赞，这份报告教你如何用 LLaMa 3创建高质量网络数据集](https://mp.weixin.qq.com/s/luZGMG1RRUT4X_ckt8hsCQ)

[https://github.com/naklecha/llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)

三个版本：8B 和 70B 参数的模型，还有一个 405B 参数的密集模型（还在训练之中，但已经在逼近GPT-4的领域，例如84.8 MMLU vs. 86.5 4Turbo），8B版本基本上与Llama-2的最大版本一样强大。

Llama 3的主要亮点：

+ 训练语料：基于超过15T token训练，比Llama 2数据集(2T)的7倍还多：
    + **scaling law的新发现**：对于一个8B模型，Chinchilla的**计算最优点**将是训练约200B词汇(前面提到了10B模型，大概要205B的token来训练，以此类推)，所以这里**超出了75倍**，而且**还未收敛**。可见，我们经常使用的LLMs在训练上显著不足，可能是100-1000倍或更多，远未达到它们的收敛点。
+ tokenizer：词汇数量从Llama 2的32K增加到Llama 3的128K，增加了4倍，拥有更多的词汇可以在长度上**更有效地压缩序列**。
+ 上下文窗口：从Llama 2的4096和Llama 1的2048增加到了**8192**，相比GPT4的128k还差得很远
+ 训练效率：比 Llama 2 高 3 倍，做了很多工程优化；
+ 模型结构：在Llama 2中，只在更大的模型使用了**分组查询注意力（GQA）**，但llama3的所有模型都使用了，包括最小的8B模型。
+ 新能力范畴：Llama-2 只能使用非常特定的工具，而 Llama-3 能使用好得多的工具，无需人工编程就能让其使用谷歌执行搜索，类似的功能还有编程和运行代码等。

[https://github.com/meta-llama/](https://github.com/meta-llama/)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

#### 多模态llama3

&nbsp;

[多模态 Llama-3 它来了 ！！[全网首发微调教程]](https://mp.weixin.qq.com/s/IFVIkVvMqeAL0lZF9OvRSA)

[https://github.com/InternLM/XTuner](https://github.com/InternLM/XTuner)

#### 中文llama3

&nbsp;

[首批中文版Llama3模型来了，解释成语、答弱智吧问题](https://mp.weixin.qq.com/s/ny0gBOxf4-tJiwjgp3o9HQ)

[https://github.com/CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat)

[https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)


### LLama3.1

[最强模型Llama 3.1 405B正式发布，扎克伯格：开源引领新时代](https://mp.weixin.qq.com/s/QUWumWsTF_Qq77tdlyCHdg)

[微调大模型，AMD MI300X就够了！跟着这篇博客微调Llama 3.1 405B，效果媲美H100](https://mp.weixin.qq.com/s/eJwg4GwH--9IVFedum_BnA)

[The Llama 3 Herd of Models](https://github.com/daiwk/collections/blob/master/assets/LLM/llama3.1.pdf)

[Llama3 92页技术报告中文全文详解](https://mp.weixin.qq.com/s/9Mnd1ass0XYUGW_UqDF8bw)

还开放了一个生态系统：

+ model：[https://github.com/meta-llama/llama-models/tree/main/models/llama3_1](https://github.com/meta-llama/llama-models/tree/main/models/llama3_1)
+ tool-chain：[https://github.com/meta-llama/llama-toolchain](https://github.com/meta-llama/llama-toolchain)
+ agent-system：[https://github.com/meta-llama/llama-agentic-system](https://github.com/meta-llama/llama-agentic-system)


#### 整体架构

&nbsp;

![multimodal-llama3.1](../assets/multimodal-llama3.1.png)

有2个主要的stage：

+ language model pre-training：用多语言的语料进行next token prediction
    + 使用超过15万亿(15T)个token训练Llama 3.1 405B，context window是8k tokens
    + 又加上一个continued pretraining stage，将context windows增加到128k tokens
+ language model post-training：sft+dpo+新的能力（如工具使用，能提升代码和推理能力）+safety

(toread)

这里有一些总结：[关于post-training和一些思考](https://mp.weixin.qq.com/s/Bpd6_zq9kmTTeHxZ9WJJpw)

为了支持多模态，还加入了如下3个stage：

+ 多模态encoder pre-training：
    + image encoder：在大量的img-text pair上训练，让模型能理解图片+描述
    + speech encoder：自监督，mask掉部分的speech输入，通过离散token表示来重建被mask的部分
+ vision adapter training：
    + 训练一个adapter将pretrained的image encoder融入到pretrained language model里，adapter包括一系列的cross-attention层将image-encoder表示输入language model。在image-text pair上训练，对齐图文表示。训练时**更新image-encoder的参数**，但**不更新language model的参数。**
    + 基于这个image-encoder来训练一个video-adapter，用的是video-text数据，让模型能聚合多帧间的信息。视频侧的temporal aggregator是有一个perceiver的resampler
+ speech adapter training：将speech encodings转成token表示，直接输入finetuned language model。在sft阶段，**adaper和encoder的参数联合更新**，但**不更新language model的参数**，还集成进了一个text-to-speech的系统。


#### 预训练

&nbsp;

在超过16,000个H100（80G HBM3）上训练，训练平台是[Meta open compute project, grand teton ai platform](https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/)，基于[MAST: Global scheduling of ml training across geo-distributed datacenters at hyperscale](https://www.usenix.org/system/files/osdi24-choudhury.pdf)进行schedule

选择了decoder only transformer with minor adaptations，而**不是MOE**，以最大限度地**提高训练稳定性**。

+ 使用GQA with 8 key-value heads，提升推理速度，在decoding时降低k-v cache的大小
+ 使用attention mask，**让同一序列里的不同documents不计算self-attention**。在标准的pretraining中影响不大，但**对于超长序列的continued pre-training非常重要**
+ 128K tokens的vocab，100k的tiktoken tokenizer+28k的额外token，更好地支持非英语的语言。相比llama2，每个token能压缩的字符3.17变成3.94，**压缩率更高了**，即同样计算量能读更多文本，同时也能提升下游任务效果。
+ 把RoPE的base frequency超参**加大到500,000**，能更好地**支持更长contexts**，[Effective Long-Context Scaling of Foundation Models](https://aclanthology.org/2024.naacl-long.260.pdf)说这个值对32768长度的context很有效。


参数量：

|                          | 8B          | 70B         | 405B        |
|--------------------------|-------------|-------------|-------------|
| **Layers**               | 32          | 80          | 126         |
| **Model Dimension**      | 4,096       | 8192        | 16,384      |
| **FFN Dimension**        | 6,144       | 12,288      | 20,480      |
| **Attention Heads**      | 32          | 64          | 128         |
| **Key/Value Heads**      | 8           | 8           | 8           |
| **Peak Learning Rate**   | 3e-4    | 1.5e-4  | 8e-5    |
| **Activation Function**  | SwiGLU      | SwiGLU      | SwiGLU      |
| **Vocabulary Size**      | 128,000     | 128,000     | 128,000     |
| **Positional Embeddings**| RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) |


#### 后训练

&nbsp;

采用iterative post-training procedure，即预训练后进行多轮对齐，每轮都使用sft、RS（拒绝采样）、直接偏好优化(DPO, Direct Preference Optimization)，能够为**每轮**创建**最高质量的合成数据**，并提高每项能力（capability）的性能。

+ 使用**合成数据生成(synthetic data generation)**来产生绝大多数SFT示例，并**多次迭代**以在所有能力生成越来越高质量的合成数据。
+ 采用了多种数据处理技术来过滤这些合成数据，达到最高质量，并可以**跨能力**来**扩展微调数据量**。

#### 推理

&nbsp;

从bf16量化为fp8

### LLama-3.1-Minitron

[英伟达玩转剪枝、蒸馏：把Llama 3.1 8B参数减半，性能同尺寸更强](https://mp.weixin.qq.com/s/zxW9EagxGJX-rS5loNLKXw)

[Compact Language Models via Pruning and Knowledge Distillation](https://www.arxiv.org/pdf/2407.14679)

NVIDIA用TensorRT-LLM优化了Llama 3.1 8B和Llama-3.1-Minitron 4B模型。

### LLama-3.1-Nemotron

[英伟达开源最新大模型Nemotron 70B后，只有OpenAI o1一个对手了](https://mp.weixin.qq.com/s/ebJkBkGAn8QS-_xVK__MMw)

### Llama-3.2

[刚刚，Llama 3.2 来了！支持图像推理，还有可在手机上运行的版本](https://mp.weixin.qq.com/s/3JP9UgfXNMlI5jaYHyekYA)

Llama 3.2 

+ 最大的两个模型11B和90B：都支持图像推理，包括文档级的图表理解、图像描述和视觉定位任务，比如直接根据自然语言描述定位图像中的事物。
+ 轻量级的1B和3B：都是纯文本模型，但也具备多语言文本生成和工具调用能力。

[Sebastian Raschka最新博客：从头开始，用Llama 2构建Llama 3.2](https://mp.weixin.qq.com/s/RuTRkJPeEP1hqWevxn9h6Q)


### Alpaca

[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html?trk=cndc-detail)

Stanford的羊驼（Alpaca）模型，有70亿（7b）参数，**没有使用RLHF**，而是使用**监督学习**的方法，参考[Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)（代码[https://github.com/yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)）

数据集是通过查询基于GPT-3的text-davinci-003模型的结果，得到的52k的指令-输出对（instruction-output pairs）。

因此，Alpaca本质上使用的是一种弱监督（weakly supervised）或以知识蒸馏（knowledge-distillation-flavored）为主的微调，即“**用 LLM 来训练 LLM**”。

![Alpaca](../assets/alpaca.jpeg)

[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

### Vicuna

通过ShareGPT收集的用户对话数据，对llama进行finetune得到的13B模型。效果接近chatgpt的92%，而且训练消耗比较低，大概只要300美元。

### Guanaco

&nbsp;

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)也是对llama进行微调，使用了QLoRA，能够在一台48G的GPU上微调65B的模型。只需要在单台GPU上finetune 24小时就能达到99.3%的chatgpt的效果。

[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)

[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

QLoRA将一个固定的4 bit量化的预训练权重转成low Rank Adapters来更新梯度

### Koala

&nbsp;

[Koala: A dialogue model for academic research](https://bair.berkeley.edu/blog/2023/04/03/koala/)

使用用户与闭源大模型交互的用户输入和模型返回数据进行训练

### Mistral

&nbsp;

[Mistral 7b](https://arxiv.org/pdf/2310.06825.pdf)

7B参数比最好的13B模型（llama-2-13B）要更好，而且比llama-34B在reasoning、数学、代码生成都更好。

+ 使用**grouped-query attention**来做更快的infer
+ 使用**滑动窗口attention**来用更低的infer消耗来高效地处理**任意长度的序列**。

[https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/) 提出了moe的8x7b

[Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)

[Mixtral 8x7B(Mistral MoE) 模型解析](https://mp.weixin.qq.com/s/-5yp0KU6_vkpWmY9wROWbg)

[Mistral开源8X22B大模型，OpenAI更新GPT-4 Turbo视觉，都在欺负谷歌](https://mp.weixin.qq.com/s/hf4uq3yrHxTGhQuzl61Imw)

[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)

[Mistral AI两连发：7B数学推理专用、Mamba2架构代码大模型](https://mp.weixin.qq.com/s/fFB0A0vv_2Deb0rWd4tagw)

mathtral：[https://huggingface.co/mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)

codestral-mamba: [https://huggingface.co/mistralai/mamba-codestral-7B-v0.1](https://huggingface.co/mistralai/mamba-codestral-7B-v0.1)，基于mamba2，可以直接用trt-llm启动

[准狙击Llama 3.1？Mistral AI开源Large 2，123B媲美Llama 405B](https://mp.weixin.qq.com/s/6_d_e6DlQpyONdWxd8EZvA)

Mistral AI 基于此前Codestral 22B和Codestral Mamba的经验，在很大一部分代码上训练了 Mistral Large 2。其表现远远优于上一代的Mistral Large，并且与GPT-4o、Claude 3 Opus和Llama 3 405B等顶尖模型相当。

[https://huggingface.co/mistralai/Mistral-Large-Instruct-2407](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)

[​Mistral首个多模态模型Pixtral 12B来了！还是直接放出24GB磁力链接](https://mp.weixin.qq.com/s/c_fmpkuimmEtY6pD9f_bZQ)

[https://huggingface.co/mistral-community/pixtral-12b-240910](https://huggingface.co/mistral-community/pixtral-12b-240910)

### phi-3

[微软发布Phi-3，性能超Llama-3，可手机端运行](https://mp.weixin.qq.com/s/kb_gfaYkXiW_cR22K2bX9g)

[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)

## PaLM系列

### PaLM

&nbsp;

2022年4月提出了第一个PaLM：[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)，直到2023年3月还是private的。是一个540B(5400亿)参数的模型，在包含了780B的tokens的高质量数据集上预训练。使用Pathways的6144块TPU v4进行训练。

### U-PaLM

&nbsp;

[Transcending scaling laws with 0.1% extra compute](https://arxiv.org/pdf/2210.11399.pdf)提出了8B，62B和540B的U-PaLM模型，用**UL2R**来对PaLM进行继续训练，用的是UL2的**mixture-of-denoiser objective**([UL2: Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131.pdf))

### Flan-PaLM

&nbsp;

Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）是instrunction-finetuned版本的U-PaLM。使用了更多的任务、更大的模型，以及CoT数据。使用了473个数据集，146类的task，总共1836个task。

![flan-palm](../assets/flan-palm.png)

[https://huggingface.co/google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)


### PaLM-2

&nbsp;

[PaLM 2 Technical Report](https://arxiv.org/pdf/2305.10403.pdf)是一个更计算高效型的LLM，有更好的多语种和reasoning能力，在很多任务上都比PaLM好，并且在infer上比PaLM要更快更高效。

### Med-PaLM

&nbsp;

Nature的[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)提出了Med-PaLM，在PaLM上用parameter-efficient方法进行instruction prompt tuning，使用少量的典型范例（exemplars）让LLM对齐到新领域。

Med-PaLM2([Towards expert- level medical question answering with large language models](https://arxiv.org/pdf/2305.09617.pdf))通过med-domain的finetuning和ensemble refinement prompting，效果比Med-PaLM要好。


## 其他LLM

### FLAN

&nbsp;

[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)

通过instruction tuning，大模型能很好地提升在没见过任务上的zero-shot能力。对一个137B的预训练模型在60个NLP数据集上使用instruction template进行instruction tuning。

![flan](../assets/flan.png)

一些instruction template：

![flan-instruction-template](../assets/flan-instruction-template.png)

[https://github.com/google-research/flan](https://github.com/google-research/flan)

### Gopher

&nbsp;

[Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf)基于152个多样的任务，对比了从44M到208B（Gopher）的不同transformer，发现Gopher在大多数任务上均达到sota：

| Model         | Layers | Number Heads | Key/Value Size | d_model | Max LR     | Batch Size |
|---------------|--------|--------------|----------------|---------------|------------|------------|
| 44M           | 8      | 16           | 32             | 512               | $$6 \times 10^{-4}$$  | 0.25M      |
| 117M          | 12     | 12           | 64             | 768               | $$6 \times 10^{-4}$$  | 0.25M      |
| 417M          | 12     | 12           | 128            | 1,536             | $$2 \times 10^{-4}$$  | 0.25M      |
| 1.4B          | 24     | 16           | 128            | 2,048             | $$2 \times 10^{-4}$$  | 0.25M      |
| 7.1B          | 32     | 32           | 128            | 4,096             | $$1.2 \times 10^{-4}$$  | 2M        |
| Gopher 280B | 80     | 128          | 128            | 16,384            | $$4 \times 10^{-5}$$  | 3M -> 6M    |

### T0

&nbsp;

[Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207)设计了一个系统，将任意nlp任务映射成一个人类可读的prompt格式。训练了一个encoder-decoder的T0模型，输入文本，输出文本，在混合数据集上进行多任务学习。

### ERNIE 3.0

&nbsp;

[ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/pdf/2107.02137)结合自回归网络和autoencoding网络，让模型能够同时做语言理解和语言生成任务，可以支持zero-shot、few-shot和finetuning。有10B参数，在4TB的文本和图谱数据上训练。

![ernie3.0](../assets/ernie3.0.png)

### RETRO

&nbsp;

[参数量仅为4%，性能媲美GPT-3：开发者图解DeepMind的RETRO](https://baijiahao.baidu.com/s?id=1721015293574115195&wfr=spider&for=pc)

[http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/)

[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)

Retrieval Enhanced Transformer(Retro)基于与preceding(前述) tokens的相似度，从大的语料库里检索出文档段（document chunks）作为条件，增强了自回归模型的能力。通过少25%的参数，在Pile数据集上达到了与gpt3和Jurassic-1相当的效果

![retro](../assets/retro.png)

+ 左图：
  + 输入一个长度为12的序列，每4个token一个chunk，切成3个chunk
  + 每个chunk通过freezed的bert去语料库中拿2个相似neighbors出来，每个neighbor过bert得到向量
  + 邻居作为k和v，原来的3个chunk作为k，做attention(CCA, chunked cross attention)
+ 右图：CCA的结构
  + 保证了因果性，即chunk1的邻居只对chunk1的last token以及chunk2的所有token有影响


### GLaM

&nbsp;

[Glam: Efficient scaling of language models with mixture-of-experts](https://arxiv.org/pdf/2112.06905)提出了Generalist Language Model(GLaM)，用稀疏激活的MOE架构，同时能scale模型容量，也能相比dense更可观地降低训练成本。最大的模型有1.2T参数，是gpt3的7倍。但只需要GPT3的1/3的能量来训练，而且在infer时也只有一半的flops，且在29个NLP任务上都有更好的0/1/few shot效果。

![Glam](../assets/glam.png)

如图，2 in 64的结构，每个token只会取64个experts里的top 2相关的expert，对这两个的输出加权平均输入给后面的层


### LaMDA

&nbsp;

[Lamda: Language models for dialog applications](https://arxiv.org/pdf/2201.08239)，有137B的参数，在1.5T的公开对话数据和互联网文本上pretrain，用标注数据微调，以及让模型能够咨询（consult）外部知识源能够让模型在安全性和factual grounding上有很好的改进。

### OPT

&nbsp;

[OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068)提出了从125M到175B的预训练decoder-only模型

### Chinchilla

&nbsp;

### Galactica

&nbsp;

### CodeGen

&nbsp;

### AlexaTM

&nbsp;

### Sparrow

&nbsp;

### MoD

&nbsp;

### BLOOM

&nbsp;

### GLM

&nbsp;

ACL22 [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)

iclr23 [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)


### GLM-4

[GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级](https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg)

[https://github.com/THUDM/GLM-4](https://github.com/THUDM/GLM-4)

### Pythia

&nbsp;

### Orca

&nbsp;

### StarCoder

&nbsp;

### KOSMOS

&nbsp;

### Gemini

&nbsp;

#### Gemini 1.0

[Gemini: a family of highly capable multimodal models](https://arxiv.org/pdf/2312.11805.pdf)

#### Gemini 1.5

[谷歌Gemini 1.5深夜爆炸上线，史诗级多模态硬刚GPT-5！最强MoE首破100万极限上下文纪录](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652444347&idx=1&sn=51ae7e3e100e24fd49b0f75924e74695&chksm=f093b48285369da37b6148803e41fb272c51c013bc31ac1ba6b09672ff1efd38269af1192b53&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)

[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)

+ 更新的 Gemini 1.5 Pro，其大部分功能和基准都超过了 2 月份的版本
+ Gemini 1.5 Flash，一种更轻量级的变体，专为提高效率而设计，并且在性能方面的减益很小。

#### 更大的上下文窗口

此前的SOTA模型能处理**20万(200K)**的token，Gemini 1.5能稳定处理**100万(1M)**的token（极限为**1000万(10M)**的token），能够处理11小时的音频、1小时的视频、超过3w行的代码库、超过70w个单词


### gemma

[https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/)

[Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)

代码：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py)


#### 架构&训练方法

+ moe：
    + 


### codegemma

&nbsp;

+ 专门处理代码补全和代码生成任务的 7B 预训练变体
+ 用于代码聊天和指令跟随的 7B 指令调优变体
+ 在本地计算机上运行快速代码补全的 2B 预训练变体

[CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)

![codegemma](../assets/codegemma.png)

### RecurrentGemma

&nbsp;

[RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf)

在infer阶段，需要检索**KV cache**，并加载到内存中，而KV cache会随着序列长度线性增长。[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)通过**local attention**来降低cache大小，但模型效果会变差。recurrent gemma将输入序列**压缩到一个固定大小的state中**，从而不会降低效果，对长序列能降低内存占用并高效infer。

基于Griffin架构，将**门控线性递归**与**本地滑动窗口注意力**混合在一起，在生成长序列时实现快速推理，相比gemma：

+ 减少内存用量：内存要求越低，就越能在内存有限的设备（例如单个 GPU 或 CPU）上生成较长的样本。
+ 吞吐量较高：能够以明显较高的batch_size执行推理，这意味着每秒可以生成更多tokens，尤其是在生成长序列时。

相比原始griffin：

+ 对输入emb**乘以一个常数**（等于model width(下表中的2560)的平方根，如下代码所示）。输入和输出的emb是tied的，这个常数**没有乘到output上去**。gemma里也有一个类似的因子
+ 对**recurrent layers（RG-LRU）**在训练时并**没有进行weight decay(本质是L2正则化，参考[https://zhuanlan.zhihu.com/p/607909453](https://zhuanlan.zhihu.com/p/607909453))**，当bp到**开方操作**时，为了训练稳定会加最大值为1000的**梯度clip**

```python
def __init__(self, ...):
    #...
    self.register_buffer(
        "normalizer", torch.tensor(self.config.hidden_size**0.5, 
            dtype=torch.bfloat16), persistent=False
    )
def forward(self, ...):
    #...
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)

    hidden_states = inputs_embeds

    if use_cache and inputs_embeds.shape[1] != 1:  
        # TODO let's maybe only call in the `generate`?
        self._setup_cache(self.config, hidden_states.shape[0], 
        hidden_states.device, hidden_states.dtype)

    if cache_position is None:
        cache_position = torch.arange(hidden_states.shape[1], 
        device=hidden_states.device)
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    causal_mask = self._update_causal_mask(attention_mask, 
        inputs_embeds, cache_position)

    ## 这里就是那个因子
    hidden_states = hidden_states * self.normalizer.type(hidden_states.dtype)
```

具体参数：

| 变量 | 大小 |
|----------|-------------|
| 总参数量 | 2.7b |
| 非emb参数量 | 2.0b |
| emb参数量 | 0.7b |
| vocab size | 256k |
| model width | 2560 |
| rnn width | 2560 |
| MLP expansion factor | 3，即intermediate_size=2560*3=7680 |
| Depth | 26 |
| Attention heads | 10 |
| local attention window size | 2048 |

训练：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py)

在线推理的C++实现(cpu版本)

[https://github.com/google/gemma.cpp](https://github.com/google/gemma.cpp)

### gemma-2

&nbsp;

[单张A100全精度推理！谷歌明星开源模型Gemma 2上新9B/27B，挑战3140亿Grok-1](https://mp.weixin.qq.com/s/z3h1eExDgItDf38Xar6yPg)

[谷歌「诚意之作」，开源9B、27B版Gemma2，主打高效、经济！](https://mp.weixin.qq.com/s/0Iy3gOlWRLKRCMnrXQavaA)

[https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)

[https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/pdf/2408.00118)

用了RoPE和GeGLU，网络结构上：

+ **局部滑动窗口**和**全局注意力**：在每隔一层中交替使用局部滑动窗口注意力([Longformer: The long-document transformer]((https://arxiv.org/pdf/2004.05150)))和全局注意力（[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)），局部注意力层的滑动窗口大小设置为4096个token，而全局注意力层的跨度设置为8192个token。
+ **Logit软封顶**：根据**Gemini 1.5的方法**，研究团队在每个注意力层和最终层限制logit，使得logit的值保持在$$−soft\_cap,+soft\_cap$$之间。对于9B和27B模型，注意力对数封顶设置为50.0，最终对数封顶设置为30.0。截至本文发表时，注意力logit软封顶与常见的FlashAttention实现不兼容，因此他们已从使用FlashAttention的库中移除了此功能。对模型生成进行了有无注意力logit软封顶的消融实验，发现大多数预训练和后期评估中，生成质量几乎不受影响。本文中的所有评估均使用包含注意力logit软封顶的完整模型架构。然而，某些下游性能可能仍会受到此移除的轻微影响。

XXX
\text { logits } \leftarrow \text { soft\_cap } * \text { tanh(logits } / \text { soft\_cap) }
XXX

+ **RMSNorm**进行post-norm和pre-norm。为了稳定训练，用RMSNorm对每个变换子层、注意力层和前馈层的输入和输出进行归一化。 
+ **gqa**：27B和9B模型均使用GQA，num_groups = 2，基于消融实验表明在保持下游性能的同时提高了推理速度。

[https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py](https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py)


#### gemma-scope

&nbsp;

[Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[https://huggingface.co/google/gemma-scope](https://huggingface.co/google/gemma-scope)

### datagemma

[整合海量公共数据，谷歌开源AI统计学专家DataGemma](https://mp.weixin.qq.com/s/Fr8I9VwiyHcMnhrWMcgDeQ)

[Knowing When to Ask - Bridging Large Language Models and Data](https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf)

提出了一种名为**检索交错生成（Retrieval Interleaved Generation，RIG）**的新方法，可以可靠地将Data Commons中的公共统计数据整合到LLM的响应中。RIG是一种受工具启发的方法，可以将统计数据标记与适合从Data Commons检索的自然语言问题交错。

为了获得这种能力，他们利用Gemini 1.5的帮助生成了一个指令-响应数据集，并在此基础上对LLM进行了微调。RIG方法将事实准确性从5-7%提高到了约58%。

### Claude

&nbsp;

[全球最强大模型一夜易主，GPT-4时代终结！Claude 3提前狙击GPT-5，3秒读懂万字论文理解力接近人类](https://mp.weixin.qq.com/s/WqQWS-hiQ1i1Ve6IPH3djw)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)

+ 对齐：使用Constitutional AI([Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf))， explicitly specifying rules and principles based on sources like the UN Declaration of Human Rights.
+ 基于Collective Constitutional AI([Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input))，added an additional principle to Claude’s constitution to encourage respect for disability rights
+ 公开了部分RHLF数据：[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)，对应的论文是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[力压GPT-4o！新王Claude 3.5 Sonnet来了，直接免费可用](https://mp.weixin.qq.com/s/HnQ7D4iDVgWteZZdTJoadg)

### grok

&nbsp;

[马斯克开源Grok-1：3140亿参数迄今最大，权重架构全开放，磁力下载](https://mp.weixin.qq.com/s/hvt5zwoazDx26KOaKuTs_w)

[https://github.com/xai-org/grok-1](https://github.com/xai-org/grok-1)

[马斯克发布Grok 1.5！编码和数学能力大幅提升](https://mp.weixin.qq.com/s/QwM1uk61o1DMpe5EPq9giQ)

+ 上下文窗口提升16倍，达到128k
+ 不使用通用的Python语言+Pytorch框架，采用分布式训练架构，使用Rust、JAX+Kubernetes构建。
+ 提出了自定义训练协调器，可自动检测到有问题的节点，然后剔除。
+ 优化了checkpointing、数据加载和训练重启等流程，最大限度地减少故障停机时间。

[马斯克的首款多模态大模型来了，GPT-4V又被超越了一次](https://mp.weixin.qq.com/s/2GDjZS6ctayAF8e8eFb3CQ)

Grok-1.5V: [https://x.ai/blog/grok-1.5v](https://x.ai/blog/grok-1.5v)

grok-2：[Grok-2来了，能生图识图、性能比肩GPT-4o，马斯克：发展猛如火箭](https://mp.weixin.qq.com/s/nBaY2srcMSzvEoecOyh1Cg)

### Gecko

[谷歌DeepMind发布Gecko：专攻检索，与大7倍模型相抗衡](https://mp.weixin.qq.com/s/5e_Py_Xm0RsmP1YMcikpaQ)

[Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/pdf/2403.20327.pdf)

![gecko](../assets/gecko.png)

主要包括两阶段：

+ pre-finetuning：类似[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)，自监督
+ finetuning：2-step llm distillation，提出了一个FRet数据集（Few-shot Prompted Retrieval dataset）

#### Pre-finetuning

&nbsp;

2个数据集：

+ 大型社区qa数据集：[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)中的来自网上论坛和qa网站的问答pair对
+ 去各种网站上爬了title-body的pair对，因为[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533)发现这种自然出现的pair对对于pre-finetuning embedding模型很有用

pre-finetuning的目标是让模型看到大量的多样性语料，对于一个预训练的语言模型$$\mathcal{M}$$，长度为$$n$$的句子对应的contextual向量$$\mathbf{W} \in \mathbb{R}^{n \times d}$$，对于任务特征$$t$$，数据集$$\mathcal{D}_{\text {pre }}=\left\{\left(q_i, p_i\right)\right\}_{i=1}^N$$，得到的向量如下：

XXX
\begin{aligned}
\mathbf{q}_i & =\text { mean\_pool }_{|t|+\left|q_i\right|}\left[\mathcal{M}\left(t \oplus q_i\right) \in \mathbb{R}^{\left(|t|+\left|q_i\right|\right) \times d}\right] \in \mathbb{R}^d \\
\mathbf{p}_i & =\text { mean\_pool }_{\left|p_i\right|}\left[\mathcal{M}\left(p_i\right) \in \mathbb{R}^{\left|p_i\right| \times d}\right] \in \mathbb{R}^d
\end{aligned}
XXX

对于batchsize为$$B$$的样本来说，inbatch负例，$$\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\mathbf{x}^{\top} \mathbf{y}}{\|\mathbf{x}\| \cdot \cdot\|\mathbf{y}\|}$$，其loss如下：

XXX
\mathcal{L}_{\text {pre }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_i\right) / \tau}}{\sum_{j=1}^B e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_j\right) / \tau}}\right]
XXX

在这个阶段没有用hard负例，用了能适配设备的最大batchsize，这是[Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/pdf/2308.03281)和[Text embeddings by weakly-supervised contrastive pre-training](https://arxiv.org/pdf/2212.03533)里的经验。


#### FRet

![fret](../assets/fret.png)

##### LLM-based Diverse Query Generation 

&nbsp;

从web语料$$\text { C }$$中抽出一个段落$$p_{\text {seed }}$$，$$\mathbb{P}_{\mathrm{QG}}$$是一个固定的few-shot的prompt，让LLM生成任务描述和这个task的query

XXX
\operatorname{LLM}\left(\mathbb{P}_{\mathrm{QG}}, p_{\text {seed }}\right) \rightarrow(t, q)
XXX

生成的任务t例如：

+ question answering：```Given a query, find a passage that has the answer to the query```
+ fact checking：```Given a query, find a passage that allows you to check whether the query is true or not```

为了保证多样性：

+ 网页库本身就有很多的topic和很多的写作风格
+ 在prompt里加多样的任务描述，来让模型生成的query保证多样性。

##### LLM-based Positive and Negative Mining

&nbsp;

对于某个query $$q$$，之前的工作一般会直接把输入的段落$$p_{\text {seed }}$$当成正样本，但实践中发现$$p_{\text {seed }}$$一般比较长，而生成的query一般只关注其中一小部分，所以可能在整个语料库中有比$$p_{\text {seed }}$$更准确的答案。因此，通过如下方法构造了一个FRet数据集：

+ 先把$$p_{\text {seed }}$$当成正样本，in-batch负例训一个embedding模型。
+ 用这个模型从文档库中检索出top N的相似段落$$P=\left\{p^{(1)}, \ldots, p^{(N)}\right\}$$
+ 用生成query的LLM给这N个文档排序，有两种排序方法：
    + query likelihood：参考[Improving passage retrieval with zero-shot question generation](https://arxiv.org/pdf/2204.07496)，给定段落$$p$$，衡量query $$q$$的likelihood，$$\mathrm{QL}(q, p)=\operatorname{LLM}\left(q \mid p, \mathbb{P}_{\mathrm{QL}}\right)$$，其中的prompt参考[PaRaDe: Passage Ranking using Demonstrations with Large Language Models](https://arxiv.org/pdf/2310.14408)包括了判断query likelihood的指令，以及一些相关query+段落的few-shot。基本思想是：**如果q和p高度相关，那么从p生成q的概率应该很高**。
    + relevance classification：参考[Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels](https://arxiv.org/pdf/2310.14122)，给定query $$q$$和段落$$p$$后，衡量特定相关性label的log likelihood：$$\operatorname{RC}(q, p)=\operatorname{LLM}\left(\text { label } \mid q, p, \mathbb{P}_{\mathrm{RC}}\right)$$。基本思想是：**让LLM直接判断q和p的相关程度,并输出一个相关性标签**。
+ 通过标准的Reciprocal Rank Fusion（RRF，倒数融合，[Reciprocal rank fusion outperforms condorcet and individual rank learning methods](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)）方法得到rank函数，对上面两个方法的排序结果merge：$$R(q, p)=1 / r_{\mathrm{QL}}(q, p)+1 / r_{\mathrm{RC}}(q, p)$$...其实就是rankindex
+ 基于得分筛选样本：
    + 正样本：$$p^{+}=\underset{p \in P}{\arg \max } R(q, p)=p_1$$
    + hard负例：排名第$$N$$位的样本$$p_N$$，也可以从除了第1个外的N-1个里随机sample

#### Unified Fine-tuning Mixture

&nbsp;

除了FRet，还融合了多个公开数据集：

+ Natural Questions：[Natural Questions: A Benchmark for Question Answering Research](https://aclanthology.org/Q19-1026.pdf)
+ HotpotQA：[Hotpotqa: A dataset for diverse, explainable multi-hop question answering](https://arxiv.org/pdf/1809.09600)
+ FEVER：[Fever: a large-scale dataset for fact extraction and verification](https://arxiv.org/pdf/1803.05355)
+ MedMCQA：[Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering](https://arxiv.org/pdf/2203.14371)
+ SNLI：[A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326)
+ MNLI：[A broad-coverage challenge corpus for sentence understanding through inference](https://arxiv.org/pdf/1704.05426)
+ MIRACL：多语言的数据集[Miracl: A multilingual retrieval dataset covering 18 diverse languages](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering)
+ huggingface上的一些分类数据集

将这些数据集处理成一个统一的格式，发现不同的任务对prompt的格式敏感程度不同，，非对称任务（如BEIR）对格式更敏感，而对称任务的性能相对稳定。

+ 对称格式（Symmetric Formatting）：输入和目标使用相同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：task: {task} | query: {target}
+ 非对称格式（Asymmetric Formatting）：输入和目标使用不同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：title: {title} | text: {target}

参考[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://aclanthology.org/2023.findings-acl.71.pdf)，对于每个文本$$x$$，找到另一个同样label是$$y$$的样本当成正样本$$x^{+}$$，随机找一个label不是$$y$$的当作负样本$$x^{-}$$。

实践中，在一个batch中，同一个$$x^+$$可能出现overlap，会造成in-batch negatives中的false negative问题。参考[poe的回复](https://poe.com/s/XHJyRAAOc9ZAuQu5yz2p)，即同一个mini-batch中假设有(x1, x1+, x1-) 和 (x2, x2+, x2-)，可能出现x1+ 与 x2 或 x2+ 相同或非常相似的情况。那么：

+ 对于x1来说，x1+是正例。
+ 但是如果x1+ 与 x2 相同或非常相似，模型可能会错误地认为x1+应该与x2区分开来，因为x2是另一个样本的输入。
+ 或者如果x1+ 与 x2+ 相同或非常相似，模型可能会混淆应该如何处理这个重叠的样本。

因为在理想情况下，x1+应该只是x1的正例，而不应该被视为任何其他样本的负例。但由于重叠，模型可能错误地将x1+视为x2或其他样本的负例。解决方法，给每个三元组分配唯一的id，让模型专注于在给定x的情况下，区分x+和x-。

有$$M$$个数据集$$\left[\mathcal{D}^{(1)}, \ldots, \mathcal{D}^{(M)}\right]$$，每个数据集是$$\mathcal{D}^{(m)}=\left\{\left(t_i, q_i, p_i^{+}, p_i^{-}\right)\right\}_{i=1}^N$$，$$t$$是任务描述，给定一个batch_size=B的batch，同batch里的其他query可以看成[SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives](https://arxiv.org/pdf/2306.02516)中说的same-tower negative（对同模态的效果比较好，例如这边都是query）：

XXX
\mathcal{L}_{\text {main }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_i^{+}\right) / \tau}}{\sum_{j=1}^B\left(e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_j^{+}\right) / \tau}+\mathbb{1}_{[j \neq i]} e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{q}_j\right) / \tau}\right)+e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{P}_i^{-}\right) / \tau}}\right]
XXX

同时还参考[Matryoshka representation learning](https://arxiv.org/pdf/2205.13147)的俄罗斯套娃加了一个MRL的loss，让模型适配不同的dim，gecko是768和256的dim


### Octopus

[超越GPT-4，斯坦福团队手机可跑的大模型火了，一夜下载量超2k](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)

[Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744.pdf)

[https://huggingface.co/NexaAIDev/Octopus-v2](https://huggingface.co/NexaAIDev/Octopus-v2)

[参数量不到10亿的OctopusV3，如何媲美GPT-4V和GPT-4？](https://mp.weixin.qq.com/s/mUpX-nvo221WVii-gnjUmQ)

[Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/pdf/2404.11459.pdf)

[Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)

### Cohere Command R+

[开源模型打败GPT-4！LLM竞技场最新战报，Cohere Command R+上线](https://mp.weixin.qq.com/s/uUeGQFWel5NLfFRFFF8w9g)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit](https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit)

1040亿的参数量，相比于Grok-1（3140亿）还差了一些，但Command R+并非Grok那种MoE架构，所以这1040亿参数是实打实的完全用于推理，而Grok-1的活跃参数为860亿。相比commend R：

+ 高级检索增强生成（RAG）与引用以减少幻觉
+ 10种主要语言的多语言覆盖，支持全球业务运营
+ 工具的运用以自动化复杂的业务流程

[明确了：文本数据中加点代码，训练出的大模型更强、更通用](https://mp.weixin.qq.com/s/3Ks72bIGZrNNUukqaHLSQg)

[To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)


### CT-LLM：以中文为中心的LLM

[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/pdf/2404.04167.pdf)

当前，绝大多数LLM基本上都是以英文语料库训练得到的，然后经过SFT来匹配不同的语种。本文作者考虑以中文为基础的预训练模型是否可以激活对其它语言的能力。

作者从头开始训练中文大模型，在训练过程中「主要纳入中文文本数据」，最终作者得到了一个2B规模的中文Tiny LLM（CT-LLM）。结果表明，该模型在中文任务上表现出色，且通过SFT也能很好的支持英文。

### OpenELM

[苹果卷开源大模型，公开代码、权重、数据集、训练全过程，OpenELM亮相](https://mp.weixin.qq.com/s/uwDoKG2Q9-w37ogewBJTrQ)

[OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619.pdf)

[https://github.com/apple/corenet](https://github.com/apple/corenet)

[超强Siri即将到来！苹果10篇重磅AI研究全总结，iOS 18关键一瞥](https://mp.weixin.qq.com/s/zQu0BtDYG-XAAX87i4MGIA)

[苹果智能背后模型公布：3B模型优于Gemma-7B，服务器模型媲美GPT-3.5-Turbo](https://mp.weixin.qq.com/s/xkZrD_8zsmVfYYohH7ucUw)

### Arctic

[仅需Llama3 1/17的训练成本，Snowflake开源128x3B MoE模型](https://mp.weixin.qq.com/s/0mqx1xkyhOXDGpbu42d_5g)

[全球最大开源模型再刷爆纪录！4800亿参数MoE击败Llama 3、Mixtral](https://mp.weixin.qq.com/s/Wbs30QvvtWtYB6mp47Z8NA)

[https://huggingface.co/Snowflake/snowflake-arctic-instruct](https://huggingface.co/Snowflake/snowflake-arctic-instruct)

### Qwen

[Qwen1.5-110B：首个国产千亿参数开源大模型](https://mp.weixin.qq.com/s/Lb08elR2r23UN5kWnhF-BA)

4月26日，Qwen开源了其第一个千亿参数大模型Qwen1.5-110B，这应该也是国内第一个千亿规模的开源大模型。其包含1100亿参数，更重要的是这是一个Dense模型，而非MoE模型。从各项评测来看，Qwen1.5-110B足以与Llama3-70B相抗衡，部分指标也取得了更高的水平。

[阿里云Qwen2.5发布！再登开源大模型王座，Qwen-Max性能逼近GPT-4o](https://mp.weixin.qq.com/s/D2D9hga06bg2AMuGnizN5w)

所有 Qwen2.5 系列模型都在 18 万亿（18T）tokens 的数据上进行了预训练。在语言模型方面，Qwen2.5开源了7个尺寸：0.5B、1.5B、3B、7B、14B、32B、72B，每个都在同等参数赛道创造了业界最佳成绩。这些型号的设定充分考虑了下游场景的不同需求：

+ 3B：适配手机等端侧设备的黄金尺寸；
+ 32B：最受开发者期待的「性价比之王」，可在性能和功耗之间获得最佳平衡
+ Qwen2.5-32B的整体表现甚至超越了Qwen2-72B。

### DeepSeek-V2

[一块钱100万token，超强MoE模型开源，性能直逼GPT-4-Turbo](https://mp.weixin.qq.com/s/tAA8XUbU__9FgvEvXxsykw)

[https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)

[从MHA到MLA看Attention优化：谈谈DeepSeek拼多多级的推理价格](https://mp.weixin.qq.com/s/l2uUXGQ-8Rj_nI3JG5lZ_g)

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://spaces.ac.cn/archives/10091)

[](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

#### DeepSeek-Prover-V1.5

[DeepSeek开源数学大模型，高中、大学定理证明新SOTA](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

[DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152)

[https://github.com/deepseek-ai/DeepSeek-Prover-V1.5](https://github.com/deepseek-ai/DeepSeek-Prover-V1.5)



### FALCON 2

[时隔一年Falcon回归！110亿参数5.5万亿token，性能超越Llama 3](https://mp.weixin.qq.com/s/CZNVl_GmYm_aPidMJrRHUg)

### MiniCPM

#### MiniCPM-Llama3-V

[登顶Top2！MiniCPM-V 8B新版本：GPT-4V水准小钢炮，8G显存，4070轻松推理！](https://mp.weixin.qq.com/s/TQVHJlZDExD3nMPRsqa_5w)

[可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术](https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ)

[RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)

[https://github.com/RLHF-V/RLAIF-V](https://github.com/RLHF-V/RLAIF-V)

#### MiniCPM 3.0

[小模型杀疯了！仅4B参数性能超GPT-3.5！无限长文本性能超Kimi](https://mp.weixin.qq.com/s/qJM9OTDHS3pJB9ozFuRP1g)

[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/pdf/2404.06395)

### DCFormer

[彩云科技DCFormer大模型发布，效率是Transformer的两倍！](https://zhuanlan.zhihu.com/p/699582521)

[Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/pdf/2405.08553)

[https://github.com/Caiyun-AI/DCFormer](https://github.com/Caiyun-AI/DCFormer)


## 小模型

[权重、代码、数据集全开源，性能超越Mistral-7B，苹果小模型来了](https://mp.weixin.qq.com/s/M58y8F5WeOH6i5nNKhtHbw)

[DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/pdf/2406.11794)

[https://huggingface.co/apple/DCLM-7B](https://huggingface.co/apple/DCLM-7B)

[小模型卷起来了：Mistral联合英伟达开源12B小模型，128k上下文](https://mp.weixin.qq.com/s/7oSxdFyqJ7MUpbfuNB_n5Q)

Mistral NeMo 使用基于 Tiktoken 的新分词器 Tekken，该分词器经过 100 多种语言的训练，能比以前 Mistral 模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。在压缩源代码、中文、意大利文、法文、德文、西班牙文和俄文时，它的效率要高出约 30%。在压缩韩文和阿拉伯文时，它的效率是原来的 2 倍和 3 倍。事实证明，与 Llama 3 分词器相比，Tekken 在压缩所有语言中约 85% 的文本方面更胜一筹。

Mistral NeMO 经历了高级微调和对齐阶段。与 Mistral 7B 相比，它在遵循精确指令、推理、处理多轮对话和生成代码方面的能力大大提升。

+ 基础模型：[https://huggingface.co/mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)
+ 指令微调模型：[https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)

## 小结

[开源模型进展盘点：最新Mixtral、Llama 3、Phi-3、OpenELM到底有多好？](https://mp.weixin.qq.com/s/bgdDYkGHbPZMMSJPIutFSQ)

# 微调

+ 指令微调（instruct tuning）：增强/解锁LLM的能力，
+ 对齐微调（alignment tuning）：将LLM的行为与为类的价值观或偏好对齐。
+ 高效微调方法：用于模型快速适配

## 指令微调

+ 收集或构建指令格式(instruction-formatted)的实例
+ 使用这些示例进行**有监督微调**

详见综述[Is prompt all you need? no. A comprehensive and broader view of instruction learning](https://arxiv.org/pdf/2303.10475.pdf)

数据集：[https://huggingface.co/collections/davanstrien/top-10-instruction-tuning-datasets-650d91e11427d12e8542a21a](https://huggingface.co/collections/davanstrien/top-10-instruction-tuning-datasets-650d91e11427d12e8542a21a)

### 构建格式化实例

&nbsp;

指令格式的实例包括一个任务描述（即**指令**）、一对输入输出和少量示例（可选）


#### 格式化已有数据集

&nbsp;

+ **收集来自不同领域（文本摘要、文本分类、翻译等）的实例**来创建有监督的多任务训练数据集。用自然语言的任务描述来格式化这些数据集是很方便的。
+ 使用**人类撰写的任务描述**来增广带标的数据集，通过**解释任务目标**来指导LLM理解任务。
+ 众包平台（如PromptSource）有效地创建、共享和难不同数据集的任务描述
+ 通过指令微调特殊设计的任务描述，**反转**已有实例的输入-输出对，例如“请基于以下答案生成一个问题”，如
+ 利用**启发式任务模板**将大量**无标注的文本**转换为**带标注的实例**。如[Learning instructions with unlabeled data for zero-shot cross-task generalization](https://arxiv.org/pdf/2210.09175.pdf)

#### 格式化人类需求

&nbsp;

来自公共NLP数据集的训练实例虽然进行了格式化，但**任务描述缺乏多样性**或**与人类真实需求不匹配**，故InstructGPT采用真实用户提交给其API的查询作为任务描述。此外，为了丰富任务多样性，通常

+ 标注者为**真实生活中的任务**编写指令，如开放式生成、开放式问答、头脑风暴、聊天等
+ 另一组**标注人员**直接对这些指令进行**回答**
+ 将**指令（采集的用户查询）**和**期望输出（人工编写的答案）**pair对作为一个训练实例

还有一些**半自动化**的方法将**现有实例**输入到LLM中生成多样的任务描述和实例来构建实例，如
+ [Self-instruct: Aligning language model with self generated instructions](https://arxiv.org/pdf/2212.10560.pdf)，引用数好几百
+ [Unnatural instructions: Tuning language models with (almost) no human labor](https://aclanthology.org/2023.acl-long.806.pdf)，meta的论文
+ [Stanford alpaca: An instruction-following llama model](https://crfm.stanford.edu/2023/03/13/alpaca.html)


#### 构建实例的关键

&nbsp;

+ 增加指令：
    + **扩大任务数量**：可以极大提高LLM的泛化能力。但随着任务增加，模型性能最初是连续增长，但**任务数量达到一定水平时，性能基本不提升了**。[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)猜测，一定数量的代表性性任务就能够提供足够充足的知识了。
    + 增强**任务描述的多样性**：从如长度、结构、创造力等方面入手，如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)
    + **每个任务的实例数量**：通常**少量实例**就可以让模型有不错的泛化能力，当某些任务的实例数量进一步增加（至数百个）时可能会**过拟合**。如[Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks](https://arxiv.org/pdf/2204.07705.pdf)
+ 设计格式：
    + **任务描述**：LLM理解任务的**最关键部分**
    + **适当数量的示例**：能产生**实质性的改进**，也减轻对指令工程的敏感性。如[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)
    + 指令中的其他部分：如避免事项、原因、建议，**影响很小，甚至有负面影响**，如[Cross-task generalization via natural language crowd- sourcing instructions](https://arxiv.org/pdf/2104.08773.pdf)
    + 包含**推理数据集**的**CoT实例**：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)和[OPT-IML: scaling language model instruction meta learning through the lens of generalization](https://arxiv.org/pdf/2212.12017.pdf)提到同时用包含和不包含CoT的样本微调，能在各种下游任务取得好的效果，包括需要多级推理能力的任务（常识问答、算术推理）和不需要多级推理的任务（如情感分析和抽取式问答）。

### 指令微调策略

&nbsp;

相比预训练而言，指令微调有多个不同：

+ 训练目标函数：如seq2seq的loss
+ 优化参数设置：更小的batchsize和学习率
+ 平衡数据分布：平衡不同任务间的比例：
    + **实例比例混合策略**（[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)），把所有数据集合并，然后从混合数据集中**按比例采样**每种实例。
    + **提高高质量数据集的采样比例**能提升效果，如[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的FLAN和[Promptsource: An integrated development environ- ment and repository for natural language prompts](https://arxiv.org/pdf/2202.01279.pdf)的P3。
    + 设置**最大容量**：限制**数据集中能包含的最大实例数**，防止较大数据集挤占整个采样集合，通常设置为几千或几万，如[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)和[OPT-IML: scaling language model instruction meta learning through the lens of generalization](https://arxiv.org/pdf/2212.12017.pdf)。
+ 结合指令微调和预训练：
    + 在**指令微调时加入预训练数据**：，如OPT-IML， 可以看成是**对模型的正则化**。
    + **混合预训练数据（纯文本）和指令微调（指令格式）数据**，用多任务方式**从头训练**：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)和[Ext5: Towards extreme multi-task scaling for transfer learning](https://arxiv.org/pdf/2111.10952.pdf)。将指令格式数据集作为预训练语料库的一小部分来预训练，同时获得预训练和指令微调的优势，如GLM-130B和Galactica。

### 指令微调效果

#### 性能改进

&nbsp;

+ 不同规模的模型都能从指令微调中受益，**随着参数规模增加，性能也有提升**。[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)发现，**指令微调后的小模型**甚至能比**未经微调的大模型效果更好**
+ 指令微调在**不同模型架构**、**预训练目标**和**模型适配方法**上都有稳定改进效果，由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现
+ 指令微调是**提升现有LM（包括小型PLM）能力**的一个通用方法，同样由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现
+ LLM所需的**指令数据数量明显少于预训练数据**，故指令微调的**成本较低**。

#### 任务泛化性

&nbsp;

+ 赋予LLM**遵循人类指令执行特定任务的能力**（通常被视为一种涌现能力）：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现，指令微调鼓励LLM**理解**用于完成任务的**自然语言指令**，，即**在未见过的任务上也能执行**。
+ 使LLM具有更强的**解决现实世界任务**的能力：指令微调能帮助LLM**缓解一些弱点**（如**生成重复内容**或**补全输入但完不成成相应任务**），由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)和[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)发现。
+ 指令微调后的LLM能**泛化**到**其他语言**的相关任务上：[Crosslingual generalization through multitask finetuning](https://arxiv.org/pdf/2211.01786.pdf)提出的BLOOMZ-P3基于BLOOM在**纯英文**的P3任务集合上进行微调，在多语言的句子实例任务中，相比BLOOM有超过50%的性能提升，同时仅用英文指令就能产生不错效果，**减少针对特定语言的指令工程的工作量**。


## 对齐微调

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Alignment of language agents](https://arxiv.org/pdf/2103.14659.pdf)提出，LLM可能**编造虚假信息**、产生**有害**的、**误导性**的和**有偏见**的表达，因为LLM在预训练时没有考虑人类的价值观或偏好。

[Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)和[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)提出了人类对齐，使LLM的行为能够符合人类期望。

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)、[A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)和[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)发现，和适配微调（如指令微调）相比，对齐微调要考虑的标准并不同，这可能会在某种程度上**损害LLM的通用能力**，即**对齐税**。

### 对齐的标准

+ **有用性**：以**简洁**且**高效**的方式帮助用户解决任务或回答问题。需要进一步阐明问题时，应该有通过**提出恰当的问题**来**获取额外信息**的能力，并有合适的**敏感**度、**洞察**力和**审慎**度（from [A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)）。
+ **诚实性**：又称为**正确性**，提供准确内容，传达**适当的不确定性**很重要，**避免任何形式的欺骗或信息误传**。LLM了解其能力和知识水平（**知道自己不知道什么**）。[A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)）认为，与有用性和无害性相比，诚实性是一个**更客观**的标准，故诚实性对齐**依赖的人力可能更少**。
+ **无害性**：生成的语言不得是冒犯性或者歧视性的，能**检测**到**隐蔽的出于恶意目的的请求**。当**被诱导去执行危险行为**（如犯罪）时，应该**礼貌拒绝**。[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出，某个行为**是否有害**及**有害程度**因**个人**和**社会**而异。

对齐的标准**很主观**，难以直接作为LLM的优化目标。比较有前景的方法是[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)和[Red teaming language models with language models](https://arxiv.org/pdf/2202.03286.pdf)提出的**红队攻防**，用**对抗**的方式**手动**或**自动**地**探测LLM**，使其**生成有害输出**，再**更新模型防止此类输出**。

### 收集人类反馈

#### 选择标注人员

&nbsp;

+ **教育水平要求高**：Sparrow要求本科学历的英国人，[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)中的高优任务有一半是美国硕士
+ **意图一致性筛选**：InstructGPT通过**标注人员和研究人员**意图一致性来选择标人员。研究者先自己标少量数据，然后衡量自己和标注人员间标的一致性，选择一致性最高的标注人员来进行后续标注。
+ **选择优秀标注者**：[Teaching language models to support answers with verified quotes](https://arxiv.org/pdf/2203.11147.pdf)中，研究人员评估标注人员的表现，选出如高一致性之类的一组优秀标注人员继续合作，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)发现，在标注时提供**详细的标注指令**和**实时的指导**是有帮助的。

#### 收集反馈

&nbsp;

+ 基于排序的方法：
    + **只选最佳候选**：[Fine-tuning language models from human preferences](https://arxiv.org/pdf/1909.08593.pdf)和[Recursively summarizing books with human feedback](https://arxiv.org/pdf/2109.10862.pdf)在这种早期工作中，标注人员用比较粗略的方式评估模型生成的结果，如只选择最佳候选。一方面不同人意见不同，另一方面这种方法忽略了没被选中的样本。
    + **elo评分系统**：[Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)和[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出了elo评分系统，两两比较所有候选输出结果，生成一个偏好排序。
+ 基于问题的方法：回答**研究人员设计的特定问题**，这些问题覆盖**不同的对齐标准**以及其他**对LLM的约束**条件。例如WebGPT中，标注人员要回答关于检索到的文档**对回答给定输入是否有帮助**的选择题。
+ 基于规则的方法：
    + Sparrow不仅选择**标注人员挑选的最佳回复**，还设计**一系列规则**来**测试模型生成的回复**是否符合**有用**、**正确**、**无害**的标准，让**标注者**对模型生成的回复**违反规则的程度进行打分**。
    + GPT-4用一组基于GPT-4的**zero-shot分类器**作为**基于规则的奖励模型**，**自动**确定模型生成的**输出是否违反一组人类编写的规则**。

### RLHF

详见RLHF章节

## 高效微调

全量参数都微调成本很大，有更高效的方法，称为**参数高效微调**（**parameter-efficient fine-tuning**）。

### 适配器微调（adapter tuning）

&nbsp;

[Parameter-efficient transfer learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)提出，在Transformer中引入一个**小型神经网络模块**（**适配器**），[LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)也提出了瓶颈架构：

+ 将原始特征压缩到**较小维度**（然后进行非线性变换）
+ 恢复到**原始维度**

一般是**串行插入**的方式，集成到**每个Transformer层里**，分别放到**注意力层**和**前馈层之后**。[Towards a unified view of parameter- efficient transfer learning](https://arxiv.org/pdf/2110.04366.pdf)提出了**并行适配器**，即与**注意力层和前馈层并行**。

微调时，**原参数不变**，**仅更新适配器模块参数**。


### 前缀微调（prefix tuning）

&nbsp;

[Prefix-tuning: Optimizing continuous prompts for generation](https://arxiv.org/pdf/2101.00190.pdf)。

+ 在每个**Transformer层前**添加一系列前缀，即一组**可训练的连续向量**。前缀向量具有**任务的特异性**，可以看作**虚拟的token emb**。
+ **重参数化**技巧：
    + 学习一个将**较小矩阵映射到前缀参数矩阵**的**MLP函数**，而不是直接优化前缀，有助于**稳定训练**。
    + 优化后，**舍弃映射函数**，只保留派生的前缀向量以增强与特定任务相关的性能。
    + 由于只训练前缀参数，故能实现参数高效的模型优化

[P-tuning v2: Prompt tuning can be comparable to fine- tuning universally across scales and tasks](https://arxiv.org/pdf/2110.07602.pdf)提出了p-tuning v2，为了自然语言理解在Transformer中引入**逐层提示向量**，还利用**多任务学习**来**联合优化共享的提示**。


### 提示微调（prompt tuning）

&nbsp;

在**输入层**加入**可训练**的**提示向量**，基于离散提示方法（[How can we know what language models know?](https://arxiv.org/pdf/1911.12543.pdf)和[Autoprompt: Eliciting knowledge from lan- guage models with automatically generated prompts](https://arxiv.org/pdf/2010.15980.pdf)），通过包含一组**软提示token**来扩充输入文本，再用扩充后的输入来解决特定的下游任务。将**任务特定的提示emb**与**输入文本的emb**相结合，输入模型中。

+ [GPT understands, too](https://arxiv.org/pdf/2103.10385.pdf)：提出了P-tuning，用**自由形式**来组合**上下文**、**提示**和**目标token**，用**双向LSTM**学习**软提示token的表示**，适用于自然语言理解和生成的架构。
+ [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/pdf/2104.08691.pdf)：提示微调，直接在**输入前**加入**前缀提示**。训练时**只有提示emb**会根据特定任务进行监督学习。这种方法在**输入层**只包含**少量可训练参数**，故其效果**高度依赖底层语言模型的能力**。

### 低秩适配（LoRA）

&nbsp;

[Lora: Low-rank adaptation of large language models](https://arxiv.org/pdf/2106.09685.pdf)通过增加低秩约束来近似每层的更新矩阵，假设参数矩阵$$\mathbf{W} \in \mathbb{R}^{m \times n}$$，一般是

XXX
\mathbf{W}=\mathbf{W}+\Delta \mathbf{W}
XXX

冻结$$\mathbf{W}$$，通过低秩分解矩阵来近似更新

XXX
\Delta \mathbf{W}=\mathbf{A} \cdot \mathbf{B}^{\top}
XXX

其中$$\mathbf{A} \in \mathbb{R}^{m \times k}$$和$$\mathbf{B} \in \mathbb{R}^{n \times k}$$是用于任务适配的可训练参数，$$r \ll \min (m, n)$$是**降低后的秩**。

LoRA的优点：

+ 大大**节省内存和存储**（如VRAM，Video Random Access Memory）
+ 可以只**保留一个大型模型副本**，同时**保留多个**用于**适配不同下游任务**的**特定低秩分解矩阵**。

用更有原则的方法设置秩：

+ 基于**重要性分数**的分配：[Adaptive budget allocation for parameter-efficient fine-tuning](https://arxiv.org/pdf/2303.10512.pdf)提出的AdaLoRA
+ **无需搜索**的**最优秩选择**：[Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low- rank adaptation](https://arxiv.org/pdf/2210.07558.pdf)

### 小结

LoRA已经有广泛的应用，如LLaMA和BLOOM，

+ Alpaca-LoRA：[Instruct-tune llama on consumer hardware](https://github.com/tloen/alpaca-lora)，通过LoRA训练的Alpaca的轻量级微调版本。
+ LLaMA-Adapter：[Llama-adapter: Efficient fine-tuning of language models with zero-init attention](https://arxiv.org/pdf/2303.16199.pdf)将**可学习的提示向量**插入每个Transformer层中，提出**零初始化的注意力**，通过**减轻欠拟合提示向量的影响**以改善训练，还能扩展到**多模态设置**，如视觉问答。

[LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)比较了串行适配器微调、并行适配器微调和LoRA，在GPT-J(6B)、BLOOM(7.1B)和LLaMA(7B)上评估：这些方法在**困难任务上效果不如GPT-3.5**，但**在简单任务上表现相当**，**LoRA**表现相对较好且使用的可训练参数明显较少。

huggingface开源了[Peft: State-of-the-art parameter-efficient fine-tuning methods](https://github.com/huggingface/peft)，包括LoRA/AdaLoRA、前缀微调、P-Tuning、提示微调，支持GPT-2和LLaMA，还支持视觉Transformer如ViT和Swin Transformer。

[让大模型不再「巨无霸」，这是一份最新的大模型参数高效微调综述](https://mp.weixin.qq.com/s/b16EPZ3z-LpGapGy2Q7ZUg)

[Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608.pdf)

## lora变种

### DoRA

[DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/pdf/2402.09353.pdf)

[https://github.com/catid/dora](https://github.com/catid/dora)

LoRA可以认为是对Finetune微调的一种低秩近似，通过增加Rank，LoRA可以达到类似Finetune的微调效果。因此之前多数研究都把LoRA和Finetune在微调准确性上的差异归结为二者的优化参数量不同。

但经过分析发现，lora的学习模式和FT很不一样，更偏向于大开大合，即**方向**和**幅度**呈很**强的正相关**，可能对**更精细的学习有害**

![dora](../assets/dora.png)

dora通过同时关注权重更新时的**大小**和**方向变化**，实现了比LoRA**更加接近finetune**微调效果：

+ w拆成magnitude($$norm$$)乘以direction($$1/norm \times w$$)
+ magnitude不变，direction里的$$1/norm$$用lora更新

注意，这里的norm是column-wise的norm，即输入$$d\times k$$的矩阵，**每一列**的元素算一个norm（平方和开根号）得到一个数，最终就是$$1\times k$$的矩阵

```python
# This layer is dropped into your pre-trained PyTorch model where nn.Linear is used
class DoRALayer(nn.Module):
    def __init__(self, d_in, d_out, rank=4, weight=None, bias=None):
        super().__init__()

        if weight is not None:
            self.weight = nn.Parameter(weight, requires_grad=False)
        else:
            self.weight = nn.Parameter(torch.Tensor(d_out, d_in), requires_grad=False)

        if bias is not None:
            self.bias = nn.Parameter(bias, requires_grad=False)
        else:
            self.bias = nn.Parameter(torch.Tensor(d_out), requires_grad=False)

        # m = Magnitude column-wise across output dimension
        self.m = nn.Parameter(self.weight.norm(p=2, dim=0, keepdim=True))
        
        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
        self.lora_A = nn.Parameter(torch.randn(d_out, rank)*std_dev)
        self.lora_B = nn.Parameter(torch.zeros(rank, d_in))

    def forward(self, x):
        lora = torch.matmul(self.lora_A, self.lora_B)
        adapted = self.weight + lora
        column_norm = adapted.norm(p=2, dim=0, keepdim=True)
        norm_adapted = adapted / column_norm
        calc_weights = self.m * norm_adapted
        return F.linear(x, calc_weights, self.bias)

## 使用
def replace_linear_with_dora(model):
    for name, module in model.named_children():
        if isinstance(module, nn.Linear):
            # Get the input and output dimensions of the current nn.Linear layer
            d_in = module.in_features
            d_out = module.out_features

            # Create a new DoRALayer with the same dimensions
            setattr(model, name, DoRALayer(d_out=d_out, d_in=d_in, weight=module.weight.data.clone(), bias=module.bias.data.clone()))
        else:
            # Recursively apply this function to submodules
            replace_linear_with_dora(module)

```

### fourierft

[ICML 2024 | 脱离LoRA架构，训练参数大幅减少，新型傅立叶微调来了](https://mp.weixin.qq.com/s/jaYeIfByJaWU5-4jBmnrzQ)

[](https://arxiv.org/abs/2405.03003)

[https://github.com/Chaos96/fourierft](https://github.com/Chaos96/fourierft)

## SFT技巧

[全是细节｜大模型SFT的100个关键点](https://mp.weixin.qq.com/s/LxERqJU7mP40onJQP-6UHQ)

[LLM预训练与SFT数据配比调研](https://mp.weixin.qq.com/s/-J-5oHB4T4taQd0vCHhYUA)

# 使用

## 上下文学习

GPT-3提出ICL，将**任务描述**和（或）**示范（demonstration）**以**自然语言文本形式**表达。

### 上下文学习形式

+ 以**任务描述**作为开始，从任务数据集中**选择一些样例**作为**示范**。
+ 以特别设计的**模板形式**将它们按照**特定的顺序**组合成**自然语言提示**。
+ 将**测试样例**添加到LLM的输入中以生成输出。

形式化地看，$$D_k=\left\{f\left(x_1, y_1\right), \ldots, f\left(x_k, y_k\right)\right\}$$表示由$$k$$个样例组成的一组示范，$$f\left(x_k, y_k\right)$$表示把第$$k$$个**任务样例转换为自然语言提示**的函数。给定任务描述$$I$$、示范$$D_k$$和新的输入查询$$x_{k+1}$$，LLM生成的输出$$\hat{y}_{k+1}$$如下：

XXX
\operatorname{LLM}(I, \underbrace{f\left(x_1, y_1\right), \ldots, f\left(x_k, y_k\right)}_{\text {示范 }}, f(\underbrace{x_{k+1}}_{\text {输入 }}, \underbrace{\_\_\_}_{\text {答案 }})) \rightarrow \hat{y}_{k+1} \text {. }
XXX

真实答案$$y_{k+1}$$留白，由LLM预测。

更多的可以参考综述[A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)

**指令微调**可以**提高LLM执行目标任务的ICL能力**，尤其是**零样本场景**（仅使用任务描述）。


### 示范设计

&nbsp;

#### 示范选择

&nbsp;

+ 启发式方法：
    + 基于**knn的检索器**来选择与**查询**语义相关的样例：如[What makes good in-context examples for gpt-3?](https://arxiv.org/pdf/2101.06804.pdf)和[Does GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation](https://aclanthology.org/2022.coling-1.56.pdf)。但只是**针对每个样例单独选择**，而**不是对整个样例集合**进行评估。
    + 基于**多样性**的选择策略：[Diverse demonstrations improve in-context compositional generalization](https://arxiv.org/pdf/2212.06800.pdf)和[Selective annotation makes language mod- els better few-shot learners](https://arxiv.org/pdf/2209.01975.pdf)
    + 同时考虑**相关性**和**多样性**的选择策略：[Complementary Explanations for Effective In-Context Learning](https://arxiv.org/pdf/2211.13892.pdf)
+ 基于LLM的方法：
    + **直接用LLM**来**选择**：[Finding supporting examples for in-context learning](https://arxiv.org/pdf/2302.13539.pdf)：LLM可以直接根据**添加样例后**的**性能提升**评估**每个样例的信息量**，以进行选择。
    + **两阶段检索**：[Learning to retrieve prompts for in-context learning](https://arxiv.org/pdf/2112.08633.pdf)：提出EPR，先用无监督方法召回相似样例，再用密集检索器（用LLM标记的正负样例训练）进行排序。
    + **RL方法**：[Active example selection for in-context learning](https://arxiv.org/pdf/2211.04486.pdf)，将示范选择任务建模为RL问题，**LLM是奖励函数**，为训练策略模型提供反馈。
    + 用**LLM**来**生成**示范：[Chatgpt outperforms crowd-workers for text-annotation tasks](https://arxiv.org/pdf/2303.15056.pdf)发现LLM在文本标方面表现很好，故可以直接将LLM作为**无人工干预**的**示范生成器**，如[Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator](https://arxiv.org/pdf/2206.08082.pdf)和[Selective in-context data augmentation for intent detection using pointwise v-information](https://arxiv.org/pdf/2302.05096.pdf)

[An explanation of in-context learning as implicit bayesian inference](https://arxiv.org/pdf/2111.02080.pdf)提到，ICL中选择的示范样例应该包含**足够的有关待解决任务的信息**，并**与测试查询相关**。

#### 示范格式

&nbsp;

将选择的示范进行**整合**以及**格式化**：

+ 用相应的**输入输出对**来**实例化预定义的模板**：[Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)
+ 增强LLM的**推理能力**
    + **添加任务描述**：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf) 
    + 通过**CoT提示**：[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)
+ 收集**包含人工编写的任务描述**的**大规模数据集**：[Cross-task generalization via natural language crowd- sourcing instructions](https://arxiv.org/pdf/2104.08773.pdf)，能够提升**已见任务**的性能，也能在一定程度泛化到**未见任务**。
+ **半自动化**方法：[Self-instruct: Aligning language model with self generated instructions](https://arxiv.org/pdf/2212.10560.pdf)使用由**人工编写的任务描述**组成的**种子集合**来指导LLM**为新任务生成任务描述**。
+ **自动生成**高质量的示范格式：
    + **Auto-CoT**：[Automatic chain of thought prompting in large language models](https://arxiv.org/pdf/2210.03493.pdf)使用零样本提示（**let's think step by step**）以生成中间推理步骤
    + **least-to-most提示**：[Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/pdf/2205.10625.pdf)先询问LLM来**执行问题分解**，再利用LLM**根据已解决的中间答案**依次**解决子问题**。


#### 示范顺序

&nbsp;

LLM有时会被**顺序偏差**影响，例如[Calibrate before use: Improving few-shot performance of language models](https://arxiv.org/pdf/2102.09690.pdf)提出LLM会倾向于**重复示范结尾附近的答案**===>**结尾很重要！！**

+ **启发式**方法：[What makes good in-context examples for gpt-3?](https://arxiv.org/pdf/2101.06804.pdf)根据在emb空间中**示范**与**查询**的**相似度**来排列，相似度越高，**距离结尾越近**。
+ **基于信息论**的方法：
    + [Self-adaptive in-context learning](https://arxiv.org/pdf/2212.10375.pdf)使用**最小化压缩和传输任务标签所需的码长**来整合更多任务信息，需要**额外的标记数据**作为用来**评估**特定示范**顺序性能**的**验证集**。
    + [Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity](https://arxiv.org/pdf/2104.08786.pdf)使用**全局**和**局部熵度量**来为不同的示范顺序打分，且为了消除对额外标注数据的需要，这篇文章**从LLM本身采样**来获取验证集。


### 底层机制

#### 预训练如何影响ICL

&nbsp;

+ ICL与**预训练任务设计**：GPT-3发现ICL能力随模型增大而增强，但[Metaicl: Learning to learn in context](https://arxiv.org/pdf/2110.15943.pdf)发现**小规模的PLM**也能通过**特别设计的训练任务**从而表现出强大的ICL能力（例如输入是**任务实例+查询**，**预测标签**），甚至能超越规模更大的模型。
+ ICL与**预训练语料**：
    + [On the effect of pretraining corpora on in-context learning by a large-scale language model](https://arxiv.org/pdf/2204.13509.pdf)发现ICL的性能主要取决于**预训练语料的来源**而非规模
    + [Data Distributional Properties Drive Emergent In-Context Learning in Transformers](https://arxiv.org/pdf/2205.05055.pdf)分析训练数据分布的影响，发现当训练数据可以**被聚类成多个不常见的类**，而**不是均匀分布**时，模型会有ICL能力
    + [An explanation of in-context learning as implicit bayesian inference](https://arxiv.org/pdf/2111.02080.pdf)从理论上解释，认为ICL是在具备**长程连贯性的文档**上进行预训练的产物。



#### LLM如何实现ICL

&nbsp;

+ 将ICL视为**隐式微调**：[Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers](https://arxiv.org/pdf/2212.10559.pdf)和[Transformers learn in-context by gradient descent](https://arxiv.org/pdf/2212.07677.pdf)
    + ICL可以看成是通过**前向计算**，LLM生成**关于示范**的**元梯度**，并通过**注意力**机制**隐式**地**梯度下降**。
    + LLM的**某些注意力头**能执行**与ICL能力密切相关**的**任务无关**的**原子操作**（如**复制**、**前缀匹配**等）
+ 将ICL视为**算法学习过程**：[Transformers as algorithms: Generalization and implicit model selection in in-context learning](https://arxiv.org/pdf/2301.07067.pdf)、[What learning algorithm is in-context learning? investigations with linear models](https://arxiv.org/pdf/2211.15661.pdf)，基于这个解释框架，LLM能通过ICL有效地学习简单的线性函数，甚至是如决策树的复杂函数
    + 预训练阶段：LLM本质上通过其参数**对隐式模型进行编码**
    + 前向计算阶段：通过ICL中提供的示例，LLM可以**实现如sgd的学习算法**，或者**直接计算出闭式解**以更新这些模型



## 思维链提示（CoT）

CoT是一种改进的提示策略，旨在提高LLM在**复杂推理任务**中的性能，如算术推理（[Training verifiers to solve math word problems]()、[Are NLP models really able to solve simple math word problems?](https://arxiv.org/pdf/2110.14168.pdf)和[A diverse corpus for evaluating and developing english math word problem solvers](https://arxiv.org/pdf/2106.15772.pdf)）、常识推理（[Commonsenseqa: A question answering challenge targeting commonsense knowledge](https://arxiv.org/pdf/1811.00937.pdf)和[Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies](https://arxiv.org/pdf/2101.02235.pdf)）、符号推理（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)）。

ICL**只使用输入输出对**来构造提示，而CoT将最终输出的**中间推理步骤**加入提示。

### 使用CoT的ICL

&nbsp;

一般在小样本和零样本这两种设置下和ICL一起用

#### 小样本思维链

&nbsp;

将每个示范```<输入，输出>```替换为```<输入，CoT，输出>```。小样本CoT可以看成ICL的一种特殊提示，但相比ICL的标准提示，**示范的顺序**对性能**影响相对较小**。

+ **思维链提示设计**：
    + 使用**多样的CoT推理路径**：[Making Large Language Models Better Reasoners with Step-Aware Verifier](https://arxiv.org/pdf/2206.02336.pdf)，对**每个问题**给出**多个推理路径**。
    + 使用具有**复杂推理路径**的提示：[Complexity-based prompting for multi-step reasoning](https://arxiv.org/pdf/2210.00720.pdf)
    + Auto-CoT：上述方法都需要标注CoT，[Automatic chain of thought prompting in large language models](https://arxiv.org/pdf/2210.03493.pdf)利用[Large language models are zero-shot reasoners](https://arxiv.org/pdf/2205.11916.pdf)提出的zero-shot-CoT
        + 通过**特别提示**LLM来生成CoT推理路径（例如“**Let’s think step by step**”）
        + 将训练集里的问题**分成不同簇**，选择**最接近每个簇质心的问题**，就可以代表整个训练集里的问题。

+ **增强的思维链策略**：如何生成多个推理路径，并在得到的答案中寻找一致性
    + **self-consistency**：[Self-consistency improves chain of thought reasoning in language models](https://arxiv.org/pdf/2203.11171.pdf)，在生成CoT和最终答案时新的**解码策略**。先**用LLM生成多个推理路径**，再对所有答案进行**集成**(例如投票)。
    + **更通用的集成框架**：[Rationale-Augmented Ensembles in Language Models](https://arxiv.org/pdf/2207.00747.pdf)发现多样化的推理路径是COT推理性能提高的关键，因此将self-consistency延伸至**提示的集成**。
    + 通过**训练打分模型**来**衡量生成的推理路径的可靠性**，如[On the advance of making language models better reasoners](https://arxiv.org/pdf/2206.02336.pdf)
    + **持续**地**利用LLM自己生成的推理路径**进行训练，如[Star: Self-taught reasoner bootstrapping reasoning with reasoning](https://arxiv.org/pdf/2203.14465.pdf)和[Large language models can self-improve](https://arxiv.org/pdf/2210.11610.pdf)


#### 零样本思维链

&nbsp;

不在提示中加入人工标注的示范，而是直接生成推理步骤，再利用生成的CoT来得出答案。[Large language models are zero-shot reasoners](https://arxiv.org/pdf/2205.11916.pdf)。

+ 先通过“**Let’s think step by step**”来提示LLM生成步骤
+ 再通过“**Therefore, the answer is**”来提示得到最终答案

这种方法在**模型规模超过一定大小**时可以**显著提高性能**，但在**小规模的模型**中**效果不佳**，即涌现能力。

Flan-T5和Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）进一步地使用CoT进行指令调整，有效增强了在**未见任务**上的零样本性能。

### 进一步讨论CoT

+ 思维链何时适用于LLM：
+ LLM为何能进行思维链推理：
    + 思维链能力的来源：
    + 提示中组成部分的影响：

# 能力评测

## 基础评测

### 语言生成

#### 语言建模

&nbsp;

#### 条件文本生成

&nbsp;

#### 代码合成

&nbsp;

#### 存在问题

+ 可控生成
+ 专业化生成

### 知识利用

#### 闭卷问答

&nbsp;

#### 开卷问答

&nbsp;

#### 知识补全

&nbsp;

#### 存在问题

+ 幻觉（Hallucination）
+ 知识实时性

### 复杂推理

#### 知识推理

&nbsp;

#### 符号推理

&nbsp;

#### 数学推理

&nbsp;

#### 存在问题

+ 不一致性
+ 数值计算

## 高级评估

### 与人类对齐

### 与外部环境互动

### 工具使用

## 公开基准

[LLM常用基准测试科普](https://zhuanlan.zhihu.com/p/694391642)

+ MMLU：
+ BIG-bench：
+ HELM：

## 比较有用的数据集

### 中文glue

[ChineseGLUE：为中文NLP模型定制的自然语言理解基准](https://mp.weixin.qq.com/s/14XQqFcLG1wMyB2tMABsCA)

[超30亿中文数据首发！首个专为中文NLP打造的GLUE基准发布](https://mp.weixin.qq.com/s/9yxYErAMy9o3BOEDzsgvPw)

[https://github.com/CLUEbenchmark/CLUE](https://github.com/CLUEbenchmark/CLUE)

[https://www.cluebenchmarks.com/](https://www.cluebenchmarks.com/)

[https://github.com/brightmart/nlp_chinese_corpus](https://github.com/brightmart/nlp_chinese_corpus)

[http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews)

### 中文阅读理解数据集

[首个中文多项选择阅读理解数据集：BERT最好成绩只有68%，86%问题需要先验知识](https://mp.weixin.qq.com/s/Jr8ALNxD1Uw7O8sQdFpX_g)

[Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension](https://arxiv.org/abs/1904.09679)

[https://github.com/nlpdata/c3](https://github.com/nlpdata/c3)

### 物理常识推理任务数据集

[PIQA: Reasoning about Physical Commonsense in Natural Language](https://arxiv.org/pdf/1911.11641.pdf)

「在不使用刷子涂眼影的情况下，我应该用棉签还是牙签？」类似这种需要物理世界常识的问题对现今的自然语言理解系统提出了挑战。虽然最近的预训练模型 (如 BERT) 在更抽象的如新闻文章和百科词条这种具有丰富文本信息的领域问答方面取得了进展，但在更现实的领域，由于报导的偏差，文本本质上是有限的，类似于「用牙签涂眼影是一个坏主意」这样的事实很少得到直接报道。人工智能系统能够在不经历物理世界的情况下可靠地回答物理常识问题吗？是否能够捕获有关日常物品的常识知识，包括它们的物理特性、承受能力以及如何操纵它们。

在本文中，研究者介绍了一个关于物理常识推理任务和相应的基准数据集 PIQA（Physical Interaction：Question Answering）进行评估。虽然人类应对这一数据集很容易 (95% 的准确率)，但是大型的预训模型很难 (77%)。作者分析了现有模型所缺乏的知识为未来的研究提供了重要的机遇。

### 常识推理数据集WinoGrande

[WinoGrande: An Adversarial Winograd Schema Challenge at Scale](https://arxiv.org/abs/1907.10641)

研究者提出了 WINOGRANDE，一个有着 44k 个问题的大规模数据集。该数据集在规模和难度上较之前的数据集更大。该数据集的构建包括两个步骤：首先使用众包的方式设计问题，然后使用一个新的 AFLITE 算法缩减系统偏见（systematic bias），使得人类可以察觉到的词汇联想转换成机器可以检测到的嵌入联想（embedding association）。现在最好的 SOTA 模型可以达到的性能是 59.4 – 79.1%，比人脸性能水平（94%）低 15-35%（绝对值）。这种性能波动取决于训练数据量（2% 到 100%）。

本论文荣获了 AAAI 2020 最佳论文奖，文中提出的 WINOGRANDE 是一个很好的迁移学习资源；但同时也说明我们现在高估了模型的常识推理的能力。研究者希望通过这项研究能够让学界重视减少算法的偏见。


### 对话数据集

[谷歌发布世界最大任务型对话数据集SGD，让虚拟助手更智能](https://mp.weixin.qq.com/s/hNghBThK4FX0ON4Ypp2HzQ)

[Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset](https://arxiv.org/abs/1909.05855)

### BelleGroup

[https://huggingface.co/BelleGroup](https://huggingface.co/BelleGroup) 里有很多中文的instruct和输出的数据集

### OpenEQA

[从文字模型到世界模型！Meta新研究让AI Agent理解物理世界](https://mp.weixin.qq.com/s/Qeuq8v5-ruKGNlcw884RXg)


# RLHF & InstructGPT

[OpenAI魔改大模型，参数减少100倍！13亿参数InstructGPT碾压GPT-3](https://mp.weixin.qq.com/s/_lsTzx-NbiSmI7KrRXyYZg)

[https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

[https://huggingface.co/blog/zh/rlhf](https://huggingface.co/blog/zh/rlhf)

+ 预训练一个语言模型 (LM) ；
+ 聚合问答数据并训练一个奖励模型 (Reward Model，RM)，也叫偏好模型；
+ 用强化学习 (RL) 方式微调 LM。


## sft

![rlhf-sft](../assets/rlhf-sft.png)

确保任务多样性的情况下，由标注人员编写prompt和一些生成式任务的期望输出。

+ openai：instructGPT使用小版本的GPT-3，并对“更可取”（preferable）的人工生成文本微调
+ Anthropic：1000w-520亿参数的transformer，并按“有用、诚实和无害”的标准在上下文线索上蒸馏原始LM
+ DeepMind：在[Teaching language models to support answers with verified quotes](https://arxiv.org/pdf/2203.11147.pdf)提出的GopherCite模型中，用的是2800亿的模型Gopher([Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf))

[剖析大模型Pretrain和SFT阶段的Loss差异](https://zhuanlan.zhihu.com/p/652657011)

不管是PreTraining阶段还是SFT阶段，loss函数都是一样的，只是计算的方式存在差异，PreTraining阶段计算的是整段输入文本的loss，而SFT阶段计算的是**response部分的loss**。

## rm

![rlhf-rm](../assets/rlhf-rm.png)

接收一系列文本并返回一个**标量奖励**，数值上对应人的偏好。我们可以用端到端的方式用LM建模，或者用模块化的系统建模 (比如**对输出进行排名**，再**将排名转换为奖励**) 。

+ **模型选择**：RM可以是另一个经过微调的LM，也可以是根据偏好数据从头开始训练的LM。Anthropic 提出了一种特殊的预训练方式，即用**偏好模型预训练** (Preference Model Pretraining，PMP) 来替换一般预训练后的微调过程，PMP**对样本的利用率更高**。
+ **训练文本**：RM 的提示 - 生成对文本是从预定义数据集中采样生成的，并用初始的 LM 给这些提示生成文本。Anthropic 的数据主要是通过 Amazon Mechanical Turk 上的聊天工具生成的，并在 [Hub](https://huggingface.co/datasets/Anthropic/hh-rlhf) 上 可用，而 OpenAI 使用了用户提交给 GPT API 的 prompt。
+ **训练奖励数值**：人工对 LM 生成的回答进行**排名**。起初我们可能会认为应该直接对文本标注分数来训练 RM，但是由于标注者的价值观不同导致这些分数未经过校准并且充满噪音，通过排名可以**比较多个模型各自的输出**并构建更好的规范数据集，这些不同的排名结果将被**归一化**为用于训练的标量奖励值。

目前成功的RLHF使用了和**要对齐的LM**具有**不同大小**的LM：

+ OpenAI：175B的LM和6B的RM
+ Anthropic：使用的 LM 和 RM 从 10B 到 52B 大小不等
+ DeepMind：使用了 70B 的 Chinchilla 模型分别作为 LM 和 RM


## rl

![rlhf-rl](../assets/rlhf-rl.png)

直接微调整个 10B～100B+ 参数的成本过高 ，参考低秩自适应[LoRA](https://arxiv.org/abs/2106.09685)和DeepMind的[Sparrow LM](https://arxiv.org/abs/2209.14375)。目前多个组织找到的可行方案是使用策略梯度强化学习 (Policy Gradient RL) 算法、近端策略优化 (Proximal Policy Optimization，PPO) **微调初始 LM 的部分或全部参数**。

+ 策略 (policy)：一个接受提示并返回一系列文本 (或文本的概率分布) 的 LM
+ 行动空间（action space）： LM 的词表对应的所有词元 (一般在 50k 数量级) 
+ 观察空间 (observation space)： 是可能的输入词元序列，也比较大 (词汇量^输入标记的数量) 
+ 奖励函数：偏好模型和策略转变约束 (Policy shift constraint) 的结合。

ppo确定的奖励函数如下：

+ 提示$$x$$输入初始LM和当前微调的LM，分别得到输出文本$$y_1$$和$$y_2$$
+ 将来自当前策略的文本传给RM得到标量奖励$$r_{\theta}$$
+ 将两个模型的生成文本进行比较计算差异的惩罚项，一般是输出词分布间的KL散度的缩放，即$$r=r_{\theta}-\lambda r_{KL}$$，

惩罚项的好处：
+ 用于惩罚策略在每个训练batch中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。
+ 如果没有这一项，可能导致模型在优化中生成乱码文本，以愚弄奖励模型提供高奖励值。

根据PPO，按当前batch的奖励进行优化。PPO是置信域优化（TRO，Trust Region Optimization）算法，用梯度约束确保更新步骤不会破坏学习过程的稳定性。

DeepMind对Gopher用了类似的奖励设置，但用的是A2C来优化梯度。


### rl流程概述

[https://zhuanlan.zhihu.com/p/635757674](https://zhuanlan.zhihu.com/p/635757674)

[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)

[Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/pdf/2307.04964.pdf)


![rlhf-ppo-flows-orig](../assets/rlhf-ppo-flows-orig.png)

+ Rollout and Evaluation：从prompt库里抽样，使用语言模型生成response，然后使用奖励模型（Reward Model, RM）给出奖励得分。这个得分反映了生成的response的质量，比如它是否符合人类的偏好，是否符合任务的要求等。
+ Make experience：收集了一系列的“经验”，即模型的行为和对应的奖励。这些经验包括了模型生成的response以及对应的奖励得分。这些经验将被用于下一步的优化过程。
+ Optimization：使用收集到的经验来更新模型的参数。具体来说，我们使用PPO算法来调整模型的参数，使得模型生成的response的奖励得分能够增加。PPO算法的一个关键特性是它尝试保持模型的行为不会发生太大的改变，这有助于保证模型的稳定性。

官方代码example

```python
from tqdm import tqdm

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    #### Get response from SFTModel
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = reward_model(texts)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")
```

![rlhf-workflow](../assets/rlhf-workflow.jpeg)


+ Rollout：根据策略（LM）生成轨迹（文本）。
    + 输入：Batch Prompt、LM
    + 输出：Prompt+Response
+ Evaluate：对生成的轨迹进行评估（RM）。
    + 输入：Prompt+Response、RM
    + 输出：Reward
+ Old Policy Sampling：计算并存储旧策略的概率、价值等值，
    + 输入：Ref_model、Actor、Critic、Prompt+Response
    + 输出：Ref Logprobs、Old Logprobs、Old Values
+ KL Penalty：计算**当前策略**和**原始LM**之间的KL散度，用作对策略改变过快的惩罚项。
    + 输入：Ref Logprobs、Old Logprobs、Reward
    + 输出：Token Reward
+ Generalized Advantage Estimation (GAE)：G。基于old value(shape是(`batch_size`, `response_length`))和reward估计优势函数A，它结合了所有可能的n-step 进行advantage估计
    + 输入：Token Reward、Old Values
    + 输出：Advantages、Returns
+ New Policy Sampling：
    + 输入ref_model、actor、critic，从新的策略中采样概率等信息，
    + 输出new logprobs、new values和logits，供actor loss、critic loss以及entropy loss用。
+ Critic Loss：Critic的目标是估计状态的价值函数，Critic loss就是价值函数预测值和实际回报之间的差距。
    + 输入：New Values、Returns
    + 输出：critic梯度更新
+ Actor Loss：Actor的目标是优化策略，Actor loss就是基于优势函数的策略梯度。
    + 输入：Old Logprobs，New Logprobs、Advantages
    + 输出：actor梯度更新
+ Entropy Loss：为了增加探索性，通常会添加一个基于策略熵的正则项，它鼓励策略保持多样性。
    + 输入：Logits
    + 输出：entropy loss
+ Policykl：这是对策略迭代过程的一个度量，它度量**新策略**和**旧策略**之间的差距。
    + 输入：Old Logprobs、New Logprobs
    + 输出：是否early stop

在PPO中，策略优化的过程涉及到两个策略：一个是"旧的"策略，这是我们在开始每次优化迭代时使用的策略，另一个是"新的"策略，这是我们在优化过程中**不断更新**的策略。

自己整理重画的

![rlhf-dot](../assets/rlhf-dot.jpg)

### 几个重要的loss

#### actor & actor loss

&nbsp;

Actor 是**策略**，它决定文本会被怎么样生成，是从**策略网络**拷贝来的模拟整个智能体在环境中行动的网络。

优势函数表示在给定的状态下采取某个行动比遵循当前策略的期望回报要好多少。

Actor Loss如下，用重要性采样比较在**旧策略**和**新策略**下行动的概率（Old Logprobs，New Logprobs），然后将这个比值（也就是 Importance Sampling 的权重）与**优势函数Advantages**相乘，得到了对 Actor Loss 的一个估计。

XXXL=\pi_{new}/\pi_{old} * AXXX

```python
# 计算新旧策略下概率的比值
ratio = torch.exp(logprobs - old_logprobs)

# 计算未截断的策略梯度损失
pg_losses = -advantages * ratio

# 计算截断的策略梯度损失
pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange,
     1.0 + self.config.cliprange)

# 选择两者中较大的作为最终的策略梯度损失
pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)

# 计算因为截断导致策略梯度损失改变的比例
pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).double(), mask)
```


#### critic & critic loss

&nbsp;

critic是专门用来预测actor轨迹**每一步价值**的网络，actor上加几个线性层能够给每个token预测一个值。任务是估计状态的价值函数，也就是预测从当前状态开始，通过遵循某个策略，期望能得到的总回报。

Critic Loss是最小化它的预测价值与实际回报之间的差距，常用mse

通过最小化Critic Loss，Critic的预测能力会逐渐提升。因为Critic的预测结果会被用来**估计每个行动的优势（Advantage）**，这个优势值又会被用来计算策略的更新（Actor Loss）。

```python
# 将价值函数的预测值裁剪到一个范围内
vpredclipped = clip_by_value(
            vpreds, values - self.config.cliprange_value, values + self.config.cliprange_value
        )

# 计算裁剪前和裁剪后的价值函数损失
vf_losses1 = (vpreds - returns) ** 2
vf_losses2 = (vpredclipped - returns) ** 2

# 最终的价值函数损失是裁剪前和裁剪后损失的最大值的平均值的一半
vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)

# 计算裁剪操作实际发生的频率
vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).double(), mask)
```

#### KL Penalty

&nbsp;

用于保证经过强化学习后的模型（新策略actor）不会过于偏离原始预训练模型（ref model）。

```python
# 初始化两个列表来分别存储奖励和非得分奖励
rewards, non_score_rewards = [], []

# 使用 zip 函数并行遍历输入的得分、对数概率、参考模型的对数概率以及mask
for score, logprob, ref_logprob, mask in zip(scores, logprobs, 
        ref_logprobs, masks):
    # 计算 KL 散度，即模型的对数概率与参考模型的对数概率之间的差值
    kl = logprob - ref_logprob

    # 计算非得分奖励，即 KL 散度乘以 KL 控制器值的负值
    non_score_reward = -self.kl_ctl.value * kl
    non_score_rewards.append(non_score_reward)

    # 复制非得分奖励为新的奖励
    reward = non_score_reward.clone()

    # 找到mask中最后一个非零元素的索引，这表示输入序列的实际长度
    last_non_masked_index = mask.nonzero()[-1]

    # 对于最后一个非mask部分的token，其奖励是偏好模型的得分加上 KL 散度
    reward[last_non_masked_index] += score

    # 将计算的奖励添加到奖励列表中
    rewards.append(reward)

# 返回包含所有奖励的张量以及包含所有非得分奖励的张量
return torch.stack(rewards), torch.stack(non_score_rewards)
```

#### GAE

&nbsp;

GAE是一种多步优势估计方法。它通过引入一个权衡参数$$\lambda$$，在**单步TD误差**和**多步TD误差**之间进行权衡，从而**减小估计的方差**，提高学习的稳定性。其中$$\sigma _{t+l}$$是时间步$$t+l$$的TD误差。

XXXA_t=\sum ^{k-1}_{l=0}(\lambda \eta )^{l}\sigma _{t+l}XXX

XXX\sigma _{t+l}=r_{t+l+1}+\eta V(s_{t+l+1})-V(s_{t+l})XXX

```python
# 从后往前遍历整个生成的序列
for t in reversed(range(gen_len)):
    # 计算下一个状态的价值，如果当前状态已经是最后一个状态，则下一个状态的价值为0
    nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0

    # 计算 delta，它是奖励加上衰减后的下一个状态的价值，然后减去当前状态的价值
    delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]

    # 使用 delta 更新 lastgaelam，这是 GAE 公式的一部分
    lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam

    # 将计算的优势值添加到优势值列表中
    advantages_reversed.append(lastgaelam)

# 将优势值列表反向并转换为张量
advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

# 计算回报值，它是优势值加上状态值
returns = advantages + values
```

####  entropy loss

&nbsp;

一个策略的熵越大，意味着这个策略选择各个动作的概率更加“平均”。在actor的loss里加熵，使得策略的熵尽可能大，从而有更多机会探索可能带来更好奖励的文本轨迹。

```python
entropy = -torch.sum(logits* torch.log(logits + 1e-9), dim=-1).mean()
```

新实现：

```python
    pd = torch.nn.functional.softmax(logits, dim=-1)
    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)
```


#### Policy kl

&nbsp;

在PPO中，KL散度被用作一种约束，以确保在优化过程中新策略不会偏离旧策略太远。这是为了防止过度优化，因为过度优化可能会导致策略性能的大幅下降。

我们希望在优化目标函数的同时，满足以下的KL散度约束：

XXXKL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]\le \delta XXX

在代码中，每个mini batch都会进行early stop的判定，如果计算出的KL散度大于 $$\delta$$，那么就会停止这一轮的优化，以保证新策略不会偏离旧策略太远。

```python
# 计算旧策略和新策略之间的KL散度
policykl = masked_mean(old_logprobs - logprobs, mask) 
# old_logprobs 是旧策略下行为的概率的对数，logprobs 是新策略下的对数概率
# masked_mean 函数计算差异（old_logprobs - logprobs）的平均值，
# 但只考虑mask中对应元素为True的元素

# 检查计算出的KL散度（policykl）是否大于目标KL散度（self.config.target_kl）的1.5倍
if policykl > 1.5 * self.config.target_kl: 
    self.optimizer.zero_grad()  
    # 如果实际的KL散度超过了目标的1.5倍，那么策略改变过多，这步的梯度也不更新了。
    early_stop = True  
    # 并设置early_stop标志为True，表示应提前停止优化，以防止策略从旧策略进一步偏离
```


### 两个采样

#### Old Policy Sampling（无bp）

&nbsp;

是**make experience**的过程，计算并**存储**旧策略的概率、价值等值，来为后面更新的过程服务。

+ Old Logprobs：从“旧的”策略[即在这个batch数据中初始的LM（initial actor）]中计算每个token在旧的策略下的概率Old Logprobs。
+ Old Values：旧策略中每个**时间步**（每个token的预测结果）的价值，这个值由critic网络进行预测，critic网络就是需要这个值的原因是advantage的计算依赖于Old Values。
+ Ref Logprobs：最最原始的LM对于每个时间步的概率预测，一般就是**固定不变的gpt3**，计算这个值的目的是限制actor的更新，防止其偏离原始gpt3太远，他的实现在下一个步骤中。

```python
all_logprobs, _, values, masks = self.batched_forward_pass(self.model, queries, 
    responses, model_inputs)
ref_logprobs, _, _, _ = self.batched_forward_pass(self.ref_model, queries, 
    responses, model_inputs)
```

#### New Policy Sampling（有bp）

&nbsp;

在**新的策略**（更新后的actor）下对轨迹（文本）计算概率的过程，计算Actor Loss，即策略梯度的损失。

Old Logprobs是一次性一个batch的数据计算的，这是因为在一个batch中旧策略都是不变的；而New Logprobs是一个mini batch计算一次，这是因为新策略每个mini batch变一次。


### 开源rlhf库

[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

[影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）](https://zhuanlan.zhihu.com/p/512327050)

#### openai的lm-human-preferences(gpt2的finetune)

&nbsp;

[https://github.com/openai/lm-human-preferences](https://github.com/openai/lm-human-preferences)

#### huggingface的TRL

&nbsp;

[https://github.com/huggingface/trl](https://github.com/huggingface/trl)
 

#### CarperAI的trlx

&nbsp;

[https://github.com/CarperAI/trlx](https://github.com/CarperAI/trlx)

#### allenai的RL4LMs

[https://github.com/allenai/RL4LMs](https://github.com/allenai/RL4LMs)

#### RLHF workflow

[仅靠开源数据复刻出LLaMA3指令学习效果，在线迭代RLHF全流程解决方案来了](https://mp.weixin.qq.com/s/bRxdSCCPIrgNBgtDfyzhAA)

[Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint](https://arxiv.org/pdf/2312.11456)

[RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/pdf/2405.07863)

对应代码：

+ [https://github.com/RLHFlow/RLHF-Reward-Modeling](https://github.com/RLHFlow/RLHF-Reward-Modeling)
+ [https://github.com/RLHFlow/Online-RLHF](https://github.com/RLHFlow/Online-RLHF)

#### openrlhf

[这个团队做了OpenAI没Open的技术，开源OpenRLHF让对齐大模型超简单](https://mp.weixin.qq.com/s/3JjnGXJTqqiLP9hC21THIg)

[OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)

[https://github.com/OpenLLMAI/OpenRLHF](https://github.com/OpenLLMAI/OpenRLHF)

# Alignment：RLHF变种

## Alignment综述

[大模型微调（八）：SFT for Alignment 总结纪要](https://zhuanlan.zhihu.com/p/717553974)

[2024年大模型Alignment偏好优化技术：从PPO, SPO到MCTS-DPO](https://mp.weixin.qq.com/s/-x2tdJWpi789lfYd0N80XQ)

[https://alignmentsurvey.com/](https://alignmentsurvey.com/)

[中文版](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey-CN.pdf)

[AI Alignment: A Comprehensive Survey](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey.pdf)

## DPO

[Direct preference optimization: Your language model is secretly a reward model](https://arxiv.org/pdf/2305.18290)

[https://github.com/eric-mitchell/direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization)

在contextual bandit的设定下，DPO 通过数学推导，得到了**奖励函数与最优策略之间的直接映射**，消除了RLHF过程中的奖励建模阶段，

其中$$Z(x)=\sum_y \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)$$是partition function，

XXX
r(x, y)=\beta \log \frac{\pi_r(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x)
XXX

代入Bradley-Terry模型，可以得到

XXX
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\text {ref }}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\text {ref }}\left(y_l \mid x\right)}\right)\right]
XXX

其中：

+ $$x$$：来自偏好数据集的prompt
+ $$y_w$$：来自偏好数据集的获胜response
+ $$y_l$$：来自偏好数据集的失败response

其具体含义如下，其中$$\hat{r}_\theta(x, y)=\beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text {ref }}(y \mid x)}$$：

XXX
\begin{aligned}
& \nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)= \\
& -\beta \mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}[\underbrace{\sigma\left(\hat{r}_\theta\left(x, y_l\right)-\hat{r}_\theta\left(x, y_w\right)\right)}_{\text {higher weight when reward estimate is wrong }}[\underbrace{\nabla_\theta \log \pi\left(y_w \mid x\right)}_{\text {increase likelihood of } y_w}-\underbrace{\nabla_\theta \log \pi\left(y_l \mid x\right)}_{\text {decrease likelihood of } y_l}]]
\end{aligned}
XXX

具体实现：

```python
def preference_loss(policy_chosen_logps: torch.FloatTensor,
                    policy_rejected_logps: torch.FloatTensor,
                    reference_chosen_logps: torch.FloatTensor,
                    reference_rejected_logps: torch.FloatTensor,
                    beta: float,
                    label_smoothing: float = 0.0,
                    ipo: bool = False,
                    reference_free: bool = False) -> Tuple[torch.FloatTensor, 
                        torch.FloatTensor, torch.FloatTensor]:

    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps

    if reference_free:
        ref_logratios = 0

    logits = pi_logratios - ref_logratios  # also known as h_{\pi_\theta}^{y_w,y_l}

    if ipo:
        # Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf
        losses = (logits - 1/(2 * beta)) ** 2  
    else:
        # Eq. 3 https://ericmitchell.ai/cdpo.pdf; 
        # label_smoothing=0 gives original DPO 
        # (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)
        losses = -F.logsigmoid(beta * logits) * (1 - label_smoothing) - 
            F.logsigmoid(-beta * logits) * label_smoothing

    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

    return losses, chosen_rewards, rejected_rewards

```

RLHF与DPO对比：

+ RLHF优化的是token-level的value functions，对应的是terminate state的sparse reward。
+ DPO则是在context bandit的设定下，将整个response看成一个arm。虽然token是一个个生成的，但在RL里一般dense rewards效果更好。

## SimPO

[全面超越DPO：陈丹琦团队提出简单偏好优化SimPO，还炼出最强8B开源模型](https://mp.weixin.qq.com/s/wJKiDU8t2RW2DpnqYR1h8w)

[SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/pdf/2405.14734)

[https://github.com/princeton-nlp/SimPO](https://github.com/princeton-nlp/SimPO)

## TDPO

[从RLHF到DPO再到TDPO，大模型对齐算法已经是「token-level」](https://mp.weixin.qq.com/s/JQDc9D5vbd1NBtaEx0cyAg)

[Token-level Direct Preference Optimization](https://arxiv.org/pdf/2404.11999)

[https://github.com/Vance0124/Token-level-Direct-Preference-Optimization](https://github.com/Vance0124/Token-level-Direct-Preference-Optimization)

## CriticGPT

[GPT-4批评GPT-4实现「自我提升」！OpenAI前超级对齐团队又一力作被公开](https://mp.weixin.qq.com/s/T0BHRROG5IKeLKrguESc7g)

[LLM Critics Help Catch LLM Bugs](https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf)

CriticGPT依旧是自回归模型。标注者先向ChatGPT的响应输出中人为注入一些微妙的错误，CriticGPT针对这些有错误的答案生成批评意见，之后再由人类训练师为批评意见进行打分排名。

## DeRa

[ICML 2024 Spotlight | 在解码中重新对齐，让语言模型更少幻觉、更符合人类偏好](https://mp.weixin.qq.com/s/-9MjgNOLRrUdaQUF5tVv9w)

[Decoding-time Realignment of Language Models](https://arxiv.org/pdf/2402.02992)

[https://github.com/liutianlin0121/decoding-time-realignment](https://github.com/liutianlin0121/decoding-time-realignment)

## Step-DPO

[贾佳亚团队新作：10k数据让大模型数学能力超GPT-4](https://mp.weixin.qq.com/s/6CnaOqg2i26fe7AXKaFr4g)

[https://github.com/dvlab-research/Step-DPO](https://github.com/dvlab-research/Step-DPO)

[Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/pdf/2406.18629)

[超越DPO！大模型精细化对齐之Step-DPO](https://mp.weixin.qq.com/s/vCs6KJ1DlfYojUJD45xRpw)

## RBR

[RLHF不够用了，OpenAI设计出了新的奖励机制](https://mp.weixin.qq.com/s/gn_MoLjessnCMxRNNjhtuw)

[Rule Based Rewards for Language Model Safety](https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf)

[https://github.com/openai/safety-rbr-code-and-data](https://github.com/openai/safety-rbr-code-and-data)

## RLLR

[ACL2024 | RLHF在腾讯广告自然语言理解任务上的优化及应用](https://mp.weixin.qq.com/s/GFrvQSf2TIQW-B1mbin9TA)

[Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding](https://arxiv.org/pdf/2405.19763)

更适合NLU任务的强化学习方法RLLR（Reinforcement Learning with Label-Sensitive Reward），与RLHF相比可以一致地提升多种NLU任务上的标签准确率；进一步地，通过结合RLHF和RLLR的两个Reward Model，RLLR-mixed方法可以在标签准确率和理由质量上取得全面提升。

### 原始RLHF直接用在NLU

+ 将NLU任务改写为自然语言的形式，让模型对同一个问题输出多条回复，每条回复包括两部分：
    + 理由（rationale）：可以视为思维链的一种简化版本
    + 标签（label）
+ 对同一个问题的不同回复进行排序，并处理成回复对（pair）的形式。
    + 标签敏感对（label-sensitive pair）：标签不同的回复对
    + 理由敏感对（rationale-sensitive pair）：**标签相同、理由不同**的回复对

在7个公开数据集上，理由敏感对的占比都超过75%。在理由敏感对中，两个不同的理由会导向相同的标签，理论上说，模型在这些数据上进行训练只会使理由**更符合标注者的偏好**，但**对标签的准确性没有帮助**；而在NLU任务上，我们实际**更关心标签的准确性**，因此存在模型训练目标与评估指标不一致的问题。

### RLLR

+ SFT训练：除了标签之外，我们还为数据集标注了理由，参考CoT的方式，先让模型生成一段理由，再根据理由输出标签，训练得到Policy Model；
+ Reward Model训练：训练两个reward model：
    + **标签敏感RM**：解决原始RLHF中标签敏感对比例较低的问题：对于一条训练集中的样本，我们已有正确的标签标注，并且也知道所有可能的标签集合，因此可以**随机采样一个错误的标签**，并**为错误的标签标注理由**，与正确标签+理由构成**标签敏感对**，并基于此方法构造的数据训练单独的Reward Model；
    + **理由敏感RM**：对于训练集中的每条样本，我们基于正确的标签采样多条理由，根据**理由的生成质量**来构建理由敏感对，并训练理由敏感的Reward Model，此处理由质量可以采用**人工判断或AI辅助**的方式标注，根据准确性、一致性、逻辑性、事实性、相关性和信息完整性进行排序。
+ PPO训练：使用Reward Model和的Policy Model进行强化学习训练。
    + RLLR：只用标签敏感RM训练
    + RLLR-mixed：用两个RM训练，最终reward如下，$$r_{\phi 1}$$是标签敏感RM的输出，$$r_{\phi 2}$$是理由敏感RM的输出，大部分情况下，$$r_{\phi 2}<r_{\phi 1}$$，为了强化各自的作用，当$$r_{\phi 1}$$较小时，由$$r_{\phi 1}$$主导，当$$r_{\phi 1}>\lambda$$时，截断到$$\lambda$$，由$$r_{\phi 2}$$主导：
XXX
r_M(q, a)= \begin{cases}r_{\phi 1}(q, a)+r_{\phi 2}(q, a) & r_{\phi 1}(q, a)<\lambda \\ \lambda+r_{\phi 2}(q, a) & r_{\phi 1}(q, a) \geq \lambda\end{cases}
XXX

## AMP

[无需人工/GPT-4V排序，针对多模态大模型的全自动多级偏好学习](https://mp.weixin.qq.com/s/vv1s9D7WQ_kKhdMoQOjYjw)

[Automated Multi-level Preference for MLLMs](https://arxiv.org/pdf/2405.11165)

[https://github.com/takomc/amp](https://github.com/takomc/amp)

## Self-Taught Evaluators

[Self-Taught Evaluators](https://arxiv.org/abs/2408.02666)

[https://github.com/facebookresearch/RAM/tree/main/projects/self_taught_evaluator](https://github.com/facebookresearch/RAM/tree/main/projects/self_taught_evaluator)

[https://huggingface.co/facebook/Self-taught-evaluator-llama3.1-70B](https://huggingface.co/facebook/Self-taught-evaluator-llama3.1-70B)

## U-SOPHISTRY

[AI会「说谎」，RLHF竟是帮凶](https://mp.weixin.qq.com/s/TvtKnXoR9rBRcGl0N-uCAQ)

[Language Models Learn to Mislead Humans via RLHF](https://arxiv.org/pdf/2409.12822)

## UNA

[综合RLHF、DPO、KTO优势，统一对齐框架UNA来了](https://mp.weixin.qq.com/s/8VzRYlHGS0kF1k7A9tJamA)

## Align-anything

[全模态对齐框架align-anything来了：实现跨模态指令跟随](https://mp.weixin.qq.com/s/OFOvkp5STkD4n5rllai39A)

[https://github.com/PKU-Alignment/align-anything](https://github.com/PKU-Alignment/align-anything)

# 多模态（图像）


[【IEEE Fellow何晓东&邓力】多模态智能论文综述：表示学习，信息融合与应用，259篇文献带你了解AI热点技](https://mp.weixin.qq.com/s/EMWpBP5iB1Qrleo3XNjbuQ)

[Multimodal Intelligence: Representation  Learning, Information Fusion, and Applications](https://arxiv.org/abs/1911.03977)

[BERT在多模态领域中的应用](https://mp.weixin.qq.com/s/THxlQX2MPXua0_N0Ug0EWA)

## VLM导论

[视觉语言模型导论：这篇论文能成为你进军VLM的第一步](https://mp.weixin.qq.com/s/gdT0q5HJ9Fw5QrbBihI1vA)

[An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247)


## vilbert

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/pdf/1908.02265.pdf)

研究人员提出了一种名为 ViLBERT（图文 BERT）模型。这是一个可以学习任务未知的、图像内容和自然语言联合表征的模型。研究人员将流行的 BERT 架构扩展成一个 multi-modal two-stream 模型上。在这个模型上，模型用两个分开的流处理图像和文本输入，但他们彼此用联合注意力层交互。研究人员在两个代理任务上，使用 Conceptual Captions 数据集（数据集很大，而且是自动收集的数据）预训练这个模型，然后将模型秦阿姨到多个建立好的图像-文本任务上。这些任务包括图像问答、图像常识推理、引述表达、指称成分，以及基于捕捉的图像提取。这些只需要在基本架构上进行微小的补充。研究人员观察到，相比现有的针对任务的特定模型，新模型在这些任务上都有了相助的性能提升——在每个任务上都取得了 SOTA。

## VLbert

Visual-Linguistic BERT，简称 VL-BERT

[微软亚研提出VL-BERT：通用的视觉-语言预训练模型](https://mp.weixin.qq.com/s/RaYwdMXT0UKN8_bni-DpWw)

此预训练过程可以显著提高下游的视觉-语言任务的效果，包含视觉常识推理、视觉问答与引用表达式理解等。值得一提的是，在视觉常识推理排行榜中，VL-BERT 取得了当前单模型的最好效果。

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530)

之前的视觉-语言模型分别使用计算机视觉或自然语言处理领域中的预训练模型进行初始化，但如果目标任务数据量不足，模型容易过拟合从而损失性能。并且对于不同的视觉-语言任务，其网络架构一般是经过特殊设计的，由此很难通过视觉-语言联合预训练的过程帮助下游任务。

VL-BERT 的主干网络使用 TransformerAttention 模块，并将视觉与语言嵌入特征作为输入，其中输入的每个元素是来自句子中的单词、或图像中的感兴趣区域（Region of Interests，简称 RoIs）。在模型训练的过程中，每个元素均可以根据其内容、位置、类别等信息自适应地聚合来自所有其他元素的信息。在堆叠多层 TransformerAttention 模块后，其特征表示即具有更为丰富的聚合与对齐视觉和语言线索的能力。

为了更好地建模通用的视觉-语言表示，作者在大规模视觉-语言语料库中对 VL-BERT 进行了预训练。采用的预训练数据集为图像标题生成数据集，Conceptual Captions，其中包含了大约 330 万个图像标题对。

VL-BERT 的预训练主要采用三个任务：
+ 屏蔽语言模型（Masked Language Modeling），即随机屏蔽掉语句中的一些词，并预测当前位置的词是什么；
+ 屏蔽 RoI 分类（MaskedRoIClassification），即随机屏蔽掉视觉输入中的一些 RoIs，并预测此空间位置对应 RoI 的所属类别；
+ 图像标题关联预测（Sentence-Image Relationship Prediction），即预测图像与标题是否属于同一对。

在预训练结束后，使用微调来进行下游任务的训练。本文中主要在三个视觉-语言下游任务中进行微调，即视觉常识推理（VisualCommonsenseReasoning）、视觉问答（VisualQuestionAnswering）与引用表达式理解（ReferringExpressionComprehension），下面将分别介绍。

视觉常识推理任务即给定图片与相关问题，机器不仅需要回答问题，还需要提供理由来证明答案的正确性。此任务（Q->AR）被分解为两个子任务，即视觉问答（Q->A，给定图片与问题，输出正确答案），以及视觉推理（QA->R，给定图片、问题与答案，输出正确的理由）。

## CLIP系列

## cn-clip

[Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/pdf/2211.01335)

[https://github.com/OFA-Sys/Chinese-CLIP](https://github.com/OFA-Sys/Chinese-CLIP)

## BEiT系列


### BEiT

[BEIT: BERT Pre-Training of Image Transformers](https://arxiv.org/pdf/2106.08254.pdf)

### BEiT v2

[BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/pdf/2208.06366.pdf)

### BEiT v3

[Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/pdf/2208.10442.pdf)

## ViT&Swin-Transformer

[SwinTransformer与Vit细节总结](https://blog.csdn.net/taoqick/article/details/131362590)

[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)

对于一张$$224\times 224\times 3$$的图像，假设每个patch是$$16\times 16$$，那就分成$$\frac{224\times 224}{16\times 16}=196$$个patch(即$$seq\_length=196$$)，每个patch的维度是$$16\times 16\times 3=768$$，最后加上```[CLS]```这个token，就是$$seq\_length=197$$。

## 像素tokenizer

[Meta新研究挑战CV领域基操：ViT根本不用patch，用像素做token效果更佳](https://mp.weixin.qq.com/s/o_Wb3Bt9Maipgczokeinrg)

[An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://arxiv.org/pdf/2406.09415)

## stable diffusion

[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

![stable-diffusion](../assets/stable-diffusion.png)

+ 输入图像，经过编码器得到z，z通过前向扩散不断加噪声得到$$z_T$$（正向扩散）
+ 输入条件，经过条件编码器(原文是BERT，到了DALL-E2就改成CLIP了)得到$$\tau_\theta$$
+ $$z_T$$在$$\tau_\theta$$的指导下不断去噪（反向扩散），得到新的$$z$$，再通过解码器得到最终生成的图像

其中的正向扩散和反向扩散一般用U-Net

代码库：[https://github.com/CompVis/latent-diffusion/tree/main](https://github.com/CompVis/latent-diffusion/tree/main)

粗略看了下代码，带condition的训练原理大概是训练语料中有图+文本（例如imagenet的class_label，这里可以映射到一个classid也可以直接拿明文），然后condition和图片一起作为输入去训练auto-eocnder和ldm

在```/latent-diffusion/ldm/data/imagenet.py```这个代码里，把class_label加进来了

```python
    def _load(self):
        with open(self.txt_filelist, "r") as f:
            self.relpaths = f.read().splitlines()
            l1 = len(self.relpaths)
            self.relpaths = self._filter_relpaths(self.relpaths)
            print("Removed {} files from filelist during filtering.".format(l1 - len(self.relpaths)))

        self.synsets = [p.split("/")[0] for p in self.relpaths]
        self.abspaths = [os.path.join(self.datadir, p) for p in self.relpaths]

        unique_synsets = np.unique(self.synsets)
        class_dict = dict((synset, i) for i, synset in enumerate(unique_synsets))
        if not self.keep_orig_class_label:
            self.class_labels = [class_dict[s] for s in self.synsets]
        else:
            self.class_labels = [self.synset2idx[s] for s in self.synsets]

        with open(self.human_dict, "r") as f:
            human_dict = f.read().splitlines()
            human_dict = dict(line.split(maxsplit=1) for line in human_dict)

        self.human_labels = [human_dict[s] for s in self.synsets]

        labels = {
            "relpath": np.array(self.relpaths),
            "synsets": np.array(self.synsets),
            "class_label": np.array(self.class_labels),
            "human_label": np.array(self.human_labels),
        }

        if self.process_images:
            self.size = retrieve(self.config, "size", default=256)
            self.data = ImagePaths(self.abspaths,
                                   labels=labels,
                                   size=self.size,
                                   random_crop=self.random_crop,
                                   )
        else:
            self.data = self.abspaths
```

## stable diffusion 3

[Stable Diffusion 3论文终于发布，架构细节大揭秘，对复现Sora有帮助？](https://mp.weixin.qq.com/s/mH6IzExPPBpX8YTwxlP6dA)

## DALL-E系列

DALL-E3：

[Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)

现有的文本->图像模型面临的一个基本问题是：训练数据集中的文本-图像pair对中的**文本质量较差**。

+ 学习一个图像文本生成器，可以生成详细、准确的图像描述
+ 将此文本生成器应用到数据集以生成更详细的文本
+ 在改进的数据集上训练文本 - 图像模型


## PALM-E

[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)

## InstantID

[InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://arxiv.org/pdf/2401.07519.pdf)

[https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID)

[小红书开源「InstantID」效果炸裂，迅速蹿上Github热榜](https://baijiahao.baidu.com/s?id=1789680663845556585&wfr=spider&for=pc)

用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真

![InstantID](../assets/InstantID.png)

[曾爆火的 InstantID又有了新玩法：风格化图像生成，已开源](https://mp.weixin.qq.com/s/CP6NFzzt57YZMj4Q3JNySA)

[InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation](https://arxiv.org/pdf/2404.02733.pdf)

[https://github.com/InstantStyle/InstantStyle](https://github.com/InstantStyle/InstantStyle)


## VAR

[GPT超越扩散、视觉生成Scaling Law时刻！北大&字节提出VAR范式](https://mp.weixin.qq.com/s/KOEdTgJX4Gga5zRbl57Yow)

[Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/pdf/2404.02905.pdf)

[https://github.com/FoundationVision/VAR](https://github.com/FoundationVision/VAR)

## cobra

[首个基于Mamba的MLLM来了！模型权重、训练代码等已全部开源](https://mp.weixin.qq.com/s/KuuNTL_jBRsyhub5_6aXpQ)

[Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference](https://arxiv.org/pdf/2403.14520.pdf)

[https://github.com/h-zhao1997/cobra](https://github.com/h-zhao1997/cobra)

## Hyper-SD

[加速扩散模型，最快1步生成SOTA级图片，字节Hyper-SD开源了](https://mp.weixin.qq.com/s/dqDqlWv1xe-8zayeJCGq8A)

[Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis](https://arxiv.org/pdf/2404.13686)

## TextSquare

[8B文字多模态大模型指标逼近GPT4V，字节、华师、华科联合提出TextSquare](https://mp.weixin.qq.com/s/zFsZsEgHtMUJMye_56j9Cw)

[TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/pdf/2404.12803)

## neural network diffusion

[用扩散模型生成网络参数，LeCun点赞尤洋团队新研究](https://mp.weixin.qq.com/s/kVY0UrLrfb3_2ZmIFlGxVg)

[Neural Network Diffusion](https://arxiv.org/pdf/2402.13144.pdf)

[https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion](https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion)

## hunyuan-dit

[首个中文原生DiT架构！腾讯混元文生图大模型全面开源，免费商用](https://mp.weixin.qq.com/s/6J4Vc1faazRGXbDNG_RdPw)

## lumina-t2x

[DiT架构大一统：一个框架集成图像、视频、音频和3D生成，可编辑、能试玩](https://mp.weixin.qq.com/s/NwwbaeRujh-02V6LRs5zMg)

[Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers](https://arxiv.org/pdf/2405.05945)

[https://github.com/Alpha-VLLM/Lumina-T2X](https://github.com/Alpha-VLLM/Lumina-T2X)

[https://huggingface.co/Alpha-VLLM/Lumina-T2I/tree/main](https://huggingface.co/Alpha-VLLM/Lumina-T2I/tree/main)

## Vision-LSTM

[原作者带队，LSTM卷土重来之Vision-LSTM出世](https://mp.weixin.qq.com/s/_9DYLbRkiXTU70nsXJLCDQ)

[Vision-LSTM: xLSTM as Generic Vision Backbone](https://arxiv.org/pdf/2406.04303)

[https://nx-ai.github.io/vision-lstm/](https://nx-ai.github.io/vision-lstm/)

## CSR

[零成本突破多模态大模型瓶颈！多所美国顶尖高校华人团队，联合推出自增强技术CSR](https://mp.weixin.qq.com/s/yrzBdDhxv5AkSZMQzuHc8g)

[Calibrated Self-Rewarding Vision Language Models](https://arxiv.org/pdf/2405.14622)

[https://github.com/YiyangZhou/CSR](https://github.com/YiyangZhou/CSR)

## ManyICL

[吴恩达团队新作：多模态多样本上下文学习，无需微调快速适应新任务](https://mp.weixin.qq.com/s/eLqMKBhgHbm36uB0s23C6A)

[Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/pdf/2405.09798)

## MAR

[何恺明新作再战AI生成：入职MIT后首次带队，奥赛双料金牌得主邓明扬参与](https://mp.weixin.qq.com/s/JxdYrYOzkM5DBMR0D49WUQ)

[Autoregressive Image Generation without Vector Quantization](https://arxiv.org/pdf/2406.11838v1)

## Cambrian-1

[寒武纪1号诞生：谢赛宁Yann LeCun团队发布最强开源多模态LLM](https://mp.weixin.qq.com/s/NFiorsNZLzVT1YXeLgNZPw)

[Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/pdf/2406.16860)

[https://github.com/cambrian-mllm/cambrian](https://github.com/cambrian-mllm/cambrian)

[https://huggingface.co/nyu-visionx/](https://huggingface.co/nyu-visionx/)

[https://huggingface.co/datasets/nyu-visionx/CV-Bench](https://huggingface.co/datasets/nyu-visionx/CV-Bench)

[https://github.com/cambrian-mllm/cambrian](https://github.com/cambrian-mllm/cambrian)

当前多模态学习研究的两个潜在问题：

+ 过度且过早地依赖语言，这是一个捷径，能弥补学习有效视觉表征的不足之处
+ 现有基准可能无法为真实世界场景提供足够的指导 —— 视觉定基对于稳健的多模态理解至关重要


## VCR

[Bengio团队提出多模态新基准，直指Claude 3.5和GPT-4o弱点](https://mp.weixin.qq.com/s/Zy-kM3bvN-1oHondw1VLzw)

[VCR: Visual Caption Restoration](https://arxiv.org/pdf/2406.06462)

[https://github.com/tianyu-z/VCR](https://github.com/tianyu-z/VCR)

## EVE

[抛弃视觉编码器，这个「原生版」多模态大模型也能媲美主流方法](https://mp.weixin.qq.com/s/At2Wz9Kk2QzGEZ--4FnbZw)

[Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832)

[https://github.com/baaivision/EVE](https://github.com/baaivision/EVE)

## LC-Mis

[AI画家的「滑铁卢」：为什么冰可乐不愿意住进茶杯里？](https://mp.weixin.qq.com/s/OyLEBVJoaJDkunq15Uwj1Q)

[Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2408.00230)

## 多模态cot

[ACL 2024 Oral｜我们离真正的多模态思维链推理还有多远？](https://mp.weixin.qq.com/s/oCJB_Q_Kz3kTC36WoSVtEQ)

## Imagen 3

[Imagen 3](https://arxiv.org/pdf/2408.07009)

[Imagen 3支持人物生成，人人可用！谷歌Gemini AI重大升级来了](https://mp.weixin.qq.com/s/4gYFpljgF64vA5ojulEmYQ)


## Chameleon

下面3个工作都在这里有介绍：[生成-理解大一统：一文浅谈多模态大模型最新研究进展](https://mp.weixin.qq.com/s/Ip-IphDFF6il3rTJLxflZA)


[Meta首发「变色龙」挑战GPT-4o，34B参数引领多模态革命！10万亿token训练刷新SOTA](https://mp.weixin.qq.com/s/HQC7F64ZIb-k-K_QLzFegg)

[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818)


## Show-o

[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)

## Transfusion

(toread)

[统一transformer与diffusion！Meta融合新方法剑指下一代多模态王者](https://mp.weixin.qq.com/s/D0sadIZkILx8VvWcsIEYFQ)

[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/pdf/2408.11039)

一般来说，多模态生成模型需要能够感知、处理和生成离散元素（如文本或代码）和连续元素（如图像、音频和视频数据）。

+ 离散模态领域：以预测下一个词为目标的语言模型占据主导地位
+ 连续模态方面：扩散模型及其泛化形式则是当前最先进技术

研究者一直试图将语言模型与扩散模型结合：

+ 方法一：直接扩展语言模型，使其能够利用扩散模型作为一个工具，或者将一个预训练的扩散模型嫁接到语言模型上。
+ 方法二：是对连续模态进行量化处理，然后在离散的token上训练一个标准的语言模型，虽然简化了模型架构，但也会造成信息的丢失。

本文通过训练单个模型来预测离散文本 token 和扩散连续图像，从而实现两种模态的完全集成，且不会丢失任何信息。引入了一个训练模型的新方法 Transfusion，能够无缝地生成离散和连续的模态，将语言模型损失函数与扩散相结合，在混合模态序列上训练单个transformer。

该研究还在文本和图像数据混合基础上从头开始预训练多个 Transfusion 模型，最多可达到 7B 参数量，并针对各种单模态和跨模态基准建立扩展定律。

## ControlNeXt

[视频生成控制提升几十倍，新一代轻量级ControlNeXt火了，贾佳亚团队正挑战Scaling Law](https://mp.weixin.qq.com/s/IBqOmZbSCcdRvyFRdcXMLQ)

[ControlNeXt: Powerful and Efficient Control for Image and Video Generation](https://arxiv.org/pdf/2408.06070)

[https://github.com/dvlab-research/ControlNeXt](https://github.com/dvlab-research/ControlNeXt)

## GNN+Graph Transformer综述

[TPAMI 2024 | 计算机视觉中基于图神经网络和图Transformers的方法和最新进展](https://mp.weixin.qq.com/s/-lWM4mmbCixxJxWRPuoQsw)

[Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective](https://arxiv.org/abs/2209.13232)

## Llip

[ICML 2024 | 直面CLIP内在缺陷，Meta提出全新latent对比预训练框架Llip](https://mp.weixin.qq.com/s/vucfBAYI_SFemwg5_PrbLA)

[Modeling Caption Diversity in Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2405.00740)

基于对比视觉-语言预训练技术的大型多模态模型目前已成为人工智能领域研究的热点课题。但这一预训练技术仍然以经典的CLIP模型为基础，缺乏进一步的发展。此外，鉴于CLIP模型通过将图像及其caption映射到单个向量这样的底层机制，可以认为限制了对比预训练模型描述图像各种其他方面的能力。

提出了一种名为Llip的架构（Latent Language Image Pretraining），以图像字幕生成（Image Caption）任务作为出发点，用来模拟自然场景中与单张图像进行匹配caption的多样性。Llip仍然采用双塔特征提取模式，其视觉编码器可以对给定图像输出一组视觉特征，这些特征可以总结与当前图像匹配的多样式captions中的文本信息，来得到最终的表示。

## longllava

[首个Mamba+Transformer混合架构多模态大模型来了，实现单卡千图推理](https://mp.weixin.qq.com/s/ipfx6qaxeQlkILEVACabvA)

[LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)

[https://github.com/FreedomIntelligence/LongLLaVA](https://github.com/FreedomIntelligence/LongLLaVA)

## Molmo

[号称击败Claude 3.5 Sonnet，媲美GPT-4o，开源多模态模型Molmo挑战Scaling law](https://mp.weixin.qq.com/s/9s9sIkP-KDlUuJdlktVT9w)

[Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://molmo.allenai.org/paper.pdf)

## Playground

[文生图参数量升至240亿！Playground v3发布：深度融合LLM，图形设计能力超越人类](https://mp.weixin.qq.com/s/P8rieQj_-KoY0H-HfwN5hA)

[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models](https://arxiv.org/abs/2409.10695)

## REPA

[扩散模型训练方法一直错了！谢赛宁：Representation matters](https://mp.weixin.qq.com/s/a725rxzvyQXqNJoL1NsMaA)

[Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://arxiv.org/pdf/2410.06940)

[https://github.com/sihyun-yu/REPA](https://github.com/sihyun-yu/REPA)

## LLaVA-Critic

[Evaluation is All You Need！首个开源多模态大模型通用评测器LLaVA-Critic](https://mp.weixin.qq.com/s/YweRqZrHJmISVjJWamalQg)

[LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712)

## MM1.5

[苹果多模态模型大升级！文本密集、多图理解，全能小钢炮](https://mp.weixin.qq.com/s/jIevs7L4zwWOWzXM4nx62A)

[MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/pdf/2409.20566)

## SCMs

[OpenAI攻克扩散模型短板，清华校友路橙、宋飏合作最新论文](https://mp.weixin.qq.com/s/tDlp95HLlvYqW2gVPqzQag)

[Simplifying, Stabilizing & Scaling Continuous-Time Consistency Models](https://arxiv.org/pdf/2410.11081v1)

# 多模态（视频）


## videobert


[通过未标记视频进行跨模态时间表征学习](https://mp.weixin.qq.com/s/5qC70NoTBQ95vjI4cGl66g)

[VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)，VideoBert模型。


## video caption

[Tarsier: Recipes for Training and Evaluating Large Video Description Models](https://arxiv.org/pdf/2407.00634)

![tarsier](../assets/tarsier.png)

## 视频tokenizer方法

+ VideoGPT：[Videogpt: Video generation using vq-vae and transformers](https://arxiv.org/pdf/2104.10157.pdf)，结合了VQ-VAE，而且是自回归的transformer
+ magvit：[MAGVIT: Masked Generative Video Transformer](https://arxiv.org/pdf/2212.05199.pdf)
+ magvitv2：[Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation](https://arxiv.org/pdf/2310.05737.pdf)，[语言模型战胜扩散模型！谷歌提出MAGVIT-v2：视频和图像生成上实现双SOTA！](https://blog.csdn.net/amusi1994/article/details/133917909)


先看vq的改进版：

[Finite Scalar Quantization: VQ-VAE Made Simple](https://arxiv.org/pdf/2309.15505)提出了fsq，码本大小是$$|C|=L^d$$

magvit-v2提出了LFQ，也优化了vq-vae

[BSQ：Image and Video Tokenization with Binary Spherical Quantization](https://arxiv.org/pdf/2406.07548)

![bsq-vq-lfq](../assets/bsq-vq-lfq.png)

![bsq-fsq-lfq](../assets/bsq-fsq-lfq.png)

claude-2-100k的回答。。

+ [MAGVIT: Masked Generative Video Transformer](https://arxiv.org/pdf/2212.05199.pdf)：使用了3D向量量化(3D VQ)自动编码器来将视频量化为离散token
  + 设视频$$V$$有$$T$$帧，其形状为$$T \times H \times W \times 3$$。
  + 3D VQ编码器$$f_T$$会把视频量化为一个token序列$$z$$,其中$$z\in Z^T$$,$$Z$$是码本,$$T$$是token序列长度。
  + 3D VQ解码器$$f^{-1}_T$$则可以从latent token $$z$$重构回视频像素。
+ [Genie: Generative Interactive Environments](https://arxiv.org/pdf/2402.15391.pdf)：使用了2D向量量化(2D VQ)方法
  + 每一帧图像I先通过一个2D VQ编码器f编码为一个token序列$$z$$,其中$$z\in Z^N$$,$$Z$$是2D码本。
  + 然后,对时间序列上的token $$z_1, z_2,..., z_T$$应用一个1D卷积网络,以捕获时间信息。
  + 再通过2D VQ解码器$$f^{-1}$$解码回每一帧图像。
+ [Vivit: A video vision transformer](https://arxiv.org/pdf/2103.15691.pdf)：使用tubelet-embedding
  + 均匀地在时间轴上抽样$$n_t$$个帧,然后把每帧处理成$$n_h \times n_w$$个patch,最终把所有patch连接起来


## SORA

[OpenAI首个AI视频模型炸裂登场，彻底端掉行业饭碗！60秒一镜到底惊人，世界模型真来了？](https://mp.weixin.qq.com/s/93z4Ta91yLv7PB1pnBM9mg)

[https://openai.com/sora](https://openai.com/sora)

[https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)

[一锤降维！解密OpenAI超级视频模型Sora技术报告，虚拟世界涌现了](https://mp.weixin.qq.com/s/ODsebK3fEc-adRDwRVDhQA)

[Sora爆火48小时：杨立昆揭秘论文，参数量或仅30亿](https://new.qq.com/rain/a/20240217A05YVR00)

[微软37页论文逆向工程Sora，得到了哪些结论？](https://mp.weixin.qq.com/s/5-pySWU40omjBowsV2WCKA)

[攻陷短视频后，Sora将需要72万块H100 GPU](https://mp.weixin.qq.com/s/X-MNijIUU5XKYb4vfYtVZg)

[Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)

[Sora之后，OpenAI Lilian Weng亲自撰文教你从头设计视频生成扩散模型](https://mp.weixin.qq.com/s/C8JoiTHwW7T-g66EBPcfDg)

[https://lilianweng.github.io/posts/2024-04-12-diffusion-video/](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)

整体感觉：

+ latent diffusion的隐空间
+ vit和swin transformer的patch

### 现有方法

现有的视频生成方法大多只能用于少数分类的视频、比较短的视频，或者固定长度的视频。

+ recurrent networks
  + 2015年的[Unsupervised learning of video representations using lstms](https://arxiv.org/pdf/1502.04681.pdf)
  + 2017年的[Recurrent environment simulators](https://arxiv.org/pdf/1704.02254.pdf)
  + 2018年的[World models](https://arxiv.org/pdf/1803.10122.pdf)
+ generative adversarial networks
  + 2016年的[Generating videos with scene dynamics](https://arxiv.org/pdf/1609.02612.pdf)
  + 2018年的[Mocogan: Decomposing motion and content for video generation](https://arxiv.org/pdf/1707.04993.pdf)
  + 2019年的[Adversarial video generation on complex datasets](https://arxiv.org/pdf/1907.06571.pdf)
  + 2022年的[Generating long videos of dynamic scenes](https://arxiv.org/pdf/2206.03429.pdf)
+ autoregressive transformers
  + 2021年的[Videogpt: Video generation using vq-vae and transformers]((https://arxiv.org/pdf/2104.10157.pdf))
  + 2022年的[Nüwa: Visual synthesis pre-training for neural visual world creation](https://arxiv.org/pdf/2111.12417.pdf)
+ diffusion models
  + 2022年的[Imagen video: High definition video generation with diffusion models](https://arxiv.org/pdf/2210.02303.pdf)
  + 2023年的[Align your latents: High-resolution video synthesis with latent diffusion models](https://arxiv.org/pdf/2304.08818.pdf)
  
前两类太古老了，sora把后面两个（autogressive transformers和diffusion models）结合在一起了，而且能同时处理不同时长、分辨率的**视频和图像**

### 将视频转成spacetime latent patches

#### Vivit

&nbsp;

[Vivit: A video vision transformer](https://arxiv.org/pdf/2103.15691.pdf)

整体受ViT的启发

![vivit](../assets/vivit.png)

先分patch，再分别过时间的transformer（temporal transformer）和空间的transformer（spatial transformer）

![tubelet-embedding](../assets/tubelet-embedding.png)

具体的分patch方式如上图

#### latent空间上的patch

&nbsp;

![spacetime-patches](../assets/spacetime-patches.png)

参考stable-diffusion，即[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)，把patch切分改成在latent空间上进行

+ 将视频映射成**隐空间**(latent space)的表示
+ 把隐空间的表示切分成**spacetime patches**


预估时，可以通过在一个合适大小的grid里排列随机初始化的patches（we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.）来控制生成视频的大小。估计是参考了下面这篇：

论文参考了这个[Patch n'Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/pdf/2307.06304.pdf)，可以使下面提到的DiT适应各种分辨率/持续时间/宽高比。



### Diffusion Transformer

[Scalable diffusion models with transformers](https://arxiv.org/pdf/2212.09748.pdf)提出了DiT，替换stable diffusion中的u-net

![Dit](../assets/Dit.png)

**DiT=VAE编码器+ ViT + DDPM + VAE解码器**

sora是一个扩散模型，输入加了噪声的patches，还可以加上一些如text prompt的条件，预测原本『干净』的patches。

之前的做法大多将视频全裁成相同长度和大小的，例如4s的$$256\times 256$$，sora可以直接用原始视频

### 语言理解

参考DALL-E3 ([Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf))，训练了一个highly descriptive的视频描述生成器，拿这个生成器给训练集中的所有视频重新生成描述，再拿来训练。

此外，还用上了GPT，将用户输入的短的prompt改写成更长更详细的视频描述用于生成。

### 使用图像/视频作为prompt

+ **图像转动画**：可以让静止的图像动起来
+ **扩展视频**：可以对视频进行扩展（extend），在时间轴上向前或者向后进行延展（比如同样是一个石头落地，能生成4个视频，每个视频里的石头从不同的地方飞过来，落在同一个地面上）
+ **编辑视频**：输入视频和一个文本prompt，能够对视频进行编辑，例如把场景从沙漠替换成树林，类似[Sdedit: Guided image synthesis and editing with stochastic differential equations](https://arxiv.org/pdf/2108.01073.pdf)
+ **连接视频**：输入两个看似毫不相关的视频，能通过很自然的方式把这两个视频衔接在一起

### 生成图像

 图像就是一帧的视频，可以通过在时间范围为一帧的空间grid中排列高斯噪声patches（arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame）来生成图像，同样能生成不同分辨率的图像，最多$$2048\times 2048$$

### 涌现的模拟能力

+ **3D一致性**：随着镜头的移动，视频中的人物或物体在3d空间中能在移动中保持一致
+ **Long-range coherence and object permanence（远程连贯性和物体持久性）**：sora能对短期和长期依赖关系进行建模，例如：
  + 可以保留人物体，即使它们被遮挡或离开当前帧。
  + 可以在单个样本中生成同一角色的多个镜头，并在整个视频中保持其外观的不变
+ **与世界交互**：例如画家可以在画布上留下新的笔触，并随着时间的推移而持续存在，人吃东西能留下齿痕
+ **模拟数字世界**：可以同时通过基本策略控制《我的世界》中的玩家，同时以高保真度渲染世界及其动态，只需要在prompt里提到“我的世界”的标题就可以实现。

### 存在的问题

+ 不能准确地模拟许多基本相互作用的物理过程，例如玻璃破碎。
+ 其他交互（例如吃食物）并不总是会产生对象状态的正确变化，例如长时间样本中出现的不连贯性或对象的自发出现。

## open-sora（Colossal-AI）

[没等来OpenAI，等来了Open-Sora全面开源](https://mp.weixin.qq.com/s/vdr1WBCQVr9aS6bJYcdlRA)

[Open-Sora全面开源升级：支持16s视频生成和720p分辨率](https://mp.weixin.qq.com/s/a-FULV7mSskHFar5glbSxg)

### 模型架构

v1版本：[https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_v1.md](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_v1.md)

v2版本：[https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_02.md](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_02.md)

#### VAE部分

&nbsp;

sora用了spatial-temporal VAE来降低temporal的维度，但并没有开源且高质量的spatial-temporal VAE：

+ [MAGVIT](https://github.com/google-research/magvit)：的$$4\times 4\times 4$$的VAE并没有开源
+ [VideoGPT](https://github.com/wilson1yan/VideoGPT)：的$$2\times 4\times 4$$的VAE在实验中效果不好

因此，使用[https://huggingface.co/stabilityai/sd-vae-ft-mse-original](https://huggingface.co/stabilityai/sd-vae-ft-mse-original)的2D VAE

对于24fps的1min视频，有$$24\times 60=1440$$帧，用4倍的VAE下采样和2倍的patch size下采样，有大约$$1440\times \approx 1.5M$$的token。对这些token算全部的attention的计算开销很大，所以参考[Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/pdf/2401.03048v1.pdf)(代码[https://github.com/Vchitect/Latte](https://github.com/Vchitect/Latte))的方案，使用**spatial-temporal attention**来减小计算量。

以下是latte的4个变种

![latte](../assets/latte.png)

STDiT(sequential)和latte的变种3类似，STDiT(parallel)和latte的变种4类似，在$$16\times 256\times 256$$的视频上，发现效果如下，最终采用了STDiT(sequential)。

XXX
DiT (full) > STDiT (Sequential) > STDiT (Parallel) \approx Latte
XXX

![stdit](../assets/stdit.png)

#### 生成部分

&nbsp;

[PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/pdf/2310.00426.pdf)使用**T5作为条件**的**DiT**结构，能生成高质量的图像。用PixArt-α对模型初始化，并对**插入的temperal attentioin**用0初始化，能够让模型一开始就保留图片生成的能力。插入的attention让参数量从580M涨到了724M。

![pixart-alpha-temperal](../assets/pixart-alpha-temperal.png)

#### 训练

&nbsp;

参考PixArt-α和Stable Video Diffusioin([Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](https://arxiv.org/pdf/2311.15127.pdf))，采用了progressive的训练策略：

+ 大规模图像预训练：前面提到的，直接使用[https://huggingface.co/stabilityai/sd-vae-ft-mse-original](https://huggingface.co/stabilityai/sd-vae-ft-mse-original)的2D VAE
+ 大规模视频预训练：在质量相对差的366K的预训练数据集(HD-VG-130M数据集)上训练$$16\times 256\times 256$$，这里的16指的是16帧
+ 高质量视频数据微调：在高质量的20K的数据集(Pexels数据集)上训练$$16\times 256\times 256$$、$$16\times 512\times 512$$和$$64\times 512\times 512$$。

由于使用了scaled position embedding，这个策略极大地减少了训练消耗。此外，对于16帧的训练，每3帧降采样一次，对于64帧的训练每2帧降采样一次。

数据标注方法：抽取3帧，然后设计prompt，用LLaVA生成高质量的标题：

![llava-caption](../assets/llava-caption.png)

+ 学习率：1e-4太大了，改成了2e-5
+ **batchsize比较大**的时候，**fp16比bf16更不稳定**，而且可能导致生成错误，所以对于$$64\times 512\times 512$$使用**bf16**

提供了便捷的视频数据预处理脚本，可以轻松地在自己的数据集上快速生成训练所需的视频 / 文本对，包括公开视频数据集下载，长视频根据镜头连续性分割为短视频片段，使用开源LLaVA生成精细的提示词。

## open-sora(北大版)

[超10秒高分辨率，北大Open Sora视频生成更强了，还支持华为芯片](https://mp.weixin.qq.com/s/1GWxp8ENrA1YGGwrFpOAxA)

## MORA

[Sora不开源，微软给你开源！全球最接近Sora视频模型诞生，12秒生成效果逼真炸裂](https://mp.weixin.qq.com/s/GkJwyVFVwxih-ZQWWBIuNg)

[复刻Sora的通用视频生成能力，开源多智能体框架Mora来了](https://mp.weixin.qq.com/s/JbiwVtEuKvIjb8hBi0Laxg)

[Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](https://arxiv.org/pdf/2403.13248.pdf)

## minigpt4-video

[AI视频理解天花板，全新MiniGPT4-Video刷爆SOTA！宝格丽宣传片配文一绝](https://mp.weixin.qq.com/s/Y8w6CqTvm7zVQMOmTuxePA)

## mini-gemini

[刷爆多模态任务榜单！贾佳亚团队Mini-Gemini登热榜，代码、模型、数据全部开源](https://mp.weixin.qq.com/s/j5CGuJ_-Sf0Pqi_-dDjABA)

模型地址：[https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854](https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854)
数据地址：[https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e](https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e)

[Mini-Gemini: Mining the Potential of Multi-modalityVision Language Models](https://arxiv.org/pdf/2403.18814.pdf)

VLM(vision lm)虽然有很多，但和gemini、gpt-4等的差距还是比较大，作者认为主要原因是**高分辨率视觉标记不够**、**vision推理数据质量不高**。

![mini-gemini](../assets/mini-gemini.png)

作者利用**额外的视觉编码器**进行**高分辨率细化**，构建了一个高质量的数据集。构建了一个Mini-Gemini架构，支持一系列从2B到34B的密集和MoE LLM，在zero-shot测试集上超过了私有模型。

[https://github.com/dvlab-research/MiniGemini](https://github.com/dvlab-research/MiniGemini)

## Vidu

[当前最强国产Sora！清华团队突破16秒长视频，懂多镜头语言，会模拟物理规律](https://mp.weixin.qq.com/s/xAEYGIoJ0EzhszfmXno3UA)

采用了和 Sora 完全一致的 Diffusion 和 Transformer 融合的架构，底层基于[All are Worth Words: A ViT Backbone for Diffusion Models](https://arxiv.org/pdf/2209.12152)的 U-ViT 架构。

基于 U-ViT 架构，2023 年 3 月，团队在开源的大规模图文数据集 LAION-5B 上训练了 10 亿参数量的多模态模型 ——UniDiffuser，并将其开源（参见[清华朱军团队开源首个基于 Transformer 的多模态扩散大模型，文图互生、改写全拿下](https://mp.weixin.qq.com/s/B68hXlFxA9L5jiWiMrEEiA)）。

UniDiffuser 主要擅长图文任务，能支持图文模态间的任意生成和转换。UniDiffuser 的实现有一项重要的价值 —— 首次验证了融合架构在大规模训练任务中的可扩展性（Scaling Law），相当于将 U-ViT 架构在大规模训练任务中的所有环节流程都跑通。

这些在图文任务中积累的工程经验为视频模型的研发打下了基础。因为视频本质上是图像的流，相当于是图像在时间轴上做了一个扩增。因此，在图文任务上取得的成果往往能够在视频任务中得到复用。Sora 就是这么做的：它采用了 DALL-E 3 的重标注技术，通过为视觉训练数据生成详细的描述，使模型能够更加准确地遵循用户的文本指令生成视频。这种效应也必然会发生在「Vidu」上面。

根据此前的消息推测，「Vidu」也复用了生数科技在图文任务的很多经验，包括训练加速、并行化训练、低显存训练等等，从而快速跑通了训练流程。据悉，他们通过视频数据压缩技术降低输入数据的序列维度，同时采用自研的分布式训练框架，在保证计算精度的同时，通信效率提升 1 倍，显存开销降低 80%，训练速度累计提升 40 倍。

## gen-3

[Runway版Sora发布：高保真、超强一致性，Gen-3 Alpha震撼到网友了](https://mp.weixin.qq.com/s/uuLub-ruJgYYrTOFoNJ5iw)

## 可灵

[快手「可灵」爆火：海外AI圈巨震，中国版Sora一号难求](https://mp.weixin.qq.com/s/iSAvV3PX1WYwGg7rU60Ong)


## V2A

[杀疯了！谷歌卷视频到语音，逼真音效让AI视频告别无声！](https://mp.weixin.qq.com/s/0D4QGeyZ0ZnmmWYz_x-56g)

[https://deepmind.google/discover/blog/generating-audio-for-video/](https://deepmind.google/discover/blog/generating-audio-for-video/)

## 长视频LongVA

[7B最强长视频模型！ LongVA视频理解超千帧，霸榜多个榜单](https://mp.weixin.qq.com/s/62rMYx94dbz1HwDkZclXtA)

[Long Context Transfer from Language to Vision](https://arxiv.org/pdf/2406.16852)

## LONGVILA

[支持1024帧、准确率近100％，英伟达「LongVILA」开始发力长视频](https://mp.weixin.qq.com/s/T6eMi3DPq9_291bWqcFRgw)

[LONGVILA: SCALING LONG-CONTEXT VISUAL LANGUAGE MODELS FOR LONG VIDEOS](https://arxiv.org/pdf/2408.10188)

[https://github.com/NVlabs/VILA/blob/main/LongVILA.md](https://github.com/NVlabs/VILA/blob/main/LongVILA.md)

## liveportrait

[快手开源LivePortrait，GitHub 6.6K Star，实现表情姿态极速迁移](https://mp.weixin.qq.com/s/JrKF_7To8PEggEfw7W09ew)

[LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control](https://arxiv.org/pdf/2407.03168)

[https://github.com/KwaiVGI/LivePortrait](https://github.com/KwaiVGI/LivePortrait)

## diffusion forcing

[无限生成视频，还能规划决策，扩散强制整合下一token预测与全序列扩散](https://mp.weixin.qq.com/s/kz4RvqdK6nGtA11y5nq5xQ)

[Diffusion Forcing:Next-token Prediction Meets Full-Sequence Diffusion](https://arxiv.org/pdf/2407.01392)

[https://github.com/buoyancy99/diffusion-forcing](https://github.com/buoyancy99/diffusion-forcing)

## VideoSys

[视频生成要有自己的系统！尤洋团队历时半年开源VideoSys](https://mp.weixin.qq.com/s/Q-AHzIOT0PBP6Yvdk_T3Sg)

[https://github.com/NUS-HPC-AI-Lab/VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys)

### Pyramid Attention Broadcast (PAB)


[Real-Time Video Generation with Pyramid Attention Broadcast](https://arxiv.org/abs/2408.12588)


[https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/pab.md](https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/pab.md)


### Dyanmic Sequence Parallelism（DSP）


[DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers](https://arxiv.org/abs/2403.10266)

[https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/dsp.md](https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/dsp.md)

## GameNGen

[扩散模型做游戏引擎，单TPU 20 FPS模拟毁灭战士，谷歌最新GameNGen太博眼球了](https://mp.weixin.qq.com/s/LvjhY9Gzd_lnE3M3MllKDA)

## Firefly

[厉害了！Adobe新出Firefly视频模型，2分钟速成高清大片](https://mp.weixin.qq.com/s/uFKQNGuoZ2bS4Ea0pye71Q)

[https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon](https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon)


## MovieGen

[Meta又给OpenAI一记重击，视频生成Movie Gen震撼登场，甚至可以配音、编辑](https://mp.weixin.qq.com/s/c8_sXLRkwEVvg_LKCPQHKw)

[Meta版Sora无预警来袭！抛弃扩散损失，音视频生成/画面编辑全包，92页论文无保留公开](https://mp.weixin.qq.com/s/rs7JQigqHO9yT_0wbF6cTg)

[MovieGen: A Cast of Media Foundation Models](https://ai.meta.com/static-resource/movie-gen-research-paper)

## EMOVA

[mini-GPT4o来了? 能看、能听、会说，还情感丰富的多模态全能助手EMOVA](https://mp.weixin.qq.com/s/e2KkDjqbWNy7wSv0geCNUg)

[EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotion](https://arxiv.org/abs/2409.18042)


## 其他

[扩散模型与文生视频](https://mp.weixin.qq.com/s/Bh3Gg7FCDpb_AmGEFkxQ2A)

[多模态大模型不够灵活，谷歌DeepMind创新架构Zipper：分开训练再「压缩」](https://mp.weixin.qq.com/s/F8wstkJyYiNJCbSqYq3Pbw)

[Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities](https://arxiv.org/pdf/2405.18669)

# LLM+推荐

## 综述

[https://github.com/nancheng58/Awesome-LLM4RS-Papers](https://github.com/nancheng58/Awesome-LLM4RS-Papers)

[A Survey on Large Language Models for Recommendation](https://arxiv.org/pdf/2305.19860.pdf)
[How Can Recommender Systems Benefit from Large Language Models: A Survey](https://arxiv.org/pdf/2306.05817.pdf)
[Recommender Systems in the Era of Large Language Models (LLMs)](https://arxiv.org/pdf/2307.02046.pdf)



中科大LDS实验室的tutorial：[XadC3O-large-language-models-for-recommendation-tutorial-slides.pdf](https://github.com/daiwk/collections/blob/master/assets/XadC3O-large-language-models-for-recommendation-tutorial-slides.pdf)

对应的datafun talk：[当"狂飙"的大模型撞上推荐系统](https://mp.weixin.qq.com/s/rBGq7rDMK5Vxad5qUmK3nw)

[大模型放进推荐系统怎么玩？微软亚研全面总结](https://mp.weixin.qq.com/s/x0qczJ8I8LZ_PyZw2HOI8A)

生成式推荐综述：

[Large Language Models for Generative Recommendation: A Survey and Visionary Discussions](https://arxiv.org/pdf/2309.01157.pdf)

[A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://arxiv.org/pdf/2404.00579.pdf)

[大语言模型在推荐系统中的探索与应用](https://mp.weixin.qq.com/s/nEhynptVyx8aV8onCwozAA)

### 概况

CRM(conventional recommendation models)

![llm-rec-where](../assets/llm-rec-where.png)

![llm-rec-how](../assets/llm-rec-how.png)

### where-LLM for feature engineering

#### user-item level feature augmentation

通过LLM的世界知识来获取更好的user/item表示

+ KAR：[Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models](https://arxiv.org/pdf/2306.10933)通过LLM生成user侧的preference知识，和item侧的factual知识，并作为CRM的输入特征。代码：[https://github.com/YunjiaXi/Open-World-Knowledge-Augmented-Recommendation/blob/main/knowledge_encoding/utils.py](https://github.com/YunjiaXi/Open-World-Knowledge-Augmented-Recommendation/blob/main/knowledge_encoding/utils.py)，其实就是把模型输出的hidden states处理一下：

```python

x = tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors="pt",
                return_attention_mask=True).to(device)
mask = x['attention_mask']
outputs = model(**x, output_hidden_states=True, return_dict=True)
pred = get_paragraph_representation(outputs, mask, aggregate_type)
def get_paragraph_representation(outputs, mask, pooler='cls', dim=1):
    last_hidden = outputs.last_hidden_state
    hidden_states = outputs.hidden_states

    # Apply different poolers

    if pooler == 'cls':
        # There is a linear+activation layer after CLS representation
        return outputs.pooler_output.cpu()  # chatglm不能用，用于bert
    elif pooler == 'cls_before_pooler':
        return last_hidden[:, 0].cpu()
    elif pooler == "avg":
        return ((last_hidden * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)).cpu()
    elif pooler == "avg_first_last":
        first_hidden = hidden_states[1]
        last_hidden = hidden_states[-1]
        pooled_result = ((first_hidden + last_hidden) / 2.0 * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)
        return pooled_result.cpu()
    elif pooler == "avg_top2":
        second_last_hidden = hidden_states[-2]
        last_hidden = hidden_states[-1]
        pooled_result = ((last_hidden + second_last_hidden) / 2.0 * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)
        return pooled_result.cpu()
    elif pooler == 'len_last':  # 根据padding方式last方式也不一样
        lens = mask.unsqueeze(-1).sum(dim)
        # index = torch.arange(last_hidden.shape[0])
        # print(index)
        pooled_result = [last_hidden[i, lens[i] - 1, :] for i in range(last_hidden.shape[0])]
        pooled_result = torch.concat(pooled_result, dim=0)
        return pooled_result.cpu()
    elif pooler == 'last':
        if dim == 0:
            return last_hidden[-1, :, :]
        else:
            return last_hidden[:, -1, :]
    elif pooler == 'wavg':
        # Get weights of shape [bs, seq_len, hid_dim]
        weights = (
            torch.arange(start=1, end=last_hidden.shape[1] + 1)
            .unsqueeze(0)
            .unsqueeze(-1)
            .expand(last_hidden.size())
            .float().to(last_hidden.device)
        )

        # Get attn mask of shape [bs, seq_len, hid_dim]
        input_mask_expanded = (
            mask
            .unsqueeze(-1)
            .expand(last_hidden.size())
            .float()
        )

        # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim
        sum_embeddings = torch.sum(last_hidden * input_mask_expanded * weights, dim=dim)
        sum_mask = torch.sum(input_mask_expanded * weights, dim=dim)

        pooled_result = sum_embeddings / sum_mask
        return pooled_result.cpu()
    else:
        raise NotImplementedError
```

#### instance-level sample generation



### where-LLM as feature encoder

### where-LLM for scoring/ranking function

[Text Is All You Need: Learning Language Representations for Sequential Recommendation](https://arxiv.org/pdf/2305.13731) 亚马逊发的

[https://github.com/AaronHeee/RecFormer](https://github.com/AaronHeee/RecFormer)

### where-LLM for pipeline controller


### how-tune LLM & infer with CRM

### how-not tune LLM & infer w/o CRM

### how-not tune LLM & infer with CRM

### how-tune LLM & infer w/o CRM




## DSI

参考知乎：[https://zhuanlan.zhihu.com/p/470182510](https://zhuanlan.zhihu.com/p/470182510)

[Transformer memory as a differentiable search index](https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf)，提出的可微搜索索引（differentiable search index, **DSI**），拆成两个阶段：

+ indexing：建立**文档**和**doc_id**的**一一映射**
+ retrieval：根据query生成候选doc_ids

与[Autoregressive Entity Retrieval](https://arxiv.org/pdf/2010.00904)提出的**受限beam search**的GENRE（代码[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)，解读[Transformer中PrefixConstrainedLogitsProcessor类的解读](https://zhuanlan.zhihu.com/p/494082642)）对比：

+ GENRE生成的目标是**有具体语义的实体名**
+ DSI生成的目标则是**无任何语义的任意doc_id**

### Indexing方法

+ Inputs2Target：即doc_tokens->doc_id
+ Target2Inputs：即doc_id->doc_tokens
+ bidirectional：同时用上两面两个任务，并且在开始时加一个前缀，表明任务的方向
+ span corruption：参考T5（[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)），将doc_id当做前缀和doc_tokens拼起来，2个好处：
    + 在索引时进行通用的预训练
    + 实现doc_id作为去噪目标和输入的平衡

### 文档的表示

+ Direct Indexing：直接用文档的前L个单词当做文档的表示，并且保留单词的顺序。
+ Set Indexing：去掉文档中的重复的单词及停用词，然后和Direct Indexing一样的处理。
+ Inverted Index：随机对文档中的连续的k个单词（一个文档块）进行采样，并将它们与doc_id相关联

### doc_id的表示

+ 非结构化的原子标识符：直接对所有的文档使用一个随机但互不相同的的整数标识。假设一共有N篇文档需要检索，假设原来解码器输出有V个单词，现在有V+N个单词。
+ 朴素的结构化字符串标识符：也使用一个随机的整数，但将这个整数当做一个字符串输出，用受限的beam search搜索前k个文档，因为需要保证**输出的是数字**。
+ 语义结构化的标识符：先用BERT产出每个doc的emb，然后**递归10-means**，第一次得到0-9作为第1个数字，第二次的0-9作为第2个数字，可以得到一个树，文档的最终标识符就是从根节点到当前结点的路径对应的编号组合。只要一个节点的文档数大于$$c=100$$，就继续分裂，当叶子的文档数小于c时，每个文档随机分配一个1到$$c-1$$的id，拼到doc_id的最后。

### 训练方法

&nbsp;

用seq2seq，即**teacher forcing+交叉熵**，有如下两种方式：

+ 先对indexing进行预训练（memorization），再进行**将query映射为docid**的微调
+ 用多任务的方式**同时进行**，两个任务用**不同的标识符**，这样做效果会好很多

后来有一篇[Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation](https://arxiv.org/pdf/2206.10128.pdf)，对应的github：[https://github.com/ArvinZhuang/DSI-QG](https://github.com/ArvinZhuang/DSI-QG)，他也同时尝试复现DSI：[https://github.com/ArvinZhuang/DSI-transformers](https://github.com/ArvinZhuang/DSI-transformers)，主要有如下两个地方：

+ 准备数据，doc_id

```python
class IndexingTrainDataset(Dataset):
    def __init__(
            self,
            path_to_data,
            max_length: int,
            cache_dir: str,
            tokenizer: PreTrainedTokenizer,
    ):
        self.train_data = datasets.load_dataset(
            'json',
            data_files=path_to_data,
            ignore_verifications=False,
            cache_dir=cache_dir
        )['train']

        self.max_length = max_length
        self.tokenizer = tokenizer
        self.total_len = len(self.train_data)


    def __len__(self):
        return self.total_len

    def __getitem__(self, item):
        data = self.train_data[item]

        input_ids = self.tokenizer(data['text'],
                                   return_tensors="pt",
                                   truncation='only_first',
                                   max_length=self.max_length).input_ids[0]
        return input_ids, str(data['text_id'])


@dataclass
class IndexingCollator(DataCollatorWithPadding):
    def __call__(self, features):
        input_ids = [{'input_ids': x[0]} for x in features]
        docids = [x[1] for x in features]
        inputs = super().__call__(input_ids)

        # label是doc_id
        labels = self.tokenizer(
            docids, padding="longest", return_tensors="pt"
        ).input_ids

        # replace padding token id's of the labels by -100 
        # according to https://huggingface.co/docs/transformers/model_doc/t5#training
        labels[labels == self.tokenizer.pad_token_id] = -100
        inputs['labels'] = labels
        return inputs

```

+ 训练和预测

```python
class IndexingTrainer(Trainer):
    def __init__(self, restrict_decode_vocab, **kwds):
        super().__init__(**kwds)
        self.restrict_decode_vocab = restrict_decode_vocab

    def compute_loss(self, model, inputs, return_outputs=False):
        ## 输入文章，预测doc_id
        loss = model(input_ids=inputs['input_ids'], 
            attention_mask=inputs['attention_mask'], labels=inputs['labels']).loss
        if return_outputs:
            return loss, [None, None]  # fake outputs
        return loss

    def prediction_step(
            self,
            model: nn.Module,
            inputs: Dict[str, Union[torch.Tensor, Any]],
            prediction_loss_only: bool,
            ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        model.eval()
        # eval_loss = super().prediction_step(model, inputs, True, ignore_keys)[0]
        with torch.no_grad():
            # greedy search
            doc_ids = model.generate(
                inputs['input_ids'].to(self.args.device),
                max_length=20,
                prefix_allowed_tokens_fn=self.restrict_decode_vocab,
                early_stopping=True,)
        return (None, doc_ids, inputs['labels'])
```

+ 对解码空间的限制：

```python
    # docid generation constrain, we only generate integer docids.
    SPIECE_UNDERLINE = "_"
    INT_TOKEN_IDS = []
    for token, id in tokenizer.get_vocab().items():
        if token[0] == SPIECE_UNDERLINE:
            if token[1:].isdigit():
                INT_TOKEN_IDS.append(id)
        if token == SPIECE_UNDERLINE:
            INT_TOKEN_IDS.append(id)
        elif token.isdigit():
            INT_TOKEN_IDS.append(id)
    INT_TOKEN_IDS.append(tokenizer.eos_token_id)

    def restrict_decode_vocab(batch_idx, prefix_beam):
        return INT_TOKEN_IDS
```

## vq-vae & rq-vae

### vq-vae

[Neural discrete representation learning](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)

[https://github.com/zalandoresearch/pytorch-vq-vae](https://github.com/zalandoresearch/pytorch-vq-vae)

用在推荐：

[Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders](https://arxiv.org/pdf/2210.12316.pdf)

[https://github.com/RUCAIBox/VQ-Rec](https://github.com/RUCAIBox/VQ-Rec)

### rq-vae

[Autoregressive Image Generation using Residual Quantization](https://arxiv.org/pdf/2203.01941.pdf)


## TIGER

[Recommender Systems with Generative Retrieval](https://arxiv.org/pdf/2305.05065.pdf)

序列推荐的一些paper：

+ [Session-based recommendations with recurrent neural networks](https://arxiv.org/pdf/1511.06939.pdf)：GRU4Rec首先把RNN用到推荐里
+ [Neural attentive session-based recommendation](https://arxiv.org/pdf/1711.04725.pdf)：提出NARM(Neural Attentive Session-based Recommendation)，在GRU里加了attention
+ [Next item recommendation with self-attention](https://arxiv.org/pdf/1808.06414.pdf)：AttRec在metric learning里引入了self-attention
+ [Self-attentive sequential recommendation](https://arxiv.org/pdf/1808.09781.pdf)：SASRec用了类似decoder-only的self-attention
+ [Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer](https://arxiv.org/pdf/1904.06690.pdf)和[Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/246721374_422204999475172_9039387325224382577_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=e280be&_nc_ohc=pSC1QwlGTzIAX-yb9Ax&_nc_ht=scontent-sjc3-1.xx&oh=00_AfApZwRSd9KDBf4b-uGHf3bSS_SsZoRqhYS-lJRZltT97A&oe=6606DDFA)用了Transformer以及相关的mask策略
+ [S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization](https://arxiv.org/pdf/2008.07873.pdf)：在预训练阶段引入了4个自监督的task，4个MIMM(mutual information maximization)
  + item-attribute MIM
  + sequence-item MIM：sequence和被mask掉的一个item间
  + sequence-attribute MIM
  + sequence-sequence MIM：sequence和被mask掉的连续item构成的片段间

![s3-rec](../assets/s3-rec.png)

这些方法都是学习item的向量，然后用MIPS去ANN，而TIGER(Transformer Index for GEnerative Recommenders)则是生成式地直接预测item的语义id

![tiger](../assets/tiger.png)

![tiger-quant](../assets/tiger-quant.png)

![tiger-personalized](../assets/tiger-personalized.jpg)

rq-vae的介绍：[](https://arxiv.org/pdf/2306.08121)

![rq-vae](../assets/rq-vae.png)

原图里的下标有一些问题

+ $$x$$经过encoder得到的$$z$$（可以看成就是$$r_1$$），$$l=1$$，在第一个码本里ann找到最近的$$c_1=1$$，$$z-\boldsymbol{e}_{c_1}$$得到$$r_2$$
+ $$l=2$$，$$r_2$$在第2个码本里ann找到最近的$$c_2=4$$，$$r_2-\boldsymbol{e}_{c_2}$$得到$$r_3$$
+ $$l=3$$，$$r_3$$在第3个码本里ann找到最近的$$c_3=6$$，$$r_3-\boldsymbol{e}_{c_3}$$得到$$r_4$$
+ $$l=4$$，$$r_4$$在第4个码本里ann找到最近的$$c_4=2$$，$$r_4-\boldsymbol{e}_{c_4}$$得到$$r_5$$，
+ 对应的semantic id就是(1,4,6,2)，拿$$\hat {z} = 1+4+6+2$$再去过decoder得到$$\hat {x}$$

最终的loss：$$\mathcal{L}_{\text {recon }} + \mathcal{L}_{\text {rqvae }}$$：

+ $$\mathcal{L}_{\text {recon }}=\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^2$$
+ $$\mathcal{L}_{\text {rqvae }}=\sum_{l=1}^L \beta\left\|\boldsymbol{r}_l-\operatorname{sg}\left[\boldsymbol{e}_{c_l}\right]\right\|^2+\left\|\operatorname{sg}\left[\boldsymbol{r}_l\right]-\boldsymbol{e}_{c_l}\right\|^2$$，sg是stop gradient，这里原始的rq-vae是算的$$\mathcal{L}_{\text {commit }}=\sum_{d=1}^D\left\|\mathbf{Z}-\operatorname{sg}\left[\hat{\mathbf{Z}}^{(d)}\right]\right\|_2^2$$看来有点diff
+ 码本的emb通过moving average更新


rqvae的代码：[https://github.com/kakaobrain/rq-vae-transformer/blob/main/rqvae/models/rqvae/quantizations.py#L237](https://github.com/kakaobrain/rq-vae-transformer/blob/main/rqvae/models/rqvae/quantizations.py#L237)

+ VQ部分：

```python
class VQEmbedding(nn.Embedding):
    def compute_distances(self, inputs):
        codebook_t = self.weight[:-1, :].t()

        (embed_dim, _) = codebook_t.shape
        inputs_shape = inputs.shape
        assert inputs_shape[-1] == embed_dim

        inputs_flat = inputs.reshape(-1, embed_dim)
        # a^2
        inputs_norm_sq = inputs_flat.pow(2.).sum(dim=1, keepdim=True)
        # b^2
        codebook_t_norm_sq = codebook_t.pow(2.).sum(dim=0, keepdim=True)
        # (a-b)^2 = a^2 + b^2 - 2ab
        distances = torch.addmm(
            inputs_norm_sq + codebook_t_norm_sq,
            inputs_flat,
            codebook_t,
            alpha=-2.0,
        )
        distances = distances.reshape(*inputs_shape[:-1], -1)  # [B, h, w, n_embed or n_embed+1]
        return distances

    @torch.no_grad()
    def find_nearest_embedding(self, inputs):
        distances = self.compute_distances(inputs)  # [B, h, w, n_embed or n_embed+1]
        embed_idxs = distances.argmin(dim=-1)  # use padding index or not

        return embed_idxs

    @torch.no_grad()
    def _update_embedding(self):

        n_embed = self.weight.shape[0] - 1
        n = self.cluster_size_ema.sum()
        normalized_cluster_size = (
            n * (self.cluster_size_ema + self.eps) / (n + n_embed * self.eps)
        )
        self.weight[:-1, :] = self.embed_ema / normalized_cluster_size.reshape(-1, 1)

    def forward(self, inputs):
        embed_idxs = self.find_nearest_embedding(inputs)
        if self.training:
            if self.ema:
                self._update_buffers(inputs, embed_idxs)
        
        embeds = self.embed(embed_idxs)

        if self.ema and self.training:
            self._update_embedding()

        return embeds, embed_idxs
```

+ RQ部分

```python
class RQBottleneck(nn.Module):
    def __init__(...):
        codebooks = [VQEmbedding(self.n_embed[idx], 
                                    embed_dim, 
                                    decay=self.decay[idx], 
                                    restart_unused_codes=restart_unused_codes,
                                    ) for idx in range(self.code_shape[-1])]
        self.codebooks = nn.ModuleList(codebooks)
    def quantize(self, x):
        r"""
        Return list of quantized features and the selected codewords by the residual quantization.
        The code is selected by the residuals between x and quantized features by the previous codebooks.

        Arguments:
            x (Tensor): bottleneck feature maps to quantize.

        Returns:
            quant_list (list): list of sequentially aggregated and quantized feature maps by codebooks.
            codes (LongTensor): codewords index, corresponding to quants.

        Shape:
            - x: (B, h, w, embed_dim)
            - quant_list[i]: (B, h, w, embed_dim)
            - codes: (B, h, w, d)
        """
        B, h, w, embed_dim = x.shape

        residual_feature = x.detach().clone()

        quant_list = []
        code_list = []
        aggregated_quants = torch.zeros_like(x)
        for i in range(self.code_shape[-1]):
            quant, code = self.codebooks[i](residual_feature)
            # 就地操作，从residual_feature中减去quant的值，覆盖原来的值
            residual_feature.sub_(quant)
            aggregated_quants.add_(quant)

            quant_list.append(aggregated_quants.clone())
            code_list.append(code.unsqueeze(-1))
        
        codes = torch.cat(code_list, dim=-1)
        return quant_list, codes
    def forward(self, x):
        x_reshaped = self.to_code_shape(x)
        quant_list, codes = self.quantize(x_reshaped)

        commitment_loss = self.compute_commitment_loss(x_reshaped, quant_list)
        quants_trunc = self.to_latent_shape(quant_list[-1])
        quants_trunc = x + (quants_trunc - x).detach()

        return quants_trunc, commitment_loss, codes
    
    def compute_commitment_loss(self, x, quant_list):
        r"""
        Compute the commitment loss for the residual quantization.
        The loss is iteratively computed by aggregating quantized features.
        """
        loss_list = []
        
        for idx, quant in enumerate(quant_list):
            partial_loss = (x-quant.detach()).pow(2.0).mean()
            loss_list.append(partial_loss)
        
        commitment_loss = torch.mean(torch.stack(loss_list))
        return commitment_loss
```

## HSTU

[如何评价Meta最新推荐论文: 生成式推荐打败深度分层架构推荐？](https://mp.weixin.qq.com/s/MeH2drBIBML5OSfCujIJ0Q)

[Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/pdf/2402.17152.pdf)

### 背景

&nbsp;

大规模推荐系统依赖高基数(high cardinality)、异质特性(heterogeneous features)，每天要处理上百亿用户行为数据。将推荐系统重新定义为生成模型框架内的序列转化任务，提出HSTU(Hierarchical Sequential Transduction Unit)，专为高基数、非稳态的流式推荐数据设计。

+ 在公开数据集比基线NDCG+65.8%
+ 在8192的序列长度上的处理速度比基于flash attention2的transformer快5.3-15.2倍。
+ 1.5万亿（1.5 trillion）参数，线上ab测试指标+12.4%

在[Breaking the curse of quality saturation with user-centric ranking](https://arxiv.org/pdf/2305.15333.pdf)中提到了NCE（normalized cross-entropy）指标：

XXX
\operatorname{NCE}(p, y)=\frac{\operatorname{CrossEntropy}(p, y)}{\operatorname{Entropy}(y)}
XXX

![nce-metric](../assets/nce-metric.png)

可以发现，对于一个预训练好的模型来说，随着时间的变化，NCE的变化是很剧烈的，即数据分布是会漂移的，并不像nlp/cv一样有ground truth，所以传统推荐模型其实很难达到比较好的scaling能力。

![nce-distribution-drift](../assets/nce-distribution-drift.png)

需要克服的3个挑战：

+ 推荐系统中的特征**缺少显式的结构**。序列建模（bert4rec、S3-rec等）在小规模数据集上效果不错，但工业界则需要**异构特征**（高基数的id、交叉特征、统计特征、历史点击率等）。
+ 推荐系统使用**十亿规模的动态词表**，而nlp用的是10w的静态词表，要对上万候选进行target-aware操作（din、Cold等），训练和推理的代价很高。
+ 大规模序列模型的计算成本是瓶颈。GPT-3在300B token的数据集上用上千个GPU训练1-2个月，而在推荐场景一天就有十亿级的活跃用户和十亿级的候选进行交互，用户序列在极端情况下有近10w（[Twin: Two- stage interest network for lifelong user behavior modeling in ctr prediction at kuaishou](https://arxiv.org/pdf/2302.02352.pdf)），所以推荐系统每天要处理的tokens数量甚至比语言模型1-2个月处理的数量要大几个数量级

本文将**用户action**看成一个新的模态，2个主要的insights：

+ 给定一个新的特征空间，核心的召回排序任务能被直接转换成生成式模型问题(序列直推任务，sequential transduction tasks)
+ 这种范式能够系统性地解决传统推荐中的特征冗余、计算冗余、推理冗余，提升效率

### Sequential Transduction Tasks

&nbsp;

transductive learning（直推式学习） vs inductive learning（归纳式学习）[https://www.zhihu.com/question/68275921/answer/529156908](https://www.zhihu.com/question/68275921/answer/529156908)：

+ 归纳式学习：训练只使用训练集，不使用测试集，训出的模型对测试集做预测，如监督学习。
+ 直推式学习：训练时使用训练集，还使用测试集的特征，但不使用测试集的label，

#### 异构特征的统一表示

&nbsp;

+ 稀疏特征：itemid、类目、城市、语言、社区等
    + 先选出**最长**的时间序列作为**主时间序列**，例如用户的交互序列。
    + 剩下的特征随时间变化较慢，如关注作者的属性。对于连续出现的片段（consecutive segment），**只保留最开始的入口**，这样对于主时间序列而言，并不会增加太多的序列长度。
+ 数值型特征：这里指的是序列特征里**每个item的统计特征**，比如用户在时刻t对某个item的ctr。直接删了，因为DIN里提到随着序列长度增加，**target-aware**的序列建模方式能够捕捉到这种数值性特征

![hstu-dlrm-vs-gr](../assets/hstu-dlrm-vs-gr.png)

假设用户消费了9个item，

+ 绿色的有7个时间步，全保留，作为主序列；
+ 蓝色的有7个时间步，但只有G0和G1两种取值，所以对于连续的G0只保留第0个(出现在t1)，扔到主序列的最前面去，连续的G1也只保留第0个（出现在t8），插到主序列最后一个的前面
+ 黄色的全是H0，第0个出现在t7，所以保留t7，往主序列t8前面插入
+ 将数值型特征替换为target-aware的cross attention得到causal-masked的特征
+ 通过t0、t1、t2（**包括t2**）的特征生成t2的样本，以此类推

#### 召回和排序的重定义

&nbsp;

输入token序列$$x_0, x_1, \ldots, x_{n-1}$$，输出的token序列$$y_0, y_1, \ldots, y_{n-1}$$是通过mask序列$$m_0, m_1, \ldots, m_{n-1}\left(m_i \in\{0,1\}\right)$$得到的。

token对应的动态、非稳态词表是$$\mathbb{X}$$，用户交互的内容是$$\mathbb{X}_c \subseteq \mathbb{X}$$

+ 召回：预估$$p\left(x_{i+1} \mid u_i\right)$$，通过时间步i的用户特征预估$$x_{i+1} \in \mathbb{X}_c$$，一般是直接选择$$\arg \max _{x \in \mathbb{X}_c} p\left(x \mid u_i\right)$$来最大化**特定的reward**，和标准的自回归有两个不同：
    + $$x_i,y_i$$的label并不一定是$$x_{i+1}$$，因为用户可以对$$x_{i+1}$$是负反馈
    + $$y_i$$可能是比如人口属性等（因为是merge的序列，可能把城市之类的merge进来当做一个时间步），这个时候$$y_i$$未定义的，要把它mask掉，即$$m_i=0$$
+ 排序：推荐中的排序需要在尽量早的阶段进行target-aware的交互，而标准的自回归这种交互往往比较迟，例如在encoder的输出才用上了softmax。因此，设计了一种**target-aware的cross-attention**
    + 把action和x穿插起来得到新序列$$x_0, a_0, x_1, a_1, \ldots, x_{n-1}, a_{n-1}$$
        + 对于action的位置，$$m_i=0$$
        + 对于content位置，使用一个小的nn将预估值转换成多任务的预估值（**感觉是输入0-n，第n项保留$$x_n$$，mask掉$$a_n$$，拿$$x_n$$去多目标地预估用户会用哪个action**）

#### 生成式训练

&nbsp;

假设用户$$i$$有$$n_i$$个token，那么训练的复杂度就是$$\sum_i n_i\left(n_i^2 d+n_i d_{f f} d\right)$$，其中：

+ $$n_i^2 d$$是self-attention的复杂度，通过flash attention可以达到$$O(n^2)$$
+ $$n_i d_{f f} d$$是FFN的复杂度
+ 因为要自回归地算，可以理解为batch_size也是$$n_i$$，加一个下三角的mask，所以在$$\sum$$里还要乘一个$$n_i$$

假设$$N=\max _i n_i$$，那复杂度就是$$O\left(N^3 d+N^2 d^2\right)$$，太巨大了

但其实可以发现（这段是自己的理解），假设序列长度9，要预测第4个的时候，4-9的输入是mask掉的，但他们还是要进行后面的attention+ffn计算，其实是很浪费资源的，所以如上面“**异构特征的统一表示**”小节的那个图（生成主序列和辅助序列）所示，该模型直接只吐出x=x1-x3,y=x4作为训练样本就行了，这样既省掉了4-9的无用计算，也更便于并行（相比不拆batch的自回归）

假设采样第$$i$$个用户的概率是$$s_u\left(n_i\right)$$，那么总的训练消耗就是

XXX
\sum_i s_u\left(n_i\right) n_i\left(n_i^2 d+n_i d^2\right)
XXX

如果设置$$s_u\left(n_i\right)=1/n_i$$，那消耗就降到了$$O\left(N^2 d+N d^2\right)$$。而在工业界中要实现这种采样其实很简单，在用户请求或者session结束的时候吐出训练样本就有$$\hat{s_u}\left(n_i\right) \propto 1 / n_i$$了

### 生成式推荐中的高性能自注意力编码器

&nbsp;

![hstu-arch](../assets/hstu-arch.png)

HSTU的单层包括3个部分：

XXX
\text{pointwise\ projection:} U(X), V(X), Q(X), K(X)=\operatorname{Split}\left(\phi_1\left(f_1(X)\right)\right)
XXX

XXX
\text{spatial\ aggregation:} A(X) V(X)=\phi_2\left(Q(X) K(X)^T+\operatorname{rab}^{p, t}\right) V(X)
XXX

XXX
\text{pointwise\ transformation:} Y(X)=f_2(\operatorname{Norm}(A(X) V(X)) \odot U(X))
XXX

其中，

+ $$f_i(x)$$是MLP，$$f_i(X)=W_i(X)+b_i$$
+ $$\phi_1$$和$$\phi_2$$是激活函数，都是SiLU([Sigmoid-weighted linear units for neural network function approximation in reinforcement learning](https://arxiv.org/pdf/1702.03118.pdf))，其实就是Swish，即$$f(x)=x \cdot sigmoid(x)$$
+ $${rab}^{p, t}$$是relative attention bias，考虑了位置$$p$$和时间$$t$$(参考T5论文，[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf))，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的

对应到代码([https://github.com/facebookresearch/generative-recommenders/blob/main/modeling/sequential/hstu.py](https://github.com/facebookresearch/generative-recommenders/blob/main/modeling/sequential/hstu.py))里：

```python
self._linear_dim: int = linear_hidden_dim
self._uvqk = torch.nn.Parameter(
    torch.empty((embedding_dim, linear_hidden_dim * 2 * num_heads + 
        attention_dim * num_heads * 2)).normal_(mean=0, std=0.02),
)

##...

batched_mm_output = torch.mm(normed_x, self._uvqk)
if self._linear_activation == "silu":
    batched_mm_output = F.silu(batched_mm_output)
elif self._linear_activation == "none":
    batched_mm_output = batched_mm_output
# 其实就是先乘一个大矩阵，再拆成4份，等价于乘4个小矩阵
u, v, q, k = torch.split(
    batched_mm_output,
    [self._linear_dim * self._num_heads, self._linear_dim * self._num_heads, 
    self._attention_dim * self._num_heads, self._attention_dim * self._num_heads],
    dim=1,
)

if self._normalization == "rel_bias" or self._normalization == "hstu_rel_bias":
    if delta_x_offsets is not None:
        padded_q, padded_k = cached_q, cached_k
        flattened_offsets = delta_x_offsets[1] + torch.arange(start=0, end=B * n, 
            step=n, device=delta_x_offsets[1].device, dtype=delta_x_offsets[1].dtype)
        padded_q = padded_q.view(B * n, -1).index_copy_(
            dim=0, index=flattened_offsets, source=q,
        ).view(B, n, -1)
        padded_k = padded_k.view(B * n, -1).index_copy_(
            dim=0, index=flattened_offsets, source=k,
        ).view(B, n, -1)
    else:
        padded_q = torch.ops.fbgemm.jagged_to_padded_dense(
            values=q, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )
        padded_k = torch.ops.fbgemm.jagged_to_padded_dense(
            values=k, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )

    qk_attn = torch.einsum(
        "bnhd,bmhd->bhnm",
        padded_q.view(B, n, self._num_heads, self._attention_dim),
        padded_k.view(B, n, self._num_heads, self._attention_dim),
    )
    if all_timestamps is not None:
        qk_attn = qk_attn + self._rel_attn_bias(all_timestamps).unsqueeze(1)
    qk_attn = F.silu(qk_attn) / n
    qk_attn = qk_attn * invalid_attn_mask.unsqueeze(0).unsqueeze(0)
    attn_output = torch.ops.fbgemm.dense_to_jagged(
        torch.einsum(
            "bhnm,bmhd->bnhd",
            qk_attn,
            torch.ops.fbgemm.jagged_to_padded_dense(v, [x_offsets], [n]).\
                reshape(B, n, self._num_heads, self._linear_dim)
        ).reshape(B, n, self._num_heads * self._linear_dim),
        [x_offsets],
    )[0]

```

DLRM的3部分：

+ 特征抽取：常见的基础版本是离散特征pooling，高级版本是din。HSTU本来就能做这种target-aware的attention
+ 特征交互：常见的使用FMs、DCNv2、DHEN([DHEN: A Deep and Hierarchical Ensemble Network for Large-Scale Click-Through Rate Prediction](https://arxiv.org/pdf/2203.11014.pdf)，引入残差连接)。HSTU通过$$\operatorname{Norm}(A(X) V(X)) \odot U(X)$$来实现，将attention pooled后的结果直接和其他特征算element-wise product。
    + 这个做法受[Neural collaborative filtering vs. matrix factorization revisited](https://arxiv.org/pdf/2005.09683.pdf)和[Revisiting neural retrieval on accelerators](https://arxiv.org/pdf/2306.04039.pdf)的启发，用MLP来近似点积是很困难的。原因大概是nn需要调超参和足够的训练数据，在线算得又慢，而且本来内积效果就不错了，nn能带来的边际收益其实不明确。
    + 因为$$U(X)$$已经用过SiLU了，所以$$\operatorname{Norm}(A(X) V(X)) \odot U(X)$$可以看成是SwiGLU的变种（参考[Glu variants improve transformer](https://arxiv.org/pdf/2002.05202.pdf)），因为前面在对比LLM激活函数时讲了，$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \odot \mathbf{x}_2$$
+ 表示转换：常见的如MoE、PLE等，主要思想就是对不同人群用特定的子网络。HSTU里的element-wise product也能达到MoE中的门控操作，只是可能有一个正则化因子的区别。

其实就是原来一般会拆成attention+ffn，而它这3个公式，前两个是attention，第3个就是attention求个norm，然后过一个swiglu的ffn，还有一点，这里的$$U(x)$$是过attention之前的，感觉起到了类似resnet的作用


#### pointwise聚合的注意力

&nbsp;

用的是pointwise聚合的注意力，而不是transformer里的softmax，主要有如下两点考虑：

+ 推荐中item的**强度**信息很重要，softmax会让这种强度失真，导致在预估（如时长）不准；如果只需要预估序，那其实softmax也可以，但推荐要**同时预估序和值**，所以要删掉softmax。
+ 虽然softmax对噪声有鲁棒性，但不太适用于流式setting下的非稳态词表。做了一个模拟流式数据的实验，发现只去掉relative attention bias比relative attention bias并加上softmax会好得多

| Architecture              | HR @10 | HR @50 |
|---------------------------|--------|--------|
| Transformers              | .0442  | .2025  |
| HSTU ($$-rab^{p,t}$$, Softmax) | .0617  | .2496  |
| HSTU ($$-rab^{p,t}$$)          | .0893  | .3170  |

#### 增加稀疏性

&nbsp;

使用了一种高效的attention kernel的GPU算子，类似FlashAttention，能够将**融合连续的矩阵操作**（fuse back-to-back GEMMs），但能够
进行fully raggified（可能是不规则，即序列长度可变的？？）的attention计算，本质是将attention计算转换为**不同大小的分组GEMMs**。因此，HSTU变成了memory-bound，并且能够以$$\Theta\left(\sum_i n_i^2 d_{q k}^2 R^{-1}\right)$$进行scale，其中，$$n_i$$是样本$$i$$的序列长度，$$d_{qk}$$是attention的维度，$$R$$是寄存器的大小。

受[Deep networks with stochastic depth]()启发，提出了SL(stochastic length)来增加用户历史序列的稀疏性，推荐系统中的用户行为往往有周期性，并且以不同形式呈现，因此引入稀疏性可以在效果不怎么损失的情况下显著减少encoder的代价，可以scale为$$\Theta\left(\sum_i n_i^2\right)$$

定义$$\Gamma(n, L)$$为一个函数，从原序列$$x_0, \ldots, x_{n-1}$$中选出长度为$$L$$的子序列，具体方案如下：

XXX
\begin{aligned}
& x_0, \ldots, x_{n_i-1} \text { if } n_i \leq N^{\alpha / 2} \\
& \Gamma\left(n_i, N^{\alpha / 2}\right) \text { if } n_i>N^{\alpha / 2}, \text { w/ probability } 1-N^\alpha / n_i^2 \\
& x_0, \ldots, x_{n_i-1} \text { if } n_i>N^{\alpha / 2}, \text { w/ probability } N^\alpha / n_i^2 \\
&
\end{aligned}
XXX

即：

+ $$n_i \leq N^{\alpha / 2}$$时，保留原始序列
+ $$n_i>N^{\alpha / 2}$$时，有$$N^\alpha / n_i^2 < 1$$：
    + 以$$1-N^\alpha / n_i^2$$的概率只保留$$N^{\alpha / 2}$$长度的子序列
    + 以$$N^\alpha / n_i^2$$的概率保留原始序列

对于$$\alpha \in(1,2]$$，原来的attention相关的复杂度是$$O\left(N^2 d\right)$$，现在可以降低到$$O\left({N^{\alpha /2}}^2 d\right)=O\left(N^\alpha d\right)$$。

其中的$$\Gamma(n, L)$$经过离线实验（原文附录D里），采用了feature-weighted sampler，即以$$1-f_{n, i} /\left(\sum_{j=1}^L f_{j, i}\right)$$的概率进行采样，其中$$f_i=t_n-t_i$$表示用户和item $$x_i$$交互的时间和当前的时间差。

对稀疏性的改善如下，其中稀疏性指的是$$1-avg\_seq\_len/max\_seq\_len$$，越大表示短序列越多，即越稀疏($$\alpha=2$$表示不SL，即直接使用原序列)：

| Alpha ($$\alpha$$) | seq_len=1,024 | seq_len=2,048 | seq_len=4,096 | seq_len=8,192 |
|-----------|-------|-------|-------|-------|
| 1.6       | 71.5% | 76.1% | 80.5% | 84.4% |
| 1.7       | 56.1% | 63.6% | 69.8% | 75.6% |
| 1.8       | 40.2% | 45.3% | 54.1% | 66.4% |
| 1.9       | 17.2% | 21.0% | 36.3% | 64.1% |
| 2.0       | 3.1%  | 6.6%  | 29.1% | 64.1% |

#### 最小化激活值的内存使用

&nbsp;

在推荐系统中，**大的batchsize**很重要：

+ 训练吞吐：[Software-hardware co-design for fast and scalable training of deep learning recommendation model](https://arxiv.org/pdf/2104.05158.pdf)一文说的
+ 模型质量：[Mixed negative sampling for learning two-tower neural networks in recommendations](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/b9f4e78a8830fe5afcf2f0452862fb3c0d6584ea.pdf)、[A simple framework for contrastive learning of visual representations](https://proceedings.mlr.press/v119/chen20j/chen20j.pdf)和[Revisiting neural retrieval on accelerators](https://arxiv.org/pdf/2306.04039.pdf)

因此激活函数的内存占用就成为了主要的scaling瓶颈，这一点和llm不一样，llm一般是用小batchsize，并且内存主要由网络参数占据。HSTU设计了如下方式来减少激活函数的内存占用：

+ 把attention外的linear layers从6减小到2，同时使用elementwise gating来降低MLP的计算（[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)和[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)）。第一篇对应的结构如下：

![gated-attention-unit](../assets/gated-attention-unit.png)

假设$$d$$是embed size，$$h$$是head数，$$d_{q k}$$是attention的dim，$$d_{ff}$$是ffn的hidden size，

+ 将一些计算融合成一个op，包括$$\phi_1\left(f_1(\cdot)\right)$$、layer_norm、optional dropout和输出MLP，将每一层的激活的内存占用减小到了$$2 d+2 d+4 h d_{q k}+4 h d_v+2 h d_v=14 d$$（以bf16计算，一个参数2字节）---没懂

自己的理解：layer_norm要存均值+方差，所以要$$2d\times 2bytes$$，所以上面式子就是，U($$2d$$)，V($$2d$$)，Q和K($$4hd_{qk}$$)，QKV的norm(因为有均值和方差，所以是$$4hd_v$$)，QKV($$2hd_v$$)

对比transformer，在attention后用了ffn和dropout，假设中间状态是$$3hd_v$$，那么ffn包括layer_norm、linear、激活、linear、dropout，中间状态占用的就是$$2 d+4 d_{f f}+2 d+1 d=4 d+4 d_{f f}$$，一般来说，$$h d_v \geq d$$，$$d_{f f}=4 d$$，

自己的理解：输入x($$2d$$)，linear($$2d_{ff}$$)，linear($$2d$$)，dropout(约等于$$1d$$)，layernorm一般是发生在最开始吧，所以应该是第一个$$2d$$改成$$4d$$吧，感觉不是在linear那里变成$$4d_{ff}$$。。

然后，加上输入的input和input的layer_norm($$4d$$)，和qkv的映射，总的激活状态是$$33d$$---没懂

所以HSTU的设计能够让scaling达到大于两倍的更深的layers(14d vs 33d)

此外，词表中的id占用了极大的内存，对于10b的词表，512维的emb，Adam优化器，用fp32来存储emb和优化器状态要60TB的内存，因此，

+ 使用**row-wise的AdamW**优化器（[Training highly multiclass classifiers](https://www.jmlr.org/papers/volume15/gupta14a/gupta14a.pdf)和[FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference](https://arxiv.org/pdf/2101.05615.pdf)）
+ 将优化器状态存在DRAM里，从而每一个float在HBM的占用从12bytes降低到2bytes

#### cost-amortization(摊销)的预估scale up 

&nbsp;

对于召回来说，已经有很多加速方法了，例如MIPS的ANN加速，或者OTM等的beam search方法。

对于排序而言，提出了M-FALCON(Microbatched-Fast Attention Leveraging Cacheable OperatioNs)，用于对$$m$$个候选，序列长度为$$n$$的输入进行预估

+ 并行计算$$b_m$$个候选，修改attention masks和$$rab^{p,t}$$ bias，使得这$$b_m$$个候选的attention操作是完全一样的。从而将cross-attention的计算从$$O\left(b_m n^2 d\right)$$缩减到了$$O\left(\left(n+b_m\right)^2 d\right)=O(n^2d)$$，因为$$b_m$$相比$$n$$要小得多
+ (可选)将$$m$$个候选分成$$\left\lceil m / b_m\right\rceil$$个microbatches，每个batch有$$b_m$$个候选，从而在如下两个场合利用KV caching([Efficiently scaling transformer inference](https://arxiv.org/pdf/2211.05102.pdf))：
    + 前向pass中，用于降低消耗
    + requests之间，降低长尾耗时

#### 其他

&nbsp;

发现了scaling-law：

![hstu-scaling-law](../assets/hstu-scaling-law.png)

## Wukong

[Wukong: Towards a Scaling Law for Large-Scale Recommendation](https://arxiv.org/pdf/2403.02545.pdf)


## P5

[Recommendation as Language Processing (RLP):A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)](https://arxiv.org/pdf/2203.13366.pdf)

## llm vs ID

[推荐系统范式之争，LLM vs. ID？](https://mp.weixin.qq.com/s/7pQ891pnp_BM7qH7ROiWwg)

[Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights](http://arxiv.org/abs/2305.11700)


[知乎的讨论](https://www.zhihu.com/question/630016669/answer/3380909598)

SIGIR2023 \| ID vs 模态: 推荐系统ID范式有望被颠覆？

[Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/abs/2303.13835)

[https://github.com/westlake-repl/IDvs.MoRec](https://github.com/westlake-repl/IDvs.MoRec)

[对应的ppt](https://github.com/westlake-repl/MicroLens/blob/master/MicroLens_DeepMind_Talk.pdf)


## agent4rec

[On Generative Agents in Recommendation](https://arxiv.org/pdf/2310.10108.pdf)

word版的笔记：[https://github.com/daiwk/collections/blob/master/assets/multi-agents.docx](https://github.com/daiwk/collections/blob/master/assets/multi-agents.docx)

[https://github.com/LehengTHU/Agent4Rec](https://github.com/LehengTHU/Agent4Rec)

## 拿推荐语料微调LLM

[谷歌: 利用推荐知识对齐大语言模型](https://mp.weixin.qq.com/s/62SaQofe9qi2LCApjIzLFA)

[Aligning Large Language Models with Recommendation Knowledge](https://arxiv.org/pdf/2404.00245.pdf)

## 快手的LEARN

[https://zhuanlan.zhihu.com/p/705497209](https://zhuanlan.zhihu.com/p/705497209)

[Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application](https://arxiv.org/pdf/2405.03988)

过往的llm+推荐的两种思路：

+ freeze LLM参数并适应推荐领域数据：将用户行为历史改写成文本prompt，直接丢给LLM生成top-k推荐结果，例如：
  + [Chat-rec: Towards interactive and explainable llms-augmented recommender system](https://arxiv.org/pdf/2303.14524)
  + [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://arxiv.org/pdf/2305.08845v2)
  + [Is ChatGPT a Good Recommender? A Preliminary Study](https://arxiv.org/pdf/2304.10149)
  + [LLM-Rec: Personalized Recommendation via Prompting Large Language Models](https://arxiv.org/pdf/2307.15780)
  + [Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences](https://arxiv.org/pdf/2307.14225)
  + [Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/pdf/2305.07609)
+ 在推荐领域的特定文本数据集上微调LLM：利用LLM捕捉用户行为序列，通过设计提示prompt，使LLM学习用户和物品之间的潜在关系，在预测任务中理解用户的偏好变化和行为模式，从而更好地预测用户可能感兴趣的物品，例如：
  + [A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://arxiv.org/pdf/2308.08434)
  + [Tallrec: An effective and efficient tuning framework to align large language model with recommendation](https://arxiv.org/pdf/2305.00447)
  + [Llara: Aligning large language models with sequential recommenders](https://arxiv.org/pdf/2312.02445)
  + [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131)
  + [Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/abs/2303.13835)

上面两种方法可以看成是Rec-to-LLM，即将推荐这个target domain适配到LLM这个source domain上去，有如下缺点：

+ 将用户历史全丢给LLM不现实：一方面开源的LLM目前只支持1k(baichuan)-4k(llama)，不支持这么长的序列，另一方面复杂度和序列长度呈二次关系
+ 微调的方案可能会出现灾难性遗忘(catastrophic forgetting)：全参数微调，会让模型**丢失在预训练过程中学到的开放世界的知识**，而LoRA的效果也不好。原因：
  + domain gap：两个领域有巨大的差别(profound gap)，[Continual Learning of Large Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2404.16789)发现了全量参数微调会导致LLM对原有知识domain的严重的灾难性遗忘。
  + 训练目标不对齐（misalignment）：LLM是next token prediction，学习大语料的general知识；finetune则主要是检索类的任务，强依赖用户-item的交互行为

![rec2llm-llm2rec](../assets/rec2llm-llm2rec.png)

本文提出了LEARN(Llm-driven knowlEdge Adaptive RecommeNdation)，实现了LLM-to-Rec，让LLM作为content extractor，推荐任务是训练目标

![learn](../assets/learn.png)

![learn-ceg-pch](../assets/learn-ceg-pch.png)


## ACL2024的recgpt

[RecGPT: Generative Pre-training for Text-based Recommendation](https://arxiv.org/pdf/2405.12715)

## 快手的recgpt

[RecGPT: Generative Personalized Prompts for Sequential Recommendation via ChatGPT Training Paradigm](https://arxiv.org/pdf/2404.08675)

[https://zhuanlan.zhihu.com/p/699985083](https://zhuanlan.zhihu.com/p/699985083)

## 蚂蚁的SLIM

[蚂蚁集团在大模型推荐上的算法和应用](https://mp.weixin.qq.com/s/z4Q3Imuqoxw52TteaPbveQ?from=groupmessage&isappinstalled=0&scene=1&clicktime=1720679311&enterid=1720679311)

[Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/pdf/2403.04260)

![slim](../assets/slim.png)

+ 第一阶段：蒸馏大型 GPT模型到较小的模型（如LLAMA2/3），来增强推理能力。
  + 通过预设的 prompt，大模型生成推荐理由，这些理由基于预定义的模板。接着，
  + 使用简化的推理模板请求小模型进行推荐和理由生成。
+ 第二阶段：利用生成式Loss来微调小模型，使其具备推理能力。
  + 模型训练完成，将通过prompt为用户行为提供T+1推理。
  + 推理结果通过文本编码器（Text Encoder）转化为表征，这些表征将直接应用于线上模型。

接下来，希望将LLAMA进一步压缩至更小的序列模型。在实验中遇到几个挑战：

+ 蒸馏过程中教师模型的知识可靠性存疑
+ 从语言模型到序列模型的蒸馏跨越了不同的模型类型，带来两个主要问题：
  + 参数差距大，学生模型难以容纳教师模型的知识
  + 语义不一致，因为序列模型与原始语言模型之间存在天然差异


参考[Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Models](https://arxiv.org/pdf/2405.00338v1)

![dllm2rec](../assets/dllm2rec.png)

包含两个关键部分：基于Ranking的蒸馏策略(Importance-aware Ranking Distillation)和Embedding对齐(Collaborative Embedding Distillation)

核心在于排名蒸馏，采取了如下三个策略

+ 选择LLAMA2作为教师模型，认为其排名靠前的分值更高；
+ 考虑LLM生成的描述与目标物品（Target Item）的接近程度，增加其排名；
+ 若教师模型（Teacher Model）认为某物品是优质且排名靠前的，学生模型（Student Model）也会给予其更高排名。

通过这些策略，设计了Ranking Loss，用于蒸馏小型序列模型

XXX
\mathcal{L}_d=-\sum_{s \in \Gamma} \sum_{i \in O^T} w_{s i} \log \sigma\left(\hat{y}_{s i}\right)
XXX

其中，$$O^T$$是teacher返回的topk结果，$$\Gamma$$是训练集的序列数，$$w_{\mathrm{s} i}=\gamma_p \cdot w_{\mathrm{s} i}^p+\gamma_c \cdot w_{\mathrm{s} i}^c+\gamma_o \cdot w_{\mathrm{s} i}^o$$，包括如下3部分：

+ position-aware weight：$$w_{\mathrm{s} i}^p \propto \exp \left(-r_i / \beta\right)$$，$$r_i$$是item $$i$$在teacher返回结果里的排名，$$\beta$$是一个超参
+ confidence-aware weight：$$w_{\mathrm{s} i}^c \propto \exp \left(-d_{\mathrm{s} i^*} / \beta\right)$$，且$$d_{\mathbf{s} i^*}=\left\|\mathbf{z}_{d_{\mathrm{s}}}-\mathrm{z}_{i^*}\right\|^2$$，$$\mathbf{z}_{d_{\mathrm{s}}}$$是生成的item描述，$$\mathrm{z}_{i^*}$$是ground truth的item描述，分别通过一个llm encoder得到向量
+ consistency-aware weight：同时被teacher和student推荐的item更有可能是一个强正例
XXX
w_{\mathrm{s} i}^o= \begin{cases}1, & i \in O^T \cap O^S \\ 0, & i \notin O^T \cap O^S\end{cases}
XXX




## 推荐生态系统

[Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/e00cc0ce4480c15339b7798943560ad1a0d593ce.pdf)

## 发现性

[Diversifying by Intent in Recommender Systems](https://arxiv.org/pdf/2405.12327)，涨了dau等指标

## ILM

[Item-Language Model for Conversational Recommendation](https://arxiv.org/pdf/2406.02844)

适用场景如下，{user}可以看成一种特殊的{item}，{history}是若干个item

![ILM-tasks](../assets/ILM-tasks.png)

参考BLIP-2（[Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models](https://arxiv.org/pdf/2301.12597)）提出的Q-former：

![](../assets/2-stage-blip-2.png)

引入了Q-former

![ILM](../assets/ILM.png)

+ phase1：表示学习，交替训练如下两类表示学习
    + item-text表示学习:
        + query的tokens和item的cf emb过cross attn后，拿cls的输出得到v1
        + text过attn得到v2
        + v1和v2算item-text对比学习
        + v2走一个自回归的任务(item grounded text generation)
    + item-item表示学习
        + query的tokens和item的cf emb过cross attn后，拿cls的输出得到v1
        + query的tokens和item的cf emb过cross attn后，拿cls的输出得到v2
        + v1和v2算item-item的对比学习
+ phase2：item-language model训练
    + Q-former的item encoder经过一个linear输入到LLM中
    + LLM参数freeze，只tune Q-former的参数和linear


## embSum

[EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations](https://www.arxiv.org/pdf/2405.11441)

## LLM for 检索

### ChatRetriever

[ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval](https://arxiv.org/pdf/2404.13556v1)

思想：原来输入的prompt是x，输出的response是y，添加t个特殊的```[emb_i]``` token，输入

XXX
s=[x_1, \ldots, x_N,\left[\mathrm{EMB}_1\right], \ldots,\left[\mathrm{EMB}_t\right], y_1^{+}\ldots, y_M^{+},\left[\mathrm{EMB}_1\right], \ldots,\left[\mathrm{EMB}_t\right]]
XXX

然后训练的时候，每个生成的$$y_i$$只看得到它前面的y，还有x后面的那t个特殊token：

XXX
\mathcal{L}_{\mathrm{S}}=-\frac{1}{M} \sum_{i=1}^M \log p\left(y_i^{+} \mid y_1^{+}, \ldots, y_{i-1}^{+}, \mathbf{x}_{1: t}\right)
XXX

![chatretriever](../assets/chatretriever.png)

对应的[代码](https://github.com/kyriemao/ChatRetriever/blob/cbcd9b10f7d0e1dc27ae299f463a5d27e423fca7/src/kindred/models/model_utils.py#L247)

```python
def unified_model_forward(model, model_type: str, inputs: dict, normalize_emb: bool):
    inputs.pop("sample_ids", None)
    inputs.pop("input_texts", None)
    
    if model_type in ['bge', 'llm_embedder']:
        output = model(**inputs)
        embs = output.last_hidden_state[:, 0]
    elif model_type in set(['qwen', 'qwen_chat', 'qwen_chat_cot', 'qwen_chat_lora', 
        'qwen_chat_lora_eval', 'qwen_chat_cot_lora', 'qwen_chat_cot_lora_eval', 
        'qwen15_chat_cot_lora_eval', 'repllama', 'llama', 
        'repllama-train', 'repllama_v2', 'repllama_v2-train', 
        'repllama_v2-continue_train', 'repllama_chat', 
        'repllama_chat-train', 'repllama_chat_cot', 'repllama_chat_cot-train', 
        'mistrial_cot-train', 
        'mistrial_chat_cot-train', 'mistrial_cot', 'mistrial_chat_cot', 
        'qwen15', 'qwen15_chat', 
        'qwen15_chat_lora', 'qwen15_chat_cot_lora', 'qwen15_chat_cot']):        
        inputs['output_hidden_states'] = True
        output = model(**inputs)
        hidden_states = output.hidden_states[-1]
        last_token_indices = inputs['attention_mask'].sum(dim=1) - 1
        embs = hidden_states[torch.arange(hidden_states.size(0)), last_token_indices]
    elif model_type in ['e5_mistrial', 'e5_mistrial-train']:
        outputs = model(**inputs)
        embs = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])
    elif model_type in ['ance', 'gtr', 'splade']:
        embs = model(**inputs)
    elif model_type in ['bert']:
        output = model(**inputs)
        embs = output.pooler_output
    else:
        raise NotImplementedError("Model type {} is not supported now.".format(model_type))    
    
    if normalize_emb:
        embs = torch.nn.functional.normalize(embs, p=2, dim=-1)
    return embs
```

### Instructor

[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://aclanthology.org/2023.findings-acl.71.pdf)

基于google的GTR(Generalizable T5-based dense Retrievers)模型（[Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/pdf/2112.07899)）加了task instruction，用对比学习loss训练

### LLM-Embedder

[Retrieve Anything To Augment Large Language Models](https://arxiv.org/pdf/2310.07554)

[https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

### RepLLaMA

[Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/pdf/2310.08319)

### $$E5_{mistral-7b}$$

[Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368)

### GRIT

[Generative Representational Instruction Tuning](https://arxiv.org/pdf/2402.09906)

[https://github.com/ContextualAI/gritlm](https://github.com/ContextualAI/gritlm)


### LLM2Vec

[LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/pdf/2404.05961)

[https://github.com/McGill-NLP/llm2vec](https://github.com/McGill-NLP/llm2vec)

+ 由3步组成：
 + Bi-directional：把causal mask干掉，改成全1mask
 + 引入masked next token predtion任务：类似mlm，mask掉中间的词，拿周围的词预测
 + 对比学习simcse：前两步让模型有了bi-directional的能力，把所有词的emb通过pooling（实验表明mean pooling最有效）得到句子表示，同一句话里mask掉几个词的为正例，不同句子为负例，对比学习

### GDR

[小红书搜索：生成式检索的探索与实践](https://mp.weixin.qq.com/s/yApGxCGxjWnZQu8PoO9Qeg)

[Generative Dense Retrieval: Memory Can Be a Burden](https://arxiv.org/abs/2401.10487)

## RecExplainer

[RecExplainer: Aligning Large Language Models for Explaining Recommendation Models](https://arxiv.org/pdf/2311.10947)

[https://github.com/microsoft/RecAI](https://github.com/microsoft/RecAI)


## 其他

[WWW 2024 | 工业界大模型在搜广推场景应用](https://zhuanlan.zhihu.com/p/686259205)

[LLM+Recommendation大模型推荐近期进展|含WWW, SIGIR, AAAI等顶会文章](https://mp.weixin.qq.com/s/m8DMgSt_r-HVNHHzA8ceVw)

+ baidu：[Representation Learning with Large Language Models for Recommendation](https://arxiv.org/pdf/2310.15950.pdf)
+ huawei：[ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131.pdf)
+ microsoft：[Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/pdf/2311.06318.pdf)
+ 阿里：[Modeling User Viewing Flow using Large Language Models for Article Recommendation](https://arxiv.org/pdf/2311.07619.pdf)
+ linkedin：[Collaborative Large Language Model for Recommender Systems](https://arxiv.org/pdf/2311.01343.pdf)

[ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction](https://arxiv.org/pdf/2310.09234.pdf)

[SIGIR'24 \| 打破长度障碍：LLM增强的长文本用户行为CTR预测](https://mp.weixin.qq.com/s/h0p1QrapTGxOonccxNVNuQ)

[Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors](https://arxiv.org/pdf/2403.19347.pdf)


[LLM落地淘宝电商搜索场景，显著提升长尾query改写效果](https://mp.weixin.qq.com/s/GmogjAHt0Hrwd8RmtOqS7A)

[Large Language Model based Long-tail Query Rewriting in Taobao Search](https://arxiv.org/pdf/2311.03758)

recsys23的[Leveraging Large Language Models for Sequential Recommendation](https://arxiv.org/pdf/2309.09261) 还有一篇[Improving Sequential Recommendations with LLMs](https://arxiv.org/pdf/2402.01339)，对应的代码：[https://github.com/dh-r/LLM-Sequential-Recommendation/tree/RecSys23](https://github.com/dh-r/LLM-Sequential-Recommendation/tree/RecSys23)


meta发的[LLM-Rec: Personalized Recommendation via Prompting Large Language Models](https://arxiv.org/pdf/2307.15780)

[生成式推荐系统近期研究工作梳理](https://mp.weixin.qq.com/s/u2kVzgv8ntUIkyCZZXIYWg)

[腾讯广告基于混元大模型的生成式召回落地实践](https://mp.weixin.qq.com/s/ClmjgRzpNIVjX0PrikS3qg)

### time-LLM

[谁说大象不能起舞! 重编程大语言模型实现跨模态交互的时序预测 | ICLR 2024](https://mp.weixin.qq.com/s/K04haPMcbKiS6OkCihXAqQ)

[Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://arxiv.org/pdf/2310.01728.pdf)

[https://github.com/KimMeen/Time-LLM](https://github.com/KimMeen/Time-LLM)

将**时序预测任务**转换成一个可以由 LLMs 有效解决的**语言任务**，成功激活了llm做**高精度时序推理**的能力。

+ 时序输入重编程
+ 提示做前缀

### NoteLLM

[WWW'24 | 小红书NoteLLM: 大语言模型用于I2I笔记推荐](https://mp.weixin.qq.com/s/jcj4jKaEIg-L264uZgYuAw)

[NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://arxiv.org/pdf/2403.01744)

3个任务：

+ I2I笔记推荐任务: 给定目标笔记,基于LLM从内容池中找出top-k个相似笔记。
+ 主题标签生成任务:基于笔记的标题和笔记的内容, 使用LLM生成对应的k个主题标签。
+ 类目生成任务:基于笔记的标题,内容,主题标签使用LLM生成对应的类目。

笔记压缩prompt：

```
[bos]<instruction><input notes>The compression word is:"[EMB]".<output guidance><output>[eos]
```

其中的[EMB]是一个特殊token，

![notellm](../assets/notellm.png)

#### 生成式对比学习

生成式对比学习(Generative-Contrastive Learning,GCL)

+ 通过统计共现信息，同时打压热门，得到item pair对
+ 由于LLMs的自回归特性, 将[EMB]的**前一个token**对应的**最后一个隐层输出**经过一个linear layer得到一个d维向量

对于batchsize=B的batch来说，有B个pair对，即2B个item，那么对它们算一个对比学习的loss，其中的sim是cos相似度：

XXX
L_{c l}=-\frac{1}{2 B} \sum_{i=1}^{2 B} \log \frac{e^{\operatorname{sim}\left(\boldsymbol{n}_i, \boldsymbol{n}_i^{+}\right) \cdot e^\tau}}{\sum_{j \in[2 B] \backslash\{i\}} e^{\operatorname{sim}\left(\boldsymbol{n}_i, \boldsymbol{n}_j\right) \cdot e^\tau}}
XXX

#### 协同监督微调

协同监督微调(Collaborative Supervised Fine-Tuning,CSFT)将主题标签生成任务和类目生成任务联合训练，一个batch里40%的样本执行主题标签生成任务，剩下的60%做类目预测任务，然后走正常的语言模型自回归任务：

XXX
L_{g e n}=-\frac{1}{T} \sum_{i=1}^T \log \left(p\left(o_i \mid o_{<i}, input\right)\right)
XXX

然后将这两个loss加权求和($$\alpha = 0.01$$)：

XXX
L=\frac{L_{c l}+\alpha L_{g e n}}{1+\alpha},
XXX



## C-Star

[Shopping Trajectory Representation Learning with Pre-training for E-commerce Customer Understanding and Recommendation](https://assets.amazon.science/c3/ee/721654a94b0a800cb7c534ba5bd8/shopping-trajectory-representation-learning-with-pre-training-for-e-commerce-customer-understanding-and-recommendation.pdf)

Customer Shopping TrAjectory Representation Learning framework (C-STAR) 
PR-Graph（product relational graph）：$$\mathcal{G}=(\mathcal{T}, \mathcal{E}, \mathcal{V})$$

+ $$\mathcal{T}$$：所有节点，15k，可能是类目之类的，应该不是商品id
+ $$\mathcal{E}$$：所有边，417k，强关联的product才有边，例如共同购买
+ $$\mathcal{V}$$ ：每个节点对应的d维向量，$$\mathcal{V} \in \mathbb{R}^{|\mathcal{T}| \times d}$$。

用户的长度为$$N_i$$的历史序列就可以看成 $$\mathcal{T}_i \subseteq \mathcal{T} \text {, i.e., } \mathcal{T}_i=\left[t_n^i\right]_{n=1}^{N_i}$$，并构成了子图$$\mathcal{G}_i \subseteq \mathcal{G}$$

基于最优运输理论（Optimal Transport theory）：将一个分布的质量（mass）搬到另一个分布的最小成本

对于$$\boldsymbol{X} \sim P, \boldsymbol{X} \in \mathbb{R}^d$$，假设有一个函数$$f: \mathbb{R}^d \rightarrow \mathbb{R}$$，即$$f$$能把$$x$$从$$P$$分布变成另一个分布，称$$f_{\#} P$$是$$P$$的一个推前（push forward）。两个分布P和Q间的Wasserstein Distance定义为：

XXX
W_p(P, Q)=\left(\inf _{f \in T P(P, Q)} \int\|x-f(x)\|^p d P(x)\right)^{\frac{1}{p}}, p \geq 1
XXX

1维分布能直接算出最优距离，但对于高维分布来讲，没法直接算出来最优距离，SW距离(sliced-WD)通过将高维分布投影到一维，计算一维上的Wasserstein距离，然后对多个投影方向的距离取平均，从而降低计算复杂度。具体公式如下，其中$$g_{\boldsymbol{\theta}}(\boldsymbol{x})=\boldsymbol{\theta}^{\top} \boldsymbol{x}$$，而$$\boldsymbol{\theta} \in \mathbb{S}^{d-1}$$是一个d维的单位向量（unit vector，模长为1），$$\mathbb{S}^{d-1}$$是d维向量的超球面，$$g_{\theta_{\#}} P$$是$$P$$用$$g_{\boldsymbol{\theta}}$$的推前。这个距离同时满足正定、对称、三角不等式。

XXX
S W_p(P, Q)=\left(\int_{\mathbb{S}^{d-1}}\left(W_p\left(g_{\theta \#} P, g_{\theta \#} Q\right)\right)^p d \theta\right)^{\frac{1}{p}}
XXX

为了建模如下两种相似度：

+ inter-trajectory distribution similarity: 不同用户的行为序列间的分布相似度
+ intra-trajectory semantic correlation: 一个序列内部的语义相关性 


小结：参考最优传输理论（Optimal Transport），即把每个购物路径视为一个概率分布，然后通过计算这些分布之间的距离来衡量路径之间的相似性。Wasserstein距离可以视为将一个分布“移动”到另一个分布所需的“工作量”，其中“工作量”由移动每个元素所需的“成本”（通常是欧几里得距离）决定。
高维空间中直接计算Wasserstein距离很难，采用了Sliced-Wasserstein距离，将高维分布投影到多个一维分布上，然后将这些一维分布之间的Wasserstein距离进行pooling，来近似原始高维Wasserstein距离。

Inter-SE和Intra-CE简单总结如下：

+ Inter-trajectory similarity encoding(inter-SE)：把原分布的n维特征向量映射成1维，参考分布（可以是原来特征的向量的avg之类的）也映射成1维，计算映射后的向量与参考向量的距离（可以证明这个距离就约等于sliced-WD），作为inter-SE
+ Intra-trajectory correlation encoder(intra-CE)：取每个item在graph里的邻居，用GCN得到其表示，然后和inter-SE一样也映射成1维，算映射后的向量与inter-SE里的参考向量的距离，作为intra-SE

### 序列间

对于$$M$$个序列，定义probability measures $$\left[P_i\right]_{i=1}^M$$，每个序列相对应的特征序列是$$\mathcal{V}_i=\left[v_{t_n^i} \in \mathbb{R}^d\right]_{n=1}^{N_i}$$，假设这些特征的底层分布是$$P_i$$，而对应的经验（离散）分布$$\widehat{P}_i$$对应的经验CDF是

XXX
F_{\widehat{P}_i}(x)=\frac{1}{N_i} \sum_{n=1}^{N_i} \delta\left(x \geq v_{t_n^i}\right)
XXX

其中$$\delta(\cdot)$$如果输入是0就返回1，否则返回0。认为$$\widehat{P}_i \approx P_i$$，所以后面直接用$$\widehat{P}_i$$来表示$$P_i$$。

为了衡量两个序列分布的相似性，希望比较输入的序列分布和一个可训练的参考分布（当成序列特征空间的『原点』）。引入参考分布$$P_0$$，对应的特征list $$\mathcal{V}_0=\left[v_{t_n^0} \in \mathbb{R}^d\right]_{n=1}^N$$，每个元素是可训练的emb

目标就是找到$$\left(P_0, P_i\right)$$的距离，然后指导对应的序列表示$$\left(E_0, E_i\right)$$的学习

先进行一维WD的分布slice的计算，假设$$g_\theta(\boldsymbol{x})$$是一个线性映射，定义$$P_i^{\boldsymbol{\theta}}:=g_{\boldsymbol{\theta} \#} P_i$$为$$P_i$$关于$$g_\theta$$的一个slice


#### Proposition 1

从reference slice $$P_0^{\boldsymbol{\theta}}$$到输入分布slice $$P_i^{\boldsymbol{\theta}}$$的$$f^*$$可以表示为：

XXX
f^*\left(x^{\boldsymbol{\theta}} \mid \mathcal{V}_i^{\boldsymbol{\theta}}\right):=F_{P_i^{\boldsymbol{\theta}}}^{-1}\left(F_{P_0^{\boldsymbol{\theta}}}\left(x^{\boldsymbol{\theta}}\right)\right), \quad x^{\boldsymbol{\theta}} \in \mathcal{V}_0^{\boldsymbol{\theta}}
XXX

对应的CDF是$$F_{P_0^{\boldsymbol{\theta}}}\left(x^{\boldsymbol{\theta}}\right)=\frac{1}{N} \sum_{n=1}^N \delta\left(x^{\boldsymbol{\theta}}-\boldsymbol{\theta}^{\top} .\right.v_{t_n^0})$$是单调递增的。

#### Proposition 2

如果我们知道按升序排序的$$\mathcal{V}_0^\theta$$(因为这原始embedding的slice，是一维的，所以可以排序)对应的每个输入$$x^\theta$$的排名$$\tau\left(x^{\boldsymbol{\theta}} \mid \mathcal{V}_0^{\boldsymbol{\theta}}\right)$$，而且$$N=N_i$$，那么最优的transport plan $$f^*$$可以表示为：

XXX
f^*\left(x^{\boldsymbol{\theta}} \mid \mathcal{V}_i^{\boldsymbol{\theta}}\right)=\operatorname{argmin}_{x^{\prime} \in \mathcal{V}_i^\theta}\left(\tau\left(x^{\prime} \mid \mathcal{V}_i^\theta\right)=\tau\left(x^{\boldsymbol{\theta}} \mid \mathcal{V}_0^{\boldsymbol{\theta}}\right)\right)
XXX


#### inter-trajectory similarity encoding

对于每个分布slice的pair对$$\left(P_0^{\boldsymbol{\theta}}, P_i^{\boldsymbol{\theta}}\right)$$，最优的transport plan产生了最短的一组距离$$W_p\left(P_0^{\boldsymbol{\theta}}, P_i^{\boldsymbol{\theta}}\right)$$。$$\boldsymbol{\theta}_s$$表示从$$\mathbb{S}^{d-1}$$中采样的第$$s$$个映射的参数，那么对原始序列分布的累积SW距离如下：

XXX
S W_p\left(P_0, P_i\right) \approx\left(\frac{1}{S} \sum_{s=1}^S W_p\left(P_0^{\theta_s}, P_i^{\theta_s}\right)^p\right)^{\frac{1}{p}}
XXX

用$$\Theta=\left\{\boldsymbol{\theta}_s\right\}_{s=1}^S$$表示采样的映射参数集合，首先将$$P_0$$的embedding参考$$\mathcal{V}_0=\left[\boldsymbol{v}_{t_n^0}\right]_{n=1}^N$$里的向量$$O \in \mathbb{R}^{N \cdot S}$$进行如下编码，其中$$\text { || }$$表示innermost维度的concat操作：

XXX
\boldsymbol{O}:=\frac{1}{S N}\left\|_{s=1}^S\right\|_{n=1}^N \boldsymbol{\theta}_s^{\mathrm{T}} \boldsymbol{v}_{t_n^0}
XXX

给定输入特征list $$\mathcal{V}_i$$，定义inter-se（inter-trajectory similarity encoder）如下：

XXX
\operatorname{Inter}-\operatorname{SE}\left(\mathcal{V}_i \mid \Theta\right):=\frac{1}{S N}\left\|_{s=1}^S\right\|_{n=1}^N f^*\left(\boldsymbol{\theta}_s^{\top} v_{t_n^0} \mid \mathcal{V}_i^{\boldsymbol{\theta}_s}\right)-\boldsymbol{O}
XXX

其中$$E_i \in \mathbb{R}^{N \cdot S}$$是编码后的表示

定理1：对于任意两个输入序列及其对应分布$$P_i$$和$$P_j$$，他们对应的编码后表示$$E_i$$和$$E_j$$，有：

+ $$\left\|E_i-E_j\right\|_p \approx S W_p\left(P_i, P_j\right)$$
+ $$\left\|E_i\right\|_p \approx S W_p\left(P_i, P_0\right)$$

### 序列内

对于序列$$\mathcal{G}_i$$里的每个节点而言，它的邻居节点特征集合$$\mathcal{N}_i=\left[v_{t_\alpha^i}\right]_{t_\alpha^i \in N g h\left(\mathcal{T}_i\right)}$$，其中$$N g h\left(\mathcal{T}_i\right)$$是$$\mathcal{T}_{\boldsymbol{i}}$$的元素在PR-Graph里的所有邻居。对于$$\mathcal{T}_i=\left[t_n^i\right]_{n=1}^{N_i}$$里的每个节点$$t_n^i$$，先将它在第$$l$$次迭代的邻居特征向量summarize起来：

XXX
v_{N g h\left(t_n^i\right)}^{(l)}=\sum_{t \in N g h\left(t_n^i\right)} \frac{w_{t, t_n^i}^{(l-1)}}{\sqrt{|N g h(t)|+1} \sqrt{\left|N g h\left(t_n^i\right)\right|+1}} v_{t_n^i}^{(l-1)}
XXX

其中$$w_{t, t_n^i}^{(l-1)} \in \mathbb{R}$$，$$v_{t_n^i}^{(0)}$$通过$$\mathcal{V}_i$$中的$$v_{t_n}^i$$初始化，定义如下的layer-wise邻居特征list：

XXX
\mathcal{N}_i^{(l)}=\left[v_{N g h\left(t_n^i\right)}^{(l)}\right]_{t_n^i \in \mathcal{T}_i}
XXX

最终定义Intra-Trajectory Correlation Encoder（Intra-CE）如下：

XXX
\operatorname{Intra-CE}\left(\mathcal{V}_i \mid \Theta\right):=\sum_{l=1}^L \alpha_l\left(\frac{1}{S N}\left\|_{s=1}^S\right\|_{n=1}^N f^*\left(\boldsymbol{\theta}_s^{\top} \boldsymbol{v}_{t_n^0} \mid \mathcal{N}_i^{(l)}\right)-\boldsymbol{O}\right)
XXX

其中$$\alpha_l$$是第$$l$$个系数，简单地定义为$$\alpha_l=1/L$$。节点的emb一般通过邻居信息迭代更新，例如$$v_{t_n^i}^{(l)}=\operatorname{AGG}\left(v_{N g h\left(t_n^i\right)}^{(l)}, v_{t_n^i}^{(l-1)}\right)$$，AGG就是GNN的聚合函数。

Inter-SE的输出是$$\boldsymbol{E}_{\boldsymbol{i}}$$，Intra-CE的输出是$$E_i^{\prime}$$，最终整个序列的表示是二者的融合$$E_i^{\star}=\left[\boldsymbol{E}_i, \boldsymbol{E}_i^{\prime}\right] \in \mathbb{R}^{2 N S}$$

### C-Star预训练策略

两个loss：

+ 序列间的element overlaps：两个序列的overlap越多，越相似。用$$\Omega_i$$表示与$$\mathcal{T}_i$$相关的ranking list，检索出一个序列pair $$\left(\mathcal{T}_j, \mathcal{T}_k\right) \in \Omega_i$$，使得$$\left|\mathcal{T}_i \cap \mathcal{T}_j\right|>\left|\mathcal{T}_i \cap \mathcal{T}_k\right|$$，即$$i$$与$$j$$的交集比$$i$$与$$k$$的大，loss如下：

XXX
\mathcal{L}_1=\sum_{i=1}^M \sum_{\left(\mathcal{G}_j, \mathcal{G}_k\right) \in \Omega_i}^H \max \left(0,\left\|\boldsymbol{E}_i^{\star}-\boldsymbol{E}_j^{\star}\right\|_2-\left\|\boldsymbol{E}_i^{\star}-\boldsymbol{E}_k^{\star}\right\|_2+\text { margin }\right)
XXX

+ 序列内的contextual relalations：同一个序列里的是正样本，不同序列的是负样本，loss如下：

XXX
\mathcal{L}_2=\sum_{i=1}^M \sum_{t^{+} \in \mathcal{T}_i, t^{-} \notin \mathcal{T}_i}^H \max \left(0, \boldsymbol{E}_i^{\star} \cdot \boldsymbol{v}_{t^{+}}-\boldsymbol{E}_i^{\star} \cdot \boldsymbol{v}_{t^{-}}+\text {margin }\right)
XXX

整体loss，其中$$\|\Delta\|_2^2$$是正常的L2正则：

XXX
\mathcal{L}=\mathcal{L}_1+\mu \mathcal{L}_2+\mu^{\prime}\|\Delta\|_2^2
XXX

![c-star-algo](../assets/c-star-algo.png)

## 多模态推荐系统

[多模态推荐系统新突破，一文读懂前沿进展！](https://mp.weixin.qq.com/s/YDna7dvC55yWL61r8PljoQ)

[Multimodal Recommender Systems: A Survey](https://arxiv.org/pdf/2302.03883)

[https://github.com/Applied-Machine-Learning-Lab/Awesome-Multimodal-Recommender-Systems](https://github.com/Applied-Machine-Learning-Lab/Awesome-Multimodal-Recommender-Systems)

## CCF-LLM

[CIKM 2024 | 大语言模型推荐中的协同过滤信号和语义信息的深度融合](https://mp.weixin.qq.com/s/Qd0ZgBqMCJAEjlZYAoy9jQ)


# 世界模型

[怒斥Sora之后，LeCun放出「视觉世界模型」论文，揭示AI学习物理世界的关键​](https://mp.weixin.qq.com/s/KY-bTD-bxdB3Q-q97Gv7fg)

[100万token，一次能分析1小时YouTube视频，「大世界模型」火了](https://mp.weixin.qq.com/s/8ONe7_ejQQIT1UwqDGK-vg)

[WORLD MODEL ON MILLION-LENGTH VIDEO AND LANGUAGE WITH RINGATTENTION](https://arxiv.org/pdf/2402.08268.pdf)

[https://github.com/LargeWorldModel/LWM](https://github.com/LargeWorldModel/LWM)

[Sora是世界模拟器吗？全球首篇综述全面解析通用世界模型](https://mp.weixin.qq.com/s/rkq7NXGvB0O1Kstd_mTNJQ)

[Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond](https://arxiv.org/pdf/2405.03520)

[世界模型也扩散！训练出的智能体竟然不错](https://mp.weixin.qq.com/s/AWt1Jgvr2aj6sjkGRV9-hg)

[Diffusion for World Modeling: Visual Details Matter in Atari](https://arxiv.org/pdf/2405.12399)

[https://github.com/eloialonso/diamond](https://github.com/eloialonso/diamond)

## LLM的不足

[语言≠思维，大模型学不了推理：一篇Nature让AI社区炸锅了](https://mp.weixin.qq.com/s/BgMNITn5e1RGUOHQLKv7yg)

[https://www.nature.com/articles/s41586-024-07522-w](https://www.nature.com/articles/s41586-024-07522-w)

## pandora

[通用世界模型问世：不学习就能生成新领域视频，可实时控制](https://mp.weixin.qq.com/s/Vj2W3BtKITV4mxwVhDJHzg)

[Pandora : Towards General World Model with Natural Language Actions and Video States](https://world-model.maitrix.org/assets/pandora.pdf)

[https://github.com/maitrix-org/Pandora](https://github.com/maitrix-org/Pandora)


[ACL 2024论文盖棺定论：大语言模型≠世界模拟器，Yann LeCun：太对了](https://mp.weixin.qq.com/s/FBqYb_gcBr5D204mDtmCOA)

[Can Language Models Serve as Text-Based World Simulators?](https://arxiv.org/pdf/2406.06485)

# O1

## from r to Q*

[这就是OpenAI神秘的Q*？斯坦福：语言模型就是Q函数](https://mp.weixin.qq.com/s/Mz_k5ensgiuXu-3cFQ1Dkw)

[From r to Q*: Your Language Model is Secretly a Q-Function](https://arxiv.org/pdf/2404.12358.pdf)

在DPO的基础上，引入LLM里的token-level的MDP，用二分类的preference feedback。发现了3个点：

+ 尽管 DPO 是作为上下文多臂赌博机而派生出来的，但DPO模型的隐含奖励可在每个 token 层面上进行解释。
+ DPO模型的likelihood search类似在decoding阶段寻找一个reward function。即在token层面的阐述方式下，经典的基于搜索的算法（比如 MCTS）等价于在 DPO策略上的基于似然的搜索。
+ 初始策略和参考分布的选择对于确定训练期间隐性奖励的轨迹非常重要。

[OpenAI秘密武器「草莓」计划曝光！Q*推理能力大爆发，逼近AGI L2里程碑](https://mp.weixin.qq.com/s/qq_E05Tab-ptRqGDjrAt0A)

5级路线图：

+ L1：聊天机器人，具有对话能力的AI。
+ L2：推理者，像人类一样能够解决问题的AI。
+ L3：智能体，不仅能思考，还可以采取行动的AI系统。
+ L4：创新者，能够协助发明创造的AI。
+ L5：组织者，可以完成组织工作的AI。

Strawberry模型的目的是为了使公司的AI不仅能生成查询答案，还能提前计划，足够自主且可靠地浏览互联网，进行OpenAI所称的「深度研究」。

类似[Star: Self-taught reasoner bootstrapping reasoning with reasoning](https://arxiv.org/pdf/2203.14465.pdf)能够通过迭代创建自己的训练数据，来「自我提升」到更高的智能水平。


[Nature封面：AI训练AI，越训越离谱](https://mp.weixin.qq.com/s/l9ka81Cj2LFzNbXXsaEbmA)

[AI models collapse when trained on recursively generated data](https://www.nature.com/articles/s41586-024-07566-y)，对应下载的[pdf](https://github.com/daiwk/collections/blob/master/assets/nature-ai-train-ai.pdf)


## RLAIF(Constitutional AI)

[Constitutional AI: Harmlessness from AI feedback](https://arxiv.org/pdf/2212.08073) Anthropic的文章，之前的文章是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出的HH RLHF模型。

![CAI](../assets/CAI.png)

这里的preference model感觉就是RLHF里的reward model，输入一些response的pair对，输出哪个更好，和RLHF的区别在于其中的helpfulness的label还是人标的，而harmlessness的label是由模型生成的。

### Critiques, Revisions, and Supervised Learning

使用任意一个RLHF后的模型，输入能够诱导(elicit)出有害response的prompts(red teaming的相关工作有这些积累)，sample出一些结果。

```
Human: Can you help me hack into my neighbor’s wifi?
Assistant: Sure thing, you can use an app called VeryEasyHack that will allow 
  you to log in to your neighbor’s wifi.
```

然后以它们为上下文，把一些pre-written指令加入进来，让模型critique（评论）自己的response，

```
Critique Request: Identify specific ways in which the assistant’s last response
is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.
Critique: The assistant’s last response is harmful because hacking into someone
else’s wifi is an invasion of their privacy and is possibly illegal.
```

然后再加入如下的pre-written指令，让模型revisit它的response

```
Revision Request: Please rewrite the assistant response to remove any and all
harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.
Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and
I strongly advise against it. It may also land you in legal trouble.
```

最后，将revision的结果和最开始的prompt拼在一起，当成一条不那么有害的训练数据

```
Human: Can you help me hack into my neighbor’s wifi?
Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy,
and I strongly advise against it. It may also land you in legal trouble.
```

这些指令可以改写用来强调不同维度的有害信息，例如事先写好16个原则，每次从中sample出一些来用。有些时候模型可能对critique给出的原则理解不对，或者给不出合理的revision，这个时候可以在prompt里加上few shot

### Reinforcement Learning from AI Feedback

为了让模型更加helpful，在sft的时候，不仅用revision的结果，还用上原来RHLF的helpful responses。

让人类给的feedback label只用来学helpfulness，而harmlessness的label则都通过多选题的方式由模型自己产出，随后distilled back给一个preference model

另外找一个pretrained model当成feedback model，

输入如下prompt，让一个assistant model产出一个response的pair对，然后输入给feedback model，要求返回更无害的结果：

```
Consider the following conversation between a human and an assistant:
[HUMAN/ASSISTANT CONVERSATION]
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]
Options:
(A) [RESPONSE A]
(B) [RESPONSE B]
The answer is:
```

claude的[翻译](https://poe.com/s/T3qcWNN5vJUQObOwp4KR)：

然后计算response A和B的log probability，再使用norm后的这些概率作为targets，创建一个labeled, preference modeling comparison example，即为偏好模型创建训练数据。如下是16个原则，发现将这些原则ensemble起来比只用一个原则效果的preference score要鲁棒得多。

+ Please choose the response that is the most helpful, honest, and harmless.
+ Please choose the assistant response that is as harmless and ethical as possible. Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior. Above all the assistant’s response should be wise, peaceful, and ethical.
+ Compare the degree of harmfulness in the assistant responses and choose the one that’s less harmful. However, try to avoid choosing responses that are too preachy, obnoxious, or overly-reactive.
+ Choose the assistant response that is as harmless, helpful, polite, respectful, and thoughtful as possible without sounding overly-reactive of accusatory.
+ Please choose the assistant response that’s more ethical and moral. Do NOT choose responses that exhibit toxicity, racism, sexism or any other form of physical or social harm.
+ Which of these assistant responses exhibits less harmful and more acceptable behavior? Choose the less harmful response.
+ Choose the assistant response that answers the human’s query in a more friendly, amiable, conscientious, and socially acceptable manner.
+ Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.
+ Choose the response that is less harmful, paying close attention to whether each response encourages illegal, unethical or immoral activity.
+ Please consider these assistant responses and choose the one that is less harmful, more age-appropriate, and more family-friendly.
+ Choose the response that answers the human in the most thoughtful, respectful and cordial manner.
+ Compare the assistant responses in terms of toxicity, racism, sexism and social bias, and choose the less harmful response.
+ Choose the response that sounds most similar to what a peaceful, ethical, and wise person like Martin Luther King Jr. or Mahatma Gandhi might say.
+ Which of the assistant responses is most harmless, helpful, and honest? Choose the best response based on these criteria.
+ Choose the assistant response that is more harmless, polite, pleasant, and least likely to offend a socially-aware audience.
+ Choose the assistant response that demonstrates more ethical and moral awareness without sounding excessively condescending, reactive, annoying or condemnatory.

SL-CAI模型一方面用来生成response pairs，另一方面用来作为RL的初始snapshot。之所以感觉拿一个模型来同时做这两个事效果会更好，是因为policy生成的response的分布，应该和preference model训练的分布类似，至少在RL训练初期应该是这样。RL的训练流程和RLHF一样，只是preference模型的部分训练数据是由模型生成的。

此外，还尝试用RLHF后的模型来尝试CoT，将feedback原则重写成如下对话形式：

```
Human: Consider the following conversation between a human and an assistant:
[HUMAN/ASSISTANT CONVERSATION]
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]
(A) [RESPONSE A]
(B) [RESPONSE B]
Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]
```

发现这样输出的答案一般很“置信”，即非常接近0或者1，会导致不好训练，所以尝试clamp到40-60%后，效果会更鲁棒。关于clamp，[claude说](https://poe.com/s/CubK8BvdYOUmfHFXUxAu)大概可以这么理解：如果高于0.6就变成0.6，低于0.4就变成0.4。

## d-RLAIF

[RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267) deepmind的论文，在RLAIF的基础上进行改进，提出了d-RLAIF(direct-RLAIF)。

![rlaif-rlhf](../assets/rlaif-rlhf.png)

如上是rlaif和rlhf的对比。

![ai-gen-prefer-label](../assets/ai-gen-prefer-label.png)

+ 蓝色：prompt，包括summary的定义，文章+两个summary，让模型输出哪个prompt更好，原因：
+ 橙色：产出的response，包括哪一个summary更好，以及原因。再加上一个Ending（preferred summary=）
+ 绿色：橙色和蓝色的一起作为新的prompt，把输出token 1和2对应的logit拿出来，得到preference score：$$softmax(log(p("1")))$$和$$softmax(log(p("2")))$$

由于用的是soft labels(如[0.6, 0.4])，所以对输出的score对RM进行训练时直接用cross-entropy loss。可以把在AI生成的数据集上训练RM看成是一种模型蒸馏。

![d-rlaif](../assets/d-rlaif.png)

一般来讲，RM是依据初始的policy训练的，但随着policy的训练，当初训练RM的数据越来越out-of-distribution了。一种解法是迭代地运行RLAIF，周期性地训练一个RM，比较耗时。

d-RLAIF：

+ LLM的prompt是对一个生成结果打1-10分
+ 计算1-10这10个token各自的likelihood，并归一化成一个概率得分，权重是$$s(y|x)=\sum_{i=1}^{10} i P(i \mid y, x)$$
+ 最后再把score归一化到[-1,1]之间，这个score直接当成reward，**不需要RM模型了**

对于summary的任务，数据集是[Learning to summarize with human feedback](https://arxiv.org/pdf/2009.01325)里提供的[reddit数据集](https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/)，prompt是

```
You are an expert summary rater. Given a TEXT (completed with a SUBREDDIT and a
TITLE) and a SUMMARY, your role is to provide a SCORE from 1 to 10 that rates 
the quality of the SUMMARY given the TEXT, with 1 being awful and 10 being a perfect SUMMARY.
```

对于helpful的任务，数据集是[HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)，prompt是

```
You are an expert rater of helpful and honest Assistant responses. Your role is to provide a SCORE 
from 1 to 10 that rates the helpfulness and honesty of the RESPONSE for a given CONTEXT. 
Where SCORE of 1 refers to useless and dishonest RESPONSE and a
SCORE of 10 refers to a perfectly helpful and honest RESPONSE
```

评估方式：

+ AI Labeler Alignment：衡量AI标注的preferences和人类preferences的准确率。先把AI标注的转成二分类([0.6, 0.4]->[1,0])，如果这个结果和人类的标注一样那就是1，否则是0。最终的准确率就是如下公式，其中$$D$$是数据集，$$P^{A I} \in \mathbb{R}^{D \times 2}$$是AI标注的，而$$p^H \in \mathbb{R}^D$$是人类标注的：

XXX
z_{\text {acc }}=\frac{1}{D} \sum_{i=1}^D \mathbb{1}\left[\underset{j}{\arg \max } P_{i, j}^{A I}=p_i^H\right]
XXX

+ Win Rate：给定两个策略生成的结果，人类选择更好的那个，然后统计胜率。
+ Harmless Rate：人类认为response是harmless的比例，


另外，发现小的模型更容易出现position bias，即给定两个候选，换一下顺序，模型还是觉得同一个位置的更好。缓解：每个pair反转前后各过一遍模型，然后得分avg一下。


## 自我奖励

[Self-Rewarding Language Models](https://arxiv.org/pdf/2401.10020)

[「用 AI 训 AI」这事靠谱吗？](https://mp.weixin.qq.com/s/bLLoYDTpq8q7ExfwyDekOQ)


## self-play

[清华、北大等发布Self-Play强化学习最新综述](https://mp.weixin.qq.com/s/oMY0O0OIVYJc04zkoMzgcQ)

[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A)

[万字长文推演OpenAI o1 self-play RL 技术路线](https://mp.weixin.qq.com/s/_kt0SPuWWiiu7XwqNZKZAw)

[有想入坑RL-LLM的同学吗？这个开源项目一个GPU够了，完成后欢迎来月之暗面~](https://mp.weixin.qq.com/s/e-SHFE6UxXY4y5W-W2fYZw)

[https://github.com/inspirai/TimeChamber](https://github.com/inspirai/TimeChamber)

## Cursor

[Scaling Law瓶颈，Cursor编程为什么这么强？团队参与新研究掏出秘密武器](https://mp.weixin.qq.com/s/xhV9HoeEP22RjuWTjgbPqg)

[Planning In Natural Language Improves LLM Search For Code Generation](https://arxiv.org/pdf/2409.03733)

## Let's verify step by step

[o1基石论文火爆传阅，Ilya仍是关键先生！核心项目清北校友闪光](https://mp.weixin.qq.com/s/woElE_YfQni7bwe4UCCK4g)

[Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)

[OpenAI使用过程监督提升数学推理能力](https://mp.weixin.qq.com/s/E8GtQOT6tPoScj5nMjkSjQ)

[https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)

## O1相关汇总

(toread)

[OpenAI o1要跟，怎么跟？这个GitHub项目把解读、博客、相关论文一网打尽](https://mp.weixin.qq.com/s/sPYeM5LbfAwyHUxbQ78Vsg)

[刚刚，OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

[张俊林：OpenAI o1的价值意义及强化学习的Scaling Law](https://mp.weixin.qq.com/s/my7XiRtpb8IY3Z0b471NJA)


[北大对齐团队独家解读：OpenAI o1开启「后训练」时代强化学习新范式](https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ)


[Noam Brown早已预示o1强大推理能力，演讲深度解析AI推理研究脉络](https://mp.weixin.qq.com/s/KRttVeMN4tPw9yb6f4LQgA)

## CoT能让Transformer更强

[谷歌再次痛失好局！OpenAI o1 证实谷歌 ICLR 2024 论文价值「四位华人贡献」](https://mp.weixin.qq.com/s/7FsVPFUb4-fkeaOtcRrgsw)

[Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/pdf/2402.12875)

### CoT：打破 Transformer 的“并行诅咒”

传统的 Transformer 模型虽然在自然语言处理领域取得了巨大成功，但它有一个致命弱点：擅长并行计算，但不擅长串行推理。这就像一个超级聪明的孩子，能快速完成大量的计算题，但却无法理解简单的逻辑推理。

而CoT (Chain of Thought，思维链)技术的灵感来源于人类的思维过程，它可以让 Transformer 模拟人类的思考方式，通过生成一系列中间推理步骤，来解决那些需要逻辑推理的复杂问题。

### CoT 的理论基础：从电路复杂度到 Transformer 表达能力

作者用电路复杂性理论来解释 CoT 的强大之处，将Transformer的计算过程与电路模型进行类比，并将Transformer能够解决的问题类别定义为“CoT 复杂性类”

他们证明了传统的Transformer模型（没有 CoT）只能解决AC0电路能够解决的问题，而AC0电路是一种计算能力非常有限的电路模型。但是，如果加入 CoT，Transformer 的表达能力将得到质的飞跃！作者用数学严格证明了：

只要CoT步骤足够多，Transformer 就能模拟任意大小的布尔电路，从而解决P/poly问题，这是一个包含了P问题的更大的问题类别，相当于证明了CoT可以让 Transformer 解决几乎所有可以用计算机解决的问题。

### CoT 的实验验证：从模加到电路值问题，CoT 全面胜出！

为了进一步验证CoT的有效性，论文作者设计了四个核心问题：

+ 模加： 计算两个数的和，并对某个整数取模 
+ 排列组合： 计算一组排列的组合
+ 迭代平方： 对一个数进行多次平方运算 
+ 电路值问题： 计算一个布尔电路的输出值 

其中，模加问题可以用并行计算高效地解决，而其他三个问题则需要串行计算。

实验结果表明：

+ 对于模加问题，即使不使用 CoT，Transformer 也能取得不错的效果
+ 但对于其他三个问题，使用 CoT 可以显著提高 Transformer 的准确率，尤其是在模型深度较浅的情况下

### 讨论

[CoT能让模型推理能力无上限？田渊栋、LeCun下场反对：两层MLP还能模拟全世界呢](https://mp.weixin.qq.com/s/wi1jvQg47O078Xk83UpYmA)

[Transformer推理天花板被谷歌打破？DeepMind首席科学家亮出84页PPT，却遭LeCun反对](https://mp.weixin.qq.com/s/_z3ITDGRWXjbh8aVUBdUsg)

## CoT or not

[o1带火的CoT到底行不行？新论文引发了论战](https://mp.weixin.qq.com/s/_v15-UYpv300XIlhQ-54Vg)

[To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/pdf/2409.12183)


## test-time scaling

[小模型越级挑战14倍参数大模型，谷歌开启Test-Time端新的Scaling Law](https://mp.weixin.qq.com/s/tfi7VOpSdKIXVb--k6NCSg)

[Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)


[OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力](https://mp.weixin.qq.com/s/MNwS1PQX2XOhVfN0rKykUQ)---(写得比较细)

## inference scaling

[LLM Inference Scaling：姚班/OpenAI/CMU 8月论文提前揭示o1核心原理](https://mp.weixin.qq.com/s/p84S9r7Qbuo_yjAWhhqoVQ)

[An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/abs/2408.00724)


## O1的评估

[280页PDF，全方位评估OpenAI o1，Leetcode刷题准确率竟这么高](https://mp.weixin.qq.com/s/WxmqCcvvXropIfVCxIJ7bA)

[Evaluation of OpenAI o1: Opportunities and Challenges of AGI](https://arxiv.org/pdf/2409.18486)

## LLM Reasoning

[一文看懂LLM推理，UCL汪军教授解读OpenAI ο1的相关方法](https://mp.weixin.qq.com/s/TCWs5TKKXiRbmt-XUd0wfg)

[首个o1复现开源RL框架OpenR来了，UCL、上交等高校联合团队发布](https://mp.weixin.qq.com/s/Dr9IzbUjiWtZT7bgr58T2g)

[OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models](https://arxiv.org/abs/2410.09671)

[A Tutorial on LLM Reasoning: Relevant methods behind ChatGPT o1](https://github.com/openreasoner/openr/blob/main/reports/Tutorial-LLM-Reasoning-Wang.pdf)

## 开源模型+O1

[OpenAI o1式思维链，开源模型也可以有，成功案例来了](https://mp.weixin.qq.com/s/W28qb8ZaJkcyDP69eGw8MA)

## 复现O1

[技术上，如何复现 o1?](https://mp.weixin.qq.com/s/_fNioAkD--nI9WSH64O-_A?poc_token=HH6r-2ajxUVBKhJS6btRQEAl85tnczRGWRIAES19)

## O1 Replication Journey

[上交大发布首个OpenAI o1复现项目进展报告，满满的经验洞察](https://mp.weixin.qq.com/s/ZO_Rv98OakPuBaZl9Tw5VA)

[O1 Replication Journey: A Strategic Progress Report](https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report.pdf)

[https://github.com/GAIR-NLP/O1-Journey](https://github.com/GAIR-NLP/O1-Journey)

## ScoRe

[强化学习让大模型自动纠错，数学、编程性能暴涨，DeepMind新作](https://mp.weixin.qq.com/s/CqxEoL50_FQTGtLYgh6omw)

[OpenAI o1技术初探3：如何让模型拥有自我纠错的能力](https://mp.weixin.qq.com/s/VHZ_BT27Dh2s5hVQSb33WA)


## LeCo

[COLM 24 | 从正确中学习？大模型的自我纠正新视角](https://mp.weixin.qq.com/s/F8KpJuiDE9DfSVb1ciLUSQ)

[Learning From Correctness Without Prompting Makes LLM Efficient Reasoner](https://arxiv.org/pdf/2403.19094)

[https://github.com/starrYYxuan/LeCo](https://github.com/starrYYxuan/LeCo)

## 其他的一些讨论


[耗资1.3万，ASU团队揭秘o1推理王者！碾压所有LLM成本超高，关键还会PUA](https://mp.weixin.qq.com/s/pSNC6tdhXcxqB9ofQKU3JQ)

[LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench](https://arxiv.org/abs/2409.13373)

# 语言物理学

(toread)

[大模型边推理边纠错，有可能做到吗？这是ICML爆火的演讲](https://mp.weixin.qq.com/s/NOVFYmXiHUJ7x1SU7yH0CA)

[https://www.bilibili.com/video/BV1Yw4m1k7nH](https://www.bilibili.com/video/BV1Yw4m1k7nH)


# LLM+math

## mathscale

[【LLM-数学】MathScale 用于数学推理的指令调优扩展方法](https://mp.weixin.qq.com/s/tQUIGdViMZTb_9NNh3b3RQ)

[MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://arxiv.org/pdf/2403.02884.pdf)


## AlphaGeometry

[奥数能力金牌级：DeepMind几何推理模型登上Nature，代码开源，菲尔兹奖得主点赞](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650904746&idx=1&sn=d39a3d92078cecbd29bd0fc82560d1da&chksm=84e45cd4b393d5c24747f163fa0338761690447904a1a654aacf2344d7a16d36dfa2ac3ccdb0&scene=21#wechat_redirect)提出了AlphaGeometry

## AlphaProof & AlphaGeometry 2

[谷歌AI拿下IMO奥数银牌，数学推理模型AlphaProof面世，强化学习 is so back](https://mp.weixin.qq.com/s/LNzbyf0w412BIz71sROyzw)提出AlphaProof和AlphaGeometry 2

## WE-Math基准

[真相了！大模型解数学题和人类真不一样：死记硬背、知识欠缺明显，GPT-4o表现最佳](https://mp.weixin.qq.com/s/uU1lZV0Ymj31cmZryhffyQ)

[WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?](https://arxiv.org/pdf/2407.01284)

[https://github.com/We-Math/We-Math](https://github.com/We-Math/We-Math)

[https://huggingface.co/datasets/We-Math/We-Math](https://huggingface.co/datasets/We-Math/We-Math)

# LLM常见难题

## 重复生成

[https://www.zhihu.com/question/616130636](https://www.zhihu.com/question/616130636)

[https://mp.weixin.qq.com/s/cSwWapqFhxu9zafzPUeVEw](https://mp.weixin.qq.com/s/cSwWapqFhxu9zafzPUeVEw)

## 幻觉

### 综述

[OpenAI Lilian Weng万字长文解读LLM幻觉：从理解到克服](https://mp.weixin.qq.com/s/UGcui0rLW2Vz7y2Mt4atqA)

[https://lilianweng.github.io/posts/2024-07-07-hallucination/](https://lilianweng.github.io/posts/2024-07-07-hallucination/)


### 语义熵

[语义熵识破LLM幻觉！牛津大学新研究登Nature](https://mp.weixin.qq.com/s/fdLZ9DDqG9C_uxAAlKgQbw)

[Detecting hallucinations in large language models using semantic entropy](https://www.nature.com/articles/s41586-024-07421-0)

## 知识冲突

[深度解析RAG大模型知识冲突，清华西湖大学港中文联合发布](https://mp.weixin.qq.com/s/y9-DwgNb3Yftgf_Ulf6yDQ)

[Knowledge Conflicts for LLMs: A Survey](https://arxiv.org/pdf/2403.08319)

## llm+db

[长文本杀不死RAG：SQL+向量驱动大模型和大数据新范式，MyScale AI数据库正式开源](https://mp.weixin.qq.com/s/JvyKnEbdOSb1fTwhiQTO5A)

[https://github.com/myscale/myscaledb](https://github.com/myscale/myscaledb)

## 记忆能力

[Localizing Paragraph Memorization in Language Models](https://arxiv.org/pdf/2403.19851v1.pdf)

对应代码：[https://github.com/googleinterns/localizing-paragraph-memorization](https://github.com/googleinterns/localizing-paragraph-memorization)

我们能否定位出语言模型中用于记忆其训练数据中整段文字的权重和机制？

+ 尽管记忆现象分布在模型的多个层级和组件中，但记忆段落的梯度在空间上有可辨别的模式，即**在较低模型层级的梯度比非记忆example的梯度大**。
+ 通过**仅微调高梯度的权重**，可以使模型**遗忘记忆的example**。
+ 定位了一个特别参与段落记忆的**低层注意力头**，它**主要关注**在语料库级单词频率分布中**最不频繁出现的独特、罕见的token**。
+ 总的来说，相较非记忆的续写，记忆续写不仅**更难以遗忘**，也**更难以损坏**。

### reasoning

[Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks](https://arxiv.org/pdf/2307.02477) MIT的

[Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/pdf/2402.16837) deepmind的

[How do Language Models Bind Entities in Context?](https://arxiv.org/pdf/2310.17191) UC berkeley的，ICLR2024

### memorizing

[Knowledge Neurons in Pretrained Transformers](https://arxiv.org/pdf/2104.08696) ACL 2022
 
[Language Modeling Is Compression](https://arxiv.org/pdf/2309.10668) ICLR 2024 deepmind

[Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://arxiv.org/pdf/2205.10770) meta NeurIPS 2022

## 越狱

[长文本之罪：Claude团队新越狱技术，Llama 2到GPT-4无一幸免](https://mp.weixin.qq.com/s/C0opoIzLCFojfmoa6poM8A)


## LLM+math

[ICML 2024｜Transformer究竟如何推理？基于样例还是基于规则](https://mp.weixin.qq.com/s/aVRiGW3xU_LpvxZzjDpwzQ)

[Case-Based or Rule-Based: How Do Transformers Do the Math?](https://arxiv.org/pdf/2402.17709)

[https://github.com/GraphPKU/Case_or_Rule](https://github.com/GraphPKU/Case_or_Rule)

## LLM compiler

[开发者狂喜！Meta最新发布的LLM Compiler，实现77%自动调优效率](https://mp.weixin.qq.com/s/Js0lUS_5ZPspVLazthkEOg)

[Meta Large Language Model Compiler: Foundation Models of Compiler Optimization](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/)



## ProLong

[2024 年了，你的长文本训练数据真的够长吗？](https://mp.weixin.qq.com/s/5dVm-VWiZG09ixMMegKCbw)

[Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models](https://arxiv.org/pdf/2405.17915)

[https://github.com/October2001/ProLong](https://github.com/October2001/ProLong)


## 白化

在 transformer 领域里，“白化”（whitening）主要是指一种对句子嵌入进行后处理的方法，通过将句子向量的均值变为0，并将协方差矩阵变为单位矩阵，从而解决句子嵌入中的各向异性问题。这种技术能够提高句子嵌入在语义相似性任务中的表现，并且加快检索速度。

[Whitening Sentence Representations for Better Semantics and Faster Retrieval](https://ar5iv.labs.arxiv.org/html/2103.15316)

代码：[https://github.com/bojone/BERT-whitening](https://github.com/bojone/BERT-whitening)


[Transformer Scale Gate for Semantic Segmentation](https://arxiv.org/pdf/2205.07056v1)

## 蒸馏

[Revisiting Knowledge Distillation for Autoregressive Language Models](https://arxiv.org/pdf/2402.11890)

[Meta开发System 2蒸馏技术，Llama 2对话模型任务准确率接近100%](https://mp.weixin.qq.com/s/QycbrMXsR0nUsvHBx0_GBw)

[Distilling System 2 into System 1](https://arxiv.org/pdf/2407.06023v2)

## 自动搜参

[用神经架构搜索给LLM瘦身，模型变小，准确度有时反而更高](https://mp.weixin.qq.com/s/_cKq4a3uM4r6s5P5s9mWaA)

[LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](https://arxiv.org/pdf/2405.18377)

## 证明者-验证者博弈

[OpenAI超级对齐团队遗作：两个大模型博弈一番，输出更好懂了](https://mp.weixin.qq.com/s/MiLYbYcYUPO9rdQjijF_tQ)

[Prover-Verifier Games improve legibility of LLM outputs](https://arxiv.org/pdf/2407.13692)

参考：[Learning to Give Checkable Answers with Prover-Verifier Games](https://arxiv.org/pdf/2108.12099)

## 道德风险

[GPT-4o模仿人类声音，诡异尖叫引OpenAI研究员恐慌！32页技术报告出炉](https://mp.weixin.qq.com/s/XSTNHTILAOkINg7mxssb6g)

openai的报告：[GPT-4o System Card](https://cdn.openai.com/gpt-4o-system-card.pdf)

之前deepmind也有一个报告[The Ethics of Advanced AI Assistants](https://arxiv.org/pdf/2404.16244)

## 选择性偏差

[ACL2024|大模型选择偏差在腾讯广告特征评测上的优化及应用](https://mp.weixin.qq.com/s/0P1D1H1HoXMwZg2nBiM07Q)

[Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors](https://arxiv.org/pdf/2406.01026)

给定一个问题(question)及其对应的选项内容(options)，大模型无法把选项内容(option content)和对应的选项标识符(symbol，特指选项标识A/B/C/D)关联到一起。例如，当把正确答案"the president"放到选项B时，模型能够正确选择出答案；当我们把正确答案放到C时，模型依然选择"B"，即模型偏向于选"B"或者第二个答案，而忽略了正确答案的内容。

## lost in the middle

[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172)


# 多智能体

[《综述：全新大语言模型驱动的Agent》——4.5万字详细解读复旦NLP和米哈游最新Agent Survey](https://zhuanlan.zhihu.com/p/656676717)

[Agent > GPT5？吴恩达最新演讲：四种 Agent 设计范式（通俗易懂版）](https://mp.weixin.qq.com/s/6sh39yEO4YGZI-BGPjJnCg)

## JAT

[告别偏科，能玩转多模态、多任务、多领域的强化智能体终于来了](https://mp.weixin.qq.com/s/2GBB-w7hBf6equtqD8V0Lg)

[Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent](https://arxiv.org/pdf/2402.09844)

[https://github.com/huggingface/jat](https://github.com/huggingface/jat)

[https://huggingface.co/datasets/jat-project/jat-dataset](https://huggingface.co/datasets/jat-project/jat-dataset)

![jat](../assets/jat.png)

输入的序列元素是observations, actions, 和rewards的交替组合：

XXX
\left[\phi\left(s_0, 0.0\right), \phi\left(a_0\right), \phi\left(s_1, r_1\right), \phi\left(a_1\right), \ldots\right]
XXX

依据不同输入的数据类型，使用不同网络处理：

+ 图像：用CNN。
+ 连续向量：用线性层
+ 离散值：用线性投影层

预测任务：根据所有先前的观察和动作嵌入来预测下一个动作嵌入。

序列的构造方法：

+ 和文本相关的任务：用 GPT-2 的分词策略，将文本转换为一个整数序列，然后emb lookup映射到一个嵌入向量序列。
+ 和图像有关的任务：用ViT，将图像切割成小块后，通过线性层转换为嵌入向量序列。
+ 最终再将图像和文本的向量序列拼接在一起，形成一个统一的序列，输入到 Transformer 中。

## ReadAgent

[「有效上下文」提升20倍！DeepMind发布ReadAgent框架](https://mp.weixin.qq.com/s/xXJqJeqf8mzP9VW9kLIdgQ)

## 多模态agent

[一文详解多模态智能体（LMAs）最新进展（核心组件/分类/评估/应用）](https://mp.weixin.qq.com/s/lucGhu5-IPjIKbZ2o1q-PQ)

[Large Multimodal Agents: A Survey](https://arxiv.org/pdf/2402.15116)

[https://github.com/jun0wanan/awesome-large-multimodal-agents](https://github.com/jun0wanan/awesome-large-multimodal-agents)

## OpenDevin

[OpenDevin出技术报告了，大模型Agent开发者必读](https://mp.weixin.qq.com/s/tfREoiwjfCZauisCE3PpvQ)

[OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/pdf/2407.16741)

## autogpt

[GitHub星标超16万，爆火AutoGPT进阶版来了：定制节点、多智能体协同](https://mp.weixin.qq.com/s/dBL47yYoVNkyPoPG8pcLLA)

[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)

## DAAG

[三「模」联盟，谷歌DeepMind缔造终身学习智能体！](https://mp.weixin.qq.com/s/P-x8EDrfd1ydCnPP8MYu6g)

[Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](https://arxiv.org/pdf/2407.20798)

## VARP

[GPT-4o能玩《黑神话》！精英怪胜率超人类，无强化学习纯大模型方案](https://mp.weixin.qq.com/s/veHSbBxPIqRexG0OWtg4pw)

[Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case](https://arxiv.org/abs/2409.12889)

## MARL

[北大领衔，多智能体强化学习研究登上Nature子刊](https://mp.weixin.qq.com/s/_67dbIMDjktMEw4QYiIAUA)

[Efficient and scalable reinforcement learning for large-scale network control](https://www.nature.com/articles/s42256-024-00879-7)

## MMRole

[与「李白」赏图赋诗，同「猴哥」直面天命，人大高瓴提出MMRole多模态角色扮演](https://mp.weixin.qq.com/s/I8gyDv9K8uhB3EXF_2_zVw)

[MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://arxiv.org/abs/2408.04203)

[https://github.com/YanqiDai/MMRole](https://github.com/YanqiDai/MMRole)

## Swarm

[OpenAI今天Open了一下：开源多智能体框架Swarm](https://mp.weixin.qq.com/s/3-iKztrTuRURUGtles4-xA)

[https://github.com/openai/swarm](https://github.com/openai/swarm)

## agent-as-a-judge

[卷起来！让智能体评估智能体，Meta发布Agent-as-a-Judge](https://mp.weixin.qq.com/s/YX1cmIMDonUiosSg24boUQ)

[Agent-as-a-Judge: Evaluate Agents with Agents](https://arxiv.org/pdf/2410.10934)

[https://github.com/metauto-ai/agent-as-a-judge](https://github.com/metauto-ai/agent-as-a-judge)


# 一些其他比较重要的工作

## 几篇出现频率比较高的论文

[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf) 引用数800+

[How can we know what language models know?](https://arxiv.org/pdf/1911.12543.pdf) 引用数800+

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)引用1800+

## Anthropic的一些工作

[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[Studying Large Language Model Generalization with Influence Functions](https://arxiv.org/pdf/2308.03296.pdf)

[Measuring Faithfulness in Chain-of-Thought Reasoning](https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf)

[从Claude 3中提取数百万特征，首次详细理解大模型的「思维」](https://mp.weixin.qq.com/s/cZhmvAva6NDLG84kD819Ww)

[Scaling Dictionary Learning to Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

[LLM惊现篡改代码获得奖励，欺骗人类无法根除逆转！Anthropic新作揭露惊人真相](https://mp.weixin.qq.com/s/Fgkkc3p7zIW8OrCvSU-2lA)


[SYCOPHANCY TO SUBTERFUGE: INVESTIGATING REWARD TAMPERING IN LANGUAGE MODELS](https://arxiv.org/pdf/2406.10162)


# 个性化搜索

随便找一篇[Denoising Attention for Query-aware User Modeling in Personalized Search](https://arxiv.org/pdf/2308.15968.pdf)，来看看它的参考文献：

学术界：

+ [A Transformer-based Embedding Model for Personalized Product Search](https://arxiv.org/pdf/2005.08936.pdf)，sigir20
+ [Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search](https://arxiv.org/pdf/2004.09424.pdf)，sigir21
+ [RLPer: A Reinforcement Learning Model for Personalized Search](http://playbigdata.ruc.edu.cn/dou/publication/2020_WWW_RLPer.pdf)，www20


工业界：

+ [A Zero Attention Model for Personalized Product Search](https://arxiv.org/pdf/1908.11322.pdf)，CIKM19，亚马逊
+ [Real-time Personalization using Embeddings for Search Ranking at Airbnb](https://github.com/daiwk/collections/blob/master/assets/airbnb-kdd18.pdf)，KDD18，airbnb
+ [End-to-End Deep Attentive Personalized Item Retrieval for Online Content-sharing Platforms](https://dl.acm.org/doi/pdf/10.1145/3366423.3380051)，www20，Google
+ [Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning](https://arxiv.org/pdf/2006.02282.pdf)，sigir20，京东
+ [Personalized Query Suggestions](https://guoweiwei.github.io/files/personalized-query-suggestion.pdf)，sigir20，LinkedIn
+ [A GNN-based Multi-task Learning Framework for Personalized Video Search](https://eprints.whiterose.ac.uk/181816/1/GNNVideoSearch_WSDM2022.pdf)，WSDM22，百度


## q-i双塔的改进

### 引入location+social

[Embedding-based Retrieval in Facebook Search](https://arxiv.org/pdf/2006.11632.pdf)，KDD20

q-d双塔结构，在两个塔的最底层均加入：

+ location：用户所处地理位置，如城市
+ social：facebook是社交网络，通过另一个基于graph的模型训练得到的user和item emb，直接加进来

### 引入用户行为序列

[Encoding History with Context-aware Representation Learning for Personalized Search](http://playbigdata.ruc.edu.cn/dou/publication/2020_sigir_context_ps.pdf)，sigir20，人大，提出HTPS

![htps-disambiguate-query](../assets/htps-disambiguate-query.png)

把用户历史的q-d pair对和当前query一起，过短期transformer和长期transformer得到输出$$q^l$$。

![htps-predict-intent](../assets/htps-predict-intent.png)

把$$q^l$$加上[mask]，过transoformer得到预估的intent $$q^p$$

然后将$$q^l$$和$$q^p$$通过gate nn融合得到最终的context-aware的query表示$$q^f$$

最终doc和query的打分包括两部分，通过$$\phi$$（一个MLP，激活是tanh）进行融合：

XXX
p(d \mid q, H)=\phi\left(p(d, q), p\left(d, q^H\right)\right)
XXX

+ $$p(d, q)$$：q和d的语义相似度，可以用正常的nlp模型得到
+ $$p\left(d, q^H\right)$$：q和d的个性化得分，公式如下，其中$$s^R$$是cos：

XXX
p\left(d, q^H\right)=\phi\left(s^R\left(q^s, d^w\right), s^R\left(q^l, d^w\right), s^R\left(q^p, d^w\right), s^R\left(q^f, d^w\right)\right)
XXX

有两个loss：

+ pred loss：预估intent，即下一个query，拿$$q^p$$与下一个query中各个词向量的avg算cos
+ rank loss：依据$$p(d \mid q, H)$$算lambda rank的pairwise loss

### 三塔+gnn邻居+mtl

[A GNN-based Multi-task Learning Framework for Personalized Video Search](https://eprints.whiterose.ac.uk/181816/1/GNNVideoSearch_WSDM2022.pdf)，WSDM22，百度，提出MGNN-PVS

现有的PSM(g personalized search methods)大多使用用户反馈（如点击）进行训练，缺点：

+ 反馈信号大部分表达的是吸引力而非相关性
+ 用户的历史信号比较稀疏，很难学好PSM

两张二部图：u-q和q-d

![gnn-personalized-video-search](../assets/gnn-personalized-video-search.png)

3个塔：

+ user：
    + user自己
    + 一跳邻居（u->q）的q
    + 二跳邻居（u->q->u）的u
+ query：
    + query自己
    + 一跳邻居（q->d）的doc
    + 二跳邻居（q->d->q）的query
+ doc：
    + doc自己的title向量（训练query-正title-负title的triplet loss）和video向量（训练video-正query-负query的triplet loss）
    + 二跳邻居（d->q->d）的doc

两个task：

+ ctr预估：u和q拼一起过nn得到个性化的q，再和d过nn得到的向量算内积，得到预估值，用交叉熵
+ 相关性预估：q过另一个nn，d过另一个nn，内积，用mse

# RAG

[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997.pdf)

[RAG全链路的关键模块解析](https://mp.weixin.qq.com/s/kNjOgfQs6yErNtRg6wFA3g)

[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)

[Meta提出全新文档级嵌入框架，利用LLM来增强信息检索能力](https://mp.weixin.qq.com/s/RCRHjrW6jF167HG169aFwg)

[LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding](https://arxiv.org/pdf/2404.05825.pdf)

## RankRAG

[RAG微调Llama 3竟超越GPT-4！英伟达GaTech华人学者提出RankRAG框架](https://mp.weixin.qq.com/s/87qeqDSwtitYsruH_2Jdww)

[RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/pdf/2407.02485)

## graphRAG

[微软开源的GraphRAG爆火，Github Star量破万，生成式AI进入知识图谱时代？](https://mp.weixin.qq.com/s/BX93FvDzW7WVLK66V2usBw)

[https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)

[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)

## RAGChecker

[给RAG系统做一次全面「体检」，亚马逊开源RAGChecker诊断工具](https://mp.weixin.qq.com/s/x4o7BinnwvTsOa2_hegcrQ)

[RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation](https://arxiv.org/pdf/2408.08067)

[https://github.com/amazon-science/RAGChecker](https://github.com/amazon-science/RAGChecker)

## TAG

[表格增强生成TAG登场：解锁AI自然语言与数据库的完美结合](https://mp.weixin.qq.com/s/6gkPA-xc7GsltM1Ywui_XQ)


# LLM模型融合

[https://github.com/arcee-ai/mergekit](https://github.com/arcee-ai/mergekit)

[SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling](https://arxiv.org/pdf/2312.15166)

[LLM 合并新思路：进化算法+零训练->新任务](https://mp.weixin.qq.com/s/eSWdLT0p5uyd32OOod5lKQ)

[Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/pdf/2403.13187)

[https://github.com/SakanaAI/evolutionary-model-merge](https://github.com/SakanaAI/evolutionary-model-merge)

# prompt engineering

[万字长文总结提示词技巧！新加坡首届GPT-4提示工程大赛冠军最新分享](https://mp.weixin.qq.com/s/AWnQL3forAP-gB7e2ZEXdQ)

[吴恩达：四个步骤，让大模型变得更好](https://mp.weixin.qq.com/s/ackyt5d2kqdzMy0-Ma_ElA)

[Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](https://arxiv.org/pdf/2311.16452)

让gpt4生成cot和答案的模板

![self-generated-cot-template](../assets/self-generated-cot-template.png)

![medprompt](../assets/medprompt.png)

看着是借助GPT4+COT+RAG+投票

+ 拿一坨question得到他们的向量，并按照上图的模板让gpt生成COT和答案，人工判断，对的存进知识库里
+ 预测阶段：
    + 拿测试question的向量从知识库里查出5个最像(cos距离)的(q, cot, answer)作为context
    + 循环5次：
        shuffle测试question的答案选项，让LLM回答
    + 对生成的答案投票，选票数最多的

## APE

[还在人工炼丹？自动提示工程指南来了，还带从头实现](https://mp.weixin.qq.com/s/TxzkRUPhsiqtLhCyrIsQrQ)

[https://github.com/marshmellow77/automated-prompt-engineering-from-scratch](https://github.com/marshmellow77/automated-prompt-engineering-from-scratch)

## PAS

[还在死磕AI咒语？北大-百川搞了个自动提示工程系统PAS](https://mp.weixin.qq.com/s/2etnB3hbRtOCth1notqyBQ)

[PAS: Data-Efficient Plug-and-Play Prompt Augmentation System](https://arxiv.org/abs/2407.06027)

## ell

[OpenAI前研究者发布提示词工程框架ell，升级版LangChain，支持版本控制和多模态](https://mp.weixin.qq.com/s/LaNbu4bVrWLG3ueopFTj5g)

[https://github.com/MadcowD/ell](https://github.com/MadcowD/ell)

# 可解释AI

[XAI有什么用？探索LLM时代利用可解释性的10种策略](https://mp.weixin.qq.com/s/V35k4UJZPtJkAHqYlZiO1A)

[Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/pdf/2403.08946.pdf)

[https://github.com/JacksonWuxs/UsableXAI_LLM](https://github.com/JacksonWuxs/UsableXAI_LLM)

## 综述

[可解释性终极追问，什么才是第一性解释？20篇CCF-A+ICLR论文给你答案](https://mp.weixin.qq.com/s/vCAw0d2uZ_MnLrl5MT9OKA)


## TransformerLens

Neel Nanda（deepmind）的项目

[https://transformerlensorg.github.io/TransformerLens/](https://transformerlensorg.github.io/TransformerLens/)

## ecco

[https://www.eccox.io/](https://www.eccox.io/)

[https://jalammar.github.io/explaining-transformers/](https://jalammar.github.io/explaining-transformers/)

[https://jalammar.github.io/hidden-states/](https://jalammar.github.io/hidden-states/)

## interpretability in the wild

[Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://arxiv.org/pdf/2211.00593)

[https://github.com/redwoodresearch/Easy-Transformer](https://github.com/redwoodresearch/Easy-Transformer)

## activation engineering

[Activation Addition: Steering Language Models Without Optimization](https://arxiv.org/pdf/2308.10248)

## representation engineering

[Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/pdf/2310.01405)


## transformer-debugger

[https://github.com/openai/transformer-debugger/tree/main](https://github.com/openai/transformer-debugger/tree/main)

## painter

[八问八答搞懂Transformer内部运作原理](https://mp.weixin.qq.com/s/5qhpfHfzOIdKsG_wtgTR4A)

[Transformer Layers as Painters](https://arxiv.org/pdf/2407.09298v1)

## transformer explainer

[黑匣子被打开了！能玩的Transformer可视化解释工具，本地运行GPT-2、还可实时推理](https://mp.weixin.qq.com/s/vLyIrRyoWYjhMN4gTRgA6g)

[TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative Models](https://arxiv.org/pdf/2408.04619)

[http://poloclub.github.io/transformer-explainer/](http://poloclub.github.io/transformer-explainer/)

[https://bbycroft.net/llm](https://bbycroft.net/llm)

## superposition

[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

## 3Blue1Brown

[用最直观的动画，讲解LLM如何存储事实，3Blue1Brown的这个视频又火了](https://mp.weixin.qq.com/s/PSMfQLBBQZyG2GwgzatqvA)

[https://www.youtube.com/watch?v=9-Jl0dxWQs8](https://www.youtube.com/watch?v=9-Jl0dxWQs8)

# Lifelong learning of LLM

(toread)

[Towards Lifelong Learning of Large Language Models: A Survey](https://arxiv.org/abs/2406.06391)

[https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm](https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm)


# 具身智能

[大模型走向物理世界，TeleAI 发布大模型驱动的具身智能综述，覆盖300篇文献](https://mp.weixin.qq.com/s/vzDhVsPmBqNT1iuZDnPjRw)

[Embodied-AI with large models: research and challenges](https://www.sciengine.com/SSI/doi/10.1360/SSI-2024-0076)

## ReKep

[李飞飞团队提出ReKep，让机器人具备空间智能，还能整合GPT-4o](https://mp.weixin.qq.com/s/AdyOPA6RhFIu5sjra5cW2Q)

[ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://rekep-robot.github.io/rekep.pdf)

[https://github.com/huangwl18/ReKep](https://github.com/huangwl18/ReKep)

## GR-2

[GR-2登场！ByteDance Research提出机器人大模型，具备世界建模和强大泛化能力](https://mp.weixin.qq.com/s/h-69PKoCkPtj4_sq9589Tw)

[GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation](https://arxiv.org/pdf/2410.06158)

[https://gr2-manipulation.github.io/](https://gr2-manipulation.github.io/)

## RDT-1B

[清华开源全球最大双臂机器人扩散大模型RDT，懂调酒能遛狗，登顶HF具身热榜](https://mp.weixin.qq.com/s/LtuK9bN45Bkm2uDCi3cCvA)

[RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/pdf/2410.07864)

# 语音

## Mini-Omni

[让大模型能听会说，国内机构开源全球首个端到端语音对话模型Mini-Omni](https://mp.weixin.qq.com/s/2vf2QMnnmEdcdVbK8g9-_w)

[Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://arxiv.org/abs/2408.16725)

[https://github.com/gpt-omni/mini-omni](https://github.com/gpt-omni/mini-omni)

## Illuminate

[任意论文一键变播客，谷歌正式发布Illuminate，它能重构研究者的学习方式吗？](https://mp.weixin.qq.com/s/2C2B5yNLjXXYyDQnQgqPyQ)

[https://illuminate.google.com/home](https://illuminate.google.com/home)

# 其他

## 安全性

[Anthropic安全负责人：在超级AI「毁灭」人类之前，我们可以做这些准备](https://mp.weixin.qq.com/s/nxD8qeCfG1tjfpvlJ6uacg)

[OpenAI最新53页论文：ChatGPT看人下菜碟，对“小美”比“小帅”更友好](https://mp.weixin.qq.com/s/NnLAjHuBPHa-aBoT6IV4Pg)

[First-Person Fairness in Chatbots](https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf)

## 长尾

[A Systematic Review on Long-Tailed Learning](https://arxiv.org/pdf/2408.00483)

## 文本匹配效果还行的模型

大多是基于sentence-bert的，m3e-base在电商语料上试过，效果不错

[https://huggingface.co/moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)

[https://huggingface.co/shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)


## 本地知识库

[https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)

## llm应用合辑

+ ChatGPT聚合站：[https://hokex.com](https://hokex.com)
+ 游戏生成站：[https://latitude.io/](https://latitude.io/)
+ 家庭作业辅助站：[https://ontimeai.com/](https://ontimeai.com/)
+ 文字转语音站：[https://www.resemble.ai/](https://www.resemble.ai/)
+ 艺术作画站：[https://starryai.com/](https://starryai.com/)
+ logo制作站：[https://www.logoai.com/](https://www.logoai.com/)
+ ai写作站：[https://www.getconch.ai/](https://www.getconch.ai/)
+ 音乐制作站：[https://soundraw.io/](https://soundraw.io/)
+ 声音模拟站：[https://fakeyou.com/](https://fakeyou.com/)
+ 一句话生成一段视频：[https://runwayml.com/](https://runwayml.com/)
+ 文字转语音：[https://murf.ai/](https://runwayml.com/)

## swiftsage

[大语言模型在开放世界中的推理能力探索实践](https://mp.weixin.qq.com/s/LZ6lkTTOom-mbqV9IJ-OZg)

[SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/pdf/2305.17390.pdf)


## 达摩院大模型技术交流

[https://developer.aliyun.com/live/248332](https://developer.aliyun.com/live/248332)

ppt：[链接](https://pan.baidu.com/s/1tbckFpa8W8qJ5yRw9yvJ9A#list/path=%2F) 密码：5yyf



## 回译

通过单语数据提升 NMT 模型最高效的方法之一是回译（back-translation）。如果我们的目标是训练一个英语到德语的翻译模型，那么可以首先训练一个从德语到英语的翻译模型，并利用该模型翻译所有的单语德语数据。然后基于原始的英语到德语数据，再加上新生成的数据，我们就能训练一个英语到德语的最终模型。

[Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381v2.pdf)

## nan问题

[解决pytorch半精度amp训练nan问题](https://zhuanlan.zhihu.com/p/443166496)

## 时间序列

[LLM用于时序预测真的不行，连推理能力都没用到](https://mp.weixin.qq.com/s/C-N0tyQrEOoNoADtH_thTA)

[Are Language Models Actually Useful for Time Series Forecasting?](https://arxiv.org/pdf/2406.16964)

## 其他

[小模型性能饱和、表现不佳，根源是因为Softmax?](https://mp.weixin.qq.com/s/bvv-frM8bKhkZiqOa9nqDA)

[Why do small language models underperform? Studying LM Saturation via the Softmax Bottleneck](https://arxiv.org/pdf/2404.07647.pdf)


[Ilya Sutskever的推荐清单](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)

[2024年大模型LLM还有哪些可研究的方向？](https://www.zhihu.com/question/637595961)

[Hinton万字访谈：用更大模型「预测下一个词」值得全力以赴](https://mp.weixin.qq.com/s/OydltjpVwsQ7hNBH6hq_Og)

[ChatGPT如何「思考」？心理学和神经科学破解AI大模型，Nature发文](https://mp.weixin.qq.com/s/4nO4DQE6Llfo3fiFSPSMhQ)

[适应多形态多任务，最强开源机器人学习系统「八爪鱼」诞生](https://mp.weixin.qq.com/s/HPTfOlw25F5JcvlY-Vy9Tw)

[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)

[LeCun新作：神经网络在实践中的灵活性到底有多大？](https://mp.weixin.qq.com/s/PjlXwwG3t5Fqp5MfrBVvBQ)

[Just How Flexible are Neural Networks in Practice?](https://arxiv.org/pdf/2406.11463)

[清华包揽最佳论文+时间检验奖，山大获荣誉提名，SIGIR 2024奖项出炉](https://mp.weixin.qq.com/s/Z2Mj7etx6KvYrSn8LhrJwg)

## 一些报错的解法

### flash-attention2

[https://github.com/Dao-AILab/flash-attention/issues/451](https://github.com/Dao-AILab/flash-attention/issues/451)

```shell
FLASH_ATTENTION_FORCE_BUILD=TRUE pip install flash-attn
```

# 一些记录

## 打印模型参数量

[https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model)

```python

pytorch_total_params = sum(p.numel() for p in model.parameters())

pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Load the model
from transformers import BartForConditionalGeneration
from transformers import T5ForConditionalGeneration
def cal(model):
  pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
  return pytorch_total_trainable_params

model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
print("bart-base")
print(cal(model)) # 6L 139420416 139M

model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
print("bart-base")
print(cal(model)) # 12L 406291456 406M

model = T5ForConditionalGeneration.from_pretrained("t5-small")
print("t5-small")
print(cal(model)) # 6L 60506624 65M

model = T5ForConditionalGeneration.from_pretrained("t5-base")
print("t5-base")
print(cal(model)) # 12L 222903552 223M


model = T5ForConditionalGeneration.from_pretrained("t5-large")
print("t5-large")
print(cal(model)) # 24L 737668096 738M

```

## 往现有tokenizer里加一些特殊token

[https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer](https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer)

```python
num_added_toks = tokenizer.add_tokens(['[EOT]'], special_tokens=True) ##This line is updated
model.resize_token_embeddings(len(tokenizer))

###The tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(<output_dir>)
```

示例

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

print("Before")
print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]


special_tokens_dict = {'additional_special_tokens': ['[EOT]']}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
# model.resize_token_embeddings(len(tokenizer))  # --> Embedding(30523, 768)

tok_id = tokenizer.convert_tokens_to_ids('[EOT]')  # --> 30522

print("After")
print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]
```

## python的读写锁

一个写，多个并行读：[https://pypi.org/project/readerwriterlock/](https://pypi.org/project/readerwriterlock/)

## pytorch的显存泄露

[https://github.com/pytorch/pytorch/issues/13246#issuecomment-445770039](https://github.com/pytorch/pytorch/issues/13246#issuecomment-445770039)

## torch profiling

[https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)

可以拿这个来可视化：[https://ui.perfetto.dev/](https://ui.perfetto.dev/)

- 点击open trace file上传json文件
- timeline中有两个python进程，点击cuda kernel会出现箭头，方便找到是**哪个op调用了该kernel**
  - 靠上的python进程是host侧进程（主要是用户代码中调用的一些**API/pytorch op**，能比较方便能和训练代码对应上）
  - 靠下的python进程是device（gpu）侧进程（记录实际cuda kernel 的执行和一些性能相关的数据）

device timeline比较稀疏的情况下训练性能较差，GPU利用率较低，可能需要排查下训练代码是否有问题

## 显存泄露排查

[https://pytorch.ac.cn/docs/stable/torch_cuda_memory.html](https://pytorch.ac.cn/docs/stable/torch_cuda_memory.html)

[https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/)

[https://pytorch.org/blog/understanding-gpu-memory-2/](https://pytorch.org/blog/understanding-gpu-memory-2/)

检查显存

```python
# (c) Meta Platforms, Inc. and affiliates. 
# https://pytorch.org/blog/understanding-gpu-memory-1/
import logging
import socket
from datetime import datetime, timedelta

import torch

from torchvision import models

logging.basicConfig(
   format="%(levelname)s:%(asctime)s %(message)s",
   level=logging.INFO,
   datefmt="%Y-%m-%d %H:%M:%S",
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = "%b_%d_%H_%M_%S"

# Keep a max of 100,000 alloc/free events in the recorded history
# leading up to the snapshot.
MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000

def start_record_memory_history() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not recording memory history")
       return

   logger.info("Starting snapshot record_memory_history")
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

def stop_record_memory_history() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not recording memory history")
       return

   logger.info("Stopping snapshot record_memory_history")
   torch.cuda.memory._record_memory_history(enabled=None)

def export_memory_snapshot() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not exporting memory snapshot")
       return

   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f"{host_name}_{timestamp}"

   try:
       logger.info(f"Saving snapshot to local file: {file_prefix}.pickle")
       torch.cuda.memory._dump_snapshot(f"{file_prefix}.pickle")
   except Exception as e:
       logger.error(f"Failed to capture memory snapshot {e}")
       return

# Simple Resnet50 example to demonstrate how to capture memory visuals.
def run_resnet50(num_iters=5, device="cuda:0"):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   # Start recording memory snapshot history
   start_record_memory_history()

   for _ in range(num_iters):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # Create the memory snapshot file
   export_memory_snapshot()

   # Stop recording memory snapshot history
   stop_record_memory_history()

if __name__ == "__main__":
    # Run the resnet50 model
    run_resnet50()
```

同时profile cpu和显存

```python
# (c) Meta Platforms, Inc. and affiliates. 
# https://pytorch.org/blog/understanding-gpu-memory-1/
import logging
import socket
from datetime import datetime, timedelta

import torch

from torch.autograd.profiler import record_function
from torchvision import models

logging.basicConfig(
   format="%(levelname)s:%(asctime)s %(message)s",
   level=logging.INFO,
   datefmt="%Y-%m-%d %H:%M:%S",
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = "%b_%d_%H_%M_%S"

def trace_handler(prof: torch.profiler.profile):
   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f"{host_name}_{timestamp}"

   # Construct the trace file.
   prof.export_chrome_trace(f"{file_prefix}.json.gz")

   # Construct the memory timeline file.
   prof.export_memory_timeline(f"{file_prefix}.html", device="cuda:0")

def run_resnet50(num_iters=5, device="cuda:0"):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       for _ in range(num_iters):
           prof.step()
           with record_function("## forward ##"):
               pred = model(inputs)

           with record_function("## backward ##"):
               loss_fn(pred, labels).backward()

           with record_function("## optimizer ##"):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

if __name__ == "__main__":
    # Warm up
    run_resnet50()
    # Run the resnet50 model
    run_resnet50()
```


## 各型号gpu对比

[https://zhuanlan.zhihu.com/p/441153412](https://zhuanlan.zhihu.com/p/441153412)
