下载本文pdf：[https://github.com/daiwk/collections/blob/master/pdfs/llm_aigc.pdf](https://github.com/daiwk/collections/blob/master/pdfs/llm_aigc.pdf)

各种学习相关代码

[https://github.com/daiwk/llms_new](https://github.com/daiwk/llms_new)

# 从word2v到Transformer

## LSTM

[超生动图解LSTM和GRU，一文读懂循环神经网络！](https://mp.weixin.qq.com/s/vVDAB2U7478yOXUT9ByjFw)


## fasttext&word2vec

注：w2v训练时的内积不是2个emb-in的内积，而是emb-in和emb-out的内积

[fasttext源码解析](https://my.oschina.net/u/3800567/blog/2877570)

+ ```Dictionary::readWord```：空格分割，一次读出来一个word
+ ```Dictionary::add```：每个word求个hash，加进词典时，id就是从0开始的序号，同时记录一下词频
+ ```Dictionary::threshold```：按词频排序，扔掉低频词
+ ```Dictionary::initNgrams```：每个词，加上前缀BOW（<）和后缀（>），然后先扔进这个词的subwords里，然后再调用``` Dictionary::computeSubwords```把这个词的ngrams也扔进它的subwords里

整个词表，是word数+bucket这么大，其中bucket表示可容纳的subwords和wordNgrams的数量，默认200w


[为什么Word2Vec训练中, 需要对负采样权重开3/4次幂？](https://zhuanlan.zhihu.com/p/144563199?utm_id=0)

[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)里提到

![](../assets/word2vec_3_4_sample.jpeg)

通过对权重开3/4次幂，可以提升低频词被抽到的概率。在保证高频词容易被抽到的大方向下，通过权重3/4次幂的方式，适当提升低频词、罕见词被抽到的概率。如果不这么做，低频词，罕见词很难被抽到，以至于不被更新到对应的Embedding。

## BPE/WordPiece分词

[【Subword】深入理解NLP Subword算法：BPE、WordPiece、ULM](https://mp.weixin.qq.com/s/U9F8G-OUCb9kunTk_tZYFw)


## Transformer原理

[从三大顶会论文看百变Self-Attention](https://mp.weixin.qq.com/s/R9FoceRsPB3ceqKpnYPvbQ)

[包学包会，这些动图和代码让你一次读懂「自注意力」](https://mp.weixin.qq.com/s/Z0--eLLiFwfSuMvnddKGPQ)

[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)

[从熵不变性看Attention的Scale操作](https://kexue.fm/archives/8823)

[Transformers Assemble（PART I）](https://mp.weixin.qq.com/s/NZM05zyUkldOwpNIbsOtDQ) 讲了3篇

[Transformers Assemble（PART II）](https://mp.weixin.qq.com/s/JdUVaQ3IyrflHvxIk5jYbQ) 又讲了三篇

### 为什么层次化softmax没人用了

[Transformer 结构中最后一层 softmax 为什么不再使用 层次化softmax了呢？](https://www.zhihu.com/question/310845030/answer/595573391?hb_wx_block=0&utm_source=wechat_session&utm_medium=social&utm_oi=632586637935251456)

主要还是计算资源的问题。

Mikolov发明word2vec的几个版本大概在13-14年前后。那个时候GPU非常少见，印象里面CMU的NLP组没有GPU，Stanford NLP lab只有6块K40。

大规模直接算softmax是在google的14年那篇seq2seq做MT的文章。为了快，把一个softmax并行在4块GPU上，每个GPU负责四分之一。那个年代，大多数NLP组全组都不会有4块GPU。

hierarchical softmax是softmax的近似，suboptimal的。当如今计算资源足够大的时候，当然包括时间和显存 (BERT 和 Elmo 都没有用hierarchical)，hierarchical softmax就逐渐退出了历史舞台。

### Transformer会不会规划未来

[Transformer本可以深谋远虑，但就是不做](https://mp.weixin.qq.com/s/1kolCWSsFAp4e9MGG089vQ)

[Do Language Models Plan for Future Tokens?](https://arxiv.org/pdf/2404.00859.pdf)

在训练期间的梯度既会为**当前token位置**的损失优化权重，也会为该序列**后面的token**进行优化，那么这二者会以怎样的比例分配资源？

+ 预缓存假设（pre-caching hypothesis）：在时间步$$t$$计算与**当前时间步的推理任务无关**但**可能对未来时间步$$t + \tau$$有用**的特征
+ 面包屑假设（breadcrumbs hypothesis）：与时间步$$t$$**最相关的特征**已经**等同**于将在**时间步$$t + \tau$$最有用**的特征。

设计了一种合成场景，其中**只能通过显式的预缓存完成任务**，即模型必须为下一token预先计算信息，**否则就无法在一次单向通过中准确计算出正确答案。**发现明显的证据说明transformer可以学习预缓存，即当必须预计算信息来最小化损失时，它们就会这样做。

但在真实语言数据上，语言模型并不会显著地准备用于未来的信息。相反，它们是计算对预测**下一个token有用的特征**——事实证明**这对未来的步骤也很有用**。


## Transformer的FLOPS和访存带宽

[https://zhuanlan.zhihu.com/p/624740065](https://zhuanlan.zhihu.com/p/624740065)

$$A$$的shape是$$m\times k$$，$$B$$的shape是$$k\times n$$，那么矩阵乘法$$AB$$需要$$m\times k\times n$$次的乘法，也需要同样多次的加法，所以FLOPS是$$2\times m\times k\times n$$

假设batchsize是$$b$$，序列长度$$s$$，原来的emb是$$d$$，即输入的是$$[b,s,d]$$，一般$$d=d_k=d_v=d_q$$，$$W_Q$$、$$W_K$$、$$W_V$$都是$$d_v\times d_v$$，对应的Q、K、V矩阵都是$$s\times d_v$$，有$$head\_num$$个头，每个头的维度$$per\_head\_d=\frac{d}{head\_num}$$

### attention的FLOPS

attention的公式：

XXX
\begin{aligned}
&Q=x W_Q, K=x W_K, V=x W_V\\
&x_{\text {out }}=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{h}}\right) \cdot V \cdot W_o+x
\end{aligned}
XXX


+ 计算3个Q、K、V：要算三次$$s\times d$$和$$d\times d_v$$的矩阵乘法，所以是：$$3\times 2\times b\times s\times d\times d_v$$
    + 输入：$$[b, s, d]$$和3个$$[b, d, d_v]$$
    + 输出：$$[b, s, d_v]$$，再把最后一维$$d_v$$拆成$$head\_num$$份，再把中间两维互换一下，得到$$[b, head\_num, s, per\_head\_d]$$
+ 计算Q和K的相似度：要算一次$$s\times d_v$$和$$d_v\times s$$的矩阵乘法，$$2\times b\times s^2\times d_k$$
    + 输入：$$[b, head\_num, s, per\_head\_d]$$和$$[b, head\_num, per\_head\_d, s]$$
    + 输出：$$[b, head\_num, s, s]$$
+ 把相似度用到V上：要算一次$$s\times s$$和$$s\times d_v$$的矩阵乘法，，$$2\times b\times s^2 \times d_v$$
    + 输入：$$[b, head\_num, s, s]$$和$$[b, head\_num, s, per\_head\_d]$$
    + 输出：$$[b, head\_num, s, per\_head\_d]$$
+ 最后过一个线性映射：要算一次$$s\times d_v$$的和$$d_v\times d_v$$的矩阵乘法，$$2\times b\times s\times d_v\times d_v$$
    + 输入：$$[b, s, d_v]$$和$$[d_v, d_v]$$
    + 输出：$$[b, s, d_v]$$

因为$$d_k=d_v=d_q=d$$，单纯计算attention总共就是$$8bsd^2 + 4bs^2d$$

### FFN的FLOPS

FFN的公式：

XXX
x=f_{\text {gelu }}\left(x_{\text {out }} W_1\right) W_2+x_{\text {out }}
XXX

在原始Transformer中，$$W_1$$的shape是$$[d,4d]$$，$$W_2$$的shape是$$[4d,d]$$，

+ 第一个线性层：$$2\times b\times s\times\ d\times 4d=8\times b\times s\times\ d^2$$
    + 输入：$$[b, s, d]$$和$$[d,4d]$$
    + 输出：$$[b, s, 4d]$$
+ 第二个线性层：$$2\times b\times s\times\ 4d\times d=8\times b\times s\times\ d^2$$
    + 输入：$$[b, s, 4d]$$和$$[4d,d]$$
    + 输出：$$[b, s, d]$$

所以一层Transformer，即attention+FFN的计算量为$$(8bsd^2 + 4bs^2d)+16bsd^2=24bsd^2+4bs^2d$$

有两点需要注意的：

+ 对NLP任务来讲，一般$$d$$是个比较固定的值，如512，而$$s$$变大，效果会更好，所以一般是$$s>d$$，所以复杂度取决于$$s$$的大小。
+ 但有些模型的初始设置不是这样的，例如GPT3的175B模型里，$$s=2048,d=12288$$，当然，算力够的话也可以把$$s$$变大

自己感觉：既然是$$24bsd^2+4bs^2d$$，其实就是$$6sd^2$$和$$s^2d$$的大小，即$$6d$$和$$s$$的大小，如果$$6d>s$$，则$$d^2$$起主导，反之$$s^2$$起主导

### DIN的FLOPS

特殊地，对于推荐中的DIN那种，看当前item和历史s个item的相关性，即q的序列长度只有1，不考虑多头，而这其实也是decoder预测下一个词时过一层Transformer的复杂度

已经有3个序列长度为$$s-1$$的QKV的cache，要算第$$s$$个词和这$$s-1$$个词的attention

+ 计算第$$s$$个词的3个Q、K、V：要算三次$$1\times d$$和$$d\times d_v$$的矩阵乘法，所以是：$$3\times 2\times b\times 1\times d\times d_v$$
    + 输入：$$[b, 1, d]$$和3个$$[b, d, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 计算Q和K的相似度：要算一次$$1\times d_v$$和$$d_v\times s$$的矩阵乘法，$$2\times b\times 1\times d_k\times s$$【这里的K是历史$$s-1$$长度的序列拼上当前词，当然对DIN来讲要去掉当前词，这里先忽略这个】
    + 输入：$$[b, 1, d_v]$$和$$[b, d_v, s]$$
    + 输出：$$[b, 1, s]$$
+ 把相似度用到V上：要算一次$$1\times s$$和$$s\times d_v$$的矩阵乘法，，$$2\times b\times 1 \times d_v \times s$$【同样地，这里的V是历史$$s-1$$长度的序列拼上当前词，当然对DIN来讲要去掉当前词，这里先忽略这个】
    + 输入：$$[b, 1, s]$$和$$[b, s, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 最后过一个线性映射：要算一次$$1\times d_v$$的和$$d_v\times d_v$$的矩阵乘法，$$2\times b\times 1\times d_v\times d_v$$
    + 输入：$$[b, 1, d_v]$$和$$[d_v, d_v]$$
    + 输出：$$[b, 1, d_v]$$
+ 第一个线性层：$$2\times b\times 1\times\ d\times 4d=8\times b\times 1\times\ d^2$$
    + 输入：$$[b, 1, d]$$和$$[d,4d]$$
    + 输出：$$[b, 1, 4d]$$
+ 第二个线性层：$$2\times b\times 1\times\ 4d\times d=8\times b\times 1\times\ d^2$$
    + 输入：$$[b, 1, 4d]$$和$$[4d,d]$$
    + 输出：$$[b, 1, d]$$

总共是$$6bd^2+2bds+2bds+2bd^2+8bd^2+8bd^2=24bd^2+4bds$$

### Transformer的访存

GPU架构的介绍参考[https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)和[https://www.zhihu.com/question/319355296/answer/2193938981](https://www.zhihu.com/question/319355296/answer/2193938981)，GPU对比CPU如下：

+ 任务模式
    + CPU由专为顺序串行处理而优化的几个核心组成
    + GPU则拥有一个由数以千计的更小、更高效的核心（专为同时处理多重任务而设计）组成的大规模并行计算架构。同时CPU相当的一部分时间在执行外设的中断、进程的切换等任务，而GPU有更多的时间并行计算。
+ 功能定位
    + CPU不但要承担计算任务还有承担逻辑控制等任务。
    + GPU在渲染画面时需要同时渲染数以百万记的顶点或三角形，故GPU的设计是可以充分支持并行计算。
+ 系统集成
    + GPU作为一类外插设备，在尺寸、功率、散热、兼容性等方面的限制远远小于CPU，这样可以让GPU有较大的显存和带宽。

以A100为例，整体架构如下

![ga100-full-gpu-128-sms](../assets/ga100-full-gpu-128-sms.png)

1. PCIE层：通过PCIE接口以外设的方式集成到服务器上。
2. 中间一坨绿色的部分是GPU的计算核心**SM(Streaming Multiprocessor)**，在A100中，一个SM有64个用于计算的Core，共108个SM(图里是GA100,有128个SM)，故共6192个Core。
3. 中间蓝色部分是L2缓存
4. NVLink：**多个GPU间进行通信**的组件，会优化GPU间的通信，提升传输效率。
5. 两侧的HBM2是显存，目前的A100的显存有两种40G and 80G

A100的SM如图所示，

![a100-sm](../assets/a100-sm.png)

GPU的显存分成两部分：

+ Global Memory：整体架构图中的**两侧HBM2**部分，例如A100 80G就有80G的global memory，**2TB/s带宽**，访问速度比较慢
+ Shared Memory：SM图中**浅蓝色**的L1 Data Cache，例如A100中每个SM中有**192KB**，访问速度比较快

从图中可见，A100的**FP16**有**312T的FLOPS**

![A100-tensorcore-performance](../assets/A100-tensorcore-performance.png)

以矩阵乘法为例，$$[M, K] \times [K, N] -> [M,N]$$，

+ 计算时间：$$2MKN/FLOPS$$
+ 访存时间为：$$(MN+MK+KN)/memory\_bandwidth$$，因为首先要**读取**$$MK$$和$$NK$$这两个矩阵，然后结果还要**写入**$$MN$$这个矩阵里。假设是fp16，占2bytes，那就还要乘以2

假设$$b=1,s=4096,d=d_k=2048$$，以计算Q和K的相似度为例，对比一下训练和预测时的**计算耗时**和**访存耗时**

+ **训练时**：$$M=4096,K=2048,N=4096$$==>**计算是瓶颈**
    + FLOPS：$$2\times b\times s^2\times d_k=2\times 1\times 4096^2\times 2048=68719476736$$
    + 计算耗时：$$FLOPS/max\_FLOPS=68719476736/(312\times 10^{12})=0.00022s=220\times 10^{-6}s=220us$$
    + 访存耗时：$$(MN+MK+KN)/memory\_bandwidth=2\times (4096\times 4096+4096\times 2048+2048\times 4096)/(2\times 10^{12})=3.35544\times 10^{-5}s=33.544\times 10^{-6}=33.5544us$$

+ **预测时**：$$M=1,K=2048,N=4096$$==>**访存是瓶颈**
    + FLOPS：$$2\times b\times 1\times d_k\times s=2\times 1\times 2048\times 4096=16777216$$ 
    + 计算耗时：$$FLOPS/max\_FLOPS=16777216/(312\times 10^{12})=5.38\times 10^{-8}s=0.0538\times 10^{-6}s=0.0538us$$
    + 访存耗时：$$(MN+MK+KN)/memory\_bandwidth=2\times (1\times 4096+1\times 2048+2048\times 4096)/(2\times 10^{12})=8.3948\times 10^{-6}s=8.3948us$$

一些常见的名词：

+ H2D：host to device，从cpu拷贝到gpu
+ D2H：device to host，从gpu拷贝到cpu

### 一些注意点


F是理论flops, B是理论访存bandwidth

延时=max(2mkn/F, (mk+kn+mn)/B)

如果是方阵m=n=k,xxx

==>越长条形的矩阵越容易进入访存bound

x=F/B

如果一个模型算出来的x比上面这个x小,那一定是访存 bound，没必要优化

美国的芯片禁令主要是减小带宽

要优化mfu：
1. 让它是计算bound
2. 加快通信

流水线：

计算和通信是2个不同硬件，算一半就通信一半，通信的时候并行算另一半，xxx

tensor core：专门算矩阵乘
cuda core：算特征函数，例如exp，elementwise乘


华为卡：算dropout很挫,模型里尽量不要有这个算子

因为dropout的随机数发生器，底层是用cuda core里的计算，但华为有一个指令没在底层支持，所以后来专门搞了一个芯片来产生随机数，但这样就有带宽问题


flash-attn的核心：搬到sram上

==>L和D的关系，决定fa的收益


triton最简单的是做fuse，即节省访存

pytorch的compile会自己fuse


## Transformer加速

### lightseq

&nbsp;

[LightSeq: A High Performance Inference Library for Transformers](https://arxiv.org/pdf/2010.13887.pdf)

[LightSeq2: Accelerated Training for Transformer-based Models on GPUs](https://arxiv.org/pdf/2110.05722.pdf)

[https://github.com/bytedance/lightseq](https://github.com/bytedance/lightseq)


# PLM：仅编码器/仅解码器/编码器+解码器

## 仅编码器的BERT

[BERT小学生级上手教程，从原理到上手全有图示，还能直接在线运行](https://mp.weixin.qq.com/s/ltVuXZ4nJh8Cb5X2UhB6tQ)

[BERT源码分析（PART I）](https://mp.weixin.qq.com/s/sSmTQ_cOLyAUV0aV0FkDvw)

[BERT源码分析（PART II）](https://mp.weixin.qq.com/s/1NDxWfBu_csu8qHV2tmmVQ)

[Dive into BERT：语言模型与知识](https://mp.weixin.qq.com/s/NjQtSKY85Np5IodRiKsrvg)

[关于BERT，面试官们都怎么问](https://mp.weixin.qq.com/s/c2PktKruzq_teXm3GAwe1Q)

主要讲了下面3篇：

[Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066)

[Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855)

[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document)


[A Primer in BERTology: What we know about how BERT works](https://arxiv.org/pdf/2002.12327.pdf)

摘要：目前，基于 Transformer 的模型已经广泛应用于自然语言处理中，但我们依然对这些模型的内部工作机制知之甚少。在本文中，来自麻省大学洛威尔分校的研究者对流行的 BERT 模型进行综述，并综合分析了 40 多项分析研究。他们还概览了对模型和训练机制提出的改进，然后描画了未来的研究方向。

[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)


[BERT系列文章汇总导读](https://mp.weixin.qq.com/s/oT2dtmfEQKyrpDTOrpzhWw)

[ALBERT、XLNet，NLP技术发展太快，如何才能跟得上节奏？](https://mp.weixin.qq.com/s/Toth-XKn2WKYkyDw6j5F3A)

[绝对干货！NLP预训练模型：从transformer到albert](https://mp.weixin.qq.com/s/Jgx9eHk9xiSOWEy0Ty3LoA)

[ALBERT一作蓝振忠：预训练模型应用已成熟，ChineseGLUE要对标GLUE基准](https://mp.weixin.qq.com/s/mvkFDy09BdKJC4Cja11PAA)

[有哪些令你印象深刻的魔改Transformer？](https://mp.weixin.qq.com/s/HS2tlT7t18cFytZVIsOXUg)

[BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！](https://mp.weixin.qq.com/s/jVSW0KDhaXuaIeOzoPmCJA)


### multi-head att 实现

输入原始的query(即from_tensor)之后, 把```[batch, from_seq, emb]```变成```[?, emb]```，其中```?=batch*from_seq```

```python
from_tensor_2d = reshape_to_matrix(from_tensor)

def reshape_to_matrix(input_tensor):
    """Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix)."""
    ndims = input_tensor.shape.ndims
    if ndims < 2:
        raise ValueError("Input tensor must have at least rank 2. Shape = %s" %
            (input_tensor.shape))
    if ndims == 2:
        return input_tensor
    width = input_tensor.shape[-1]
output_tensor = tf.reshape(input_tensor, [-1, width])
    return output_tensor
```

然后再接一个fc，把```[?, emb]```变成```[?, head_num * per_head]```，一般```head_num * per_head=emb```。

```python
query_layer = tf.layers.dense(
        from_tensor_2d,
        num_attention_heads * size_per_head,
        activation=query_act,
        name="query",
        kernel_initializer=create_initializer(initializer_range))
```

因为```?=batch*from_seq```，所以可以直接做如下变换

```python
query_layer = transpose_for_scores(query_layer, batch_size,
        num_attention_heads, from_seq_length,
        size_per_head)
```

实际就是把```?```拆开成```batch, from_seq```，整个变成```[batch, from_seq, head_num, per_head]```，然后做了个 transpose，把1和2互换了下，得到```[batch, head_num, from_seq, per_head]```

```python
def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
        seq_length, width):
    output_tensor = tf.reshape(
            input_tensor, [batch_size, seq_length, num_attention_heads, width])

    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
    return output_tensor
```

然后key也做完全一样的操作(不过处理的是to_tensor，如果是self-attention，那to_tensor=from_tensor), 得到```[batch, head_num, to_seq, per_head]```：

```python
to_tensor_2d = reshape_to_matrix(to_tensor)
key_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=key_act,
        name="key",
        kernel_initializer=create_initializer(initializer_range))

key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,
        to_seq_length, size_per_head)
```

然后就算$$QK^T$$了，注意这里对key取了转置，也就是```[batch, head_num, from_seq, per_head]```乘以```[batch, head_num, per_head, to_seq]```，得到的结果是```[batch, head_num, from_seq, to_seq]```：

```python
attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
attention_scores = tf.multiply(attention_scores,
    1.0 / math.sqrt(float(size_per_head)))

if attention_mask is not None:
    # `attention_mask` = [B, 1, F, T]
    attention_mask = tf.expand_dims(attention_mask, axis=[1])

    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
    # masked positions, this operation will create a tensor which is 0.0 for
    # positions we want to attend and -10000.0 for masked positions.
    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0

    # Since we are adding it to the raw scores before the softmax, this is
    # effectively the same as removing these entirely.
    attention_scores += adder
attention_probs = tf.nn.softmax(attention_scores)
attention_probs = dropout(attention_probs, attention_probs_dropout_prob)
```

然后看下value的操作：

```python
value_layer = tf.layers.dense(
        to_tensor_2d,
        num_attention_heads * size_per_head,
        activation=value_act,
        name="value",
        kernel_initializer=create_initializer(initializer_range))

# `value_layer` = [batch, to_seq, head_num, per_head]
value_layer = tf.reshape(
            value_layer,
            [batch_size, to_seq_length, num_attention_heads, size_per_head])

# `value_layer` = [batch, head_num, to_seq, per_head]
value_layer = tf.transpose(value_layer, [0, 2, 1, 3])

# `context_layer` = [batch, head_num, from_seq, per_head]
context_layer = tf.matmul(attention_probs, value_layer)

# `context_layer` = [batch, from_seq, head_num, per_head]
context_layer = tf.transpose(context_layer, [0, 2, 1, 3])
```

再确认一点，$$softmax(QK^T)$$是```[batch, head_num, from_seq, to_seq]```，而$$V$$是```[batch, head_num, to_seq, per_head]```，所以context_layer是```[batch, head_num, from_seq, per_head]```

最后，再搞一下，变回```[batch, from_seq, head_num * per_head]```：

```python
if do_return_2d_tensor:
# `context_layer` = [B*F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size * from_seq_length, num_attention_heads * size_per_head])
else:
# `context_layer` = [B, F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size, from_seq_length, num_attention_heads * size_per_head])
```

如上过程是$$Concat(head_1, ..., head_h)$$，其中$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$。包装在了函数```attention_layer```之中，我们注意到原文还有一个大小为$$hd_v \times d_{model}$$的$$W^O$$，也就是大小为$$d_{model}\times d_{model}$$，再看看源码。。也就是说，正常的bert里，```attention_heads```就只有一个元素，然后接了个```hidden_size```的fc，而前面的代码里也提到了```hidden_size```正好就是$$d_{model}$$，所以这就是$$W^O$$。

```python
attention_heads = []
with tf.variable_scope("self"):
    attention_head = attention_layer(xxxxx)
    attention_heads.append(attention_head)
    attention_output = None
    if len(attention_heads) == 1:
        attention_output = attention_heads[0]
    else:
        # In the case where we have other sequences, we just concatenate
        # them to the self-attention head before the projection.
        attention_output = tf.concat(attention_heads, axis=-1)
    # Run a linear projection of `hidden_size` then add a residual
    # with `layer_input`.
    with tf.variable_scope("output"):
        attention_output = tf.layers.dense(
            attention_output,
            hidden_size,
            kernel_initializer=create_initializer(initializer_range))
        attention_output = dropout(attention_output, hidden_dropout_prob)
        attention_output = layer_norm(attention_output + layer_input)

```

关于 mask，可以看看这个[https://juejin.im/post/5b9f1af0e51d450e425eb32d](https://juejin.im/post/5b9f1af0e51d450e425eb32d)

摘抄一下：

什么是padding mask呢？回想一下，我们的每个批次输入序列长度是不一样的！也就是说，我们要对输入序列进行对齐！具体来说，就是给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。
具体的做法是，把这些位置的值加上一个非常大的负数(可以是负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！

而sequence mask是为了使得decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。
那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为1，下三角的值权威0，对角线也是0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。

### masked-language-model的实现

[https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L240](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L240)


如下，其中```hidden_size```就是是$$d_{model}$$：

```python
def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,
                         label_ids, label_weights):
  """Get loss and log probs for the masked LM."""
  input_tensor = gather_indexes(input_tensor, positions)

  with tf.variable_scope("cls/predictions"):
    # We apply one more non-linear transformation before the output layer.
    # This matrix is not used after pre-training.
    with tf.variable_scope("transform"):
      input_tensor = tf.layers.dense(
          input_tensor,
          units=bert_config.hidden_size,
          activation=modeling.get_activation(bert_config.hidden_act),
          kernel_initializer=modeling.create_initializer(
              bert_config.initializer_range))
      input_tensor = modeling.layer_norm(input_tensor)

    # The output weights are the same as the input embeddings, but there is
    # an output-only bias for each token.
    output_bias = tf.get_variable(
        "output_bias",
        shape=[bert_config.vocab_size],
        initializer=tf.zeros_initializer())
    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    label_ids = tf.reshape(label_ids, [-1])
    label_weights = tf.reshape(label_weights, [-1])

    one_hot_labels = tf.one_hot(
        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)

    # The `positions` tensor might be zero-padded (if the sequence is too
    # short to have the maximum number of predictions). The `label_weights`
    # tensor has a value of 1.0 for every real prediction and 0.0 for the
    # padding predictions.
    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])
    numerator = tf.reduce_sum(label_weights * per_example_loss)
    denominator = tf.reduce_sum(label_weights) + 1e-5
    loss = numerator / denominator

  return (loss, per_example_loss, log_probs)
```

其中的gather如下：

```python
def gather_indexes(sequence_tensor, positions):
  """Gathers the vectors at the specific positions over a minibatch."""
  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
  batch_size = sequence_shape[0]
  seq_length = sequence_shape[1]
  width = sequence_shape[2]

  flat_offsets = tf.reshape(
      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
  flat_positions = tf.reshape(positions + flat_offsets, [-1])
  flat_sequence_tensor = tf.reshape(sequence_tensor,
                                    [batch_size * seq_length, width])
  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
  return output_tensor
```

注意调用时传的是如下参数

```python
    (masked_lm_loss,
     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
         bert_config, model.get_sequence_output(), model.get_embedding_table(),
         masked_lm_positions, masked_lm_ids, masked_lm_weights)
```

### BERT的可解释性

[ACL 2019 \| 理解 BERT 每一层都学到了什么](https://mp.weixin.qq.com/s/w2Cwo--GTKp5o8YKRtbl7g)

[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document)

探索BERT深层次的表征学习是一个非常有必要的事情，一是这可以帮助我们更加清晰地认识BERT的局限性，从而改进BERT或者搞清楚它的应用范围；二是这有助于探索BERT的可解释性

## 更复杂的BERT

[站在BERT肩膀上的NLP新秀们（PART III）](https://mp.weixin.qq.com/s/CxcyX5V9kBQDW8A4g0uGNA)

[BERT时代与后时代的NLP](https://mp.weixin.qq.com/s/U_pYc5roODcs_VENDoTbiQ)

[美团BERT的探索和实践](https://mp.weixin.qq.com/s/qfluRDWfL40E5Lrp5BdhFw)

[Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展](https://mp.weixin.qq.com/s/dF3PtiISVXadbgaG1rCjnA)

### 中文BERT

#### WWM

[哈工大讯飞联合实验室发布基于全词覆盖的中文BERT预训练模型](https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&mid=2650794872&idx=1&sn=dccd856283bdd4edcdad08cf75506697&chksm=8f477e93b830f7850e6c0ffe684264f704c6fcc4e126a6300b5ae33916aa676a279206e1e4ce&mpshare=1&scene=1&srcid=0701DAFsQt28gF1hGzH4llaM&pass_ticket=8wChBZeeRNV5mWLFKMXfVyWjwTb94XookbbSJiYpmEClqUrpybiGPpfilXkL5UQN#rd)

[https://github.com/ymcui/Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)

论文：[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)

#### ERNIE

参考[中文任务全面超越BERT：百度正式发布NLP预训练模型ERNIE](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650758722&idx=1&sn=6742b0f86982890d78cb3ec3be9865b3&scene=0#wechat_redirect)

[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223.pdf)

使用entity-level masking和phrase-level masking两种mask方法

输入的每个样本由5个 ';' 分隔的字段组成，数据格式：

+ token_ids
+ sentence_type_ids：两句话，第一句都是0，第二句都是1
+ position_ids
+ seg_labels：分词边界信息: 0表示词首、1表示非词首、-1为占位符, 其对应的词为 CLS 或者 SEP；
+ next_sentence_label

例如：

```shell
1 1048 492 1333 1361 1051 326 2508 5 1803 1827 98 164 133 2777 2696 983 121 4 19 9 634 551 844 85 14 2476 1895 33 13 983 121 23 7 1093 24 46 660 12043 2 1263 6 328 33 121 126 398 276 315 5 63 44 35 25 12043 2;0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55;-1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 -1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 -1;0
```

和bert在mask上的区别：

![ernie-bert-masking-diff](../assets/ernie-bert-masking-diff.png)

一个句子的不同level的mask方式：

![ernie-different-mask-level](../assets/ernie-different-mask-level.png)


[ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FOR LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1907.12412v1.pdf)

![ernie-2.0-loss](../assets/ernie-2.0-loss.png)

### 跨语言

#### XLM

[Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond](https://arxiv.org/abs/1812.10464)，XLM的主要思想还是来自于这篇文章，借用了BERT的框架最后成了XLM。本文提出了LASER（Language-Agnostic SEntence Representations）

XLM：facebook提出[Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)，加了language emb

+ 无监督的方法：只依赖单语种数据（monolingual data）
+ 有监督的方法：对平行语料使用新的跨语言loss

![xlm](../assets/xlm.png)


[Facebook最新语言模型XLM-R：多项任务刷新SOTA，超越单语BERT](https://mp.weixin.qq.com/s/6oK-gevKLWDwdOy4aI7U7g)

XLM-R

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

来自facebook。针对多种跨语言的传输任务，大规模地对多语言语言模型进行预训练可以显著提高性能。在使用超过 2TB 的已过滤 CommonCrawl 数据的基础上，研究者在 100 种语言上训练了基于 Transformer 的掩模语言模型。该模型被称为 XLM-R，在各种跨语言基准测试中，其性能显著优于多语言 BERT（mBERT），其中 XNLI 的平均准确度为+ 13.8％，MLQA 的平均 F1 得分为+ 12.3％，而 FQ 的平均 F1 得分为+ 2.1％ NER。XLM-R 在低资源语言上表现特别出色，与以前的 XLM 模型相比，斯瓦希里语（Swahili）的 XNLI 准确性提升了 11.8％，乌尔都语（Urdu）的准确性提升了 9.2％。研究者还对获得这些提升所需的关键因素进行了详细的实证评估，包括（1）积极转移和能力稀释；（2）大规模资源资源的高低性能之间的权衡。最后，他们首次展示了在不牺牲每种语言性能的情况下进行多语言建模的可能性。XLM-Ris 在 GLUE 和 XNLI 基准测试中具有强大的单语言模型，因此非常具有竞争力。

### 更长序列

#### transformer-xl

[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)

![vanilla-segment](../assets/vanilla-segment.png)

最简单的处理长文本方法：对长文本直接切成多个segment，每个segment独立过transformer，**segment间是没有信息流动的**

![transformer-xl](../assets/transformer-xl.png)

transformer-xl的处理方法：参考RNN的**隐藏记忆单元**，对**上一个segment**计算的隐藏状态序列进行**fixed和cached**，并在模型处理下一个新的segment时将其缓存为**可重用的扩展上下文**。

如图，第k+1层的第i个元素，用到了第k层的第i-1, i-2, ...,i - segment_len+1这几个元素，所以**对于最顶层的来讲，实际看到的窗口就更长了**。为此还提出了**相对位置编码**，即使用两个token的**相对距离**代替之前的绝对位置。

此外，[Stabilizing Transformers for Reinforcement Learning](https://arxiv.org/abs/1910.06764)说明了为什么标准的transformer架构很难在强化学习中优化。研究者同时提出了一种架构Gated Transformer-XL(GTrXL)，可以很好地提升transformer架构和变体的稳定性，并加速学习，可以超过LSTM，在多任务学习 DMLab-30 基准上达到 SOTA 的水平。

#### XLNet

[XLNet : 运行机制及和 Bert 的异同比较](https://mp.weixin.qq.com/s/VCCZOKJOhCEjxfnoLSuRKA)

[Transformer-XL与XLNet笔记](https://mp.weixin.qq.com/s/g7I_V5a3Puy9uK11A--Xqw)

[什么是XLNet中的双流自注意力](https://mp.weixin.qq.com/s/9QmhN4KfukCtAxzprKDbAQ)

[20项任务全面碾压BERT，CMU全新XLNet预训练模型屠榜（已开源）](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650764408&idx=1&sn=92196097be1a5f993ef02de0bac8128d&chksm=871ab006b06d3910ec88e57598d6c8b1a38dead073b3f417b793ba71ac4750ae2a8263537fa2&mpshare=1&scene=1&srcid=&pass_ticket=%2BD9Ask8qPVeDCkEHTF8NEBVBQX9YmDDkPy9VdMIfOYJ2VtpyHOOhIYdS3wUnvPjn#rd)

参考[拆解XLNet模型设计，回顾语言表征学习的思想演进](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650765039&idx=3&sn=5f21b702a06b2b3c12c1e5f327c0b744&chksm=871ab291b06d3b87a7b35ff2e69bbaa6863e7737510783f0d7913bc575e759a6662242e6b97a&scene=0&xtrack=1&pass_ticket=6OQo9SLhUprzhz9WVqt5LanZi%2Bu5pLbXWpLouCtQ6gkfTHAGY5Li3M%2BDR0n3drA2#rd)

[他们创造了横扫NLP的XLNet：专访CMU博士杨植麟](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650767073&idx=1&sn=3a1014852f0ba8caaee9fdfadd344503&chksm=871aba9fb06d33891731e38d7ca2010516a73cff72c1569c89962160d0fc15ef0b747f9ff7f5&scene=0&xtrack=1&pass_ticket=Kz97uXi0CH4ceADUC3ocCNkjZjy%2B0DTtVYOM7n%2FmWttTt5YKTC2DQT9lqCel7dDR#rd)

[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)

代码：[https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)

+ 自回归（autoregressive, AR）：主要的论文有这几篇：[Semi-supervised sequence learning](https://arxiv.org/abs/1511.01432)、[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)、[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)。通过一个autoregressive的模型来估计文本语料库的概率分布。也就是给定一个文本序列$$\mathbf{x}=\left(x_{1}, \cdots, x_{T}\right)$$，AR将likelihood因式分解(factorize)成一个前向的乘积$$p(\mathbf{x})=\prod_{t=1}^{T} p\left(x_{t} | \mathbf{x}_{<t}\right)$$，或者是一个后向的乘积$$p(\mathbf{x})=\prod_{t=T}^{1} p\left(x_{t} | \mathbf{x}_{>t}\right)$$。由于 AR 语言模型仅被训练用于编码**单向(uni-directional)语境（前向或后向）**，因而在深度双向语境建模中效果不佳。而下游语言理解任务通常需要双向语境信息，导致 AR 语言建模无法实现有效预训练。
+ 自编码（autoencoding, AE）：相关的预训练模型不会进行明确的密度估计(explicit density estimation)，而是从残缺的(corrupted)输入中**重建原始数据**。例如bert，使用一定比例的```[MASK]```，然后预测被mask掉的是什么东西。由于目标并不是密度估计，所以在重建的时候，可以考虑双向的上下文信息。但存在如下两个问题：
    + **finetune**时的真实数据**缺少**预训练期间使用的```[MASK]```这些**mask信息**，这导致**预训练和微调效果的差异（pretrain-finetune discrepancy）**。
    + 输入中要预测的token是被mask掉的，所以无法像AR那样使用乘积rule来建立联合概率分布。也就是说，给定未mask的 token，BERT**假设预测的token**之间**彼此独立**，这其实是对自然语言中普遍存在的**高阶、长期依赖关系**的一种**过度简化**。

基于这些优缺点，提出了一种泛化的自回归预训练模型XLNet：

+ Permutation Language Model(PLM)：在自回归LM模式下，最大化所有可能的**因式分解顺序**的对数似然，学习双向语境信息；
    + 把原来的```[MASK]```这个token干掉了，转而用attention mask来搞
    + 引入了排列：即原来是1234，可以输入3241，这个时候改一下mask就行
+ 引入了Transformer-XL的主要思路：**相对位置编码**以及**分段RNN机制**，实践已经证明这两点对于长文档任务是很有帮助的

![](../assets/xlnet.png)

### 更多的任务

#### MT-DNN

[Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/abs/1901.11504)

### 其他改进

#### RoBERTa

参考[重回榜首的BERT改进版开源了，千块V100、160GB纯文本的大模型](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650766934&idx=2&sn=54c479dd8e8e69cd9617b9a1962443e1&chksm=871aba28b06d333e0d5dc64754b7280776ba5c17577831077dc5cd2caa4c2335beec3afd8d34&scene=0&xtrack=1&pass_ticket=zAXdHORK5tTx549e9RwAgNcm7bjJrH4ENwbbTYVrAZDqpsE%2Fu1hY63b%2FoRfnZQdM#rd)

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)

+ 修改了一些超参
+ 删掉nsp任务
+ 更大的batchsize和学习率


[RoBERTa中文预训练模型，你离中文任务的「SOTA」只差个它](https://mp.weixin.qq.com/s/EKFa40rLQlnEVuu9V7GDmg)

[https://github.com/brightmart/roberta_zh](https://github.com/brightmart/roberta_zh)


#### DeBERTa

[Deberta: Decoding-enhanced bert with disentangled attention](https://arxiv.org/pdf/2006.03654.pdf)

+ disentangled attention mechanism：每个词使用2个向量，$$H$$编码内容，$$P$$编码相对位置，i和j间的attention如下：

XXX
\begin{array}{r}
\mathrm{A}_{\mathrm{i}, \mathrm{j}}=\left\{\mathrm{H}_{\mathrm{i}}, \mathrm{P}_{\mathrm{i} \mid \mathrm{j}}\right\} \times\left\{\mathrm{H}_{\mathrm{j}}, \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}\right\}^{\mathrm{T}} \\
=\mathrm{H}_{\mathrm{i}} \mathrm{H}_{\mathrm{j}}^{\mathrm{T}}+\mathrm{H}_{\mathrm{i}} \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}^{\mathrm{T}}+\mathrm{P}_{\mathrm{i} \mid \mathrm{j}} \mathrm{H}_{\mathrm{j}}^{\mathrm{T}}+\mathrm{P}_{\mathrm{i} \mid \mathrm{j}} \mathrm{P}_{\mathrm{j} \mid \mathrm{i}}^{\mathrm{T}}
\end{array}
XXX

+ 预训练：使用enhanced mask decoder，把绝对位置信息引入解码层，来预估被mask掉的token
+ finetune：virtual adversarial training

#### ELECTRA

[2019最佳预训练模型：非暴力美学，1/4算力超越RoBERTa](https://mp.weixin.qq.com/s/_R-Bp5lLov-QIoPRl6fFMA)

[ELECTRA: 超越BERT, 19年最佳NLP预训练模型](https://mp.weixin.qq.com/s/fR5OrqxCv0udKdyh6CHOjA)

[ELECTRA: pre-training text encoders as discriminators rather than generators](https://openreview.net/pdf?id=r1xmh1btvb)

使用新的预训练task：RTD(replaced token detection)：

+ 使用一个**生成网络**采样出token，替换一些原有的token
+ 训练一个**判别模型**，预估一个token**是否是被替换的**

RTD比MLM**更sample-efficient**，因为RTD只做二分类，而MLM需要做全词表的softmax

#### Matryoshka

[俄罗斯套娃 (Matryoshka) 嵌入模型概述](https://mp.weixin.qq.com/s/H3LWIs4hBQ-b_XLxICw7Dg)

[Matryoshka representation learning](https://arxiv.org/pdf/2205.13147.pdf)

俄罗斯套娃嵌入模型旨在将**更重要的信息存储在早期的维度中**，将不太重要的信息存储在后面的维度中。俄罗斯套娃嵌入模型的这一特点允许我们截断模型产生的原始 (大) 嵌入，同时仍保留足够的信息以在下游任务上表现良好。

基本原理：假设是一个L分类的多分类问题，把原来的x先通过F映射到d，然后取前m维出来，再通过W把m映射到L，去和label算loss，再乘以权重c。这么搞几次，相当于搞了多个mlp，把不同的m映射到L。

假设输入一个llm产出的4096维向量，那这个llm就是F，假设是一个2分类问题，期望降维到(64,128,1024)，那就是取前64出来，映射成2维，算个loss，再取前128维出来，映射成2维，算个loss，再取前1024出来，算个loss

但这样比较粗暴，合理的方式是在gpt的建议下，通过如下方式来循序渐进地完成降维：

```python
multi_stage_cfg = [
    {"stage": "stage1_high_dim", "dims": [4096, 2048], "weights": [0.7, 0.3]},
    {"stage": "stage2_mid_dim", "dims": [2048, 1024, 512], "weights": [0.5, 0.3, 0.2]},
    {"stage": "stage3_low_dim", "dims": [512, 128, 64], "weights": [0.2, 0.3, 0.5]},
]

def train_stage(model, mrl_dims, mrl_weights, output_dir,
    train_dataset, eval_dataset, lr=5e-6, bs=32, acc=2, 
    max_steps=None, stage_name=""):
    
    if wandb.run:
        wandb.finish()  
    if rank == 0:
        wandb.init(project="xxx", name=stage_name)
    loss_fn = losses.MultipleNegativesRankingLoss(model)
    loss = losses.MatryoshkaLoss(model, loss_fn, 
        matryoshka_dims=mrl_dims, matryoshka_weights=mrl_weights)

    trainer = SentenceTransformerTrainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        loss=loss,
    )
    trainer.train()
    # 其他    

def train():
    for stage_cfg in multi_stage_cfg:
        if stage_cfg["stage"] == "stage1_high_dim":
            model = load_model(base_model_path)
        stage_dir = f"./outputs/{stage_cfg['stage']}"
        print(f"\n Training {stage_cfg['stage']}... dims={stage_cfg['dims']}")
        train_stage(
            model=model,
            mrl_dims=stage_cfg["dims"],
            mrl_weights=stage_cfg["weights"],
            output_dir=stage_dir,
            stage_name=stage_cfg["stage"],
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            lr=5e-6,
            bs=bs,
            acc=acc,
            max_steps=max_steps 
        )
        torch.cuda.empty_cache()
        torch.cuda.synchronize()  
        gc.collect()
        model = load_model_continue(stage_dir)

```


![matryoshka-representation-learning](../assets/matryoshka-representation-learning.png)

[https://sbert.net/examples/training/matryoshka/README.html](https://sbert.net/examples/training/matryoshka/README.html)

训练+预测

```python
model_name = "./bge-large-zh-v1.5" 
out_model_name = "./new-bge-large-zh-v1.5"
model = SentenceTransformer(model_name)


sentences = ["这是一个测试句子", "这是另一个例子"]
embeddings = model.encode(sentences)
print(f"嵌入维度：{embeddings.shape}")

train_dataset = Dataset.from_dict({
    "anchor": ["It's nice weather outside today.", "He drove to work."],
    "positive": ["It's so sunny.", "He took the car to the office."],
})
loss = losses.MultipleNegativesRankingLoss(model)
loss = losses.MatryoshkaLoss(model, loss, [768, 512, 256, 128, 64])

trainer = SentenceTransformerTrainer(
    model=model,
    train_dataset=train_dataset,
    loss=loss,
)
trainer.train()
trainer.save_model(out_model_name)

model = SentenceTransformer(out_model_name)

matryoshka_dim = 64
embeddings = model.encode(
            [
                "The weather is so nice!",
                "It's so sunny outside!",
                "He drove to the stadium.",
                ]
            )
embeddings = embeddings[..., :matryoshka_dim] # Shrink the embedding dimensions
print(embeddings.shape)

```

注意，如果嵌入已经归一化，那么在截断后它们将不再归一化，因此你**可能需要重新归一化**。

```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim
import torch.nn.functional as F

model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)

matryoshka_dim = 64
embeddings = model.encode(
    [
        "search_query: What is TSNE?",
        "search_document: t-distributed stochastic xxx.",
        "search_document: Amelia Mary Earhart was xxx.",
    ],
    convert_to_tensor=True,
)
# The Nomic team uses a custom architecture, 
# making them recommend Layer Normalization before truncation
embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))
embeddings[..., :matryoshka_dim] # Shrink the embedding dimensions

similarities = cos_sim(embeddings[0], embeddings[1:])
# => tensor([[0.7154, 0.4468]])
```

#### Piccolo2

[拿下SOTA！最强中文Embedding模型对标OpenAI，技术路线公开](https://mp.weixin.qq.com/s/G9izxyzyIHtGDb6kAYD-qg)

[Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](https://arxiv.org/pdf/2405.06932)

[https://huggingface.co/sensenova/piccolo-large-zh-v2](https://huggingface.co/sensenova/piccolo-large-zh-v2)

#### Conan-Embedding

[通过负样本挖掘炼出更强Embedding模型](https://mp.weixin.qq.com/s/z0jgPnvaPO6RTzgFzB8TFQ)

[Conan-embedding: General Text Embedding with More and Better Negative Samples](https://arxiv.org/pdf/2408.15710)

#### MEXMA

[MEXMA: Token-level objectives improve sentence representations](https://arxiv.org/abs/2409.12737)

[https://huggingface.co/facebook/MEXMA](https://huggingface.co/facebook/MEXMA)

[https://github.com/facebookresearch/mexma](https://github.com/facebookresearch/mexma)

#### ModernBert

[时隔6年，谷歌BERT终于有替代品了！更快更准更长，还不炒作GenAI](https://mp.weixin.qq.com/s/QDtl2Q_BHdL1PZw5wCUBaw)

[Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/pdf/2412.13663)

## 更小的BERT

[BERT 瘦身之路：Distillation，Quantization，Pruning](https://mp.weixin.qq.com/s/ir3pLRtIaywsD94wf9npcA)

### albert

[刚刚，Google发布24个小型BERT模型，直接通过MLM损失进行预训练](https://mp.weixin.qq.com/s/s0ysFH4CRvsHY1Gp3b4DPQ)

[ALBERT：用于语言表征自监督学习的轻量级 BERT](https://mp.weixin.qq.com/s/0V-051qkTk9EYiuEWYE3jQ)

[谷歌ALBERT模型V2+中文版来了：之前刷新NLP各大基准，现在GitHub热榜第二](https://mp.weixin.qq.com/s/nusSlw28h4bOlw5hDsc-Iw)

[超小型BERT中文版横空出世！模型只有16M，训练速度提升10倍](https://mp.weixin.qq.com/s/eVlNpejrxdE4ctDTBM-fiA)

[https://github.com/brightmart/albert_zh](https://github.com/brightmart/albert_zh)


[预训练小模型也能拿下13项NLP任务，谷歌ALBERT三大改造登顶GLUE基准](https://mp.weixin.qq.com/s/kvSoDr0E_mvsc7lcLNKmgg)

ALBERT 模型在 GLUE、RACE 和 SQuAD 基准测试上都取得了新的 SOTA 效果，并且参数量还少于 BERT-large。

[ALBERT: a lite bert for self-supervised learning of language representations](https://openreview.net/pdf?id=H1eA7AEtvS)

通过对词嵌入矩阵进行因式分解，再为下游任务共享不同层的所有参数，这样可以大大降低 BERT 的参数量。

还提出了一种新型句间连贯性损失函数，它可以强迫模型学习句间的连贯性表达，从而有利于各种下游 NLP 任务。

ALBERT 通过两个参数削减技术克服了扩展预训练模型面临的主要障碍。

+ 对嵌入参数化进行因式分解：将大的嵌入矩阵分解为两个小的矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离使得隐藏层的增加更加容易，同时不显著增加词汇嵌入的参数量。
+ 跨层参数共享：避免参数量随着网络深度的增加而增加。

两种技术都显著降低了 BERT 的参数量，同时不对其性能造成明显影响，从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1/18，训练速度却是后者的 1.7 倍。这些参数削减技术还可以充当某种形式的正则化，可以使训练更加稳定，而且有利于泛化。

为了进一步提升 ALBERT 的性能，还引入了一个自监督损失函数，用于句子级别的预测（SOP）。SOP 主要聚焦于**句间连贯**，用于解决原版 BERT 中下一句预测（NSP）损失低效的问题。

albert_tiny：

input_ids先查word_embeddings(`\(V\times E=21118*128)`)，得到dim=128的表示，再查word_embeddings_2(`\(E\times M =128*312\)`)，得到dim=312的表示。

搞positionembedding时，并不用输入0 1 2...，只需要做一些slice的变换就行了

```python
    with tf.control_dependencies([assert_op]):
      full_position_embeddings = tf.get_variable(
          name=position_embedding_name,
          shape=[max_position_embeddings, width],
          initializer=create_initializer(initializer_range))
      # Since the position embedding table is a learned variable, we create it
      # using a (long) sequence length `max_position_embeddings`. The actual
      # sequence length might be shorter than this, for faster training of
      # tasks that do not have long sequences.
      #    
      # So `full_position_embeddings` is effectively an embedding table
      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
      # perform a slice.
      position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                     [seq_length, -1]) 
      num_dims = len(output.shape.as_list())

      # Only the last two dimensions are relevant (`seq_length` and `width`), so
      # we broadcast among the first dimensions, which is typically just
      # the batch size.
      position_broadcast_shape = [] 
      for _ in range(num_dims - 2):
        position_broadcast_shape.append(1)
      position_broadcast_shape.extend([seq_length, width])
      position_embeddings = tf.reshape(position_embeddings,
                                       position_broadcast_shape)
      output += position_embeddings
```

然后会通过create_attention_mask_from_input_mask把input_ids和input_mask搞一下，得到attention_mask去和attention做mask，主要是算loss啥的，把后面的mask掉不算

### distillbert

参考[小版BERT也能出奇迹：最火的预训练语言库探索小巧之路](https://mp.weixin.qq.com/s/a0d0b1jSm5HxHso9Lz8MSQ)

1.4w个stars。。

[https://huggingface.co/transformers](https://huggingface.co/transformers)

[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)


### tinybert

[TinyBERT：模型小7倍，速度快8倍，华中科大、华为出品](https://mp.weixin.qq.com/s/VL7TSHmZPKD-xGdOxNmnHw)

[TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

提出了一个two-stage learning framework，在pre-training阶段和task-specific阶段都进行distill。

相比baseline，只有28% parameters和31%的inference时间

在glue上，7.5x小，infer上有9.4x快。

[哪吒”出世！华为开源中文版BERT模型](https://mp.weixin.qq.com/s/He6Xujoe5Ieo95Tshx7PnA)

[NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204)

[https://github.com/huawei-noah/Pretrained-Language-Model](https://github.com/huawei-noah/Pretrained-Language-Model)

[华为诺亚方舟开源哪吒、TinyBERT模型，可直接下载使用](https://mp.weixin.qq.com/s/cqYWllVCgWwGfAL-yX7Dww)


### reformer

[哈希革新Transformer：这篇ICLR高分论文让一块GPU处理64K长度序列](https://mp.weixin.qq.com/s/QklCVuukfElVDBFNxLXNKQ)

[Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB)

[https://github.com/google/trax/blob/master/trax/models/research/reformer.py](https://github.com/google/trax/blob/master/trax/models/research/reformer.py)

大型的 Transformer 往往可以在许多任务上实现 sota，但训练这些模型的成本很高，尤其是在序列较长的时候。在 ICLR 的入选论文中，我们发现了一篇由谷歌和伯克利研究者发表的优质论文。文章介绍了两种提高 Transformer 效率的技术，最终的 Reformer 模型和 Transformer 模型在性能上表现相似，并且在长序列中拥有更高的存储效率和更快的速度。论文最终获得了「8，8，6」的高分。在最开始，文章提出了将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度。此外，研究者使用可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数）。最终的 Reformer 模型和 Transformer 模型在性能上表现相同，同时在长序列中拥有更高的存储效率和更快的速度。

[大幅减少GPU显存占用：可逆残差网络(The Reversible Residual Network)](https://mp.weixin.qq.com/s/j6-x9ANF9b3Q1I_os_LJSw)

### LTD-bert

[内存用量1/20，速度加快80倍，腾讯QQ提出全新BERT蒸馏框架，未来将开源](https://mp.weixin.qq.com/s/W668zeWuNsBKV23cVR0zZQ)

### Q-bert

[AAAI 2020 \| 超低精度量化BERT，UC伯克利提出用二阶信息压缩神经网络](https://mp.weixin.qq.com/s/0qBlnsUqI2I-h-pFSgcQig)

[Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/pdf/1909.05840.pdf)

### Adabert

[推理速度提升29倍，参数少1/10，阿里提出AdaBERT压缩方法](https://mp.weixin.qq.com/s/mObuD4ijUCjnebYIrjvVdw)

[AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search](https://arxiv.org/pdf/2001.04246v1.pdf)


## 仅解码器的GPT 

[AI也能精彩表达：几种经典文本生成模型一览](https://mp.weixin.qq.com/s/GfP76I-BzzQcyLqQJoeXxw)

### GPT

2018年的[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。

![gpt1](../assets/gpt1.png)

微调阶段为每种下游任务专门设计：

+ 分类：输入一段文本，经过transformer，最后接一个Linear
+ entailment(蕴含)：输入2段文本，premise(假设)和hypothesis(假说)，经过transformer，最后接一个Linear
+ 相似度：输入2段文本a和b，a+b过transformer，b+a过transformer，再合起来接一个Linear
+ 多选题：输入context+答案1过transformer+Linear，答案2、答案3同样操作，将3个输出合在一起求softmax

### GPT2

2019年的[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模。

+ layernorm前移到每个sub-block之前
+ additional layernorm在最后的self-attention block才加上
+ 修改初始化方法，以考虑残差路径上的累积并缩放残差层的权重


[15亿参数最强通用NLP模型面世！Open AI GPT-2可信度高于所有小模型](https://mp.weixin.qq.com/s/nu2egJuG_yxIVfW9GfdlCw)

中文GPT2

[只需单击三次，让中文GPT-2为你生成定制故事](https://mp.weixin.qq.com/s/FpoSNNKZSQOE2diPvJDHog)

[https://github.com/imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)

[https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb](https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb)


[语言模型秒变API，一文了解如何部署DistilGPT-2](https://mp.weixin.qq.com/s/5B8bN2kplB4t1ctYJjN1zw)

huggingface的distill gpt-2：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

### nanogpt代码解读

简化版的gpt，
tiktoken：gpt2中使用的开源分词工具，比huggingface的tokenizer快得多

```python
import tiktoken
enc = tiktoken.get_encoding("gpt2")

# 字节对编码过程，我的输出是[31373, 995]
encoding_res = enc.encode("hello world")
print(encoding_res)

# 字节对解码过程，解码结果：hello world
raw_text = enc.decode(encoding_res)
print(raw_text)
```

类似的：[https://github.com/karpathy/llm.c](https://github.com/karpathy/llm.c)

[https://github.com/karpathy/build-nanogpt/tree/master](https://github.com/karpathy/build-nanogpt/tree/master)

fork了一个：[https://github.com/daiwk/build-nanogpt](https://github.com/daiwk/build-nanogpt)

GPT的整体结构：

```python
class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            # 这里有n层，每层是一个Block
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            # 最后一层需要加一个layer norm
            ln_f = nn.LayerNorm(config.n_embd),
        ))
        # 最后一层的linear
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # weight sharing scheme
        self.transformer.wte.weight = self.lm_head.weight

        # init params
        self.apply(self._init_weights)
```

```python
class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x):
        # 这里对原来的add&norm进行了改造，保留一个从始至终的+x的操作，让梯度能够直通输入
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
```

```python
class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)
        self.gelu    = nn.GELU(approximate='tanh')
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        return x
```

```python
class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.c_proj.NANOGPT_SCALE_INIT = 1
        # regularization
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        # nh is "number of heads", hs is "head size", and C (number of channels) = nh * hs
        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer
        qkv = self.c_attn(x)
        q, k, v = qkv.split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side
        # output projection
        y = self.c_proj(y)
        return y
```


## 编码器+解码器

### T5

[谷歌T5模型刷新GLUE榜单，110亿参数量，17项NLP任务新SOTA](https://mp.weixin.qq.com/s/YOMWNV5BMI9hbB6Nr_Qj8w)

[谷歌最新T5模型17项NLP任务霸榜SuperGLUE，110亿参数量！](https://mp.weixin.qq.com/s/rFT37D7p0MiS8XGZM35bYA)

[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)

将所有NLP任务都建模成text-to-text的生成任务，

mT5（[mt5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/pdf/2010.11934.pdf)）是T5的变种，基于新的Common Crawl的数据集（包括101种语言）上预训练

### MASS

[MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450v5)

bert只使用了Transformer的encoder部分，其下游任务也主要是适用于自然语言理解（NLU），对于类似文本摘要、机器翻译、对话应答生成等自然语言生成（NLG）任务显然是不太合适的。MASS 采用了编码器-解码器框架，并尝试在**给定部分句子**的情况下**修复整个句子**。MASS输入句子包含了一些连续的 Token，并且中间会带有一些**连续的Mask**，模型的任务是**预测出被Mask掉的词**是什么。相比 BERT 只有编码器，MASS 联合训练编码器与解码器，能获得更适合机器翻译的表征能力。

训练步骤主要分为两步：

+ Encoder： 输入为被随机mask掉连续部分token的句子，使用Transformer对其进行编码；这样处理的目的是可以使得encoder可以更好地捕获没有被mask掉词语信息用于后续decoder的预测；
+ Decoder： 输入为与encoder同样的句子，但是mask掉的正好和encoder相反，和翻译一样，使用attention机制去训练，但只预测encoder端被mask掉的词。该操作可以迫使decoder预测的时候更依赖于source端的输入而不是前面预测出的token，防止误差传递。


### BART

[多项NLP任务新SOTA，Facebook提出预训练模型BART​](https://mp.weixin.qq.com/s/1-EJ36-lY9YZSLBG5c2aaQ)

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)

BART是一个适用于seq2seq模型的去噪自编码器。预训练包括两个阶段：

+ 使用**任意噪声**函数**破坏文本**
+ 用seq2seq模型来**重建原始文本**


# 一些思考

[The Second Half](https://ysymyth.github.io/The-Second-Half/)

[OpenAI姚顺雨：欢迎来到AI下半场！](https://zhuanlan.zhihu.com/p/1896310535865233616)

数十年来，人工智能主要致力于开发新的训练方法和模型。

我们不再仅仅问：“我们能否训练一个模型来解决X问题？”而是问：“我们应该训练人工智能去做什么，以及我们如何衡量真正的进步？”要在下半场取得成功，我们需要及时转变思维方式和技能组合，这些可能更接近产品经理的思维方式。

[Welcome to the Era of Experience](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)

[强化学习之父当头一棒：RL版「苦涩的教训」来了！通往ASI，绝非靠人类数据](https://mp.weixin.qq.com/s/MGrSWh-wcBrLKPboUmg05w)

[被《经验时代》刷屏之后，剑桥博士长文讲述RL破局之路](https://mp.weixin.qq.com/s/dLfbu56f02BtQR4bCjzNfw)

[生成式AI进入第二幕：交大携手创智学院提出「认知工程」，AI新纪元开始了](https://mp.weixin.qq.com/s/G3WTW2RiYNA6epyQoZZ9Ng)

[Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/pdf/2504.13828)

[https://github.com/GAIR-NLP/cognition-engineering](https://github.com/GAIR-NLP/cognition-engineering)

[中文版](https://github.com/GAIR-NLP/cognition-engineering/blob/main/assets/Cognition_Engineering_zh.pdf)，自己转存了[一份](https://github.com/daiwk/collections/blob/master/assets/Cognition_Engineering_zh.pdf)

[刚刚！北大校友Lilian Weng最新博客来了：Why We Think](https://mp.weixin.qq.com/s/fcDRzd3cwuM_JOnQW5XRMQ)

[https://lilianweng.github.io/posts/2025-05-01-thinking/](https://lilianweng.github.io/posts/2025-05-01-thinking/)


## 上半场

上半场的游戏：专注于构建新的模型和方法（如Alexnet、transformer、GPT-3），而评估和基准测试（如Imagenet、WMT'14翻译）是次要的。原因：

+ 方法比任务更难、更令人兴奋，需要非凡的洞察力和工程能力；而定义任务往往更简单：只是把人类已经做的事情（比如翻译、图像识别或国际象棋）变成基准测试，没有太多洞察力甚至工程能力。
+ 方法也往往比单独的任务更通用、更广泛适用，因此它们特别有价值。

## 方案(the recipe)

在强化学习中，有三个关键组成部分：**算法**、**环境**和**先验知识**。长期以来，强化学习研究人员主要关注**算法**（例如REINFORCE、DQN、actor-critic、PPO、TRPO等），即智能体学习的智力核心，而**将环境和先验知识视为固定或最小化的**。

在深度强化学习时代，环境很重要：如果你忽略环境，你可能会构建一个只在玩具环境中表现出色的“最优”算法。所以，OpenAI设想的就是一旦我们**将所有数字世界变成一个环境**，用聪明的强化学习算法解决它，我们就拥有了数字通用人工智能（AGI），就搞了如下的项目：

+ gym：各种游戏的标准强化学习环境
+ World of Bits和Universe项目：将互联网或计算机变成一个游戏。

在GPT-3出现之后，才发现缺失的部分是**先验知识**，需要强大的语言预训练（language pre-training），将一般常识和语言知识提炼到模型中，然后可以对其进行微调。因此，强化学习中最重要的部分可能甚至不是强化学习算法或环境，而是**先验知识**，而这些先验知识可以**通过与强化学习完全无关的方式获得**。

**思考**（thinking，或者说**推理**，reasoning），是一种奇怪的动作（action），它并不直接影响外部世界，而且**推理的空间是开放的、组合上是无限的**。在经典的强化学习理论中，这是一个糟糕的交易，使得决策变得不可能。但是，通过在任何强化学习环境的动作空间中加入推理，我们利用语言预训练的**先验知识来实现泛化**，并且我们可以在不同的决策中使用**灵活的测试时计算**。可以参考论文[ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

一旦我们有了正确的强化学习**先验知识（语言预训练）**和**强化学习环境（将语言推理加入到动作中）**，事实证明强化学习**算法可能就是最不重要的部分**了。因此我们有了o系列、R1、深度研究、计算机使用智能体，还有更多即将出现的东西。

## 下半场

上半场的游戏是

+ 开发**新的训练方法或模型**，不断**改进基准测试的效果**。
+ 创建**更难的基准测试**，并继续这个循环。

但目前这种游戏正在被破坏，因为：

+ 方案本质上已经**标准化并工业化**了，**无需太多的新想法**。一个针对特定任务的新方法可能提升5%，但下一个o系列模型可能没有明确针对这个任务，但能提升30%
+ 即使有更难的基准测试，很快（并且越来越快）它们也会被方案解决，下图是最近5年基准测试的准确率变化情况：

![](../assets/second-half-progress.png)

但问题是：人工智能已经在国际象棋和围棋中击败了世界冠军，在SAT和律师资格考试中超越了大多数人类，并在国际信息学奥林匹克竞赛（IOI）和国际数学奥林匹克竞赛（IMO）中达到了金牌水平。但世界并没有发生太大变化，至少从经济和GDP的角度来看。这个问题可以称为**效用（utility）**问题，根源可能很简单：我们的**评估设置**与**现实世界的设置**在很多基本方式上不同：

+ 评估“应该”**自动**运行：通常智能体接收任务输入，自主执行操作，然后接收任务奖励。但现实中，智能体需要**和人类互动**。因此需要新的基准测试，例如：
    + 将真实人类纳入其中，例如[https://lmarena.ai/](https://lmarena.ai/)
    + 将用户模拟纳入其中，例如下图的[tau-bench](https://arxiv.org/abs/2406.12045)

![](../assets/tau-bench.png)

+ 评估“应该”**独立同分布**（i.i.d.）运行：如果你有一个包含500个任务的测试集，你独立运行每个任务，平均任务指标，然后得到一个总体指标。但现实中，你是**按顺序解决任务**，而不是并行解决。例如一个工程师在越来越熟悉代码库的过程中，会越来越擅长解决具体的代码问题，而目前的评测做不到这一点。所以需要**长期记忆**方法（而且确实有[AGENT WORKFLOW MEMORY](https://arxiv.org/pdf/2409.07429)），但学术界没有合适的基准测试来证明这种需求。

下半场的游戏可能是：

+ 开发**针对现实世界效用**的新的评估设置或任务。
+ 用方案解决它们，或者用新组件增强方案。继续这个循环。

上半场的参与者在解决视频游戏和考试，充满了渐进式的方法和模型，而下半场的参与者可以通过构建有用的产品来建立价值数十亿甚至数千亿美元的公司，使用通用方案会轻易击败渐进式方法，除非你创造出打破方案的新假设。

## 如何持续地评估AI产品

[姚顺雨提到的「AI下半场」，产品评估仍被误解](https://mp.weixin.qq.com/s/ToqvRCuBXrDkNUH1Pe1Jfw)

[https://eugeneyan.com/writing/eval-process/](https://eugeneyan.com/writing/eval-process/)

总有人以为再加个工具、添个指标，或者让大语言模型当裁判（LLM-as-judge），就能解决问题拯救产品。这根本是在回避核心问题，逃避真正该做的工作。评估并非一劳永逸，也不是什么快速起效的方法——它是**运用科学方法的持续实践**，是评估驱动开发，是AI输出的持续监测。

构建产品评估体系，本质上是一个**不断提问、实验和分析的循环**。

![](../assets/ai-product-cycle.png)

+ **观察数据**：审视输入内容、AI输出结果，以及用户与系统的交互情况。数据会告诉我们系统哪里运转良好，更重要的是，哪里会出问题。发现这些故障模式才是有效改进的起点。
+ **标注数据**：优先处理问题输出。这意味着要**对成功和失败的样本进行标记**，建立**平衡且有代表性的数据集**。理想情况下，正负样本应该五五开，并覆盖各类输入场景。这个数据集将成为针对性评估的基础，帮我们追踪已发现问题的改进情况。
+ **提出假设**：为什么会出现这个错误？可能是RAG检索没返回相关上下文，也可能是模型处理复杂（有时自相矛盾）的指令时力不从心。通过分析检索文档、推理轨迹和错误输出等数据，我们能确定要优先修复的问题以及要验证的假设。
+ **设计实验验证假设**：比如重写提示词、更新检索组件或切换不同模型。好的实验要能明确验证假设是否成立，最好还设置基线对照组进行比较。
+ **测量(measure)和错误分析**：这不同于随意的感觉判断，**必须量化**实验改动是否真有效果：准确率提升了吗？缺陷减少了吗？新版本在对比测试中表现更优吗？无法量化的改进根本不算改进。
+ **实验成功就应用更新，失败就继续迭代**。

评估驱动的开发（Eval-driven development，EDD）：开发AI功能前，先通过产品评估**定义成功标准**，确保从第一天就有**明确目标**和**可衡量的指标**。

**评估指引开发方向**。我们先评估基线（比如简单提示词）获取基准数据。之后每个提示词调整、系统更新和迭代都要评估：简化提示词提升了准确性吗？检索更新增加了相关文档召回率吗？还是反而让效果变差了？

EDD是如下的**写评估->做改动->跑评估->整合改进**的循环，确保了可衡量的改进，建立的不是模糊的直觉判断，而是扎根于软件工程实践的反馈闭环。

![](../assets/edd.png)

自动化评估工具（LLM-as-judge）也离不开**人工监督**。如果我们不主动审查AI输出和用户反馈，再多自动评估工具也救不了产品。

要评估和监测 AI 产品，通常需要**采样输出**并**标注质量缺陷**。有了足够多高质量标注数据，我们就能**校准自动评估工具**，使其与人类判断一致。这可能涉及测量二元标签的召回率/准确率，或通过两两比较决定输出之间的相关性。校准后的评估工具能有效扩展AI系统的持续监测能力。

但自动评估工具不能取代人工监督。我们仍需要定期采样、标注数据，分析用户反馈。理想情况下，我们应该设计能够**通过用户交互获取隐式反馈**的产品。不过，显式反馈虽然不那么频繁，偶尔也会有偏见，但也很有价值。**自动化评估工具本质上是人工标注与反馈流程的放大器。**

![](../assets/auto-monitor.png)

## 趋势

[AI开源狂飙，OpenAI们慌了！GenAI大洗牌，2025趋势深度解读](https://mp.weixin.qq.com/s/h47YCdzC9hUk64Sr3IBHlg)

[硅谷今夜集体失眠！互联网女皇340页AI报告猛料刷屏，大佬熬夜头秃](https://mp.weixin.qq.com/s/QqWM36GvUc8FwS2FqYmk-w)

[AI 推理成本暴跌，「互联网女皇」 Mary Meeker 从中看到了什么？](https://mp.weixin.qq.com/s/Iy2PxBLtE6n3EOI_vjdOcQ)

# LLM概述

PLM（pretrained language models），即bert等

## LLM简史

+ 2017年的[Learning to generate reviews and discovering sentiment](https://arxiv.org/pdf/1704.01444.pdf)尝试用rnn来实现智能系统
+ 2018年的gpt1：[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。确定对自然语言文本建模的基本原则为**预测下一个单词**。
+ 2019年的gpt2：[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模，**无需使用标记数据进行显式微调**。
    + 参考[The natural language decathlon: Multitask learning as question answering](https://arxiv.org/pdf/1806.08730.pdf)中**多任务求解的概率形式**： $$p(output|input,task)$$ 。
    + 提出“由于特定任务的有监督目标与无监督目标（语言建模）相同，只是在序列的子集上进行评估，因此，无监督目标的全局最小值也是有监督目标的全局最小值”，即每个NLP任务可以看作**世界文本子集的单词预测问题**，如果模型有足够能力来复原世界文本，无监督语言建模可以解决各种问题。
    + 仅无监督与监督微调的SOTA相比效果还是不太行。虽然GPT2模型规模相对较小，但如对话等任务在其基础上做微调还是能拿到很好的效果的，例如[DIALOGPT : Large-scale generative pre-training for conversational response generation](https://arxiv.org/pdf/1911.00536.pdf)、[End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2](https://aclanthology.org/2020.acl-main.54.pdf)
+ 2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning（这点在**3.3亿的BERT和15亿的gpt2中看不到**）。**预训练和ICL有相同的语言建模范式**：预训练预测给定上下文条件下的后续文本序列，ICL预测正确的任务解决方案，其可被格式化为给定任务描述和示范下的文本序列。
+ GPT-3的两种改进方法：
    + 使用代码数据训练：GPT-3主要问题是缺乏对复杂任务的推理能力，2021年openai提出了Codex（[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)），在github代码上微调的GPT。[A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more](https://arxiv.org/pdf/2112.15594.pdf)发现Codex能解决非常困难的编程问题，还能在数学问题上有显著提升。[Text and code embeddings by contrastive pre-training](https://arxiv.org/pdf/2201.10005.pdf)提出了训练文本和代码emb的对比学习，在线性探测分类、文本搜索、代码搜索等任务上有所提升。GPT-3.5就是在基于代码的GPT（code-davinci-002）的基础上开发的。
    + 与人类对齐：2017年openai就在[learning from human preference](https://openai.com/research/learning-from-human-preferences)的博客中提出了应用强化学习来学习由人类标的偏好比较，此后2021年7月openai发表了PPO。2020年GPT-2用RL进行微调，[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)也做了相似工作。2022年提出了RLHF的InstructGPT([Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf))，其中的**SFT就对应于常说的指令微调**。在openai的博客[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research)中提出了训练AI系统的3个有前途的方向：**使用人类反馈、协助人类评估、做对齐研究**。
+ 2022年的ChatGPT：用类似InstructGPT的方式进行训练，专门**对对话能力进行优化**，将人类生成的对话（**扮演用户和AI两个角色**）与InstructGPT数据集结合起来**以对话形式生成**。
+ 2023年的GPT-4：将文本输入扩展到**多模态信号**。此外，
    + 提升安全性：在RLHF训练中加入**额外的安全奖励信号**，采用多种干预策略如Anthropic提出的[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)提到的红队评估（read teaming）机制以减轻幻觉、隐私和过度依赖问题。
    + 改进的优化方法：使用**可预测扩展**（predictable scaling）的机制，使用模型训练期间的一小部分计算量**以预测最终性能**。
    + 迭代部署的工程方案：[Lessons learned on language model safety and misuse](https://openai.com/research/language-model-safety-and-misuse)，遵循5阶段的开发和部署生命周期来开发模型和产品。

## LLM列表（持续更新中）

+ 百亿：除了LLaMA（最大650亿）和NLLB（最大545亿），大多数在100亿-200亿之间，通常需要**数百甚至上千**个GPU或TPU。
+ 千亿：OPT、OPT-IML、BLOOM和BLOOMZ与GPT-3(175B)大致相同，GLM有1300亿，Galactica有1200亿，通常需要**数千**个GPU或者TPU。

| ckpt? | 模型 |发布时间 | 大小 | 预训练数据规模 | 硬件 | 训练时间  |
|---|---|---|---|---|---|---|
| Y | [T5](https://arxiv.org/pdf/1910.10683.pdf) | 2019.10| 11B |  1万亿tokens | 1024 TPU v3  |  - |
| N | [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) | 2020.05 | 175B |  3000万亿tokens | -  |  - |
| N | [GShard](https://arxiv.org/pdf/2006.16668.pdf) | 2020.06 | 600B |  1万亿tokens | 2048 TPU v3 | 4天 |
| Y | [mT5](https://arxiv.org/pdf/2010.11934.pdf) | 2020.10 | 13B |  1万亿tokens | -  |  - |
| Y | [PanGu-$$\alpha$$](https://arxiv.org/pdf/2104.12369.pdf) | 2021.04 | 13B |  1.1TB | 2048 Ascend 910 | - |
| Y | [CPM-2](https://arxiv.org/pdf/2106.10715.pdf) | 2021.06 | 198B |  2.6TB | - | - |
| N | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | 2021.07 | 12B |  1000万亿tokens | - | - |
| N | [ERNIE 3.0](https://arxiv.org/pdf/2107.02137.pdf) | 2021.07 | 10B |  3750亿tokens | 384 v100 | - |
| N | [Jurassic-1](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | 2021.08 | 178B | 3000亿tokens | 800 GPU | - |
| N | [HyperCLOVA](https://arxiv.org/pdf/2109.04650.pdf) | 2021.09 | 82B | 3000亿tokens | 1024 A100 | 13.4天 |
| N | [FLAN](https://arxiv.org/pdf/2109.01652.pdf) | 2021.09 | 137B | - | 128 TPU v3 | 60小时 |
| N | [Yuan 1.0](https://arxiv.org/pdf/2110.04725.pdf) | 2021.10 | 245B | 1800亿tokens | 2128 GPU | - |
| Y | [T0](https://arxiv.org/pdf/2211.01786.pdf) | 2021.10 | 11B | - | 512 TPU v3 | 27小时 |
| N | [Anthropic](https://arxiv.org/pdf/2112.00861.pdf) | 2021.12 | 52B | 4000亿tokens | - | - |
| N | [WebGPT](https://arxiv.org/pdf/2112.09332.pdf) | 2021.12 | 175B |  - | - | - |
| N | [Gopher](https://arxiv.org/pdf/2112.11446.pdf) | 2021.12 | 280B |  3000亿tokens | 4096 TPU v3 | 920小时 |
| N | [ERNIE 3.0 Titan](https://arxiv.org/pdf/2112.12731.pdf) | 2021.12 | 260B |  - | - | - |
| N | [GLaM](https://arxiv.org/pdf/2112.06905.pdf) | 2021.12 | 1200B | 2800亿tokens | 1024 TPU v4 | 574小时 |
| N | [LaMDA](https://arxiv.org/pdf/2201.08239.pdf) | 2022.01 | 137B |  7680亿tokens | 1024 TPU v3 | 57.5天 |
| N | [MT-NLG](https://arxiv.org/pdf/2201.11990.pdf) | 2022.01 | 530B | 2700亿tokens | 4480 80G A100 | - |
| N | [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf) | 2022.02 | 41B | 9670亿tokens | - | - |
| N | [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) | 2022.03 | 175B |  - | - | - |
| N | [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) | 2022.03 | 70B | 1.4万亿tokens | - | - |
| Y | [CodeGen](https://arxiv.org/pdf/2203.13474.pdf) | 2022.03 | 16B | 5770亿tokens | - | - |
| Y | [GPT-NeoX-20B](https://arxiv.org/pdf/2204.06745.pdf) | 2022.04 | 20B | 825GB | 96 40G A100 | - |
| Y | [Tk-Instruct](https://arxiv.org/pdf/2204.07705.pdf) | 2022.04 | 11B |  - | 256 TPU v3 | 4小时 |
| N | [PaLM](https://arxiv.org/pdf/2204.02311.pdf) | 2022.04 | 540B | 7800亿tokens | 6144 TPU v4 | - |
| Y | [UL2](https://arxiv.org/pdf/2205.05131.pdf) | 2022.05 | 20B |  825GB | 96 40G A100 | - |
| Y | [OPT](https://arxiv.org/pdf/2205.01068.pdf) | 2022.05 | 175B | 1800亿tokens | 992 80G A100 | - |
| Y | [NLLB](https://arxiv.org/pdf/2207.04672.pdf) | 2022.07 | 54.5B |  - | - | - |
| N | [AlexaTM](https://arxiv.org/pdf/2208.01448.pdf) | 2022.08 | 20B | 1.3万亿tokens | 128 A100 | 120天 |
| N | [Sparrow](https://arxiv.org/pdf/2209.14375.pdf) | 2022.09 | 70B | 64 TPU v3 | - | - |
| N | [WeLM](https://arxiv.org/pdf/2209.10372.pdf) | 2022.09 | 10B | 3000亿tokens | 128 A100 40G | 24天 |
| N | [U-PaLM](https://arxiv.org/pdf/2210.11399.pdf) | 2022.10 | 540B | - | 512 TPU v4 | 5天 |
| N | [Flan-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | 512 TPU v4 | 37小时 |
| N | [Flan-U-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | - | - |
| Y | [GLM](https://arxiv.org/pdf/2210.02414.pdf) | 2022.10 | 130B | 4000亿tokens | 768 40G A100 | 60天 |
| Y | [Flan-T5](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 11B |  - | - | - |
| Y | [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) | 2022.11 | 176B | 3660亿tokens | 384 80G A100 | 105天 |
| Y | [mT0](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 13B |  - | - | - |
| Y | [Galactica](https://arxiv.org/pdf/2211.09085.pdf) | 2022.11 | 120B | 1060亿tokens | - | - |
| Y | [BLOOMZ](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 176B |  - | - | - |
| Y | [OPT-IML](https://arxiv.org/pdf/2212.12017.pdf) | 2022.12 | 175B |  - | 128 40G A100 | - |
| Y | [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) | 2023.02 | 65B | 1.4万亿tokens | 2048 80G A100 | 21天 |
| N | [GPT-4](https://arxiv.org/pdf/2303.08774.pdf) | 2023.03 | - |  - | - | - |
| Y | [CodeGeeX](https://arxiv.org/pdf/2303.17568.pdf) | 2022.09 | 13B | 8500亿tokens | 1536 Ascend 910 | 60天 |
| N | [PanGU-$$\Sigma$$](https://arxiv.org/pdf/2303.10845.pdf) | 2023.03 | 1085B | 3290亿tokens | 512 Ascend 910 | 100天 |
| Y | [Pythia](https://arxiv.org/pdf/2304.01373.pdf) | 2023.04 | 12B | 3000亿tokens | 256 40G A100 | - |

可以直接把对应的md丢给gpt，叫它导出一个excel，然后就可以自定义排序或者画散点图看了


## LLM开源库

+ transformers：huggingface的库
+ [deepspeed](https://github.com/microsoft/DeepSpeed)：微软的库，与pytorch兼容，训练了MT-NLG、BLOOM等模型，包括各种分布式训练优化技术，如**内存优化**（**ZeRO**、**梯度检查点**等）和**管道并行**。
+ megatron-lm：英伟达的库，同样包括各种分布式训练技术，包括**模型和数据并行**、**混合精度**训练和**FlashAttention**。（[Megatron-lm: Training multi-billion parameter language models using model parallelism](https://arxiv.org/pdf/1909.08053.pdf)、[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)和[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)）
+ [jax](https://github.com/google/jax)：google的库，允许用户在**带有硬件加速（GPU或TPU）**的情况下进行**数组的高效运算**，可以在**各种设备**高效计算，支持**自动微分**和**即时编译**等功能。
+ [colossal-AI](https://arxiv.org/pdf/2110.14883.pdf)：HPC-AI Tech的库，基于pytorch，可以使用[PatrickStar](Patrickstar: Parallel training of pre-trained models via a chunk-based memory management)提出的方法优化异构内存管理，分布了基于LLaMA的[ColossalChat](https://medium.com/pytorch/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+ [BMTrain](https://github.com/OpenBMB/BMTrain)：openBMB的库，强调代码简洁、低资源占用和高可用性
+ [FastMoE](Fastmoe: A fast mixture-of-expert training system)：专门用于MoE模型的训练库，基于pytorch，简化了将transformer转换为MoE模型的过程
+ [semantic-kernel](https://github.com/microsoft/semantic-kernel)：微软的开源库

![AI的4场景战役](../assets/4wars-in-aistck.png)

一些开源的小模型：[从零训练的 1B 以下小模型汇总](https://mp.weixin.qq.com/s/IWuMj6ywge2NAUhYmYQBLA)

## 一些综述

+ [Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media](https://github.com/daiwk/collections/blob/master/assets/LLM/foundation%20models%20NLP.pdf)
+ [大规模语言模型：从理论到实践](../assets/LLM/LLM-TAP.pdf)，[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf)邱锡鹏等
+ 人大的大模型综述：[https://github.com/RUCAIBox/LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)，[自己存了一份pdf](https://github.com/daiwk/collections/blob/master/assets/LLM/LLM_Survey_Chinese.pdf)，（**！！！本文大部分内容按这个来组织！！！**），出书和课件了：[https://llmbook-zh.github.io/](https://llmbook-zh.github.io/)
+ [Talking about large language models](https://arxiv.org/pdf/2212.03551.pdf)
+ [Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)，引用数2k+
+ [A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt](https://arxiv.org/pdf/2302.09419.pdf)，唐杰等
+ [Pre-Trained Models: Past, Present and Future](https://arxiv.org/pdf/2106.07139.pdf)
+ [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)
+ [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/pdf/2105.10311.pdf)
+ [A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)
+ [Towards reasoning in large language models: A survey](https://arxiv.org/pdf/2212.10403.pdf)
+ [Reasoning with language model prompting: A survey](https://arxiv.org/pdf/2212.09597.pdf)
+ [Dense Text Retrieval based on Pretrained Language Models: A Survey](https://arxiv.org/pdf/2211.14876.pdf)
+ [Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://zhuanlan.zhihu.com/p/395795968)
+ [如何高效部署大模型？CMU最新万字综述纵览LLM推理MLSys优化技术](https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ)：[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)
+ [一篇对大语言模型（LLMs）进行全面、深入分析的43页综述（Word2Vec作者出品）](https://mp.weixin.qq.com/s/5fbx0lM9V-Q7xYbeDauuHw)==>[Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)，[自己存了一份](https://github.com/daiwk/collections/blob/master/assets/LLM/Large%20Language%20Models-%20A%20Survey.pdf)
+ [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/pdf/2308.07107v3.pdf)
+ [awesome-LLM-resourses](https://github.com/WangRongsheng/awesome-LLM-resourses)
+ [14天速成LLM高手！大佬开源学习笔记，GitHub狂揽700星](https://mp.weixin.qq.com/s/aDkH9E5b0yNd1J_Kthkh5Q)：[https://github.com/hesamsheikh/ml-retreat](https://github.com/hesamsheikh/ml-retreat)
+ [GitHub超火开发者路线图库有AI学习路线了！star数近30万](https://mp.weixin.qq.com/s/P9hWHGsiWcfEfMq54E02xg)：[https://github.com/kamranahmedse/developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)
+ [写的真好，万字长文串烧LLM大模型技术原理](https://mp.weixin.qq.com/s/ArTr-Df10LiTkJ4k9zlmZA)：[https://zhuanlan.zhihu.com/p/713794852](https://zhuanlan.zhihu.com/p/713794852)


[大模型面试八股](https://zhuanlan.zhihu.com/p/643560888)

[大模型八股答案（一）——基础知识](https://zhuanlan.zhihu.com/p/643829565)

[大模型八股答案（二）——训练框架](https://zhuanlan.zhihu.com/p/643836163)

[Jeff Dean演讲回顾LLM发展史，Transformer、蒸馏、MoE、思维链等技术都来自谷歌](https://mp.weixin.qq.com/s/5EUQlD5isnEEzXBw1AJEig)

## 课程

[https://github.com/mlabonne/llm-course](https://github.com/mlabonne/llm-course)

[https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)

[https://github.com/decodingml/llm-twin-course](https://github.com/decodingml/llm-twin-course)


## 扩展法则(scaling law)

### openai的扩展法则

2020年,openai的[Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)通过拟合模型在不同数据大小（2000w到230亿个token）、不同的模型大小（7.68亿到15亿个**非嵌入参数**）的性能，提出了在**计算预算**$$c$$的条件下，$$L$$是用nats表示的交叉熵损失，模型性能与**模型规模**$$N$$、**数据集规模**$$D$$以及**训练计算量**$$C$$间存在如下幂律关系：

XXXL(N)=(\frac{N_c}{N})^{\alpha _N}, {\alpha}_N\sim 0.076,N_c\sim 8.8\times 10^{13}XXX

XXXL(D)=(\frac{D_c}{D})^{\alpha _D}, {\alpha}_D\sim 0.05,N_c\sim 5.4\times 10^{13}XXX

XXXL(C)=(\frac{C_c}{C})^{\alpha _C}, {\alpha}_C\sim 0.05,C_c\sim 3.1\times 10^{8}XXX

其中，$$N_c$$表示非嵌入参数数量，$$D_c$$表示训练token数量,$$C_c$$表示FP-days。

[Go Wider Instead of Deeper](https://arxiv.org/pdf/2107.11817)说了，transformer效果的提升**不在于计算量的变大**，而应该在于通过**提升模型的hidden dim**来增加模型参数量

### Chinchilla扩展法则

DeepMind在[Training compute-optimal large language models](https://arxiv.org/pdf/2203.15556.pdf)中提出了Chichilla扩展法则来指导LLM**最优计算量**的训练。通过变化更大范围的模型大小（7000w到160亿参数）和数据大小（50亿到5000亿个token）进行实验，拟合了如下的扩展法则：

XXX
L(N, D)=E+\frac{A}{N^\alpha}+\frac{B}{D^\beta}
XXX

其中$$E=1.69,A=406.4,B=410.7,\alpha = 0.34, \beta =0.28$$，通过在约束条件$$C\approx 6ND$$下优化损失$$L(N,D)$$，将计算预算最优地分配给模型大小和数据大小的方法：

XXX
N_{o p t}(C)=G\left(\frac{C}{6}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{6}\right)^b
XXX

其中$$a=\frac{\alpha}{\alpha+\beta}$$，$$b=\frac{\beta}{\alpha+\beta}$$，$$G$$是由$$A,B,\alpha,\beta$$计算出的扩展系数。

随着计算预算的增加，

+ openai的扩展法则更偏向于将更大预算分给**模型大小**，因为其对比各模型时使用了固定的训练数据量和学习率等超参，低估了数据量的作用。每增加10倍的计算量，应该让数据集大小增加为约1.8倍，模型参数量增加为约5.5倍。即**模型参数量更加的重要**。
+ Chinchilla扩展法则认为**模型大小和数据大小要同比例增加**，即$$a$$和$$b$$取值差不多。因为其在无视模型大小的前提下，发现设置与数据量差不多match的学习率能获得更好的loss。每增加10倍的计算量，应该让数据集大小增加为约3.16倍，模型参数量也增加为约3.16倍。即**数据集大小和模型参数量一样重要**。

然而，有一些能力（如涌现）无法根据扩展法则进行预测，只有当模型达到一定规模时才会出现。

![chinchilla-law](../assets/chinchilla-law.png)

飘红的就是常见的10B模型，大概要205B的token来训练，能达到**计算最优点**，当然**并不一定是loss最小的点**，这个可以参考llama3的现象

### scaling law的一些讨论

[Scaling Laws 又失灵了？谷歌新研究：扩散模型不是越大越好](https://mp.weixin.qq.com/s/ia9L6jr_lwowYHgLI1k_4g)

[Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://arxiv.org/pdf/2404.01367.pdf)

[腾讯混元、北大发现Scaling law「浪涌现象」，解决学习率调参难题](https://mp.weixin.qq.com/s/ff5_O0H5VQNkArKroJkEZQ)

[Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling](https://arxiv.org/pdf/2405.14578)

SIGIR24最佳论文：[Scaling Laws For Dense Retrieval](https://arxiv.org/pdf/2403.18684)

[中科大联合华为诺亚提出Entropy Law，揭秘大模型性能、数据压缩率以及训练损失关系](https://mp.weixin.qq.com/s/F4OFP1lzAGH4RSXcXBw7mw)

#### 词表的scaling law

&nbsp;

[NeurIPS 2024 | 大模型的词表大小，同样适用于Scaling Law](https://mp.weixin.qq.com/s/_DTvTMCtrW9WV3vELjU9jw)

[Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)

[https://github.com/sail-sg/scaling-with-vocab/](https://github.com/sail-sg/scaling-with-vocab/)

#### scaling law for precision

[Scaling Laws终结，量化无用，AI大佬都在审视这篇论文](https://mp.weixin.qq.com/s/JhtOlj5Y4UYM3W3koeMmqw)

[Scaling Laws for Precision](https://arxiv.org/abs/2411.04330)

你训练的 token 越多，你需要的精度就越高。

### Densing law

[LLM最大能力密度100天翻一倍！清华刘知远团队提出Densing Law](https://mp.weixin.qq.com/s/O_jtO2ZuL11XB9GlaURsWg)

[Densing Law of LLMs](https://arxiv.org/pdf/2412.04315v2)

## 涌现能力

![llm-capabilities](../assets/llm-capabilities.png)

涌现能力：在小型模型中不存在而在大型模型中产生的能力，当规模达到一定程度时，性能显著提升，超出随机水平（参考
[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)）。与物理学中的**相变**现象类似（物质从一种相（状态）转变为另一种相的过程，通常伴随着能量的吸收或释放，并且涉及不同的物理性质，例如固体、液体和气体之间的转变）。

[普林斯顿DeepMind用数学证明：LLM不是随机鹦鹉！「规模越大能力越强」有理论根据](https://mp.weixin.qq.com/s/oYYuqbelBfCCSLW4Qo4POA)

[A Theory for Emergence of Complex Skills in Language Models](https://arxiv.org/abs/2307.15936)：


![涌现](../assets/emergent%20ability.png)

LLM的3种典型涌现能力及其对应代表模型：

### 上下文学习(in-context learning)

GPT-3（[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)）提出，只要提供一个自然语言指令和/或几个任务演示，语言模型就能通过完成输入文本的词序列的方式来为测试实例生成预期输出，不用额外的梯度更新。

+ ICL能力小模型不具备：1750亿的GPT-3有ICL能力，但GPT-1和GPT-2无此能力。
+ ICL能力取决于具体下游任务：130亿的GPT-3能在算术任务上有ICL，但1750亿的GPT-3在波斯语QA上无能为力。


### 指令遵循(instruction following)

使用**自然语言描述的混合多任务数据集进行微调（指令微调）**，LLM在**未见过的以指令形式描述的任务**上表现出色，具有更好的泛化能力。例如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)、[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)、[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)。

在[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的实验中，当模型大小达到680亿时，经过指定微调的LaMDA-PT开始在未见过的任务上显著优于未微调的模型，而80亿或更小的模型则没有这个现象。

在[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)的实验中，PaLM至少在620亿参数上才能在4个评估基准的各种任务上表现良好。

[精准0误差，输入价格打骨折！OpenAI官宣API支持结构化输出，JSON准确率100％](https://mp.weixin.qq.com/s/257SBcB2hr-xKPNYkUEErQ)


### 逐步推理(multi-step reasoning)

对于涉及多个推理步骤的复杂任务（如数学），可以使用**思维链（Chain-of-Thought, CoT）**提示策略（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)），让LLM通过**利用中间推理步骤的提示机制**来解决这类任务。

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)发现，CoT在模型大于600亿的PaLM和LaMBDA变体中能够提升在算术推理基准任务的效果，而当模型大于1000亿时，相比标准提示的优势更明显。

[How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

## LLM关键点

如何让LLM能够**通用**且**有能力**？

### 扩展

更大的模型、数据规模和更多的训练计算，但计算预算是有限的，可以用扩展法更高效地分配计算资源，如Chinchilla在**相同计算预算下增加训练token数**，优于更大模型规模的Gopher，同时需要数据清理。

### 训练

+ 分布式的训练框架：包括DeepSpeed（[Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters](https://dl.acm.org/doi/abs/10.1145/3394486.3406703)）和Megatron-LM（[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)和[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)）
+ 优化技巧：有助于提升训练稳定性和模型性能，如**重新开始以克服训练损失激增**（[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)）和**混合精度训练**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

### 能力引导

当LLM执行某些特定任务时，可能不会显式地展示出其通用求解器的能力，**设计合适的任务指令或具体的ICL策略**可以**激发**这种能力，例如

+ 通过**包含中间推理步骤的CoT提示**
+ 使用**自然语言表达的任务描述**，对LLM进行**指令微调**

### 对齐微调

由于预训练语料库包括高质量和低质量的数据，LLM可能生成有毒、偏见甚至有害的内容，要让LLM和人类价值观保持一致，如**有用性、诚实性和无害性**。RLHF相关工作如[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)能够产生高质量、无害的回答（例如拒绝回答侮辱性问题）。


### 工具操作

LLM本质是基于海量文本语料库进行文本生成训练的，对于不适合以文本形式表达的任务表现不佳（如数字计算），且其能力受限于预训练数据，无法获取最新信息。可以利用外部工具：

+ [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/pdf/2302.04761.pdf)能利用计算器进行准确计算
+ [Webgpt: Browser-assisted question-answering with human feed- back](https://arxiv.org/pdf/2112.09332.pdf)能利用搜索引擎检索未知信息

# LLM数据集

## 常用数据集

llm中文数据集：[https://juejin.cn/post/7238921093553438779](https://juejin.cn/post/7238921093553438779)

+ Books：
    + [BookCorpus](https://arxiv.org/pdf/1506.06724.pdf)：超过11000本电子书，用于GPT和GPT-2。
    + [Gutenberg](https://www.gutenberg.org/)：超过70000本文学作品，包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共领域，用于MT-NLG和LLaMA。
    + Books1和Books2：比BookCorpus大得多，但未公开，用于GPT-3。
+ CommonCrawl：最大的开源网络爬虫数据库之一，**百万亿字节**，有大量噪音和低质信息，需要过滤，有如下4个子集：
    + [C4](https://www.tensorflow.org/datasets/catalog/c4)：包括en（806G，训练T5、LaMDA、Gopher、UL2）、en.noclean（6T）、realnewslike（36G）、webtextlike（17G）、multilingual（38T，训练mT5）。
    + [CC-Stories](https://arxiv.org/pdf/1806.02847.pdf)：31G，内容以故事的形式展示
    + [CC-News](https://arxiv.org/pdf/1907.11692.pdf)：76G
    + [RealNews](https://arxiv.org/pdf/1905.12616.pdf)：120G
+ Reddit Links：Reddit上的帖子，高赞通常比较有用，可以拿来创建高质量数据集。
    + [WebText](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)：由Reddit上的高赞链接组成，未公开，对应的开源版是[OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)。
    + [Pushshift.io](https://arxiv.org/pdf/2001.08435.pdf)：实时更新的数据集，包括Reddit自创建以来的历史数据，有数据存储，也有实用工具，供用户搜索、总结和统计分析。
+ Wikipedia：大部分文章使用写作风格，并支持引用，英语版本用于大多数LLM，如GPT-3、LaMDA、LLaMA，还有多语言版。
+ Code：包括开源许可证的公共代码库（如github）和与代码相关的问答平台（如StackOverflow）,Google公开了[BigQuery](https://cloud.google.com/bigquery?hl=zh-cn)数据集，CodeGen用的BIGQUERY是其的一个子集。
+ 其他：
    + [The Pile](https://arxiv.org/pdf/2101.00027.pdf)有800G，包括书籍、网站、代码、科学论文和社交媒体平台，有22个子集，用于GPT-J(6B)、CodeGen(16B)、Megatron-Turing NLG（530B）。
    + [ROOTS](https://arxiv.org/pdf/2303.03915.pdf)由各种小数据集组成，共1.6T，包括59种语言（自然语言和编程语言），用于BLOOM。


## 数据收集

### 数据获取

+ 通用文本数据：
    + 网页：例如CommonCrawl，同时需要过滤和处理以提高质量
    + 对话文本：公共对话数据如PushShift.io，对于在线社交媒体的对话数据，可以**转换成树形结构**，每句话与回应其的话相连。多方的对话树可以划分为预训练语料库中的多个子对话。过度引入对话数据可能会有潜在风险（[OPT: open pre-trained transformer language models](https://arxiv.org/pdf/2205.01068.pdf)）：陈述性指令和直接疑问句被错误地认为是对话的开始，导致指令的有效性下降。
    + 书籍：更正式的长文本，利于**学习语言知识**、**建模长期依赖关系**、**生成叙述性和连贯的文本**。
+ 专用文本数据：
    + 多语言文本：BLOOM的预训练语料中包括了46种语言，PaLM包含了122种
    + 科学文本：如arxiv论文、科学教材、数学 网页等，通常需要特定的标记化和预处理。
    + 代码：一是编程问答社区，二是开源代码仅为。对应长距离依赖和准确的执行逻辑，可能是复杂推理能力的来源。将推理任务格式化为代码形式还能帮LLM生成更准确的结果（如[Language models of code are few-shot commonsense learners](https://arxiv.org/pdf/2210.07128.pdf)和[Autoformalization with large language models](https://arxiv.org/pdf/2205.12615.pdf)）

### 数据预处理

+ 质量过滤：有一些基于分类器的方法，例如维基百科的数据为正样本，负采样其他数据训练二分类器，但这种方法会删除方言、口语和社会语言的高质量文本，可能导致有偏、减少多样性。还有启发式的方法，主要包括：
    + 基于语言的过滤：如果该llm主要用于某种语言，可以把其他语言删了
    + 基于度量的过滤：利用生成文本的评估度量（如**perplexity**）来检测和删除不自然的句子
    + 基于统计的过滤：如**标点符号分布**、**符号和单词比例**、**句子长度**等
    + 基于关键词的过滤：删除噪声或无用元素，如**HTML标签**、**超链接**、**模板**、**攻击性词语**等。
+ 去重：[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)中发现重复数据会降低多样性，可能导致训练不稳定。下面3个级的去重都很有用
    + 句子级：删掉包含重复单词和短语的句子，因为可能在语言建模中引入**重复模式**（[The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751.pdf)）(后面的章节会讲)
    + 文档级：通过文档间的表层特征（如n-gram或单词重合率）来删掉重复文档
    + 数据集级：训练集中删掉测试集可能出现的重复文本，防止训练集和评估集间的重叠
+ 隐私去除：删掉可识别个人信息（PII），如基于关键词（姓名、地址、电话号码）识别。另外，[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/pdf/2202.06539.pdf)发现LLM在隐私攻击下的脆弱性可能归因于预训练语料中存在**重复PII数据**。
+ 分词：可以直接利用已有分词器，也可以使用专门为预训练语料库设计的分词器，如SentencePiece，而且**BPE**(byte pair encoding)能**确保分词后的信息不会丢失**，但其中的如NFKC([Unicode normalization forms](https://unicode.org/reports/tr15/))的**归一化技术**可能会**降低分词的性能**。

### 预训练语料的重要性

+ 混合来源：不同领域和场景的数据能让LLM有更强大的泛化能力。需要**仔细设置数据分布**，Gopher对数据分布消融，发现增加书籍数据可以提升捕捉长期依赖的能力，增加c4数据集比例可以提升其在c4验证集上的效果，但单独训练过多的某个领域数据会影响LLM在其他领域的泛化能力。
+ 数据量：模型性能方面，**数据大小**也能看到与模型大小类似的**扩展法则**。LLaMA发现，用更多数据训练更长时间，较小的模型也能实现良好性能。
+ 数据质量：Gopher、GLaM和T5都发现，在清理后的数据上训练能提升llm效果。数据的重复可能导致『双下降现象』（[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)和[Deep double descent: Where bigger models and more data hurt](https://arxiv.org/pdf/1912.02292.pdf)），甚至会导致训练不稳定。此外，[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)还发现，**重复数据会降低LLM从上下文复制的能力**，从而影响**ICL中的泛化能力**。

注：**双下降**指的是随着模型复杂性的增加，可能**loss先下降，然后再升高，最后又下降**：
+ 当模型的复杂性低于数据的复杂性时，增加模型的复杂性可以帮助减少训练误差。
+ 当模型的复杂性超过数据的复杂性时，增加模型的复杂性反而可能导致训练误差增加。这是因为模型开始过拟合数据，捕获数据中的噪声而非实际的模式。
+ 当模型的复杂性远大于数据的复杂性时，训练误差可能再次开始减少。这是因为模型有足够的能力来对数据的噪声进行平滑，同时仍然能够捕获数据的实际模式。

## benchmark

MMLU

[Measuring massive multitask language understanding](https://arxiv.org/pdf/2009.03300)

# LLM模型架构

## MoE原理

[MoE模型的前世今生](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

[MoE 系列论文解读：Gshard、FastMoE、Tutel、MegaBlocks 等](https://mp.weixin.qq.com/s/T5eJZWGH3yRpK9bxmvhhTA)

[CMU开源GRIFFIN：一种新颖的无需训练的MoE方法，提高大模型的生成效率！](https://mp.weixin.qq.com/s/33ISRxfXp4Z7OCN1dBKGyA)

[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/pdf/2404.01365.pdf)

[From Sparse to Soft Mixtures of Experts](https://arxiv.org/pdf/2308.00951): softmoe

MoE的核心问题是一个**如何把token分配给哪个专家**的**离散优化**问题，有如下离散+稀疏的分配方法，都需要**辅助loss**来**平衡每个专家的负载**，以减少drop tokens：

+ 线性规划
+ 强化学习
+ 人为固定规则
+ 最优运输方法
+ 贪心topk token-choose-expert
+ 贪心topk expert-choose-token

![softmoe](../assets/softmoe.png)

在softmoe中，假设N个token，S个slot，E个expert

代码：

[https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97](https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97)和[https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128](https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128)

[算法、系统和应用，三个视角全面读懂混合专家（MoE）](https://mp.weixin.qq.com/s/3UEFwy87f8O4H1Tt4VVnig)


[A Survey on Mixture of Experts](https://arxiv.org/pdf/2407.06204)

![moe-gating](../assets/moe-gating.png)

[从ACL 2024录用论文看混合专家模型（MoE）最新研究进展](https://mp.weixin.qq.com/s/BCsRHvHnn3B8oOODjim-Kg)

### Dynamic MoE

[Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/pdf/2403.07652)

[https://github.com/ZhenweiAn/Dynamic_MoE](https://github.com/ZhenweiAn/Dynamic_MoE)



### XMoE

是上面两种方法的并集

[XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection](https://arxiv.org/pdf/2403.18926)

[https://github.com/ysngki/XMoE](https://github.com/ysngki/XMoE)

### HyperMoE

[HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/pdf/2402.12656)

[https://github.com/Bumble666/Hyper_MoE](https://github.com/Bumble666/Hyper_MoE)

### Expert Pruning

[Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/pdf/2402.14800)

[https://github.com/Lucky-Lance/Expert_Sparsity](https://github.com/Lucky-Lance/Expert_Sparsity)

### MixLoRA

[Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/pdf/2402.15896)

### ESFT

[Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv.org/pdf/2407.01906)



### 多模态MoE

[混合专家更有主见了，能感知多模态分情况行事，Meta提出模态感知型专家混合](https://mp.weixin.qq.com/s/1FNqu0CwPmMFuDTMhli7WA)

[Chameleon: Mixed-modal early-fusion foundation models](https://arxiv.org/pdf/2405.09818)单一Transformer架构，可以根据下一个token的预测目标，对由离散图像和文本token组成的混合模态序列进行建模，从而在不同模态之间进行无缝推理和生成。然而对于Chameleon这样各种模态会在模型训练的早期混合起来的模型，想要拓展它的能力，需要投入大量算力。

[MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)使用路由式稀疏架构（routed sparse architecture）

### HMOE

[优化传统MoE结构，腾讯混元团队提出专家差异化新思路](https://mp.weixin.qq.com/s/gOXGL_MReneAZCmO7lBIng)

[HMoE: Heterogeneous Mixture of Experts for Language Modeling](https://arxiv.org/pdf/2408.10681)

在 HMoE 中，**每个专家的大小不再相同**，从而赋予了每个专家不同的表达能力。这种差异化设计使得路由可以根据专家的实际能力动态分配不同难度的 token，有效解决了专家专业化程度不足的问题。 

### OLMoE

[OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/pdf/2409.02060)

[https://huggingface.co/allenai/OLMoE-1B-7B-0924](https://huggingface.co/allenai/OLMoE-1B-7B-0924)

[https://github.com/allenai/OLMoE](https://github.com/allenai/OLMoE)

### 元象MOE

[中国最大开源MoE模型，255B参数无条件免费商用，元象发布](https://mp.weixin.qq.com/s/vX0gt2YfsbUbrCpvNBFHZw)

XVERSE-MoE-A36B，该模型总参数255B，激活参数36B，达到100B模型性能的「跨级」跃升。

[https://github.com/xverse-ai/XVERSE-MoE-A36B](https://github.com/xverse-ai/XVERSE-MoE-A36B)

### MoEUT

[Jurgen、曼宁等大佬新作：MoE重塑6年前的Universal Transformer，高效升级](https://mp.weixin.qq.com/s/CckHYrUpwCpft53-lZ4DwA)

[MoEUT: Mixture-of-Experts Universal Transformers](https://arxiv.org/pdf/2405.16039)

[https://github.com/robertcsordas/moeut](https://github.com/robertcsordas/moeut)

### MoA

[无问芯穹提出混合稀疏注意力方案MoA，加速长文本生成，实现最高8倍吞吐率提升](https://mp.weixin.qq.com/s/rjGAJfusY_CHSx3Q0SHVmg)

[https://github.com/thu-nics/MoA](https://github.com/thu-nics/MoA)

[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](https://arxiv.org/abs/2406.14909)

### GRIN

[专家模型不要专家并行！微软开源MoE新路径](https://mp.weixin.qq.com/s/pt-AlH_z4e3PNiKC9Iyz7A)

[GRIN: GRadient-INformed MoE](https://arxiv.org/abs/2409.12136)


## 主流框架

+ **编码器-解码器架构(encoder-decoder)**：标准Transformer，如T5、BART，**只有少数LLLM还用这种结构**，如Flan-T5
+ **因果解码器架构(causual decoder)**：也叫**decoder-only**，**单向注意力掩码**，输入和输出token通过解码器以相同方式进行处理，以GPT系列为代表，现有大部分LLM都是这种架构，如OPT、BLOOM、Gopher等。
+ **前缀解码器架构(prefix decoder)**：修正因果解码器的掩码机制，使其能**对前缀token执行双向注意力**，并且**仅对生成的token执行单向注意力**（和encoder-decoder类似），即[Unified language model pre-training for natural language understanding and generation](https://arxiv.org/pdf/1905.03197.pdf)提出的uni-lm。[What language model architecture and pretraining objective works best for zero-shot generalization?](https://arxiv.org/pdf/2204.05832.pdf)建议不从头开始预训练，而是**继续训练因果编码器，然后将其转换成前缀编码器以加速收敛**。例如U-PaLM从PaLM演化而来，还有GLM-130B也是这种架构。

![uni-lm](../assets/uni-lm.png)

[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)

对于这3种架构，都可以用**MoE**进行扩展，每个输入的**一小部分神经网络权重**被**稀疏激活**，如[Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)和GLaM。[Unified scaling laws for routed language models](https://arxiv.org/pdf/2202.01169.pdf)发现，通过**增加专家数量或总参数大小**，性能会有显著改进。

### 讨论：为什么现在的LLM都是Decoder only的架构？

&nbsp;

[https://www.zhihu.com/question/588325646/answer/2940298964](https://www.zhihu.com/question/588325646/answer/2940298964)

+ **泛化性能强**：ICML 22的[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf).在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在**各种下游任务上zero-shot泛化性能最好**；另外，ACL23的[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/pdf/2212.10559.pdf)等工作表明，decoder-only模型相当于基于给出的几个示例**隐式地进行梯度下降**，对应的in-context learning泛化能力更强，
+ **秩**的讨论：[Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)的讨论，$$n\times d$$和$$d\times n$$相乘后（$$n\gg d$$）再加上softmax后，秩不超过$$d$$，而decoder-only中有一个下三角矩阵的mask，所以输入的是一个下三角矩阵，而下三角矩阵的行列式是对角线之积，且有softmax，对角线肯定大于0，所以是满秩的(行列式不为0-->矩阵经过变换后不会有一行或者一列全为0-->当前矩阵满秩)
+ 预训练**任务难度**更大：相比encoder-decoder，decoder-only架构里**每个位置能接触到的信息更少**，故难度更高，当模型大小和数据量够的时候，上限更高
+ 隐式学习了**位置信息**：[Transformer Language Models without Positional Encodings Still Learn Positional Information](https://aclanthology.org/2022.findings-emnlp.99.pdf)，encoder里对语序的区分能力较弱，需要结合position encoding，而causual attention隐式地具备了这种建模位置的能力。
+ **工程效率**：支持**复用kv-cache**，对多轮对话更友好，**『DIN的FLOPS』**一节里有讲

[盛名一时的BERT哪去了？这个问题的答案昭示了LLM范式的转变](https://mp.weixin.qq.com/s/fKeorQYwRlmmepuG1_aJlQ)

[What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising)

#### 去噪目标

&nbsp;

**去噪目标**指的是span corruption任务的任意变体，即填充(infilling)或填空(fill in the blank)。表达方式有很多，比如span长度、随机性、sentinel token等。

+ BERT类的模型中，大部分是**in-place**的去噪目标，例如对mask tokens的分类head，
+ T5的做法则是通过encoder-decoder或decoder-only模型来处理数据变换，即把masked token move to the back给模型预测。

去噪目标的效果很好，可以作为常规语言建模的**补充目标**，但不足以单独作为目标，因为去噪有两个缺点：

+ **更少的loss exposure**：在去噪目标中，只有**少量token会被mask和学习**，而常规语言建模则接近100%，使得**每个FLOP的样本效率非常低**
+ 比常规语言建模更不自然：以一种奇怪的方式重新设定输入输出格式，不太适合少样本学习

#### PrefixLM vs decoder-only

&nbsp;

注：归纳偏置(inductive bias)指模型在预测未遇到的输入时，做的一些假设的集合，例如最小描述长度（奥卡姆剃刀）指的就是当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。

对语言模型来说，双向注意力是一种有趣的**归纳偏置**，相比于较小规模的场景，双向注意力**在规模较大时可能就没那么重要了**，或者可能对不同的任务或模态有不同的影响，例如PaliGemma就用的prefixLM（[PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/pdf/2407.07726)）

![paligemma](../assets/paligemma.png)

PrefixLM也存在缓存问题，是这类架构的一个固有缺陷

#### encoder-decoder的优缺点

&nbsp;

+ 相比decoder-only的优势：endocer不受causal的限制，可以激进地试各种pooling/linear attention，因此可以offload一些**不那么重要的context到encoder中**，也可以**让encoder更小**，例如[Charformer](https://arxiv.org/pdf/2106.12672)
+ 相比prefixLM的缺点：输入和目标必须分配固定的预算，例如输入预算是1024token，那么encoder就**必须pad**到1024，而这可能会浪费大量计算。相反，在 PrefixLM 中，输入和目标可以直接concat起来，从而可以缓解这个问题


## 组件配置

### 标准化（norm）

&nbsp;

LN(layer norm)能缓解LLM训练不稳定的问题，其位置很重要。

![pre-ln](../assets/pre-ln.jpeg)

+ 前置LN：最初Transformer使用后置LN，但大多数LLM采用前置LN以实现更稳定的训练，尽管会有一些性能损失([On layer normalization in the transformer architecture](https://arxiv.org/pdf/2002.04745.pdf))。[Sandwich-LN](https://arxiv.org/pdf/2105.13290.pdf)在残差连接前添加额外的LN，虽然能避免数值爆炸，但有时会无法稳定LLM的训练，可能导致训练崩溃（[GLM-130B: an open bilingual pre-trained model](https://arxiv.org/pdf/2210.02414.pdf)）
+ [RMS Norm](https://arxiv.org/pdf/1910.07467.pdf)：训练和性能都不错，在Gopher和Chinchilla里使用
+ [Deep Norm](https://arxiv.org/pdf/2203.00555.pdf)：比LN有更好的训练稳定性，和后标准化一起用在GLM-130B里

![deep-norm](../assets/deep%20norm.png)

+ [DyT](https://arxiv.org/pdf/2503.10622)：用$$tanh(\alpha x)$$替换掉layer norm里的normalize

```python
class DyT(nn.Module):
    def __init__(self, num_features, alpha_init_value=0.5):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        return x * self.weight + self.bias
```

此外，**在emb后直接加额外的LN**能提升训练稳定性，但会导致**显著的性能下降**([What language model to train if you have one million GPU hours?](https://arxiv.org/pdf/2210.15424.pdf))，在后来的LLM中**被移除**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

[神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！](https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ)

[On the Nonlinearity of Layer Normalization](https://arxiv.org/pdf/2406.01255)

### 激活函数

&nbsp;

FFN中的激活函数：

+ [GeLU](https://arxiv.org/pdf/1606.08415.pdf)：大部分都是这个
+ [GLU(gated linear units)的变体](https://arxiv.org/pdf/2002.05202.pdf)：应用在PaLM和LaMDA等模型中，如SwiGLU和GeGLU有更好的效果，但在FFN中的参数量比GeLU要大50%


原始Transformer中

XXX\operatorname{FFN}\left(x, W_1, W_2, b_1, b_2\right)=\max \left(0, x W_1+b_1\right) W_2+b_2XXX

T5中把bias干掉了

XXX\operatorname{FFN}_{\operatorname{ReLU}}\left(x, W_1, W_2\right)=\max \left(x W_1, 0\right) W_2XXX

然后，$$\operatorname{GELU}(x)=x \Phi(x)$$，同时$$\operatorname{Swish}_\beta(x)=x \sigma(\beta x)$$，接下来

XXX\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes(x V+c)XXX

XXX\operatorname{Bilinear}(x, W, V, b, c)=(x W+b) \otimes(x V+c)XXX

XXX\operatorname{ReGLU}(x, W, V, b, c)=\max (0, x W+b) \otimes(x V+c)XXX

XXX\operatorname{GEGLU}(x, W, V, b, c)=\operatorname{GELU}(x W+b) \otimes(x V+c)XXX

XXX\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)XXX

对应起来就是

XXX\operatorname{FFN}_{\mathrm{GLU}}\left(x, W, V, W_2\right)=(\sigma(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {Bilinear }}\left(x, W, V, W_2\right)=(x W \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\operatorname{ReGLU}}\left(x, W, V, W_2\right)=(\max (0, x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {GEGLU }}\left(x, W, V, W_2\right)=(\operatorname{GELU}(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {SwiGLU }}\left(x, W, V, W_2\right)=\left(\operatorname{Swish}_1(x W) \otimes x V\right) W_2XXX


### 位置编码

&nbsp;

Transformer的self-attention有转换不变性，故要位置编码以引入绝对或相对位置信息来建模序列。

+ 绝对位置编码：
    + 正弦函数：原始Transformer中使用
    + 可学习的位置编码：LLM中常用
+ 相对位置编码：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)提出，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的，根据**k和q之间的偏移量**生成emb
+ Alibi：[Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf)提出，使用**k和q之间距离的惩罚**来给注意力分数加bias，[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf)发现其有更好的**零样本泛化能力**和更强的**外推能力**，能够在**比训练序列更长的序列**上表现良好。
+ RoPE：[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)提出，**k和q之间的分数用相对位置信息计算**，利于建模长序列，在PaLM、LLaMA、GLM-130B中都有应用。
  

[Transformer升级之路：RoPE的底数设计原则](https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ)

[Base of RoPE Bounds Context Length](https://arxiv.org/pdf/2405.14591)

[Decoder-only的LLM为什么需要位置编码？](https://mp.weixin.qq.com/s/3sBYrKyEPP93nwigaOAAAA)

[HuggingFace工程师亲授：如何在Transformer中实现最好的位置编码](https://mp.weixin.qq.com/s/_vhyBrTM041FfHEohwLqMA)

(toread)

### 注意力机制和Bias

+ 稀疏注意力：[Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf))，**计算复杂度更低**，GPT-3用了
+ FlashAttention：[Flashattention: Fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/pdf/2205.14135.pdf)，考虑显存访问
+ 其他attention：如[Random feature attention](https://arxiv.org/pdf/2103.02143.pdf)、[Big bird: Transformers for longer sequences](https://arxiv.org/pdf/2007.14062.pdf)
+ 移除bias：PaLM和Galactica中将bias删了，能够增加训练稳定性。


### 优化器

&nbsp;

[月之暗面开源改进版Muon优化器，算力需求比AdamW锐减48%，DeepSeek也适用](https://mp.weixin.qq.com/s/E65ULmjlK7Lv81dqvAubcQ)

### 小结

#### 归一化位置

&nbsp;

sublayer表示FFN或self-attention模块

| 方法 | 公式 | 
|------|---------------|
| post Norm | $$\operatorname{Norm}(\mathbf{x}+\operatorname{Sulayerb}(\mathbf{x}))$$ |
| pre Norm | $$\mathbf{x}+\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x}))$$ |
| Sandwich Norm | $$\mathbf{x}+\operatorname{Norm}(\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x})))$$ |

#### 归一化方法

| 方法 | 公式 | 
|------|---------------|
|Layer Norm| $$\frac{\mathrm{x}-\mu}{\sqrt{\sigma}} \cdot \gamma+\beta, \quad \mu=\frac{1}{d} \sum_{i=1}^d x_i, \quad \sigma=\sqrt{\frac{1}{d} \sum_{i=1}^d(x_i-\mu)^2}$$ |
|RMSNorm| $$\frac{\mathrm{x}}{\operatorname{RMS}(\mathrm{x})} \cdot \gamma, \quad \operatorname{RMS}(\mathbf{x})=\sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}$$ |
|Deep Norm| $$LayerNorm (\alpha \cdot \mathbf{x}+\operatorname{Sublayer}(\mathbf{x}))$$ |

#### 激活函数

| 方法 | 公式 | 
|------|-----------------------|
|ReLU| $$\operatorname{ReLU}(\mathbf{x})=\max (\mathbf{x}, \mathbf{0})$$ |
| GeLU | $$\operatorname{GeLU}(\mathbf{x})=0.5 \mathrm{x} \otimes[1+\operatorname{erf}(\mathbf{x} / \sqrt{2})], \quad \operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$$ |
|Swish | $$\operatorname{Swish}(\mathbf{x})=\mathbf{x} \otimes \operatorname{sigmoid}(\mathbf{x})$$ |
|SwiGLU|$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$ |
|GeGLU|$$\operatorname{GeGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{GeLU}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$|

#### 位置嵌入

+ $$A_{ij}$$：**q和k之间**的**注意力分数**
+ $$r_{i-j}$$：基于**q和k之间偏移**的可学习标量
+ $$\mathbf{R}_{\theta, i-j}$$：旋转角度为$$t\cdot \theta$$的旋转矩阵

| 方法 | 公式 | 
|------|--------------|
|绝对位置编码| $$\mathbf{x}_i=\mathbf{x}_i+\mathbf{p}_i$$ |
|相对位置编码|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T+r_{i-j}$$|
|RoPE|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T$$|
|Alibi|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T-m(i-j)$$|


## 预训练任务

### 语言建模

&nbsp;

语言建模是**仅解码器LLM**的常见目标，给定token序列$$\mathbf{x}=\left\{x_1, \ldots, x_n\right\}$$，旨在基于序列中前面的token，自回归地预估目标token：

XXX
\mathcal{L}_{L M}(\mathbf{x})=\sum_{i=1}^n \log P\left(x_i \mid x_{<i}\right)
XXX


对应到代码里：

```python
hidden_states = outputs[0]
logits = self.lm_head(hidden_states)

# gemma系列模型会做soft-cap
cap = self.config.logits_soft_cap
logits = nn.functional.tanh(logits / cap) * cap

logits = logits.float()
loss = None
if labels is not None:
    # Shift so that tokens < n predict n
    ## 假设句子长度为4，词表有3个词，
    ## logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3], [0.6, 0.1, 0.3] ]
    ## labels: [0, 2, 1, 0]
    ## shift_logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3]
    shift_logits = logits[..., :-1, :].contiguous()
    ## shift_labels: [2, 1, 0]
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    # [bs, seq_len - 1, vocab_size] 变为 [(bs * (seq_len - 1)), vocab_size]
    shift_logits = shift_logits.view(-1, self.config.vocab_size)
    # [bs, seq_len - 1] 变为 [(bs * (seq_len - 1))]
    shift_labels = shift_labels.view(-1)
    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels)
```

**前缀解码器**架构使用的是前缀语言建模任务，其loss**不涉及对前缀内token的预测**，故预训练时**涉及的序列中token较少**，故当预训练token数相同时，前缀语言模型的**性能往往略低**于传统语言模型任务。

另外，自回归的loss：

+ 训练时：是可以并行的，因为每个位置的label是已知的，可以并行算，
+ 预测时：是串行的，因为得预测完了第t个词，才能去预测第t+1个词。

### 去噪自编码

&nbsp;

DAE是BERT待模型的常见任务，即MLM（masked language model），输入$$\mathbf{x}_{\backslash \tilde{\mathbf{x}}}$$是一些**有随机替换区间的损坏文本**，目标是恢复被替换的token $$\tilde{\mathbf{x}}$$：

XXX
\mathcal{L}_{D A E}(\mathbf{x})=\log P\left(\tilde{\mathbf{x}} \mid \mathbf{x}_{\backslash \tilde{\mathbf{x}}}\right)
XXX

在T5和GLM-130B中使用，**自回归地恢复替换区间**。

### 其他任务

multi-token prediction，一次预估未来的k个词

[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)

![multi-token-prediction](../assets/multi-token-prediction.png)


# 典型LLM简介

llm榜单：

[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

![llm-families](../assets/llm-families.png)


## GPT系列

### GPT3

&nbsp;

2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning。

### CODEX

&nbsp;

[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)

能够解析自然语言，并生成代码。CODEX是gpt3在github上收集的代码语料上进行finetune得到的，并且在微软的copilot中使用。

### WebGPT

&nbsp;

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[https://openai.com/blog/webgpt/](https://openai.com/blog/webgpt/)

为了回答开放性问题，使用基于文本的浏览器对gpt3进行finetune，包括如下3个步骤：

+ 学习使用人类示范（demonstration）数据来模仿人类的浏览行为
+ 学习一个reward函数来预测人类偏好
+ 用强化学习和拒绝采样来优化reward函数


注：[重要性采样和拒绝采样](https://zhuanlan.zhihu.com/p/664233442)

+ **重要性采样**的关键是**降低方差**：因为相同的样本量，用$$\pi(x)$$分布采样得到的结果方差较大(或者是$$\pi(x)$$不好采样)，而用$$p(x)$$采样的样本得到的结果方差较小，用来估计原分布$$\pi(x)$$
+ **拒绝采样**：引入易于采样的分布$$Q(x)$$，然后从中随机地筛掉某些样本(根据**接受概率**接受或者拒绝样本)，使得剩下的样本服从分布$$P(x)$$

拒绝采样的步骤：

+ 从辅助分布$$Q(x)$$中采样得到样本$$x_i$$
+ 计算接受概率$$A = P(x_i) / (M \times Q(x_i))$$，其中$$M$$是一个常数，满足$$P(x) \leq M \times Q(x)$$对于所有$$x$$成立
+ 以概率$$A$$接受样本$$x_i$$，即生成一个随机数$$u$$，如果$$u \leq A$$，则接受样本$$x_i$$；否则拒绝样本$$x_i$$。

重复上述步骤，直到获得足够数量的样本。


### InstructGPT

&nbsp;

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

sft+rm+rl，在最小性能降低的情况下，提升了生成结果的真实性，并降低了毒害性

### ChatGPT&GPT-4

&nbsp;

2022.11.30推出了ChatGPT，基于GPT3.5，即InstructGPT的兄弟

2023.3推出了GPT-4，多模态LLM，能输入图像和文本

o3/o4-mini：[Agent 要被吃进大模型了](https://mp.weixin.qq.com/s/89QkOqY3vWzQvPPbGkBodQ)

## LLaMA系列

### LLaMA

&nbsp;

2023年2月发布，[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)，开源的LLaMA-13B比 GPT3 175B在很多任务上都更好

参考代码：
[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

[https://github.com/meta-llama/llama](https://github.com/meta-llama/llama)

之前的工作考虑的是在训练预算有限的前提下，如何提升模型性能（2022年deepmind的[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)的Chinchilla）,llama考虑在预测时的预算。例如chinchilla是一个10b的模型在200b的token上训练，但其实一个7b的模型当用了1T的token后，性能仍在提升。LLama-13b比gpt3在大多数benchmark上好，但size只有1/10，在一个GPU上就能跑。

llama只用公开数据训练，而Chinchilla、PaLM、GPT-3都有自己的未公开数据集。其他的OPT、GPT-NeoX、BLOOM、GLM虽然也只用公开数据集，但打不过PaLM-62B或者Chinchilla

#### 预训练数据

&nbsp;

+ English CommonCrawl(67%)：使用CCNet pipeline，去重、用fasttext把非英文的页面删了，用n-gram把低质内容删了。此外，还训了一个线性模型，对页面进行分类：作为维基百科的引用 vs 随机采样的页面，最后把不属于引用这个类别的页面删了
+ C4(15%)：与CCNet类似，主要区别在质量过滤是基于启发式的规则，如标点符号的存在，或者词数和句子数
+ github(4.5%)：使用Google BigQuery里的公开github数据集，只用Apache、BSD和MIT证书的。低质判断是启发式规则，如字母数字占比、行的长度等，用正则删掉head等样式，最终以文件粒度进行去重。
+ wikipedia(4.5%)：2022年6-8月的数据，包括20种语言
+ Gutenberg and Books3(4.5%)：两个书籍数据集，对有90%以上内容重复的书籍做去重。
+ Arxiv(2.5%)：拿原始的tex文件，删掉first section之前的东西，还有一些注释、宏
+ Stack Exchange(2%)：高质量的问答网站，按答案的分数排序

![llama_data](../assets/llama_data.png)

tokenizer：BPE，使用sentencepiece的实现。将所有numbers切成单个数字，回退到字节去处理未知的utf8字符（fallback to bytes to decompose unknown UTF-8 characters）

总共有1.4T的token，对大部分训练数据，每个token在训练时只用了一次，除了维基和book大概用了两次。

附：gpt4说：当我们说"一个token只训练一次"，我们其实是在说在一个epoch（一个完整遍历训练集的过程）中，我们只遍历一次完整的数据集。如果一个特定的token在数据集中出现多次，那么在一个epoch中，这个token就会被用来训练模型多次。

![llama](../assets/llama_params.png)


![一些大模型](../assets/LLM/WechatIMG322.jpg)


#### 网络结构

&nbsp;

+ SwiGLU激活函数(PaLM)：取代ReLU，[Glu variants improve trans- former](https://arxiv.org/abs/2002.05202)，把PaLM里的$$4d$$改了$$2/34d$$

说白了就是输入$$x$$，SwiGLU激活完是$$swish(w_1(x)) * w_3(x)$$，其中swish又叫silu，是$$f(x)=x \cdot sigmoid(x)$$

然后再过一个$$w_2$$，得到$$w_2(swish(w_1(x)) * w_3(x))$$就是最终的ffn输出

以下是transformers里的实现：[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act] ## 默认是silu，即swish

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) 
                for i in range(self.config.pretraining_tp)], dim=-1)

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

这个是llama官方代码的实现：[https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345](https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345)

```python
class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
        ffn_dim_multiplier: Optional[float],
    ):
        """
        Initialize the FeedForward module.

        Args:
            dim (int): Input dimension.
            hidden_dim (int): Hidden dimension of the feedforward layer.
            multiple_of (int): Value to ensure hidden dimension is 
                a multiple of this value.
            ffn_dim_multiplier (float, optional): 
                Custom multiplier for hidden dimension. Defaults to None.

        Attributes:
            w1 (ColumnParallelLinear): Linear transformation for the first layer.
            w2 (RowParallelLinear): Linear transformation for the second layer.
            w3 (ColumnParallelLinear): Linear transformation for the third layer.

        """
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if ffn_dim_multiplier is not None:
            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```


+ Rotary embeddings(GPTNeo)：删掉原来的绝对位置编码，加上rotary positional embedding(RoPE)，网络的每一层都加，参考[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)
+ pre-normalization(gpt3)：提升训练**稳定性**，对每个子层的输入做norm，而非输出。此外，使用的是RMSNorm函数([Root mean square layer normalization](https://arxiv.org/abs/1910.07467))取代标准的layer-norm
    + layernorm计算单个样本在单层中所有激活的均值和标准差，并使用这些统计数据来归一化该层的激活。
    + RMSnorm只计算激活的平方根均值（RMS），而不是标准差。这样做的一个好处是计算上更简单，因为它省去了计算均值的步骤，只关注激活的规模（scale）而非其准确的分布。$$\operatorname{RMSNorm}\left(x_i\right)=\frac{x_i}{\sqrt{\frac{1}{H} \sum_{j=1}^H x_j^2}+\epsilon}$$，其中$$H$$是该层的神经元个数，而且也不用求均值

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        Initialize the RMSNorm normalization layer.

        Args:
            dim (int): The dimension of the input tensor.
            eps (float, optional): A small value added to the denominator 
                for numerical stability. Default is 1e-6.

        Attributes:
            eps (float): A small value added to the denominator 
                for numerical stability.
            weight (nn.Parameter): Learnable scaling parameter.

        """
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        Apply the RMSNorm normalization to the input tensor.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The normalized tensor.

        """
        # rsqrt(x)= 1/ sqrt(x)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        Forward pass through the RMSNorm layer.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor after applying RMSNorm.

        """
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

优化器：AdamW，cosine学习率schedule，最终学习率是最大学习率的10%。0.1的weight decay和1.0的gradient cliping，使用2000steps的warmup

#### 训练加速

&nbsp;

+ 对causal multi-head attention加速：实现在[http://github.com/facebookresearch/xformers](http://github.com/facebookresearch/xformers)中，降低内存使用和运行时间，参考[self-attention does not need $$o(n^2)$$ memory](https://arxiv.org/pdf/2112.05682.pdf)，以及[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)。思想是
    + 不存储attention weights
    + 不计算被mask的key/query得分
+ 减少xxx：


### LLaMA2

&nbsp;

2023年7月，[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

+ 基于公开数据集预自监督地训练一个llama-2
+ llama-2-chat模型:
    + sft后得到初始版本
    + 使用RLHF迭代地更新（拒绝采样+ppo） 

![llama-2-chat](../assets/llama-2-chat.png)

[https://zhuanlan.zhihu.com/p/636784644](https://zhuanlan.zhihu.com/p/636784644)

使用了GQA（grouped query attention）(参考[Gqa: Training generalized multi-query transformer models from multi-head checkpoints](https://arxiv.org/pdf/2305.13245.pdf))，在注意力机制中对**K/V进行参数共享**的方案，可以在**推理**过程中**减小KV缓存**。

![gqa](../assets/gqa.png)


### LLaMA3

#### 原始LLaMa3

&nbsp;

2024年4月

[开源大模型Llama 3王者归来！最大底牌4000亿参数，性能直逼GPT-4](https://mp.weixin.qq.com/s/KCyL8WTzXutPQ_k0Vl9Vwg)

[Llama 3超大杯有何惊喜？Meta会一直开源吗？当初为何笃信元宇宙？扎克伯格新访谈回应一切](https://mp.weixin.qq.com/s/e2n4ttcT8raDU877t53GPQ)

[Llama 3细节公布！AI产品总监站台讲解：Llama系列超庞大生态系统](https://mp.weixin.qq.com/s/iDAlop_LNv9evZtfPMPyUg)

[OpenAI 前创始成员、特斯拉自动驾驶前负责人 Andrej Karpathy 发表 Meta Llama 3 笔记](https://mp.weixin.qq.com/s/701PSyi954QHz_mt6Ddn-Q)

[Karpathy称赞，从零实现LLaMa3项目爆火，半天1.5k star](https://mp.weixin.qq.com/s/1poG0tEjmym1456mmR66nQ)

[Karpathy点赞，这份报告教你如何用 LLaMa 3创建高质量网络数据集](https://mp.weixin.qq.com/s/luZGMG1RRUT4X_ckt8hsCQ)

[https://github.com/naklecha/llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)

三个版本：8B 和 70B 参数的模型，还有一个 405B 参数的密集模型（还在训练之中，但已经在逼近GPT-4的领域，例如84.8 MMLU vs. 86.5 4Turbo），8B版本基本上与Llama-2的最大版本一样强大。

Llama 3的主要亮点：

+ 训练语料：基于超过15T token训练，比Llama 2数据集(2T)的7倍还多：
    + **scaling law的新发现**：对于一个8B模型，Chinchilla的**计算最优点**将是训练约200B词汇(前面提到了10B模型，大概要205B的token来训练，以此类推)，所以这里**超出了75倍**，而且**还未收敛**。可见，我们经常使用的LLMs在训练上显著不足，可能是100-1000倍或更多，远未达到它们的收敛点。
+ tokenizer：词汇数量从Llama 2的32K增加到Llama 3的128K，增加了4倍，拥有更多的词汇可以在长度上**更有效地压缩序列**。
+ 上下文窗口：从Llama 2的4096和Llama 1的2048增加到了**8192**，相比GPT4的128k还差得很远
+ 训练效率：比 Llama 2 高 3 倍，做了很多工程优化；
+ 模型结构：在Llama 2中，只在更大的模型使用了**分组查询注意力（GQA）**，但llama3的所有模型都使用了，包括最小的8B模型。
+ 新能力范畴：Llama-2 只能使用非常特定的工具，而 Llama-3 能使用好得多的工具，无需人工编程就能让其使用谷歌执行搜索，类似的功能还有编程和运行代码等。

[https://github.com/meta-llama/](https://github.com/meta-llama/)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

#### 多模态llama3

&nbsp;

[多模态 Llama-3 它来了 ！！[全网首发微调教程]](https://mp.weixin.qq.com/s/IFVIkVvMqeAL0lZF9OvRSA)

[https://github.com/InternLM/XTuner](https://github.com/InternLM/XTuner)

#### 中文llama3

&nbsp;

[首批中文版Llama3模型来了，解释成语、答弱智吧问题](https://mp.weixin.qq.com/s/ny0gBOxf4-tJiwjgp3o9HQ)

[https://github.com/CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat)

[https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)


### LLama3.1

[最强模型Llama 3.1 405B正式发布，扎克伯格：开源引领新时代](https://mp.weixin.qq.com/s/QUWumWsTF_Qq77tdlyCHdg)

[微调大模型，AMD MI300X就够了！跟着这篇博客微调Llama 3.1 405B，效果媲美H100](https://mp.weixin.qq.com/s/eJwg4GwH--9IVFedum_BnA)

[The Llama 3 Herd of Models](https://github.com/daiwk/collections/blob/master/assets/LLM/llama3.1.pdf)

[Llama3 92页技术报告中文全文详解](https://mp.weixin.qq.com/s/9Mnd1ass0XYUGW_UqDF8bw)

还开放了一个生态系统：

+ model：[https://github.com/meta-llama/llama-models/tree/main/models/llama3_1](https://github.com/meta-llama/llama-models/tree/main/models/llama3_1)
+ tool-chain：[https://github.com/meta-llama/llama-toolchain](https://github.com/meta-llama/llama-toolchain)
+ agent-system：[https://github.com/meta-llama/llama-agentic-system](https://github.com/meta-llama/llama-agentic-system)


#### 整体架构

&nbsp;

![multimodal-llama3.1](../assets/multimodal-llama3.1.png)

有2个主要的stage：

+ language model pre-training：用多语言的语料进行next token prediction
    + 使用超过15万亿(15T)个token训练Llama 3.1 405B，context window是8k tokens
    + 又加上一个continued pretraining stage，将context windows增加到128k tokens
+ language model post-training：sft+dpo+新的能力（如工具使用，能提升代码和推理能力）+safety

(toread)

这里有一些总结：[关于post-training和一些思考](https://mp.weixin.qq.com/s/Bpd6_zq9kmTTeHxZ9WJJpw)

为了支持多模态，还加入了如下3个stage：

+ 多模态encoder pre-training：
    + image encoder：在大量的img-text pair上训练，让模型能理解图片+描述
    + speech encoder：自监督，mask掉部分的speech输入，通过离散token表示来重建被mask的部分
+ vision adapter training：
    + 训练一个adapter将pretrained的image encoder融入到pretrained language model里，adapter包括一系列的cross-attention层将image-encoder表示输入language model。在image-text pair上训练，对齐图文表示。训练时**更新image-encoder的参数**，但**不更新language model的参数。**
    + 基于这个image-encoder来训练一个video-adapter，用的是video-text数据，让模型能聚合多帧间的信息。视频侧的temporal aggregator是有一个perceiver的resampler
+ speech adapter training：将speech encodings转成token表示，直接输入finetuned language model。在sft阶段，**adaper和encoder的参数联合更新**，但**不更新language model的参数**，还集成进了一个text-to-speech的系统。


#### 预训练

&nbsp;

在超过16,000个H100（80G HBM3）上训练，训练平台是[Meta open compute project, grand teton ai platform](https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/)，基于[MAST: Global scheduling of ml training across geo-distributed datacenters at hyperscale](https://www.usenix.org/system/files/osdi24-choudhury.pdf)进行schedule

选择了decoder only transformer with minor adaptations，而**不是MOE**，以最大限度地**提高训练稳定性**。

+ 使用GQA with 8 key-value heads，提升推理速度，在decoding时降低k-v cache的大小
+ 使用attention mask，**让同一序列里的不同documents不计算self-attention**。在标准的pretraining中影响不大，但**对于超长序列的continued pre-training非常重要**
+ 128K tokens的vocab，100k的tiktoken tokenizer+28k的额外token，更好地支持非英语的语言。相比llama2，每个token能压缩的字符3.17变成3.94，**压缩率更高了**，即同样计算量能读更多文本，同时也能提升下游任务效果。
+ 把RoPE的base frequency超参**加大到500,000**，能更好地**支持更长contexts**，[Effective Long-Context Scaling of Foundation Models](https://aclanthology.org/2024.naacl-long.260.pdf)说这个值对32768长度的context很有效。


参数量：

|                          | 8B          | 70B         | 405B        |
|--------------------------|-------------|-------------|-------------|
| **Layers**               | 32          | 80          | 126         |
| **Model Dimension**      | 4,096       | 8192        | 16,384      |
| **FFN Dimension**        | 6,144       | 12,288      | 20,480      |
| **Attention Heads**      | 32          | 64          | 128         |
| **Key/Value Heads**      | 8           | 8           | 8           |
| **Peak Learning Rate**   | 3e-4    | 1.5e-4  | 8e-5    |
| **Activation Function**  | SwiGLU      | SwiGLU      | SwiGLU      |
| **Vocabulary Size**      | 128,000     | 128,000     | 128,000     |
| **Positional Embeddings**| RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) |


#### 后训练

&nbsp;

采用iterative post-training procedure，即预训练后进行多轮对齐，每轮都使用sft、RS（拒绝采样）、直接偏好优化(DPO, Direct Preference Optimization)，能够为**每轮**创建**最高质量的合成数据**，并提高每项能力（capability）的性能。

+ 使用**合成数据生成(synthetic data generation)**来产生绝大多数SFT示例，并**多次迭代**以在所有能力生成越来越高质量的合成数据。
+ 采用了多种数据处理技术来过滤这些合成数据，达到最高质量，并可以**跨能力**来**扩展微调数据量**。

#### 推理

&nbsp;

从bf16量化为fp8

+ 推理：对精度要求较高，对数值范围要求偏低，故一般用FP8-E4M3
+ 训练：Gradients通常对精度损失不太敏感，但需要更高的范围(例如clip变小的时候，模型效果会变差，说明grad的range是比较大的)，故一般用FP8-E5M2

![](../assets/fp8.jpeg)

- 离线量化： 在推理前，利用离线dump数据/训练依赖/权重本身，产生量化scale，保存在模型中，推理中直接使用。history 
- 在线量化： 推理中，利用输入数据在线计算scale，需要在线统计a/b的amax，推理速度慢。就是current scale

### LLama-3.1-Minitron

[英伟达玩转剪枝、蒸馏：把Llama 3.1 8B参数减半，性能同尺寸更强](https://mp.weixin.qq.com/s/zxW9EagxGJX-rS5loNLKXw)

[Compact Language Models via Pruning and Knowledge Distillation](https://www.arxiv.org/pdf/2407.14679)

NVIDIA用TensorRT-LLM优化了Llama 3.1 8B和Llama-3.1-Minitron 4B模型。

### LLama-Nemotron

[英伟达253B开源新王登场，Llama 4三天变陪衬！直逼DeepSeek-R1成推理天花板](https://mp.weixin.qq.com/s/QbUTBKG9vrIVTA-6qII2gg)

[英伟达开源最新大模型Nemotron 70B后，只有OpenAI o1一个对手了](https://mp.weixin.qq.com/s/ebJkBkGAn8QS-_xVK__MMw)

[公开模型一切，优于DeepSeek-R1，英伟达开源Llama-Nemotron家族](https://mp.weixin.qq.com/s/Ofw7l6XPNNinXvFReGI3vw)

[Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/pdf/2505.00949)

[https://github.com/NVIDIA/NeMo-Aligner/tree/llama-nemotron-dev](https://github.com/NVIDIA/NeMo-Aligner/tree/llama-nemotron-dev)

[https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo)

[https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b](https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b)

post training的数据集：

[https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)

开源了LN-Nano（8B）、LN-Super（49B）与LN-Ultra（253B），还有独立变体LN-UltraLong（8B，支持超长上下文）。

#### 构建面向推理优化的模型

&nbsp;

LN-Super和LN-Ultra通过Puzzle框架实现高效推理优化。Puzzle是一个神经网络架构搜索（Neural Architecture Search, NAS）框架，能够在真实部署约束条件下，将大型语言模型转化为面向硬件高效的变体([Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146))

![](../assets/llama-nemotron-puzzle.png)

以Llama 3 Instruct模型为起点（LN-Super(49B)基于Llama3.3-70B-Instruct，LN-Ultra(253B)基于Llama 3.1-405B-Instruct），Puzzle通过逐模块局部蒸馏（block-wise local distillation）方法构建可替代的Transformer模块库。每个模块独立训练且可**并行处理**，允许每个替代模块在**精度与效率之间进行权衡**（accuracy-efficiency tradeoff），模块变体主要包括以下几种类型：

+ **移除注意力机制（Attention removal）**：部分模块**完全省略注意力机制**，从而显著减少计算开销和KV cache内存占用。 
+ **可变FFN维度（Variable FFN dimensions）**：通过调整**FFN的中间维度**，在不同粒度下实现模型压缩（如将隐藏层维度压缩至原始的87%、75%、50%，甚至低至 10%）。

尽管Puzzle同样支持其他结构替换方式（如多GQA中不同的kv head数、线性注意力替代方案、以及不执行操作的替换模块），但实际评估结果表明，在优化LN-Super和LN-Ultra两个模型的总体吞吐量与内存节省方面，最有效的技术仍是移除注意力机制与FFN压缩。

在模块库构建完成后，Puzzle通过逐层选取模块的方式组装完整模型。模块选择过程由**整数混合规划**（Mixed-Integer Programming, MIP）**求解器**控制，在给定的约束条件下（如硬件兼容性、最大推理延迟、总内存预算或指定推理吞吐量）确定效率最优的模块配置。

由于每一层支持多个具有不同精确度–效率权衡方案的模块变体，Puzzle允许用户精确定位至任何位于精度-效率帕累托前沿（Pareto frontier）上的模型配置点。例如，Puzzle 可生成满足特定智能体系统（agentic systems）或部署流程所需约束（如内存不可超出上限或端到端响应时间严格受限）的模型。

**用FFN融合实现纵向压缩（Vertical Compression with FFN Fusion）**：针对LN-Ultra模型，引入了一种额外的压缩技术——FFN融合（FFN Fusion），该方法旨在降低模型的序列深度，并进一步缩短推理延迟。利用Puzzle移除部分注意力层后的结构特性：在这种结构下，模型中经常会出现**连续的FFN模块序列**。FFN Fusion会识别出这类序列，并将其替换为**更少但更宽的FFN层**，这些宽层可并行执行，从而减少序列处理步骤的数量，同时保留模型的表达能力，还能显著提升计算资源的利用率，特别是在多GPU环境中，可以有效降低跨层通信带来的开销。

在NAS之后，进行了continue pretraining(CPT)：

+ LN-Super：使用([Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146)里的Distillation Mix数据集，以知识蒸馏目标函数训练了400亿个Token
+ LN-Ultra：首先使用相同的蒸馏数据集进行了650亿Token的蒸馏训练，随后在Nemotron-H第四阶段预训练数据集上进行了额外880亿Token的持续预训练。

LN-Ultra不仅实现了与基准模型Llama 3.1-405B-Instruct相当的性能，还在多个关键基准测试上取得超越。说明**即使进行激进的架构优化，也可通过短周期的蒸馏与预训练恢复并提升模型性能。**

#### 合成数据

&nbsp;

设计了「detailed thinking on/off」指令机制，在合成数据中**显式标记是否需要展开详细推理过程**，引导模型在训练中学习何时进行逐步思考、展示推理链条，何时直接给出简明答案。指令为「on」时，模型输出完整的中间推理过程并展示解题思路；指令为「off」时，模型仅呈现最终结果。

拿deepseek-r1、llama-4等作为teacher来生产数据，产出的数据总共33,011,757条，里面66%是数学的，30%是代码的（这两部分里都是reasoning off占大头），剩下的是科学和instruction following

#### SFT

&nbsp;

基于合成数据来进行SFT，不同大小的student用的seq_len、epoch数不太一样，也有一些小trick，详见原文

#### 推理能力强化学习

&nbsp;

只蒸馏的话，student很难超越teacher，所以需要强化学习，考虑到资源问题，只对LN-Ultra搞了GRPO

两个reward：

+ 准确率奖励（Accuracy rewards）：每个训练样本提供标准答案（数字、句子或段落），使用Llama-3.3-70B-Instruct模型判定策略模型响应是否与标准答案一致。
+ 格式奖励（Format rewards）：和DeepSeek一致，在detailed thinking on时，需将推理过程置于```<think></think>```标签之中；而在off时，确保不包含思考标签。

一些trick：

+ 样本过滤：由LN-Super为每道题生成8个独立回答，计算通过率，并过滤通过率大于等于0.75的样本，提升总体训练数据难度。
+ 课程学习（curriculum learning）：渐进式批处理策略（progressive batching），使用预计算通过率作为样本难度指标，在固定batch size下，动态计算每个batch的目标难度分布。该分布以高斯函数建模，从早期批次集中在高通过率（简单样本），逐步过渡至后期批次的低通过率（高难度样本）。每个batch中，样本按目标分布随机分配，并根据不同通过率池中剩余样本量进行容量填充。**确保样本难度在batch层面逐步递进，同时batch内部保持随机性。**
+ 基于fp8的生成阶段：修改vLLM，支持bf16转fp8，然后用fp8做gemm

#### 用于偏好优化的强化学习

&nbsp;

+ 指令跟随：在前面的科学推理任务的强化学习之后，对LN-Super和LN-Ultra再进行短期的强化学习，提升指令跟随能力。参考[Instruction-following evaluation for large language models](https://arxiv.org/abs/2311.07911)，生成包含1至10条详细指令的合成提示词，用RLOO（[Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms](https://arxiv.org/abs/2402.14740)）在batch128跑了120个steps，用自定义指令跟随验证器作为奖励函数。发现能同时提升指令跟随任务和推理类任务的效果。
+ RLHF：增强模型的通用协助能力（helpfulness）与多轮聊天能力
    + LN-Super：用iterative online RPO（[ Reward-aware preference optimization: A unified mathematical framework for model alignment](https://arxiv.org/abs/2502.00203)）来最大化HelpSteer2数据集上的Llama-3.1-Nemotron-70B-Reward
    + LN-Ultra：一样的方法，只是换成了GRPO
    + LN-Nano：2轮的offline RPO
        + 第一轮：用混合了reasoning和非reasoning的数据，来提升reasoning控制能力
        + 第二轮：用使用当前正在训练（优化）的策略生成的数据（on-policy的数据），来提升指令遵循能力

### Llama-3.2

[刚刚，Llama 3.2 来了！支持图像推理，还有可在手机上运行的版本](https://mp.weixin.qq.com/s/3JP9UgfXNMlI5jaYHyekYA)

Llama 3.2 

+ 最大的两个模型11B和90B：都支持图像推理，包括文档级的图表理解、图像描述和视觉定位任务，比如直接根据自然语言描述定位图像中的事物。
+ 轻量级的1B和3B：都是纯文本模型，但也具备多语言文本生成和工具调用能力。

[Sebastian Raschka最新博客：从头开始，用Llama 2构建Llama 3.2](https://mp.weixin.qq.com/s/RuTRkJPeEP1hqWevxn9h6Q)

### Llama-3.3

[新版Llama 3 70B反超405B！Meta开卷后训练，谷歌马斯克都来抢镜](https://mp.weixin.qq.com/s/6Iv4VzMlYrkmSsAo_IRGTg)

Llama 3.3能用70B实现405B的效果，主要是“运用了后训练技术的最新进展”，其中包括在线偏好优化（online preference optimization）。

[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)

[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)


### Llama-4

[Meta深夜开源Llama 4！首次采用MoE，惊人千万token上下文，竞技场超越DeepSeek](https://mp.weixin.qq.com/s/Kqk36zfBrZvu6IpCp2lBbw)

[https://ai.meta.com/blog/llama-4-multimodal-intelligence/](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)


| 模型 | 结构 | 参数量 | 特点 |
|------|-----|------|---------|
| Llama 4 Behemoth | 多模态MoE，288B激活，16专家 | 2T | 还没训完，暂未开源|
| Llama 4 Maverick | 多模态MoE，17B激活，128专家 | 400B |推理+编程与deepseek v3相当 |
| Llama 4 Scout | 多模态MoE，17B激活，16专家 | 109B|10M上下文窗口 |

+ 架构：
    + iRoPE架构：使用无位置嵌入的交错注意力层（interleaved attention layers），并通过推理时的温度缩放([Scalable-Softmax Is Superior for Attention](https://arxiv.org/pdf/2501.19399))来增强长上下文泛化能力。
    + 原生多模态设计：通过早期融合将文本和视觉token无缝整合到统一的模型骨干中，能够使用大量未标记的文本、图像和视频数据对模型进行联合预训练。此外，还改进了Llama 4中的视觉编码器，基于MetaCLIP，以更好地使编码器适应LLM。
+ 训练：
    + MetaP：能够可靠地设置模型超参数，例如每层的学习率和初始化规模。选定的超参数在不同批量大小、模型宽度、深度和训练 token 值之间具有良好的迁移性。
    + 数据：200种语言的数据，比llama3多了10倍的多语言token；训练所用的数据混合总量超过30万亿（30T）token，是Llama 3预训练数据混合量的两倍多，涵盖了多样化的文本、图像和视频数据集。
    + fp8训练：兼具质量并确保高FLOPs利用率，使用FP8和32K GPU预训练Llama 4 Behemoth模型时，实现了每GPU有390 TFLOPs。
+ mid-training：
    + 利用专门的数据集扩展长上下文，使得Llama 4 Scout具备业界领先的1000万(10M)输入上下文长度。
+ post-training：
    + 轻量级监督微调（SFT）> 在线强化学习（RL）> 轻量级直接偏好优化（DPO）。发现SFT和DPO可能会过度约束模型，限制在线RL阶段的探索能力:
        + 轻量级监督微调（SFT）：Meta 使用Llama模型作为评判，移除了超过50%的标记为简单的数据，并在剩余较难的数据集上进行SFT。剪枝比例：2T的模型95%，较小模型50%
        + RL：
            + 通过policy model进行pass@k分析，采样难度较高的prompt，并构建难度逐渐增加的训练课程
            + 训练过程中动态过滤掉零advantage的prompt，并构建包含多种能力的混合prompt训练batch
            + 从多种系统指令中采样，确保模型在推理和编码任务中保持指令遵循能力(这个可能讲的是dpo？)
        + 轻量级直接偏好优化（DPO）：处理与模型响应质量相关的边缘情况，有效实现了模型智能与对话能力的良好平衡。
+ 新的框架：
    + 现有的分布式训练框架为了将所有模型加载到内存中而牺牲了计算内存，Meta的新基础设施能够灵活地将不同模型分配到不同的GPU上，并根据计算速度在多个模型之间平衡资源。
    + 对MoE并行化的设计进行了优化
    + 完全异步的在线强化学习训练框架

### Alpaca

[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html?trk=cndc-detail)

Stanford的羊驼（Alpaca）模型，有70亿（7b）参数，**没有使用RLHF**，而是使用**监督学习**的方法，参考[Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)（代码[https://github.com/yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)）

数据集是通过查询基于GPT-3的text-davinci-003模型的结果，得到的52k的指令-输出对（instruction-output pairs）。

因此，Alpaca本质上使用的是一种弱监督（weakly supervised）或以知识蒸馏（knowledge-distillation-flavored）为主的微调，即“**用 LLM 来训练 LLM**”。

![Alpaca](../assets/alpaca.jpeg)

[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

### Vicuna

通过ShareGPT收集的用户对话数据，对llama进行finetune得到的13B模型。效果接近chatgpt的92%，而且训练消耗比较低，大概只要300美元。

### Guanaco

&nbsp;

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)也是对llama进行微调，使用了QLoRA，能够在一台48G的GPU上微调65B的模型。只需要在单台GPU上finetune 24小时就能达到99.3%的chatgpt的效果。

[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)

[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

QLoRA将一个固定的4 bit量化的预训练权重转成low Rank Adapters来更新梯度

### Koala

&nbsp;

[Koala: A dialogue model for academic research](https://bair.berkeley.edu/blog/2023/04/03/koala/)

使用用户与闭源大模型交互的用户输入和模型返回数据进行训练

### Mistral

&nbsp;

[Mistral 7b](https://arxiv.org/pdf/2310.06825.pdf)

7B参数比最好的13B模型（llama-2-13B）要更好，而且比llama-34B在reasoning、数学、代码生成都更好。

+ 使用**grouped-query attention**来做更快的infer
+ 使用**滑动窗口attention**来用更低的infer消耗来高效地处理**任意长度的序列**。

[https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/) 提出了moe的8x7b

[Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)

[Mixtral 8x7B(Mistral MoE) 模型解析](https://mp.weixin.qq.com/s/-5yp0KU6_vkpWmY9wROWbg)

[Mistral开源8X22B大模型，OpenAI更新GPT-4 Turbo视觉，都在欺负谷歌](https://mp.weixin.qq.com/s/hf4uq3yrHxTGhQuzl61Imw)

[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)

[Mistral AI两连发：7B数学推理专用、Mamba2架构代码大模型](https://mp.weixin.qq.com/s/fFB0A0vv_2Deb0rWd4tagw)

mathtral：[https://huggingface.co/mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)

codestral-mamba: [https://huggingface.co/mistralai/mamba-codestral-7B-v0.1](https://huggingface.co/mistralai/mamba-codestral-7B-v0.1)，基于mamba2，可以直接用trt-llm启动

[准狙击Llama 3.1？Mistral AI开源Large 2，123B媲美Llama 405B](https://mp.weixin.qq.com/s/6_d_e6DlQpyONdWxd8EZvA)

Mistral AI 基于此前Codestral 22B和Codestral Mamba的经验，在很大一部分代码上训练了 Mistral Large 2。其表现远远优于上一代的Mistral Large，并且与GPT-4o、Claude 3 Opus和Llama 3 405B等顶尖模型相当。

[https://huggingface.co/mistralai/Mistral-Large-Instruct-2407](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)

[​Mistral首个多模态模型Pixtral 12B来了！还是直接放出24GB磁力链接](https://mp.weixin.qq.com/s/c_fmpkuimmEtY6pD9f_bZQ)

[https://huggingface.co/mistral-community/pixtral-12b-240910](https://huggingface.co/mistral-community/pixtral-12b-240910)

[发力了，Mistral对标ChatGPT全面升级le Chat，还祭出超大杯多模态模型](https://mp.weixin.qq.com/s/sABAaxKNGM-odULaM0-_Zg)

[最强代码模型刷新：Mistral新品上线即登顶，上下文窗口增至256k](https://mp.weixin.qq.com/s/BaZhEs7CV8-kQzdrkcMU2A)

[单个4090就能跑，Mistral开源多模态小模型，开发者：用来构建推理模型足够香](https://mp.weixin.qq.com/s/t173XAigACpw9NWPePhUUQ)

### phi-3

[微软发布Phi-3，性能超Llama-3，可手机端运行](https://mp.weixin.qq.com/s/kb_gfaYkXiW_cR22K2bX9g)

[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)

### phi-4

[微软首个多模态Phi-4问世，56亿参数秒杀GPT-4o！LoRA华人大佬带队](https://mp.weixin.qq.com/s/UQK5b2-ZobnQJwr73p5U8Q)

[https://huggingface.co/microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)

## PaLM系列

### PaLM

&nbsp;

2022年4月提出了第一个PaLM：[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)，直到2023年3月还是private的。是一个540B(5400亿)参数的模型，在包含了780B的tokens的高质量数据集上预训练。使用Pathways的6144块TPU v4进行训练。

### U-PaLM

&nbsp;

[Transcending scaling laws with 0.1% extra compute](https://arxiv.org/pdf/2210.11399.pdf)提出了8B，62B和540B的U-PaLM模型，用**UL2R**来对PaLM进行继续训练，用的是UL2的**mixture-of-denoiser objective**([UL2: Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131.pdf))

### Flan-PaLM

&nbsp;

Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）是instrunction-finetuned版本的U-PaLM。使用了更多的任务、更大的模型，以及CoT数据。使用了473个数据集，146类的task，总共1836个task。

![flan-palm](../assets/flan-palm.png)

[https://huggingface.co/google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)


### PaLM-2

&nbsp;

[PaLM 2 Technical Report](https://arxiv.org/pdf/2305.10403.pdf)是一个更计算高效型的LLM，有更好的多语种和reasoning能力，在很多任务上都比PaLM好，并且在infer上比PaLM要更快更高效。

### Med-PaLM

&nbsp;

Nature的[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)提出了Med-PaLM，在PaLM上用parameter-efficient方法进行instruction prompt tuning，使用少量的典型范例（exemplars）让LLM对齐到新领域。

Med-PaLM2([Towards expert- level medical question answering with large language models](https://arxiv.org/pdf/2305.09617.pdf))通过med-domain的finetuning和ensemble refinement prompting，效果比Med-PaLM要好。


## 其他LLM

### FLAN

&nbsp;

[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)

通过instruction tuning，大模型能很好地提升在没见过任务上的zero-shot能力。对一个137B的预训练模型在60个NLP数据集上使用instruction template进行instruction tuning。

![flan](../assets/flan.png)

一些instruction template：

![flan-instruction-template](../assets/flan-instruction-template.png)

[https://github.com/google-research/flan](https://github.com/google-research/flan)

### Gopher

&nbsp;

[Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf)基于152个多样的任务，对比了从44M到208B（Gopher）的不同transformer，发现Gopher在大多数任务上均达到sota：

| Model         | Layers | Number Heads | Key/Value Size | d_model | Max LR     | Batch Size |
|---------------|--------|--------------|----------------|---------------|------------|------------|
| 44M           | 8      | 16           | 32             | 512               | $$6 \times 10^{-4}$$  | 0.25M      |
| 117M          | 12     | 12           | 64             | 768               | $$6 \times 10^{-4}$$  | 0.25M      |
| 417M          | 12     | 12           | 128            | 1,536             | $$2 \times 10^{-4}$$  | 0.25M      |
| 1.4B          | 24     | 16           | 128            | 2,048             | $$2 \times 10^{-4}$$  | 0.25M      |
| 7.1B          | 32     | 32           | 128            | 4,096             | $$1.2 \times 10^{-4}$$  | 2M        |
| Gopher 280B | 80     | 128          | 128            | 16,384            | $$4 \times 10^{-5}$$  | 3M -> 6M    |

### T0

&nbsp;

[Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207)设计了一个系统，将任意nlp任务映射成一个人类可读的prompt格式。训练了一个encoder-decoder的T0模型，输入文本，输出文本，在混合数据集上进行多任务学习。

### ERNIE 3.0

&nbsp;

[ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/pdf/2107.02137)结合自回归网络和autoencoding网络，让模型能够同时做语言理解和语言生成任务，可以支持zero-shot、few-shot和finetuning。有10B参数，在4TB的文本和图谱数据上训练。

![ernie3.0](../assets/ernie3.0.png)

### 文心X1和4.5

&nbsp;

[提前免费！百度连发两款模型，我们实测：能听歌看电影，还会蛐蛐人](https://mp.weixin.qq.com/s/XKim3L7__c9VXJ2bj4FYhA)

推理模型文心X1：

+ 应用了递进式强化学习训练方法
+ 具备 “长思维链”，采用了思维链和行动链耦合的端到端训练方式，不是只学习思考或者只学习行动，而是把思考和行动结合起来
+ 采用了多元统一的奖励系统

文心4.5：

+ FlashMask 动态注意力掩码：[FlashMask: Efficient and Rich Mask Extension of FlashAttention](https://arxiv.org/abs/2410.01359)，通过列式稀疏掩码表示方法，将传统的二维稠密掩码矩阵转换为一维的行索引区间表示，从而显著降低了存储复杂度，从$$O(N^2)$$降低到$$O(N)$$。极致的加速了大模型训练效率，尤其是长序列场景下的训练效率。
+ 多模态异构专家扩展技术：结合多模态数据处理与混合专家模型（MoE）的创新架构，旨在通过针对不同模态特点构建的**异构专家模块**，解决多模态融合中的梯度不均衡问题，提升模型的多模态融合能力。多模态异构专家扩展技术在多个应用场景中展现了显著优势，例如多模态问答任务，这种技术能够更高效地处理复杂的多模态数据。
+ 时空维度表征压缩技术：把图片和视频里的关键信息进行浓缩的方法。在不丢失重要细节的情况下，在时空维度对图片和视频的语义表征进行高效压缩，让这些数据变得更小、更高效。这样一来，训练多模态模型（也就是同时处理文字、图片、视频等多种数据的模型）时就能更快、更省资源。
+ 基于知识点的大规模数据构建技术：知识分级采样优先把重要的、核心的知识挑出来，接着把挑出来的知识点进行压缩，去掉多余的部分，把相关的知识融合在一起。对于那些比较少见、不太好找的知识点，专门合成一些相关内容，确保这些重要的小众知识也不会被遗漏。这样一来，模型幻觉大大降低。
+ 基于自反馈的Post-training技术：通过多种评估方式不断优化模型的后训练方法。简单来说，它让模型在训练过程中不断反思自己的表现，根据不同的评价标准调整自己，从而让模型变得更稳定、更可靠。这种技术还能显著提升预训练模型理解人类意图的能力，让模型的回答更符合人类的期望。

### RETRO

&nbsp;

[参数量仅为4%，性能媲美GPT-3：开发者图解DeepMind的RETRO](https://baijiahao.baidu.com/s?id=1721015293574115195&wfr=spider&for=pc)

[http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/)

[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)

Retrieval Enhanced Transformer(Retro)基于与preceding(前述) tokens的相似度，从大的语料库里检索出文档段（document chunks）作为条件，增强了自回归模型的能力。通过少25%的参数，在Pile数据集上达到了与gpt3和Jurassic-1相当的效果

![retro](../assets/retro.png)

+ 左图：
  + 输入一个长度为12的序列，每4个token一个chunk，切成3个chunk
  + 每个chunk通过freezed的bert去语料库中拿2个相似neighbors出来，每个neighbor过bert得到向量
  + 邻居作为k和v，原来的3个chunk作为k，做attention(CCA, chunked cross attention)
+ 右图：CCA的结构
  + 保证了因果性，即chunk1的邻居只对chunk1的last token以及chunk2的所有token有影响


### GLaM

&nbsp;

[Glam: Efficient scaling of language models with mixture-of-experts](https://arxiv.org/pdf/2112.06905)提出了Generalist Language Model(GLaM)，用稀疏激活的MOE架构，同时能scale模型容量，也能相比dense更可观地降低训练成本。最大的模型有1.2T参数，是gpt3的7倍。但只需要GPT3的1/3的能量来训练，而且在infer时也只有一半的flops，且在29个NLP任务上都有更好的0/1/few shot效果。

![Glam](../assets/glam.png)

如图，2 in 64的结构，每个token只会取64个experts里的top 2相关的expert，对这两个的输出加权平均输入给后面的层


### LaMDA

&nbsp;

[Lamda: Language models for dialog applications](https://arxiv.org/pdf/2201.08239)，有137B的参数，在1.5T的公开对话数据和互联网文本上pretrain，用标注数据微调，以及让模型能够咨询（consult）外部知识源能够让模型在安全性和factual grounding上有很好的改进。

### OPT

&nbsp;

[OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068)提出了从125M到175B的预训练decoder-only模型

### Chinchilla

&nbsp;

### Galactica

&nbsp;

### CodeGen

&nbsp;

### AlexaTM

&nbsp;

### Sparrow

&nbsp;

### MoD

&nbsp;

### BLOOM

&nbsp;

### GLM

&nbsp;

ACL22 [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)

iclr23 [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)


### GLM-4

[GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级](https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg)

[https://github.com/THUDM/GLM-4](https://github.com/THUDM/GLM-4)

### Pythia

&nbsp;

### Orca

&nbsp;

### StarCoder

&nbsp;

### KOSMOS

&nbsp;

### Claude

&nbsp;

[全球最强大模型一夜易主，GPT-4时代终结！Claude 3提前狙击GPT-5，3秒读懂万字论文理解力接近人类](https://mp.weixin.qq.com/s/WqQWS-hiQ1i1Ve6IPH3djw)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)

+ 对齐：使用Constitutional AI([Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf))， explicitly specifying rules and principles based on sources like the UN Declaration of Human Rights.
+ 基于Collective Constitutional AI([Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input))，added an additional principle to Claude’s constitution to encourage respect for disability rights
+ 公开了部分RHLF数据：[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)，对应的论文是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[力压GPT-4o！新王Claude 3.5 Sonnet来了，直接免费可用](https://mp.weixin.qq.com/s/HnQ7D4iDVgWteZZdTJoadg)

[大模型代肝，自动刷《崩铁》升级材料，Claude操纵计算机还能这么用！](https://mp.weixin.qq.com/s/F8CX6_VWebWLy26NeRNQsw)

[The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/pdf/2411.10323)

[https://github.com/showlab/computer_use_ootb](https://github.com/showlab/computer_use_ootb)

### grok

&nbsp;

[马斯克开源Grok-1：3140亿参数迄今最大，权重架构全开放，磁力下载](https://mp.weixin.qq.com/s/hvt5zwoazDx26KOaKuTs_w)

[https://github.com/xai-org/grok-1](https://github.com/xai-org/grok-1)

[马斯克发布Grok 1.5！编码和数学能力大幅提升](https://mp.weixin.qq.com/s/QwM1uk61o1DMpe5EPq9giQ)

+ 上下文窗口提升16倍，达到128k
+ 不使用通用的Python语言+Pytorch框架，采用分布式训练架构，使用Rust、JAX+Kubernetes构建。
+ 提出了自定义训练协调器，可自动检测到有问题的节点，然后剔除。
+ 优化了checkpointing、数据加载和训练重启等流程，最大限度地减少故障停机时间。

[马斯克的首款多模态大模型来了，GPT-4V又被超越了一次](https://mp.weixin.qq.com/s/2GDjZS6ctayAF8e8eFb3CQ)

Grok-1.5V: [https://x.ai/blog/grok-1.5v](https://x.ai/blog/grok-1.5v)

grok-2：[Grok-2来了，能生图识图、性能比肩GPT-4o，马斯克：发展猛如火箭](https://mp.weixin.qq.com/s/nBaY2srcMSzvEoecOyh1Cg)

[目前对Grok 3分析最为透彻的一篇文章](https://mp.weixin.qq.com/s/h5cFZnU4T7K7YscMYc61pQ)


### Octopus

[超越GPT-4，斯坦福团队手机可跑的大模型火了，一夜下载量超2k](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)

[Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744.pdf)

[https://huggingface.co/NexaAIDev/Octopus-v2](https://huggingface.co/NexaAIDev/Octopus-v2)

[参数量不到10亿的OctopusV3，如何媲美GPT-4V和GPT-4？](https://mp.weixin.qq.com/s/mUpX-nvo221WVii-gnjUmQ)

[Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/pdf/2404.11459.pdf)

[Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)

### Cohere Command R+

[开源模型打败GPT-4！LLM竞技场最新战报，Cohere Command R+上线](https://mp.weixin.qq.com/s/uUeGQFWel5NLfFRFFF8w9g)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit](https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit)

1040亿的参数量，相比于Grok-1（3140亿）还差了一些，但Command R+并非Grok那种MoE架构，所以这1040亿参数是实打实的完全用于推理，而Grok-1的活跃参数为860亿。相比commend R：

+ 高级检索增强生成（RAG）与引用以减少幻觉
+ 10种主要语言的多语言覆盖，支持全球业务运营
+ 工具的运用以自动化复杂的业务流程

[明确了：文本数据中加点代码，训练出的大模型更强、更通用](https://mp.weixin.qq.com/s/3Ks72bIGZrNNUukqaHLSQg)

[To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)


### CT-LLM：以中文为中心的LLM

[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/pdf/2404.04167.pdf)

当前，绝大多数LLM基本上都是以英文语料库训练得到的，然后经过SFT来匹配不同的语种。本文作者考虑以中文为基础的预训练模型是否可以激活对其它语言的能力。

作者从头开始训练中文大模型，在训练过程中「主要纳入中文文本数据」，最终作者得到了一个2B规模的中文Tiny LLM（CT-LLM）。结果表明，该模型在中文任务上表现出色，且通过SFT也能很好的支持英文。

### OpenELM

[苹果卷开源大模型，公开代码、权重、数据集、训练全过程，OpenELM亮相](https://mp.weixin.qq.com/s/uwDoKG2Q9-w37ogewBJTrQ)

[OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619.pdf)

[https://github.com/apple/corenet](https://github.com/apple/corenet)

[超强Siri即将到来！苹果10篇重磅AI研究全总结，iOS 18关键一瞥](https://mp.weixin.qq.com/s/zQu0BtDYG-XAAX87i4MGIA)

[苹果智能背后模型公布：3B模型优于Gemma-7B，服务器模型媲美GPT-3.5-Turbo](https://mp.weixin.qq.com/s/xkZrD_8zsmVfYYohH7ucUw)

### Arctic

[仅需Llama3 1/17的训练成本，Snowflake开源128x3B MoE模型](https://mp.weixin.qq.com/s/0mqx1xkyhOXDGpbu42d_5g)

[全球最大开源模型再刷爆纪录！4800亿参数MoE击败Llama 3、Mixtral](https://mp.weixin.qq.com/s/Wbs30QvvtWtYB6mp47Z8NA)

[https://huggingface.co/Snowflake/snowflake-arctic-instruct](https://huggingface.co/Snowflake/snowflake-arctic-instruct)

### FALCON 2

[时隔一年Falcon回归！110亿参数5.5万亿token，性能超越Llama 3](https://mp.weixin.qq.com/s/CZNVl_GmYm_aPidMJrRHUg)

### MiniCPM

#### MOEfication

[大模型隐藏玩家上桌：DeepSeek 向左，面壁向右](https://mp.weixin.qq.com/s/DrtUv-7fMh8NqdaSHHJ8uQ)

[MoEfication：Transformer Feed-forward layers are Mixtures of Experts](https://arxiv.org/abs/2110.01786)

#### MiniCPM-Llama3-V

[登顶Top2！MiniCPM-V 8B新版本：GPT-4V水准小钢炮，8G显存，4070轻松推理！](https://mp.weixin.qq.com/s/TQVHJlZDExD3nMPRsqa_5w)

[可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术](https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ)

[RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)

[https://github.com/RLHF-V/RLAIF-V](https://github.com/RLHF-V/RLAIF-V)

#### MiniCPM 3.0

[小模型杀疯了！仅4B参数性能超GPT-3.5！无限长文本性能超Kimi](https://mp.weixin.qq.com/s/qJM9OTDHS3pJB9ozFuRP1g)

[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/pdf/2404.06395)


### DCFormer

[彩云科技DCFormer大模型发布，效率是Transformer的两倍！](https://zhuanlan.zhihu.com/p/699582521)

[Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/pdf/2405.08553)

[https://github.com/Caiyun-AI/DCFormer](https://github.com/Caiyun-AI/DCFormer)

### hunyuan

[腾讯混元又来开源，一出手就是最大MoE大模型](https://mp.weixin.qq.com/s/GVFhlelNgrIDnzIIyW3yXg)

[Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265)

[https://github.com/Tencent/Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large)

[https://llm.hunyuan.tencent.com/](https://llm.hunyuan.tencent.com/)

### hunyuan turbo-s

[https://github.com/Tencent/llm.hunyuan.turbo-s](https://github.com/Tencent/llm.hunyuan.turbo-s)


### 阶跃星辰

[在「最难LLM评测榜单」上，阶跃万亿参数模型拿下中国第一](https://mp.weixin.qq.com/s/rGv2r_-owZ3jIx3IPk6XfQ)

[六小虎「阶跃星辰」疯狂了！连发6款大模型，多模态霸榜第一](https://mp.weixin.qq.com/s/qcFGKQlX6OnMoiMPJCze_w)

### Tülu 3(基于llama3.1)

(toread)

[这才是真开源模型！公开「后训练」一切，性能超越Llama 3.1 Instruct](https://mp.weixin.qq.com/s/sTtBkVkqy0CQtpzcR6SN-A)

[TÜLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/pdf/2411.15124)

[https://github.com/allenai/open-instruct](https://github.com/allenai/open-instruct)

提出了具有可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)，可能和openai的rft有关系

[https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)

### baichuan

[百川新模型超GPT-4o近20%，首创自约束训练方案突破瓶颈，主打「领域增强」](https://mp.weixin.qq.com/s/7oupuln3BQHPa7zpfGdMXQ)

Baichuan4-Finance

评测benchmark：[https://github.com/FLAME-ruc/FLAME/tree/main](https://github.com/FLAME-ruc/FLAME/tree/main)

### minimax-01

[MiniMax开源4M超长上下文新模型！性能比肩DeepSeek-v3、GPT-4o](https://mp.weixin.qq.com/s/pj6XZbyZu_mx2KRwIF1Q9g)

[MiniMax押注线性注意力，让百万级长文本只用1/2700算力｜对话MiniMax-01架构负责人钟怡然](https://mp.weixin.qq.com/s/NigAnui9fXbfresW8KIX-Q)

[MiniMax-01: Scaling Foundation Models with Lightning Attention](https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf)

m层lightning attention后再接1层softmax attention，参数456B，每次推理激活45.9B。



## post training的一些总结

[工业界主流大语言模型后训练(Post-Training)技术总结](https://mp.weixin.qq.com/s/sm2HEKJaeIUy9yXZCj0L-g)

[长文 | 大模型Post-Training总结](https://mp.weixin.qq.com/s/mL3Zc85zoHCjNOaTX2icTA)


## 小模型

[权重、代码、数据集全开源，性能超越Mistral-7B，苹果小模型来了](https://mp.weixin.qq.com/s/M58y8F5WeOH6i5nNKhtHbw)

[DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/pdf/2406.11794)

[https://huggingface.co/apple/DCLM-7B](https://huggingface.co/apple/DCLM-7B)

[小模型卷起来了：Mistral联合英伟达开源12B小模型，128k上下文](https://mp.weixin.qq.com/s/7oSxdFyqJ7MUpbfuNB_n5Q)

Mistral NeMo 使用基于 Tiktoken 的新分词器 Tekken，该分词器经过 100 多种语言的训练，能比以前 Mistral 模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。在压缩源代码、中文、意大利文、法文、德文、西班牙文和俄文时，它的效率要高出约 30%。在压缩韩文和阿拉伯文时，它的效率是原来的 2 倍和 3 倍。事实证明，与 Llama 3 分词器相比，Tekken 在压缩所有语言中约 85% 的文本方面更胜一筹。

Mistral NeMO 经历了高级微调和对齐阶段。与 Mistral 7B 相比，它在遵循精确指令、推理、处理多轮对话和生成代码方面的能力大大提升。

+ 基础模型：[https://huggingface.co/mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)
+ 指令微调模型：[https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)

[大模型已过时，小模型SLM才是未来？苹果正在研究这个](https://mp.weixin.qq.com/s/vAa1Tmse-Sn_nhaceWC1lg)

[Computational Bottlenecks of Training Small-scale Large Language Models](https://arxiv.org/pdf/2410.19456)

[研究大模型门槛太高？不妨看看小模型SLM，知识点都在这](https://mp.weixin.qq.com/s/sg7HveGDjMEj-ZcHS3YizA)

[A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/abs/2411.03350)

[https://github.com/FairyFali/SLMs-Survey](https://github.com/FairyFali/SLMs-Survey)



### BitNet

[微软开源爆火1.58bit大模型推理框架！千亿参数模型量化后单CPU可跑，速度每秒5-7个token](https://mp.weixin.qq.com/s/gerCRxj4eULOut9PtMlNog)

[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)

[https://github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)

### Zamba 2

[7B新王登基！Zamba 2完胜同级模型，推理效率比Llama 3提升20%，内存用量更少](https://mp.weixin.qq.com/s/6_dQod3hS1IJ_xU09x_jrg)

用了mamba 2

[Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/pdf/2405.16712)

+ hf版本：[https://huggingface.co/Zyphra/Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)
+ 纯pytorch版本：[https://github.com/Zyphra/Zamba2](https://github.com/Zyphra/Zamba2)



## 小结

[开源模型进展盘点：最新Mixtral、Llama 3、Phi-3、OpenELM到底有多好？](https://mp.weixin.qq.com/s/bgdDYkGHbPZMMSJPIutFSQ)



# 新的模型结构

## 长上下文的问题

[长序列（Long Context）大模型笔记](https://mp.weixin.qq.com/s/nUX3MbKbyxw6b6mhgoL-Mw?poc_token=HK37E2ejmhfX-g6qnVG3pIxFJXZEVk-vPpwriYAj)

[LLM长上下文的问题](https://mp.weixin.qq.com/s/5e5HtxJrxNuhsVxrKsL2gA)

以中文为例，大部分模型每个token对应的中文字数都>1.5个字，所以200k的token就对应30w字的上下文

对长文本的几个要求：

+ 在文本比较长的时候，还能保证通顺，**ppl要足够低**
+ 能attention到前面提过的细节，**不能自我矛盾**

注意：如果训练时是2k长度的语料，而推理设定8k窗口，那么PPL会急剧上升，因为

+ RoPE不能很好地处理没有训练过的**位置编码**
+ 推理时**注意力机制**所处理的token数量远超训练时的数量，导致注意力机制的崩坏

### 两阶段训练方式

+ 直接输入连续长文本（如书籍）
+ 多个中等文本拼接，再通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。

如果简单地增加长度，例如从4k变到32k，长度增加8倍，为了加速计算需要缓存中间结果（如QK的结果是$$s^2$$的空间复杂度），所以显存会扩大$$8^2=64$$倍。一般的做法是2阶段：

+ 第一阶段：用2k或者4k训练一个基础模型，让模型学好**文本内容**和**短位置关系**
+ 第二阶段：用**比第一阶段小的数据量优化**模型在**长上下文的效果**，具体做法见下节

### 针对位置编码的插值类方法

#### 线性插值

&nbsp;

[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf)

#### NTK-Aware Interpolation

&nbsp;

#### NTK-by-parts

&nbsp;

#### Dynamically NTK Scaled RoPE

&nbsp;

### 针对attention score的缩放方法

#### YaRN

&nbsp;

#### logn

&nbsp;

### lossless long context

[专访月之暗面杨植麟：lossless long context is everything](https://mp.weixin.qq.com/s/UMY0qZsCGh87KnW4wjfvoA)

## 外推问题

[语言模型窗口外推技术综述](https://mp.weixin.qq.com/s/5CqrlUZHrciMAFmeV2oYdw)

### longRoPE

[LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens](https://mp.weixin.qq.com/s/4ryyv59ofNOD--RCSdqktQ)

### CoPE

[解决Transformer根本缺陷，CoPE论文爆火：所有大模型都能获得巨大改进](https://mp.weixin.qq.com/s/JxB6JU6MxO3709mkg7penw)

[Contextual Position Encoding: Learning to Count What’s Important](https://arxiv.org/pdf/2405.18719)

### DAPE

[NeurIPS 2024 | Transformer长度外推，全新位置编码DAPE大幅提升模型性能](https://mp.weixin.qq.com/s/-7YsAMYYO92nItRJbqSrpw)

[DAPE: Data-Adaptive Positional Encoding for Length Extrapolation](https://arxiv.org/abs/2405.14722)

[https://github.com/chuanyang-Zheng/DAPE](https://github.com/chuanyang-Zheng/DAPE)


## retrieval head

[Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/pdf/2404.15574)


## Hawk & Griffin

[RNN效率媲美Transformer，谷歌新架构两连发：同等规模强于Mamba](https://mp.weixin.qq.com/s/RtAZiEzjRWgqQw3yu3lvcg)

[Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/pdf/2402.19427)

### 简介

global attention在infer阶段的效率和序列长度成二次关系，而且序列长度与KV cache呈线性增长关系。[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)提出的multi-query attention(MQA)可以通过一个**constant factor**来**减小cache size**，部分缓解这个问题，但cache还是与序列长度线性相关。

循环模型能够将整个序列压缩到一个**fixed-size的hidden state**，并通过迭代进行更新。但要想取代transformer，rnn不仅需要在效果上可比，还要相似的硬件效率，相关工作如下：

+ [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396)
+ [Long range language modeling via gated state spaces](https://arxiv.org/pdf/2206.13947)提出了GSS block
+ [Simplified state space layers for sequence modeling](https://arxiv.org/pdf/2208.04933)
+ [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ [Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)
+ [Hyena hierarchy: Towards larger convolutional language models](https://arxiv.org/pdf/2302.10866)
+ [Mamba: Linear-time sequence modeling with selective state spaces](https://arxiv.org/pdf/2312.00752)

本文提出了新的**RG-LRU层**，一种新的gated linear recurrent layer，并基于此提出了将MLP和RG-LRU结合的Hawk，还有将MLP和RG-LRU与local attention混合的Griffin。

![griffin-hawk](../assets/griffin-hawk.png)

+ 对于最多超过7B的模型，Hawk和Griffin发现了**held-out loss**和**训练FLOPS**间的**power law scaling**（上图左边）
+ Hawk-3B比Mamba-3B在下游任务上要好，而训练的token数只有mamba的一半；Griffin-7B和14B与Llama-2效果相当，而训练数据只有其1/7
+ 训练在TPU-v3上完成，用JAX中的Pallas实现了RG-LRU(Real-Gated Linear Recurrent Unit)层的内核([https://jax.readthedocs.io/en/latest/pallas/index.html](https://jax.readthedocs.io/en/latest/pallas/index.html))，减小内存transfer
+ infer阶段比MQA的**吞吐高很多**（上图右边），而且对长序列有**更低的latency**
+ 训练时在长序列上比transformer更好，而且能够高效地学习**复制和检索**的任务。但如果没有finetune，直接对比pretrain的效果，transformer会更好

### 网络结构

包括三大类组件：

+ residual block: 
+ MLP block：
+ temporal-mixing block


![griffin-arch](../assets/griffin-arch.png)

#### Residual Block

&nbsp;

受pre-norm transformer的启发，用的RMSNorm

#### MLP block

&nbsp;

参考[Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083)（类似star两个W直接element-wise product），采用了gated MLP，即如下两个linear的结果（输入维度$$D$$，输出维度都是$$MD,M=3$$）进行element-wise product(类似GeGLU（[Glu variants improve transformer]((https://arxiv.org/pdf/2002.05202.pdf))）)，再过一个linear

+ 直接过一个linear，但没有激活
+ linear后加一个GeLU激活（实际用的```nn.functional.gelu(input, approximate="tanh")```）

看recurrentGemma-2b的[config](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json)，发现总共有26个block，一个block依次包括：

+ rg-lru或者MQA，每个block里的选择方式是(rg,rg,att,rg,rg,att,rg,rg,att,...)
+ 再过一个gated MLP

```python
  "_block_types": [
    "recurrent",
    "recurrent",
    "attention"
  ],
  "num_hidden_layers": 26,
  "attention_window_size": 2048,

```

#### Temporal-mixing block

&nbsp;

##### global Multi-Query Attention(MQA)

&nbsp;

[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)中为了加速推理，采用了MQA的方法，本文固定了head的维度为$$D_{head}=128$$，head的个数$$H$$也固定，且$$HD_{head}=D$$，所以model的维度$$D$$需要是128的倍数。没用绝对位置编码，用了RoPE。

##### local(sliding-window) MQA

&nbsp;

[Longformer: The long-document transformer](https://arxiv.org/pdf/2004.05150)提出了可以用local attention，即滑动窗口attention。让每个位置只和前面的固定个tokens去算attentioin，可以

+ 降低计算的FLOPS
+ 让KV cache的size的上界变成了**window的size**，从而不是序列长度的二次关系

##### RG-LRU

&nbsp;

类似GSS block，也类似Mamba，输入$$D$$，分别过一个linear得到两个分支，均是$$D_{RNN}$$：

+ 分支1：过GeLU激活，同上，实际用的```nn.functional.gelu(input, approximate="tanh")```
+ 分支2：
  + 参考[Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)的H3模型里的Shift-SSM，过一个Conv1D，其temporal filter dim是4
  + Conv1D的参数只有$$4D_{RNN}$$，因此再过一个RG—LRU模块

两个分支的输出element-wise product一下，再过一个linear得到$$d$$。

conv1d参考torch官方[doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)

+ 输入：$$\left(N, C_{i n}, L_{i n}\right)$$
+ 输出：$$\left(N, C_{\text {out }}, L_{\text {out }}\right)$$

其中，

XXX
L_{\text {out }}=\left\lfloor\frac{L_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel\_size }-1)-1}{\text { stride }}+1\right\rfloor
XXX

当```groups==in_channels and out_channels=K*in_channels```，也叫depthwise convolution，K是depthwise乘子。即**每个输入通道都有自己的卷积核，并且只作用于该通道**。

```python
        ## conv1d_width = 4, 
        ## 对图像卷积来讲，padding就是让输出的shape不小于输入shape
        ## 输入是hidden_size=2560，而lru_width也是2560
        self.conv_1d = nn.Conv1d(
                    config.lru_width, # in_channels
                    config.lru_width, # out_channels
                    kernel_size=config.conv1d_width,
                    groups=config.lru_width,
                    padding=config.conv1d_width - 1,
                )
        
        ## 输入[bs, seq_len, hidden_size]
        x_branch = self.linear_x(input_states)
        ## 变成[bs, hidden_size, seq_len]
        x_branch = x_branch.transpose(1, 2)
        if use_cache:
            if cache_position.shape[0] != 1:  # prefill
                self.conv1d_state = nn.functional.pad(x_branch, 
                    (self.conv1d_width - x_branch.shape[-1] - 1, 0))
                x_branch = self.conv_1d(x_branch)[..., :seq_len]
            else:  # decoding
                conv_state = torch.cat((self.conv1d_state, x_branch), -1)
                x_branch = torch.sum(conv_state * self.conv_1d.weight[:, 0, :], dim=-1) 
                    + self.conv_1d.bias
                x_branch = x_branch.unsqueeze(-1)
                self.conv1d_state = conv_state[:, :, 1:]
        else:
            # 前面维度不变，最后一维截断到seq_len
            x_branch = self.conv_1d(x_branch)[..., :seq_len]
```


参考[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)提出的LRU(Linear Recurrent Unit)，并参考传统LSTM和GRU引入了gate：

XXX
\begin{aligned}
r_t & =\sigma\left(W_a x_t+b_a\right), \quad \text { recurrence gate } \\
i_t & =\sigma\left(W_x x_t+b_x\right), \quad \text { input gate } \\
a_t & =a^{c r_t}, \\
h_t & =a_t \odot h_{t-1}+\sqrt{1-a_t^2} \odot\left(i_t \odot x_t\right) .
\end{aligned}
XXX

其中，recurrent weight $$a=\sigma(\Lambda)$$是一个对角矩阵，$$\Lambda$$是一个可学习的参数。$$c$$是一个常数8，为了**计算稳定**，在log-space计算$$a^{c r_t}$$，即先算出$$\log a_t$$，再取exp。

XXX
\log a_t=\log a^{c r_t}=\log \sigma(\Lambda)^{c r_t}=-\operatorname{csoftplus}(\Lambda) \odot r_t 
XXX

(这个公式可能有点问题，感觉应该是$$-\operatorname{csoftplus}(-\Lambda) \odot r_t$$，不过$$\Lambda$$是一个可学习的nn.Parameter，其实这个错误无所谓吧)

```python
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.block_width = config.lru_width // self.num_attention_heads

        self.recurrent_param = nn.Parameter(torch.empty([config.lru_width]))
        self.input_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.input_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))

        self.recurrent_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.recurrent_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))
        self.recurrent_states = None

    def forward(xxx):
        ## reshape成适合多头的情况
        reshape_act = activations.reshape(batch_size * seq_len, self.num_attention_heads, self.block_width)
        ## (num_attention_heads, batch_size * seq_len, block_width)
        reshape_act = reshape_act.permute(1, 0, 2)

        ## 批量矩阵乘法（baddbmm），在reshape_act和self.input_gate_weight之间进行，
        ## 并加上偏置self.input_gate_bias。这一步计算输入门的原始值。
        res = torch.baddbmm(self.input_gate_bias[:, None, :], reshape_act, self.input_gate_weight)
        input_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        ## 类似input_gate
        res = torch.baddbmm(self.recurrent_gate_bias[:, None, :], reshape_act, self.recurrent_gate_weight)
        recurrent_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        # Compute the parameter `A` of the recurrence.
        # 上面的公式
        log_recurrent_gate = -8.0 * recurrent_gate * nn.functional.softplus(self.recurrent_param)
        recurrent_gate = torch.exp(log_recurrent_gate)
```

还有如下几个特点：

+ $$W_a$$和$$W_x$$用LeCun init初始化
+ 初始化$$\Lambda$$，使得在训练开始时，$$a^c$$均匀分布在0.9和0.999之间，类似[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ 不像大部分SSM（例如[Hippo: Recurrent memory with optimal polynomial projections](https://arxiv.org/pdf/2008.07669)）基于orthogonal polynomials（正交多项式）理论进行初始化。
+ 也不像[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)在底层的连续系统上进行离散化定义
+ 不像原始的LRU用了复数，虽然[ On the universality of linear recurrences followed by nonlinear projections](https://arxiv.org/pdf/2307.11888v1)说复数的表达能力更强，但在实践中对语言模型并没有什么用（[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)）

##### gate特点

&nbsp;

先复习一下LSTM：

![lstm-peephole-lstm](../assets/lstm-peephole-lstm.png)

GRU：

+ 重置门（reset gate）：如果重置门关闭，会**忽略掉历史信息**，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的**输入门和遗忘门合并**，用于控制**历史信息对当前时刻隐层输出的影响**。如果更新门接近1，会把历史信息传递下去。

![gru](../assets/gru.png)

+ 两个gate只和$$x_t$$有关，**和$$h_{t-1}$$无关**。
+ input gate $$i_t$$和LSTM类似，直接对输入$$x_t$$进行filter或者scale down。
+ recurrent gate $$r_t$$和之前的gate机制不同：
    + mamba里的selection机制和GRU的**update gate**类似，在**之前的状态**和**当前输入$$x_t$$**之间进行插值（interpolate），功能类似LSTM的**forget gate**，能够**reset状态，并遗忘之前的信息**
    + 本文的recurrent gate则类似于在LRU的更新和之前的隐藏状态之间进行插值，能够**有效地丢弃输入**，并且**保持之前历史里的所有信息**。使得模型在处理不相关或重复输入（**uniformative inputs**）时，更能达到**超指数的记忆能力**，以更有效地保留有用信息(因为这个gate**和$$h_{t-1}$$无关**)。


## feedback attention memory

[TransformerFAM: Feedback attention is working memory](https://arxiv.org/pdf/2404.09173.pdf)

## infini-attention

[Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143.pdf)

[【重磅】谷歌重塑Transformer：无限记忆力，无限长输入，LLM基础研究重大突破](https://mp.weixin.qq.com/s/bV2b9uJ4GFQPhhggHT3VIA)

将**压缩记忆**整合进标准的点积注意力机制，并在单个Transformer块内同时实现了**掩码局部注意力**和**长期线性注意力机制**

![infini-attention](../assets/infini-attention.png)

与transformer-xl对比：

![infini-attention-vs-transformer-xl](../assets/infini-attention-vs-transformer-xl.png)

[https://github.com/mustafaaljadery/gemma-2B-10M](https://github.com/mustafaaljadery/gemma-2B-10M)

[https://github.com/dingo-actual/infini-transformer](https://github.com/dingo-actual/infini-transformer)

## MEGA

[Mega: Moving average equipped gated attention](https://arxiv.org/pdf/2209.10655.pdf)

[https://github.com/facebookresearch/mega](https://github.com/facebookresearch/mega)

### 标准的self-attention公式

&nbsp;

对于序列长度=$$n$$的输入$$\boldsymbol{X}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\right\} \in \mathbb{R}^{n \times d}$$，输出$$\boldsymbol{Y}=\left\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_n\right\} \in \mathbb{R}^{n \times d}$$

XXX
\boldsymbol{Y}=\operatorname{Attn}(\boldsymbol{X})=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \boldsymbol{V}
XXX

XXX
\boldsymbol{Q}=\boldsymbol{X} W_q+b_q,\boldsymbol{K}=\boldsymbol{X} W_k+b_k,\boldsymbol{V}=\boldsymbol{X} W_v+b_v
XXX

其中，$$\text { Attn : } \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$$，$$W_q, W_k, W_v \in \mathbb{R}^{d \times d}$$，$$b_q, b_k, b_v \in \mathbb{R}^d$$，而且有两种定义方式：

+ $$f(\cdot)=f_{\text {softmax }}(\cdot)$$，同时$$\tau(\boldsymbol{X})=\sqrt{d}$$，这是经典操作[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
+ $$f(\cdot)=f_{\mathrm{relu}^2}(\cdot)$$即$$relu(x)^2$$，同时$$\tau(\boldsymbol{X})=n$$，这是[Primer: Searching for efficient transformers for language modeling](https://arxiv.org/pdf/2109.08668.pdf)和[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)提出的

![gau](../assets/gau.png)

对于$$h$$个attention heads，计算$$\boldsymbol{A}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \in \mathbb{R}^{n \times n}$$需要的时间和空间复杂度都是$$O(hn^2)$$

### EMA

&nbsp;

对于输出的序列$$\boldsymbol{Y}$$采用Exponential Moving Average(指数滑动平均)如下

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha}) \odot \mathbf{y}_{t-1}
XXX

其中，$$\boldsymbol{\alpha} \in(0,1)^d$$表示权重衰减，$$\odot$$是element-wise product

$$\boldsymbol{\alpha}$$越大，$$1-\boldsymbol{\alpha}$$越小，对历史的衰减也越快：

![ema](../assets/ema.png)

EMA的计算看成$$n$$个**独立的卷积**，可以通过**FFT(快速傅立叶变换)**来加速计算。具体。。。再看看

**Damped EMA**：由于输入的x是d维向量，可以引入一个因子$$\boldsymbol{\delta} \in(0,1)^d$$让EMA更鲁棒：

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha} \odot \boldsymbol{\delta}) \odot \mathbf{y}_{t-1}
XXX

### MEGA

&nbsp;

EMA可以看成是一种**与位置相关的归纳偏置（inductive bias）**，即假设当前位置与之前位置满足滑动平均的关系，而attention矩阵的计算其实并没有考虑位置信息，所以可以把二者结合一下。

![mega](../assets/mega.png)

其中的multi-dimensional damped EMA大致流程如下：

+ **先变成h维**：先把$$\boldsymbol{X} \in \mathbb{R}^{n \times d}$$通过矩阵$$\beta$$映射成$$\boldsymbol{U}\in \mathbb{R}^{n \times h}$$
+ **计算EMA**：然后通过EMA得到$$\boldsymbol{h}\in \mathbb{R}^{n \times h}$$（具体$$\mathbf{h}_t^{(j)}=\boldsymbol{\alpha}_j \odot \mathbf{u}_t^{(j)}+\left(1-\boldsymbol{\alpha}_j \odot \boldsymbol{\delta}_j\right) \odot \mathbf{h}_{t-1}^{(j)}$$）
+ **再变回d维**：再通过一个矩阵$$\eta$$变回$$\boldsymbol{Y}\in \mathbb{R}^{n \times h}$$

然后看整个流程：

+ 先计算EMA

XXX
\begin{aligned}
\boldsymbol{X}^{\prime} & =\operatorname{EMA}(\boldsymbol{X}) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Z} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_z+b_z\right) & & \in \mathbb{R}^{n \times z}
\end{aligned}
XXX

+ 再基于EMA的结果计算QK，基于原始的X计算V：

XXX
\begin{array}{ll}
\boldsymbol{Q}=\boldsymbol{\kappa}_q \odot \boldsymbol{Z}+\boldsymbol{\mu}_q & \in \mathbb{R}^{n \times z} \\
\boldsymbol{K}=\boldsymbol{\kappa}_k \odot \boldsymbol{Z}+\boldsymbol{\mu}_k & \in \mathbb{R}^{n \times z} \\
\boldsymbol{V}=\phi_{\text {silu }}\left(\boldsymbol{X} W_v+b_v\right) & \in \mathbb{R}^{n \times v}
\end{array}
XXX

+ 计算带位置bias的attention：

XXX
\boldsymbol{O}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}+\boldsymbol{b}_{\mathrm{rel}}\right) \boldsymbol{V} \quad \in \mathbb{R}^{n \times v}
XXX

+ 通过reset gate $$\boldsymbol{\gamma}$$和update gate $$\boldsymbol{\varphi}$$计算输出

XXX
\begin{aligned}
\boldsymbol{\gamma} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_\gamma+b_\gamma\right) & & \in \mathbb{R}^{n \times v} \\
\boldsymbol{\varphi} & =\phi_{\text {sigmoid }}\left(\boldsymbol{X}^{\prime} W_{\varphi}+b_{\varphi}\right) & & \in \mathbb{R}^{n \times d} \\
\hat{\boldsymbol{H}} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_h+(\boldsymbol{\gamma} \boldsymbol{O}) U_h+b_h\right) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Y}&=\boldsymbol{\varphi} \odot \hat{\boldsymbol{H}}+(1-\boldsymbol{\varphi}) \odot \boldsymbol{X} \quad & & \in \mathbb{R}^{n \times d}
\end{aligned}
XXX


这里把attention里的softmax改成了如下的laplace函数：

XXX
f_{\text {laplace }}(x ; \mu, \sigma)=0.5 \times\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma \sqrt{2}}\right)\right]
XXX

其中，$$\operatorname{erf}(x)=\frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} \mathrm{~d} t=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \mathrm{~d} t$$是[误差函数](https://zh.wikipedia.org/wiki/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0)，

为了让laplace逼近$$f_{\text {relu }^2}$$，对于$$x=\sqrt{2}$$这个点，求解如下方程

XXX
\begin{aligned}
& f_{\text {relu2 }}(\sqrt{2})=f_{\text {laplace }}(\sqrt{2}) \\
& f_{\text {relu2 }}^{\prime}(\sqrt{2})=f_{\text {laplace }}^{\prime}(\sqrt{2})
\end{aligned}
XXX

可以得到$$\mu=\sqrt{1 / 2}$$，$$\sigma=\sqrt{1 / 4 \pi}$$，对应的曲线和准确率如下：

![laplace-vs-relu2](../assets/laplace-vs-relu2.png)

### mega-chunk

![mega-chunk](../assets/mega-chunk.png)

将序列切分成长度固定为$$c$$的$$k=n/c$$个chunk，对**每个chunk独立计算**上面的attention，这样复杂度就变成了$$O(kc^2)=O(nc)$$，由于有EMA，所以这样做还是能够保持一定程度的长距离依赖。

## MEGALODON

[Meta无限长文本大模型来了：参数仅7B，已开源](https://mp.weixin.qq.com/s/VML5hExo5iPsyEavxzIZSA)

[革命新架构掀翻Transformer！无限上下文处理，2万亿token碾压Llama 2](https://mp.weixin.qq.com/s/xgP9P51gjqJ93FYSWfPeaA)

[MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

[https://github.com/XuezheMax/megalodon](https://github.com/XuezheMax/megalodon)

[Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

基于MEGA进行改进，能够同时实现

+ 高效训练（减少通信和计算量）
+ 高效推理（保持恒定的KV缓存）


## MOD

[Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/pdf/2404.02258.pdf)


## multi-head moe

[微软让MoE长出多个头，大幅提升专家激活率](https://mp.weixin.qq.com/s/ZCRyb63M2DL4hOQh7uxxaw)

[Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)

[https://github.com/yushuiwx/MH-MoE](https://github.com/yushuiwx/MH-MoE)

## Lory

[150B token从头训练，普林斯顿Meta发布完全可微MoE架构Lory](https://mp.weixin.qq.com/s/UKIXGJTFzSeSZvoTe_c9CQ)

[Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](https://arxiv.org/pdf/2405.03133)

## Perciever

[Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf)

## Aaren

[Bengio等人新作：注意力可被视为RNN，新模型媲美Transformer，但超级省内存](https://mp.weixin.qq.com/s/mRt2A1n1CmO7uqzuLQHxkw)

[Attention as an RNN](https://arxiv.org/pdf/2405.13956)

### many-to-one RNN

对query向量$$q$$的attention可以看成是一个函数，对输入的$$N$$个token $$x_{1: N}$$通过他们的key和value $$\left\{\left(k_i, v_i\right)\right\}_{i=1}^N$$ 变换成输出 $$\text { Attention }\left(q, k_{1: N}, v_{1: N}\right)$$，假设$$\bar{s}_i=\operatorname{dot}\left(q, k_i\right)$$，那么输出就是

XXX
o_N=\sum_{i=1}^N \operatorname{softmax}(s)_i v_i=\frac{\sum_{i=1}^N \exp \left(s_i\right) v_i}{\sum_{i=1}^N \exp \left(s_i\right)}=\frac{\hat{a}_N}{\hat{c}_N}
XXX

可以发现，分子和分母其实都可以写成递推形式$$\hat{a}_k=\hat{a}_{k-1}+\exp \left(s_k\right) v_k$$和$$\hat{c}_k=\hat{c}_{k-1}+\exp \left(s_k\right)$$，由于直接这么做可能会产生很大或者很小的值（例如算exp），所以可以通过如下方式进行缓解：计录到第k步的最大值$$m_k=\max _{i \in\{1, \ldots, k\}} s_i$$，然后减掉它，即$$a_k=\sum_{i=1}^k \exp \left(s_i-m_k\right) v_i$$和$$c_k=\sum_{i=1}^k \exp \left(s_i-m_k\right)$$，这样就可以改写为如下形式：

XXX
\begin{aligned}
a_k & =a_{k-1} \exp \left(m_{k-1}-m_k\right)+v_k \exp \left(s_k-m_k\right) \\
c_k & =c_{k-1} \exp \left(m_{k-1}-m_k\right)+\exp \left(s_k-m_k\right) \\
m_k & =\max \left(m_{k-1}, s_k\right)
\end{aligned}
XXX

其中第一行其实就是$$s_i-m_k=s_i-m_k+m_{k-1}-m_k$$，然后exp一下，$$exp(s_i-m_k)=exp(s_i-m_k)exp(m_{k-1}-m_k)$$，总结成如下图：

![attention-rnn-cell](../assets/attention-rnn-cell.png)

所以Attention的RNN cell就是输入$$\left(a_{k-1}, c_{k-1}, m_{k-1}, q\right)$$，输出$$\left(a_k, c_k, m_k, q\right)$$，初始的状态是$$\left(a_0, c_0, m_0, q\right)=(0,0,0,q)$$

然后就可以将之前的attention看成如下几类many-to-one的RNN了：

+ 传统attention只计算最后的一个输出
+ self-attention使用**输入token**作为初始状态
+ Perceiver的cross-attention使用**依赖input的隐变量**作为初始状态

![many-to-one-rnn](../assets/many-to-one-rnn.png)

对于新来的token而言：

+ 传统的RNN一般是流式地输入数据，因此只需要O(1)的内存和计算就行了
+ Transformer需要把这个新token当成一个初始状态加进来，所以需要把之前时间步的再重新算一次，需要O(N)的计算
+ Perceiver中的隐变量是依赖输入的，而且这个新token会改变value，因此初始状态也会变，所以需要从头算一遍，需要O(NL)的计算，L是隐变量个数（可以参考代码[perceiver_torch](https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py)和如下gpt4o的回答）

![perceiver](../assets/perceiver.png)

### many-to-many RNN

要计算$$\left\{o_i=\operatorname{Attention}\left(q, x_{1: i}\right)\right\}_{i=1}^N$$，通过如下的并行前缀扫描算法，对于$$N$$个序列数据的$$N$$个前缀，通过关联运算符$$\oplus$$并行计算，能够高效地通过$$\left\{x_k\right\}_{k=1}^N$$计算$$\left\{\bigoplus_{i=1}^k x_i\right\}_{k=1}^N$$

![parallel-prefix-scan](../assets/parallel-prefix-scan.png)

由于$$\operatorname{Attention}\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)=o_k=\frac{a_k}{c_k}$$，为了计算$$\left\{\text { Attention }\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)\right\}_{k=1}^N$$，只需要先按这个并行扫描算法计算$$\left\{a_k\right\}_{k=1}^N$$、$$\left\{c_k\right\}_{k=1}^N$$和$$\left\{m_k\right\}_{k=1}^N$$，再把$$a_k$$和$$c_k$$结合起来就行。

接来来定义三元组$$\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right)$$，其中

+ $$A$$：一些下标的集合
+ $$\mathrm{m}_A=\max _{i \in A} s_i$$
+ $$\mathrm{u}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right)$$
+ $$\mathrm{w}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right) v_i$$

所以并行扫描算法的输入是$$\left\{\left(\mathrm{m}_{\{i\}}, \mathrm{u}_{\{i\}}, \mathrm{W}_{\{i\}}\right)\right\}_{i=1}^N=\left\{\left(s_i, 1, v_i\right)\right\}_{i=1}^N$$，再来定义操作$$\oplus$$：

XXX
\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right) \oplus\left(\mathrm{m}_B, \mathrm{u}_B, \mathrm{w}_B\right)=\left(\mathrm{m}_{A \cup B}, \mathrm{u}_{A \cup B}, \mathrm{w}_{A \cup B}\right)
XXX

其中，

+ $$\mathrm{m}_{A \cup B}=\max \left(\mathrm{m}_A, \mathrm{~m}_B\right)$$
+ $$\mathrm{u}_{A \cup B}=\mathrm{u}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{u}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$
+ $$\mathrm{w} _{A \cup B}=\mathrm{w}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{w}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$

这个并行扫描算法最终输出下式，即$$\left\{\left(m_k, c_k, a_k\right)\right\}_{k=1}^N$$：

XXX
\left\{\left(\mathrm{m}_{\{1, \ldots, k\}}, \mathrm{u}_{\{1, \ldots, k\}}, \mathrm{w}_{\{1, \ldots, k\}}\right)\right\}_{k=1}^N=\left\{\left(m_k, \sum_{i=1}^k \exp \left(s_i-m_k\right), \sum_{i=1}^k \exp \left(s_i-m_k\right) v_i\right)\right\}_{k=1}^N
XXX


![many-to-many-rnn](../assets/many-to-many-rnn.png)

### Aaren

Aaren(attention as a recurrent newral network)的结构如下：

XXX
\begin{aligned}
h_1^{(0)}, \ldots, h_N^{(0)} & \leftarrow x_1, \ldots, x_N \\
{\left[h_1^{(j+1)}, \ldots, h_N^{(j+1)}\right] } & \leftarrow \operatorname{Aaren}\left(q^{(j)},\left[h_1^{(j)}, \ldots, h_N^{(j)}\right]\right)
\end{aligned}
XXX

transformer的query是输入的token，而Aaren的query token $$q$$是在训练的过程中通过bp学习的。迭代地计算$$y_k$$只需要常数级的计算，因为它依赖$$h_{k-1}$$和$$x_k$$。

transformer：

+ 使用kv cache时需要线性的内存
+ 需要保存所有之前的tokens，包括在中间层的那些

aaren：

+ 只需要常数级的内存
+ 不需要保存之前的所有tokens

![stacking-aaren](../assets/stacking-aaren.png)

## Matmul-free

[从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗](https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag)

## H2O attention

[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/pdf/2306.14048)

+ sparsity for small cache size：kv cache很稀疏，只要5%效果就可以了
+ Heavy-Hitters for low miss rate：一小部分的token贡献了大部分attention score
+ 贪心法选择需要干掉的token：假设总共需要保留k个token，对于第i个token，加进来后，遍历集合里的所有token，看干掉哪一个对attention score影响最小，就把它干掉

## TransNAR

[拯救Transformer推理能力！DeepMind新研究TransNAR：给模型嵌入「算法推理大脑」](https://mp.weixin.qq.com/s/YPICpkYHAC7zTLC_0M_XkQ)

[Transformers meet Neural Algorithmic Reasoners](https://arxiv.org/pdf/2406.09308)

基于：[Neural Algorithmic Reasoning](https://arxiv.org/pdf/2105.02761)

## softmax数学原理

[通向概率分布之路：盘点Softmax及其替代品](https://mp.weixin.qq.com/s/tA9kJqD279dHnivzPkvbjg)

把softmax的分母记为$$Z(x)$$，其对数是max的一个光滑近似：

XXX
\begin{array}{r}
\log Z(\boldsymbol{x})=\log \sum_{j=1}^n e^{x_j}=\operatorname{logsumexp}(\boldsymbol{x}) \\
\lim _{\tau \rightarrow 0^{+}} \tau \operatorname{logsumexp}(\boldsymbol{x} / \tau)=\max (\boldsymbol{x})
\end{array}
XXX

当$$\tau$$取1时，可以得到$$\operatorname{logsumexp}(\boldsymbol{x}) \approx \max (\boldsymbol{x})$$

## StreamingLLM

[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453)

[深度解析streamingLLM之无限长文本生成能力](https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&mid=2247484158&idx=1&sn=666bd5fed13815b5765124646206bac5&chksm=cea54ae8f9d2c3fea1f3d5dfc94fc3d48abbd02a1278584bfd76ff8fe52ce59ce8966ced9cd5&token=562866785&lang=zh_CN#rd)

attention sink：输入给LLM推理开头的几个intial tokens是非常特殊的，就像水龙头一样，出水那一瞬间吸引了人们大量的attention。而且intial tokens与生成token的绝对距离距离和语义信息都不重要，重要的是这第一个或者前面几个token。

softmax需要所有位置的值的总和为1，因此必须给某些位置权重，即使这个位置对输出没有任何贡献，可能导致在backward的过程中产生错误的权重更新，而这个错误在后续的过程中很难被纠正。因此，模型倾向于将不必要的注意力值转嫁给特定的token。其实在[https://www.evanmiller.org/attention-is-off-by-one.html](https://www.evanmiller.org/attention-is-off-by-one.html)就提出了，要给softmax的分母+1：$$(\operatorname{softmax_1}(\mathrm{x}))_i=\frac{\exp \left(x_i\right)}{\sum_j \exp \left(x_j\right)+1}$$

当每个token都趋向负无穷时，softmax的极限是1/k（k是token数），也就是会让每个token的概率都是1/k

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}(\operatorname{softmax}(x))_i=\frac{1}{k}>0
XXX

而$$$$softmax_1$$$$因为分母有个1（即$$exp(0)=1$$），相当于在最前面引入了一个或者多个的无意义的global_token。同样当每个token都趋向负无穷时，因为分母已经有一个1了，所以可以让其他正常token都趋于0

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}\left(\operatorname{softmax}_1(x)\right)_i=0
XXX


[https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

集成到trt-llm里了：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)

+ trtllm-build时，加上```--streamingllm enable```
+ infer时，加上```--sink_token_length```设置sink的token数，加上```--max_attention_window_size```设置sliding_window

## 是否还要RAG?

[谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强](https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg)

## memory^3

[鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆](https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw)

[Memory3 : Language Modeling with Explicit Memory](https://arxiv.org/pdf/2407.01178)

## TSLLM

[没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练](https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg)

[AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training](https://arxiv.org/pdf/2309.17179)

[https://github.com/waterhorse1/LLM_Tree_Search](https://github.com/waterhorse1/LLM_Tree_Search)

## PEER

[单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE](https://mp.weixin.qq.com/s/-oocmPNRT5ddvNwIvYxiQA)

[MoE也有Scaling Law，「百万专家」利用率近100%！DeepMind华人挑战MoE极限](https://mp.weixin.qq.com/s/tBe9DZvzB6NB8HhLYP31CQ)

[Mixture of A Million Experts](https://arxiv.org/pdf/2407.04153)

## TTT

[彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态](https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ)

[Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620)

[https://github.com/test-time-training/ttt-lm-jax](https://github.com/test-time-training/ttt-lm-jax)

[https://github.com/test-time-training/ttt-lm-pytorch](https://github.com/test-time-training/ttt-lm-pytorch)

![ttt](../assets/ttt.png)

[连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路](https://mp.weixin.qq.com/s/tfrG21mfteVAkjqYx5mDsQ)

[The Surprising Effectiveness of Test-Time Training for Abstract Reasoning](https://ekinakyurek.github.io/papers/ttt.pdf)

## Axiomatic Training

[6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码](https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ)

[公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4](https://mp.weixin.qq.com/s/Yera281WG8RNAOrRUYjH6g)

[Teaching Transformers Causal Reasoning through Axiomatic Training](https://arxiv.org/pdf/2407.07612v1)

训练大模型新范式——**公理框架（Axiomatic Framework）**，作者从头开始训练了6700万参数的模型，仅使用了简单的因果链作为训练数据。在推断复杂图表中的因果关系时，**67M模型的表现超越了十亿级参数LLM**，甚至可以与GPT-4相媲美。

## PNN

[AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键](https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw)

[Training of Physical Neural Networks](https://arxiv.org/pdf/2406.03372)

## JRT

[小技巧大功效，「仅阅读两次提示」让循环语言模型超越Transformer++](https://mp.weixin.qq.com/s/zdPlK4IHeEiW0ikmQMPJUA)，注：这里的transformer++指的就是llama2的架构

[Just read twice: closing the recall gap for recurrent language models](https://arxiv.org/pdf/2407.05483)

[https://github.com/HazyResearch/prefix-linear-attention](https://github.com/HazyResearch/prefix-linear-attention)

![jrt](../assets/jrt.png)

### order的重要性

&nbsp;

线性attention的recurrent state很小，存储空间有限，往往很难选择要存储哪些state。

如果集合A和集合B有交集，且A元素比B多，先出现集合A的时候，需要存储整个A，而先出现集合B的时候，则只需要存储集合B

### JRT-Prompt

&nbsp;

上下文学习任务以$$(\mathcal{C}, \mathcal{Q}, \mathcal{Y})$$作为输入，$$\mathcal{C}$$为一些上下文来源（如文档或代码存储库），$$\mathcal{Q}$$为给定上下文时对模型的一些问题或请求，$$\mathcal{Y}$$为答案。

+ 使用自回归的标准上下文学习模型$$\mathcal{A}$$，输入$$\mathcal{C}$$和$$\mathcal{Q}$$，并根据正确的完成情况$$Y$$来评估生成的输出$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q})$$。
+ JRT-PROMPT：在提示模型输出答案之前会在上下文中重复提示中的信息（如问题和文档），例如$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q}, \mathcal{C}, \mathcal{Q})$$。在上下文第二次出现时，模型根据完整的上下文来决定存储哪些信息。

示例：

```shell
# 原来的prompt
## input: 
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年

# JRT的prompt
## input：
百度成立于哪一年？百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年
```

### base的linear transformer

&nbsp;

通过$$\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{\tilde{d}}$$，使得$$\phi\left(\boldsymbol{q}_i\right)^{\top} \phi\left(\boldsymbol{k}_j\right) \approx \exp \left(\boldsymbol{q}_i^{\top} \boldsymbol{k}_j / \sqrt{d}\right) .$$

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i\left(\phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j\right)}{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i \phi\left(\boldsymbol{k}_j\right)}
XXX

先计算k和v的乘法，时间和空间复杂度是$$\mathcal{O}(N d \tilde{d})$$，而softmax attention是$$O\left(N^2 d\right)$$

infer阶段包括两个phases：

+ prefill：并行处理prompt，得到两个state：
    + KV-state：$$\boldsymbol{s}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j$$
    + K-state：$$\boldsymbol{z}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top}$$
+ decoding：计算如下3步，其中$$\boldsymbol{s}_i \in \mathbb{R}^{d \times \tilde{d}}$$，$$\boldsymbol{z}_i \in \mathbb{R}^{\tilde{d}}$$，一个decode step有$$O(1)$$的时间和空间复杂度，而softmax attention加上kv-caching有$$O(N)$$
    + $$\boldsymbol{s}_i=\boldsymbol{s}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top} \boldsymbol{v}_i$$
    + $$\boldsymbol{z}_i=\boldsymbol{z}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top}$$
    + $$\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{s}_i}{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{z}_i}$$

### JRT-RNN架构

&nbsp;

PLA（Prefix Linear Attention）受Prefix-LM启发，主要有2个特点：

+ prefix-LM在encoder和decoder的projection是共享的，而JRT-RNN的encoder用$$\boldsymbol{k}_e, \boldsymbol{v}_e$$，decoder用$$\boldsymbol{k}_d, \boldsymbol{v}_d$$
+ 编码器使用了non-causal的线性注意力，而解码器使用标准causal线性注意力。

prefill阶段，并行地对长度为$$l$$的prompt进行如下计算，如果长度$$l< M$$，进行left-pad到$$M$$长度

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}\right)}{\phi\left(\boldsymbol{v} q_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top}\right)}
XXX

初始化如下：

XXX
\boldsymbol{s}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}\right),\ \boldsymbol{z}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top}\right)
XXX

对于decoding阶段，输出$$y_i, i>M$$，和base的linear transformer一样，不需要修改

训练loss是ntp和mlm的混合，假设序列长度是$$N$$，前$$M$$个token算MLM，后面的$$N-M$$个token算NTP：

XXX
\mathcal{L}=\frac{w_1 \mathcal{L}_{\mathrm{NTP}}+w_2 \mathcal{L}_{\mathrm{MLM}}}{w_1+w_2}
XXX

### 效率提升

虽然Linear attention比softmax attention要快，但其实不如精心优化的softmax attention（如flash attention），[Simple linear attention language models balance the recall-throughput tradeof](https://arxiv.org/pdf/2402.18668)实现了一个io-aware的kernel（[https://github.com/HazyResearch/based/](https://github.com/HazyResearch/based/)），在prefill阶段精细地partitioning & storing暛的matrix-valued recurrent state across warp-registers，PLA参考这个实现了自己的加速版。


## LM-steer

ACL 2024

[LM-Steer: Word Embeddings Are Steers for Language Models](https://arxiv.org/pdf/2305.12798)

[https://github.com/Glaciohound/LM-Steer](https://github.com/Glaciohound/LM-Steer)

发现词向量空间上的线性变换空间等价于对语言模型生成样式的调节，并以此设计了名为 LM-Steers 的语言模型调控方法。我们发现词向量的这种调节作用普遍存在于各种尺寸的语言模型中。它只需要学习原始模型 0.2% 的参数就可以引导各种风格。在语言模型去毒化和生成情感控制等任务上，LM-Steers 可以实现与最先进的受控生成方法相当或更好的性能，同时保持更好的生成质量平衡。学习到的 LM-Steer 还可以充当文本风格的解读器：它可以解释各种文本样式与词向量哪些维度相关，并且可以用于寻找最具代表性的文本片段。 LM-Steer 可通过显式计算来在不同语言模型之间转移，而不需要额外训练。我们还可以简单地通过缩放 LM-Steer 来实现风格的连续控制，或者实现多种生成控制的组合。

## razerattention

减少kv-cache的大小

[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://www.arxiv.org/pdf/2407.15891)

受anthropic的induction head([In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html))启发:

+ short head：对长文没有任何响应
+ long head(induction head)：对长文能直接找到对应位置，并且对那附近的词有很强的信号

## MLP-Mixer

[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)

## sigmoid attention

[Sigmoid注意力一样强，苹果开始重新审视注意力机制](https://mp.weixin.qq.com/s/4DvgsqkyNcj6HBrAHTygCA)

[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/pdf/2409.04431)

[https://github.com/apple/ml-sigmoid-attention](https://github.com/apple/ml-sigmoid-attention)

证明了从理论上讲，与 softmax 注意力相比，具有sigmoid注意力的Transformer是**通用函数逼近器**，并且受益于**改进的正则化**。

## lstm+transformer

[LSTM+Transformer王炸创新，荣登Nature](https://mp.weixin.qq.com/s/OTwbZEy-v93-fzvh1Z4m2Q)

## AnyGraph

[港大黄超团队推出AnyGraph, 首次揭秘图大模型的Scaling Law](https://mp.weixin.qq.com/s/iqQi4ZP7FdpHnMxZByOyoQ)

[AnyGraph: Graph Foundation Model in the Wild](https://arxiv.org/pdf/2408.10700)

[https://github.com/HKUDS/AnyGraph](https://github.com/HKUDS/AnyGraph)


## 液态神经网络

[给机器人装上「虫脑」？非Transformer液态神经网络终于来了！MIT CSAIL负责人创业成果](https://mp.weixin.qq.com/s/oowid3yCpFNTALCvgdSwXg)

Liquid Foundation Models（LFM），1B、3B和40B LFM在各个规模上均能实现SOTA性能，同时保持更小的内存占用和更高效的推理。2020年就有了[Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439)

## 差分transformer

[这篇论文非常火！差分Transformer竟能消除注意力噪声，犹如降噪耳机](https://mp.weixin.qq.com/s/hG_S85HkyAkTFAI2iQjl6g)

[Differential Transformer](https://arxiv.org/pdf/2410.05258)

Transformer往往会过度关注不相关的上下文，即注意力噪声（attention noise），而差分Transformer则能放大对答案范围的注意力并消除噪音，从而增强上下文建模的能力。

## SparseLLM

[NeurIPS 2024｜SparseLLM：突破性全局剪枝技术，大语言模型稀疏化革命](https://mp.weixin.qq.com/s/mfdkUFXCsB50iqKgxv7hQQ)

[SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)

[https://github.com/BaiTheBest/SparseLLM](https://github.com/BaiTheBest/SparseLLM)

## minLSTM+minGRU

[图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?](https://mp.weixin.qq.com/s/ueid-TAw-9OjtKFKA5lqSw)

[Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201v1)

## MixCon

[北大林宙辰团队全新混合序列建模架构MixCon：性能远超Mamba](https://mp.weixin.qq.com/s/FfCYq1Bx6d7NARKvDPwz2Q)

[MixCon: A Hybrid Architecture for Efficient and Adaptive Sequence Modeling](https://zhouchenlin.github.io/Publications/2024-ECAI-MixCon.pdf)

## self-lengthen

[阿里千问提出Self-Lengthen，大模型实现自迭代扩展输出长度](https://mp.weixin.qq.com/s/1m1hUkhs3altxjYP6IxUVw)

[Language Models Can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933)

[https://github.com/QwenLM/Self-Lengthen](https://github.com/QwenLM/Self-Lengthen)

## BLT

[Tokenization不存在了？Meta最新研究，无需Tokenizer的架构来了](https://mp.weixin.qq.com/s/7ju-PjPZVPrBLQ1qFnFoKw)

[Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)

## R2L tokenizer

[从2019年到现在，是时候重新审视Tokenization了](https://mp.weixin.qq.com/s/zmeFYfxWD1nZq_MocgGeeQ)

[https://huggingface.co/spaces/huggingface/number-tokenization-blog](https://huggingface.co/spaces/huggingface/number-tokenization-blog)

## memory layers at scale

[Meta探索大模型记忆层，扩展至1280亿个参数，优于MoE](https://mp.weixin.qq.com/s/R5JOMkLVbbI7yPJmdcT2pQ)

[Memory Layers at Scale](https://arxiv.org/pdf/2412.09764)

[https://github.com/facebookresearch/memory](https://github.com/facebookresearch/memory)

## TITANS

(toread)

[谷歌新架构一战成名，打破Transformer记忆瓶颈，姚班校友钟沛林新作](https://mp.weixin.qq.com/s/APE_CJ4rEQYV8ngyaTAXWw)

[近8年后，谷歌Transformer继任者「Titans」来了，上下文记忆瓶颈被打破](https://mp.weixin.qq.com/s/EUCZ1oSuyzR9M9X9r5SYBw)

[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663v1)

## Transformer^2

[Transformer^2要做「活」的AI模型，动态调整权重，像章鱼一样适应环境](https://mp.weixin.qq.com/s/_vdA_KygkFWqFE5Xm6CDRg)

[TRANSFORMER2 : SELF-ADAPTIVE LLMS](https://arxiv.org/pdf/2501.06252)

## MTA

[Multi-Token突破注意力机制瓶颈，Meta发明了一种很新的Transformer](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650963426&idx=2&sn=4cb5432a3b547f12136a50fdebbb19f8)

[Multi-Token Attention](https://arxiv.org/pdf/2504.00927)

## Linear MOE

[Linear-MoE：线性注意力遇上混合专家的开源实践](https://mp.weixin.qq.com/s/Li6fP14bdwQweykdV6F3_g)

[Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts](https://arxiv.org/abs/2503.05447)

[https://github.com/OpenSparseLLMs/Linear-MoE](https://github.com/OpenSparseLLMs/Linear-MoE)

## 记忆能力

[最新发现！每参数3.6比特，语言模型最多能记住这么多](https://mp.weixin.qq.com/s/DAoNui-_u0IlBjHl16wn-g)

[How much do language models memorize?](https://arxiv.org/pdf/2505.24832)

# Gemmini&Gemma系列

## Gemini 1.0

[Gemini: a family of highly capable multimodal models](https://arxiv.org/pdf/2312.11805.pdf)

## Gemini 1.5

[谷歌Gemini 1.5深夜爆炸上线，史诗级多模态硬刚GPT-5！最强MoE首破100万极限上下文纪录](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652444347&idx=1&sn=51ae7e3e100e24fd49b0f75924e74695&chksm=f093b48285369da37b6148803e41fb272c51c013bc31ac1ba6b09672ff1efd38269af1192b53&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)

[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)

+ 更新的 Gemini 1.5 Pro，其大部分功能和基准都超过了 2 月份的版本
+ Gemini 1.5 Flash，一种更轻量级的变体，专为提高效率而设计，并且在性能方面的减益很小。

此前的SOTA模型能处理**20万(200K)**的token，Gemini 1.5能稳定处理**100万(1M)**的token（极限为**1000万(10M)**的token），能够处理11小时的音频、1小时的视频、超过3w行的代码库、超过70w个单词


## gemma

[https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/)

[Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)

代码：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py)


## codegemma

+ 专门处理代码补全和代码生成任务的 7B 预训练变体
+ 用于代码聊天和指令跟随的 7B 指令调优变体
+ 在本地计算机上运行快速代码补全的 2B 预训练变体

[CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)

![codegemma](../assets/codegemma.png)

## RecurrentGemma

[RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf)

在infer阶段，需要检索**KV cache**，并加载到内存中，而KV cache会随着序列长度线性增长。[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)通过**local attention**来降低cache大小，但模型效果会变差。recurrent gemma将输入序列**压缩到一个固定大小的state中**，从而不会降低效果，对长序列能降低内存占用并高效infer。

基于Griffin架构，将**门控线性递归**与**本地滑动窗口注意力**混合在一起，在生成长序列时实现快速推理，相比gemma：

+ 减少内存用量：内存要求越低，就越能在内存有限的设备（例如单个 GPU 或 CPU）上生成较长的样本。
+ 吞吐量较高：能够以明显较高的batch_size执行推理，这意味着每秒可以生成更多tokens，尤其是在生成长序列时。

相比原始griffin：

+ 对输入emb**乘以一个常数**（等于model width(下表中的2560)的平方根，如下代码所示）。输入和输出的emb是tied的，这个常数**没有乘到output上去**。gemma里也有一个类似的因子
+ 对**recurrent layers（RG-LRU）**在训练时并**没有进行weight decay(本质是L2正则化，参考[https://zhuanlan.zhihu.com/p/607909453](https://zhuanlan.zhihu.com/p/607909453))**，当bp到**开方操作**时，为了训练稳定会加最大值为1000的**梯度clip**

```python
def __init__(self, ...):
    #...
    self.register_buffer(
        "normalizer", torch.tensor(self.config.hidden_size**0.5, 
            dtype=torch.bfloat16), persistent=False
    )
def forward(self, ...):
    #...
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)

    hidden_states = inputs_embeds

    if use_cache and inputs_embeds.shape[1] != 1:  
        # TODO let's maybe only call in the `generate`?
        self._setup_cache(self.config, hidden_states.shape[0], 
        hidden_states.device, hidden_states.dtype)

    if cache_position is None:
        cache_position = torch.arange(hidden_states.shape[1], 
        device=hidden_states.device)
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    causal_mask = self._update_causal_mask(attention_mask, 
        inputs_embeds, cache_position)

    ## 这里就是那个因子
    hidden_states = hidden_states * self.normalizer.type(hidden_states.dtype)
```

具体参数：

| 变量 | 大小 |
|----------|-------------|
| 总参数量 | 2.7b |
| 非emb参数量 | 2.0b |
| emb参数量 | 0.7b |
| vocab size | 256k |
| model width | 2560 |
| rnn width | 2560 |
| MLP expansion factor | 3，即intermediate_size=2560*3=7680 |
| Depth | 26 |
| Attention heads | 10 |
| local attention window size | 2048 |

训练：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py)

在线推理的C++实现(cpu版本)

[https://github.com/google/gemma.cpp](https://github.com/google/gemma.cpp)

## gemma-2

[单张A100全精度推理！谷歌明星开源模型Gemma 2上新9B/27B，挑战3140亿Grok-1](https://mp.weixin.qq.com/s/z3h1eExDgItDf38Xar6yPg)

[谷歌「诚意之作」，开源9B、27B版Gemma2，主打高效、经济！](https://mp.weixin.qq.com/s/0Iy3gOlWRLKRCMnrXQavaA)

[https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)

[https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/pdf/2408.00118)

用了RoPE和GeGLU，网络结构上：

+ **局部滑动窗口**和**全局注意力**：在每隔一层中交替使用局部滑动窗口注意力([Longformer: The long-document transformer]((https://arxiv.org/pdf/2004.05150)))和全局注意力（[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)），局部注意力层的滑动窗口大小设置为4096个token，而全局注意力层的跨度设置为8192个token。
+ **Logit软封顶**：根据**Gemini 1.5的方法**，研究团队在每个注意力层和最终层限制logit，使得logit的值保持在$$−soft\_cap,+soft\_cap$$之间。对于9B和27B模型，注意力对数封顶设置为50.0，最终对数封顶设置为30.0。截至本文发表时，注意力logit软封顶与常见的FlashAttention实现不兼容，因此他们已从使用FlashAttention的库中移除了此功能。对模型生成进行了有无注意力logit软封顶的消融实验，发现大多数预训练和后期评估中，生成质量几乎不受影响。本文中的所有评估均使用包含注意力logit软封顶的完整模型架构。然而，某些下游性能可能仍会受到此移除的轻微影响。

XXX
\text { logits } \leftarrow \text { soft\_cap } * \text { tanh(logits } / \text { soft\_cap) }
XXX

+ **RMSNorm**进行post-norm和pre-norm。为了稳定训练，用RMSNorm对每个变换子层、注意力层和前馈层的输入和输出进行归一化。 
+ **gqa**：27B和9B模型均使用GQA，num_groups = 2，基于消融实验表明在保持下游性能的同时提高了推理速度。

[https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py](https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py)


[Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[https://huggingface.co/google/gemma-scope](https://huggingface.co/google/gemma-scope)

## datagemma

[整合海量公共数据，谷歌开源AI统计学专家DataGemma](https://mp.weixin.qq.com/s/Fr8I9VwiyHcMnhrWMcgDeQ)

[Knowing When to Ask - Bridging Large Language Models and Data](https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf)

提出了一种名为**检索交错生成（Retrieval Interleaved Generation，RIG）**的新方法，可以可靠地将Data Commons中的公共统计数据整合到LLM的响应中。RIG是一种受工具启发的方法，可以将统计数据标记与适合从Data Commons检索的自然语言问题交错。

为了获得这种能力，他们利用Gemini 1.5的帮助生成了一个指令-响应数据集，并在此基础上对LLM进行了微调。RIG方法将事实准确性从5-7%提高到了约58%。

## gemini 2.0

[OpenAI深夜被狙，谷歌Gemini 2.0掀翻牌桌！最强智能体组团击毙o1](https://mp.weixin.qq.com/s/RfTGAlYVlH1RlTzuzugrKQ)

[谷歌“狙击”OpenAI，发布新一代大模型！主打Agent+多模态](https://mp.weixin.qq.com/s/c39MbjULBW5M_8vdO-VUXQ)

[我扒出了Gemini 2.0超实时多模态幕后的黑科技，第六代TPU芯片Trillium！](https://mp.weixin.qq.com/s/BZpL9PwvCWmR4R6h2502Kw)

[谷歌逆风翻盘暴击OpenAI，90天王者归来！44页报告押注25年三大技术前沿](https://mp.weixin.qq.com/s/b0yqPre6wMlP59cH6Qrbdw)

[data and ai trends report 2024](https://services.google.com/fh/files/misc/data_ai_trends_report.pdf)

## gemini 2.5

[DeepSeek逼出谷歌新推理模型：40分优势超GPT4.5登顶竞技场，支持原生多模态，但依然败给了“竹竿问题”](https://mp.weixin.qq.com/s/U75bF7gQ6wlQlaflQvrIQQ)

[谷歌终于登顶一次了！最强推理模型Gemini 2.5 Pro实测体验，真的有点东西](https://mp.weixin.qq.com/s/unlZoM5oCH3HCw2kxy6X_w)

[谷歌首款混合推理Gemini 2.5登场，成本暴降600%！思考模式一开，直追o4-mini](https://mp.weixin.qq.com/s/Tx6dbqKwPTx9dqAW1xzT3w)

Gemini 2.5 Pro的上下文窗口是1M tokens，并且支持原生多模态：可以理解庞大数据集并处理来自不同信息源的复杂问题，包括文本、音频、图像、视频，甚至是整个代码库。2.5 pro擅长创造视觉上引人注目的Web应用程序和智能体代码。

[刚刚，Gemini 2.5 Pro升级，成编程模型新王](https://mp.weixin.qq.com/s/6wv6tUVYsJPfaN-7HxUjoQ)


[52页PPT，谷歌Gemini预训练负责人首次揭秘！扩展定律最优解](https://mp.weixin.qq.com/s/IB_Bum3Md5mafAaEVxk9QA)

[https://vladfeinberg.com/assets/2025-04-24-princeton-talk.pdf](https://vladfeinberg.com/assets/2025-04-24-princeton-talk.pdf)

## gemini 2.0 flash thinking

[推理最强也最快，谷歌发布Gemini 2.0 Flash Thinking，全面超越o1-preview](https://mp.weixin.qq.com/s/NkTP17j6HYIz95sHxCameA)

[https://ai.google.dev/gemini-api/docs/thinking-mode?hl=zh-cn](https://ai.google.dev/gemini-api/docs/thinking-mode?hl=zh-cn)

[https://colab.research.google.com/github/google-gemini/cookbook/blob/main/gemini-2/thinking.ipynb](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/gemini-2/thinking.ipynb)

我自己复制的一个：[https://colab.research.google.com/github/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb](https://colab.research.google.com/github/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb)

对应的github：[https://github.com/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb](https://github.com/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb)

## gemma-3

[Google 发布多模态开源模型 Gemma 3：性能与功能全面升级，附技术报告英中对照版](https://mp.weixin.qq.com/s/WyBNvIsPKMGjSMk_F2Xrpg)

+ 架构设计：Gemma 3基于decoder-only架构，与前两代Gemma模型类似。但在细节上有所创新，例如采用GQA和RMSNorm，还引入了**QK-norm**替代Gemma 2中的软封顶机制，以提升性能和稳定性。
+ 训练策略：Gemma 3的训练过程包括预训练和指令微调两个阶段。在预训练阶段，模型使用知识蒸馏技术，从大规模数据中学习语言和视觉表示。训练数据涵盖了文本、图像以及多语言等多种类型，且经过严格的筛选和清洗，以减少有害内容和低质量数据的影响。在指令微调阶段，Gemma 3 采用了新颖的训练方法，重点关注数学、聊天、指令遵循和多语言等能力的提升。

[一台3090就能跑Gemma 3 27B！谷歌发布Gemma 3全系QAT版模型](https://mp.weixin.qq.com/s/OlfazHQpsrkQ_CtyQvXbJg)


# DeepSeek系列

## DeepSeek-V2

[一块钱100万token，超强MoE模型开源，性能直逼GPT-4-Turbo](https://mp.weixin.qq.com/s/tAA8XUbU__9FgvEvXxsykw)

[https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)

[从MHA到MLA看Attention优化：谈谈DeepSeek拼多多级的推理价格](https://mp.weixin.qq.com/s/l2uUXGQ-8Rj_nI3JG5lZ_g)

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://spaces.ac.cn/archives/10091)

![](../assets/mla-deepseekmoe.png)

主要包括2部分：

### MLA

输入第$$t$$个token $$h_t$$，分别经过一个$$W^Q$$、$$W^K$$和$$W^V$$得到$$d$$维的$$q_t=W^Qh_t$$、$$k_t=W^Kh_t$$和$$v_t=W^Vh_t$$，split成$$n_h$$份，每份的dim是$$d_h=d/n_h$$，**每一份就是图里的一条竖线**

+ MHA(Multi-head Attention)：每个head各自算注意力，再拼接
+ GQA(Grouped-Query Attention)：出自[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)，将所有Head分为$$g$$个组（g可以整除h），**每组共享**同一对K、V
+ MQA(Multi-Query Attention)：出自[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)，让**所有Attention Head共享同一个K、V**
+ MHA(Multi-head Latent Attention)：
    + 降维：输入的$$h_t$$先过一个$$W^{DKV}\in R^{d_c\times d}$$，得到$$c_t^{KV}=W^{DKV}h_t$$，其中$$d_c\ll d$$
    + 升维：$$k^C_t=W^{UK}c_t^{KV}$$，$$v^C_t=W^{UV}c_t^{KV}$$，其中$$W^{UK}\in R^{d\times d_c}$$，$$W^{UV}\in R^{d\times d_c}$$
    + 同时对q也降维，不过是降到$$d'_c$$，$$c^Q_t=W^{DQ}h_t$$，$$q^C_t=W^{UQ}c_t^Q$$，其中$$c^Q_t\in R^{d'_c}$$，$$W^{UQ}\in R^{d_hn_h\times d'_c}$$
    + 为了兼容RoPE，
        + $$q^R_t=RoPE(W^{QR}c^Q_t)$$，其中$$W^{QR}\in R^{d^R_hn_h\times d'_c}$$，再split成$$n_h$$份：$$[q^R_{t,1};q^R_{t,2};...;q^R_{t,n_h}]$$，每一份的dim是$$d^R_h$$
        + $$k^R_t=RoPE(W^{KR}h_t)$$
        + $$q_{t,i}=[q^C_{t,i};q^R_{t,i}]$$，$$k_{t,i}=[k^C_{t,i};k^R_t]$$，维度都是$$d_h+d^R_h$$，
        + 再拿这个q和k去算attn，$$o_{t,i}=\sum ^t_{j=1}softmax_j(\frac{q^T_{t,i}k_{j,i}}{\sqrt {d_h+d^R_h}})v^C_{j,i}$$
        + $$u_t=W^O[o_{t,1};o_{t,2};...;o_{t,n_h}]$$

![](../assets/mha-gqa-mqa-mla.png)

| attn | KV Cache per Token | Capability |
|-------|---------------------------------|-------|
| MHA | $$2n_hd_hl$$ | Strong   |
| GQA | $$2n_gd_hl$$ | Moderate  |
| MQA | $$2d_hl$$  | Weak  |
| MLA | $$(d_c + d_h^R)l \approx \tfrac{9}{2}d_hl$$ | Stronger |

最终在计算时，需要cache的是图中的蓝色部分，所以是$$(d_c + d_h^R)l$$，在Deepseek-v2中，设置$$d_c=4d_h, d^R_h=d_h/2$$

![](../assets/mla-kvcache.jpeg)

### DeepSeekMoE

&nbsp;

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066)

[https://github.com/deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)

FFN部分的MoE，相比传统的[GShard](https://arxiv.org/pdf/2006.16668.pdf) 等MoE架构，DeepSeekMoE用了**更细粒度的专家分配机制**，并将部分专家设置为**共享专家**。

细粒度专家分配：假设有$$N$$个专家，即么对于原来的每个专家，把它的intremediate hidden dim分成$$m$$份，每一份的长度是原来的$$\frac{1}{m}$$，这样就有了$$mN$$个专家，原来选$$k$$个，现在就选$$mK$$个

$$\mathbf{u}_t$$表示FFN输入的第$$t$$个token，$$N_s$$是shared experts的数量，$$N_r$$是routed experts的数量，$$K_r$$表示被激活的router专家数量 

XXX
\begin{aligned}
& \mathbf{h}_t^{\prime}=\mathbf{u}_t+\sum_{i=1}^{N_s} \operatorname{FFN}_i^{(s)}\left(\mathbf{u}_t\right)+\sum_{i=1}^{N_r} g_{i, t} \operatorname{FFN}_i^{(r)}\left(\mathbf{u}_t\right), \\
& g_{i, t}= \begin{cases}s_{i, t}, & s_{i, t} \in \operatorname{Topk}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_r\right\}, K_r\right), \\
0, & \text { otherwise },\end{cases} \\
& s_{i, t}=\operatorname{Softmax}_i\left(\mathbf{u}_t^T \mathbf{e}_i\right),
\end{aligned}
XXX

device-limited routing：保证每个token的目标专家**分布在最多$$M$$个设备上**

+ 首先选择有最高相关性（即上面的$$s_{i,t}$$）的$$M$$个设备出来
+ 再从这$$M$$个设备里选出top-K个专家出来

负载均衡的辅助loss：

+ expert-level的负载均衡loss：$$\alpha_1$$是超参

XXX
\begin{aligned}
\mathcal{L}_{\text {ExpBal }} & =\alpha_1 \sum_{i=1}^{N_r} f_i P_i, \\
f_i & =\frac{N_r}{K_r T} \sum_{t=1}^T \mathbb{1}(\text { Token } t \text { selects Expert } i), \\
P_i & =\frac{1}{T} \sum_{t=1}^T s_{i, t},
\end{aligned}
XXX

+ device-level的负载均衡loss：把所有routed专家分成$$D$$组$$\left\{\mathcal{E}_1, \mathcal{E}_2, \ldots, \mathcal{E}_D\right\}$$，并把每一组放到一个单独的设备上：

XXX
\begin{aligned}
\mathcal{L}_{\text {DevBal }} & =\alpha_2 \sum_{i=1}^D f_i^{\prime} P_i^{\prime} \\
f_i^{\prime} & =\frac{1}{\left|\mathcal{E}_i\right|} \sum_{j \in \mathcal{E}_i} f_j \\
P_i^{\prime} & =\sum_{j \in \mathcal{E}_i} P_j
\end{aligned}
XXX

+ 通信负载均衡loss：如果一个设备收到的token比较多，它的通信代价也比较大。因为要选$$M$$个设备出来，总共有$$T$$个词，所以，鼓励每个设备最多传输$$MT$$个隐层状态去其他设备，同时从其他设备接收到$$MT$$左右个的隐层状态。

XXX
\begin{aligned}
\mathcal{L}_{\text {CommBal }} & =\alpha_3 \sum_{i=1}^D f_i^{\prime \prime} P_i^{\prime \prime} \\
f_i^{\prime \prime} & =\frac{D}{M T} \sum_{t=1}^T \mathbb{1}(\text { Token } t \text { is sent to Device } i) \\
P_i^{\prime \prime} & =\sum_{j \in \mathcal{E}_i} P_j
\end{aligned}
XXX

## DeepSeek-Prover-V1.5

[DeepSeek开源数学大模型，高中、大学定理证明新SOTA](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

[DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152)

[https://github.com/deepseek-ai/DeepSeek-Prover-V1.5](https://github.com/deepseek-ai/DeepSeek-Prover-V1.5)

## Deepseek-v3

[4篇DeepSeek神作论文速读，屠龙少年的名言：You play the game I play the rule.](https://mp.weixin.qq.com/s/AxwK_gtWQfxzT9aqPaVRGw)

[DeepSeek-V3 正式发布](https://mp.weixin.qq.com/s/iFZOQsUNkpkXPDvOkE99wQ)

[论文](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)

下载了一个：[https://github.com/daiwk/collections/blob/master/assets/DeepSeek_V3.pdf](https://github.com/daiwk/collections/blob/master/assets/DeepSeek_V3.pdf)

[DeepSeek-V3技术报告解读](https://mp.weixin.qq.com/s/-Ggy94bM8k-AaYarTMSI5Q)

[Deepseek V3 预训练策略解读](https://mp.weixin.qq.com/s/TzaaOOwe32V1IXxiLwcKGw)

[MiniMax-01 与 DeepSeek-V3 对比](https://mp.weixin.qq.com/s/x7sN0RwaJjzxHiAReslSRQ)

[DeepSeek绕开CUDA垄断，V3论文细节再挖出！英伟达护城河不存在了？](https://mp.weixin.qq.com/s/t4VTGX7fMTFpur8F91fvjQ)

直接写PTX（并行线程执行），PTX是专门为其GPU设计的中间指令集架构，位于高级GPU编程语言（如CUDA C/C++或其他语言前端）和低级机器代码（流处理汇编或SASS）之间。PTX是一种接近底层的指令集架构，将GPU呈现为数据并行计算设备，因此能够实现寄存器分配、线程/线程束级别调整等细粒度优化，这些是CUDA C/C++等语言无法实现的

[首次披露！DeepSeek V3 发布软硬一体协同训练论文，公开「降成本」秘诀](https://mp.weixin.qq.com/s/LFOsOzINZNmyVDMRtGk83g)

[刚刚，DeepSeek首曝V3降成本秘诀！软硬协同突破Scaling天花板](https://mp.weixin.qq.com/s/JgIZD0IMxsH55Vg6-lfgkQ)

[DeepSeek-V3再发论文，梁文锋署名，低成本训练大模型的秘密揭开了](https://mp.weixin.qq.com/s/nTZH03aIIG1tQa7uiRc__A)

[Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/pdf/2505.09343)


## NSA

[刚刚！DeepSeek梁文锋亲自挂名，公开新注意力架构NSA](https://mp.weixin.qq.com/s/SlhNdGNFpGlicIkMUE21og)

[【手撕NSA】DeepSeek新作-原生稀疏注意力-超长文(附代码)](https://mp.weixin.qq.com/s/-sbxKky1qWBVYzuyNMg5jw)

[Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)

## FlashMLA

[“源神”DeepSeek！突破H800性能上限，FlashMLA重磅开源，算力成本还能降](https://mp.weixin.qq.com/s/OZmMTW4JyiP4GWkzShlqAg)

[https://github.com/deepseek-ai/FlashMLA](https://github.com/deepseek-ai/FlashMLA)

## DeepEP

[DeepSeek开源第二弹，为MoE和EP量身定制的通信库！暂和英伟达显卡绑定](https://mp.weixin.qq.com/s/2ecxmq9zbwFlzOalprok0Q)

[https://github.com/deepseek-ai/DeepEP](https://github.com/deepseek-ai/DeepEP)

## DeepGEMM

[DeepSeek-R2曝5月前上线！第三弹DeepGEMM 300行代码暴击专家优化内核](https://mp.weixin.qq.com/s/4aKxWIpb2-9DZKrSSAYoWg)

[https://github.com/deepseek-ai/DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)


## DualPipe

[DeepSeek一口气开源3个项目，还有梁文锋亲自参与，昨晚API大降价](https://mp.weixin.qq.com/s/aeK5eJMwqNuqJaZczKrhgQ)

[https://github.com/deepseek-ai/DualPipe](https://github.com/deepseek-ai/DualPipe)

## EPLB

[https://github.com/deepseek-ai/eplb](https://github.com/deepseek-ai/eplb)

## profile-data

[https://github.com/deepseek-ai/profile-data](https://github.com/deepseek-ai/profile-data)

## 3fs

[DeepSeek开源周最后一天：让数据处理「从自行车升级到高铁](https://mp.weixin.qq.com/s/p8KGnYIqmCkXPF4jjRiKjQ)

[https://github.com/deepseek-ai/3FS](https://github.com/deepseek-ai/3FS)

在3fs上的数据处理框架：[https://github.com/deepseek-ai/smallpond](https://github.com/deepseek-ai/smallpond)

## 推理系统

[https://zhuanlan.zhihu.com/p/27181462601](https://zhuanlan.zhihu.com/p/27181462601)

中文版：[https://github.com/deepseek-ai/open-infra-index](https://github.com/deepseek-ai/open-infra-index)

对应英文版：[https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md](https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)


优化目标：更大的**吞吐**，更低的**延迟**。
方案：大规模跨节点专家并行(Expert Parallelism, EP)

EP好处：

+ 增加batchsize，提高gpu矩阵乘法效率，**提升吞吐**
+ 专家分散在不同GPU，每个GPU只计算少量专家，访存更少，**降低延迟**

EP的复杂性：

+ 引入了跨节点传输，需要设计合理计算流程使**计算**和**传输**可以**同步进行**
+ 因为多节点，需要**DP**（数据并行），不同DP间要**负载均衡**

### 大规模跨节点专家并行

V3/R1的结构：从第4层开始是MOE，每层有**1个共享**专家和**256个路由**专家，每次激活8个路由专家，即**8 in 256**，稀疏度很高，因此需要用**更大的overall batchsize**，才能给每个专家提供足够的专家batchsize

+ Prefill：
    + 路由专家EP32
    + MLA和共享专家DP32
    + 一个部署单元是4节点（32卡），每张卡包括：
        + 9个路由专家（共9*32=288个专家），其中：
            + 1个冗余专家，共1*32=32个
            + 8个路由专家，共8*32=256个
        + 1个共享专家，共1*32=32个
+ Decode：
    + 路由专家EP144
    + MLA和共享专家DP144，
    + 一个部署单元是18节点(18*8=144卡)，每张卡包括：
        + 2个路由专家，共2*144=288
            + 总共32个冗余专家
            + 总共288-32=256个正常的路由专家 
        + 1个共享专家，共1*144=144个

### 计算通信重叠

多机多卡的EP会引入较大的通信开销，可以用双batch重叠来掩盖通信开销，提升整体吞吐：

+ **prefill阶段**：2个batch的计算和通信交错进行，一个batch计算时可以掩盖另一个batch的通信开销。即attn->dispatch->mlp->shared/combine这4步（这里是shared和combine并行）

![](../assets/overlap-prefill.png)

+ **decode阶段**：不同阶段的执行时间有差别， attention部分拆成了两个stage（图中的attn-0（attn前的一些proj操作等）和attn-1（核心的attn计算等）），共5个stage的流水线（attn0->attn1->dispatch/shared->mlp->combine）来实现计算和通信的重叠（这里是shared和dispatch并行）。

![](../assets/overlap-decode.png)

细节可以参考[https://github.com/deepseek-ai/profile-data](https://github.com/deepseek-ai/profile-data)

### 尽可能地负载均衡

尽可能为每个GPU分配均衡的计算负载+通信负载

+ prefill load balancer：
    + 问题：不同DP实例上的请求数、长度不同，导致core-attention的计算量、dispatch发送量不同
    + 目标：
        + 各GPU的**计算量**尽量相同==>core-attention计算负载均衡
        + **输入的token数量**尽量相同==>dispatch发送量负载均衡，避免部分GPU处理时间过长
+ decode load balancer：
    + 问题：不同DP实例上的请求数量、长度不同，导致core-attention计算量（与 KVCache 占用量相关）、dispatch发送量不同
    + 目标：
        + 各GPU的**KVCache占用量**尽量相同==>core-attention计算负载均衡
        + **请求数量**尽量相同==>dispatch发送量负载均衡
+ expert-parallel load balancer：
    + 问题：对于给定MoE模型，存在一些**天然的高负载专家**，导致不同GPU的专家计算负载不均衡
    + 目标：每个GPU上的**专家计算量均衡**==>最小化所有GPU的dispatch接收量(即MOE的输入)的最大值

### 架构图

![](../assets/deepseek-infer.jpg)

prefill和decode拆了2个service，并且每个前面有负载均衡，内部有EP的负载均衡

### 线上效果

H800，精度和训练一致：

+ 矩阵计算和dispatch传输用fp8
+ core-attn计算和combine用bf16

白天用所有节点部署服务，晚上减少节点，峰值278节点（每个节点8卡H800，共278 * 8=2224卡，前面提到了prefill要32卡，decode要144卡，其实就是32x+14y=2224），平均占用226.75个节点，每张卡2美金1小时，那就是2 * 226.75 * 8 * 24 = 87072美金/天

+ 输入总token数608B，有342B命中kvcache缓存，命中率56.3%
+ 输出总token数168B，平均输出速度20~22tps(tokens per second)，这里指的应该是**单请求**的生成速度，平均每输出一个token的KVCache长度是4989。
+ 平均每台H800(8卡)的吞吐量：
    + prefill任务：输入吞吐约 73.7k tokens/s（含缓存命中）。
        + 推测：每秒每8卡输入的token数是608000000000 / 226.75/86400=31k，好像对不太上这个73.7k，可能猜测：缓存那部分非常快，非缓存很慢，而这31k是假设全天流量不变的平均值
    + decode任务：输出吞吐约 14.8k tokens/s。
        + 推测：每秒每8卡输出的token数是168000000000 / 226.75 /86400=8575，前面提到了每个请求每秒的输出token数是20~22，那就相当于每8卡机器每秒能处理有8575/21=408个请求，所以总的返回qps大概是408 * 226.75=9.25k

## GRM+SPCT

[刚刚，DeepSeek公布推理时Scaling新论文，R2要来了？](https://mp.weixin.qq.com/s/TbPWc4ooFyf0it5pEaLRUA)

[Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495)

critique：生成的文本评价，即评论

### RM分类

![](../assets/rm-patterns.png)

RM可以进行如下分类：

+ reward生成范式：输入query和一些responses
  + scalar：只输出reward值
  + semi-scalar：生成critique和scalar
  + generative：只输出critique
+ 打分范式：输入query和2个responses
  + pointwise：输出每个response的得分
  + pairwise：输出二者的相对得分

几种常见RM：

+ Bradley-Terry，scalar + pointwise
+ PairRM：scalr + pairwise
+ CLoud：semi-scalr + pointwise
+ LLM-as-a-Judge：semi-scalr或者generative + pairwise
+ Pointwise GRM：generative + pointwise

### SPCT

核心：principle（准则）不再只是辅助性的输入，而是使模型能在生成过程中**主动生成**并**基于此生成对应的critique**

Self-Principled Critique Tuning (SPCT)

![](../assets/spct.png)

+ 训练（2阶段）：输入query+2个response
  + RFT（rejective fine-tuning）：使GRM能够生成格式正确且适配多种输入类型的principle与critique
    + 使用预训练的pointwise-GRM，采样得到N个priciple + response，和goundtruth比较(groundtruth对应的reward是不是比其他N-1个结果对应的reward高)，满足下面条件的扔了，剩下的构造成一个数据集
      + 全对的就是too-easy的，扔掉；
      + 错误的也扔掉
    + 同时还有一定比例的hint sampling，即把groundtruth提前泄露到GRM的prompt里去，让模型倾向于去对齐groudtruth
  + RL（rule-based online RL）：通过不断优化生成的准则和评论，进一步增强泛化型奖励生成能力。
    + 也是用的GRPO，只有accuracy reward，没format reward，加大KL-penalty
+ 推理：输入query+2个response
  + 并行采样出一堆的principles+critiques，即m条结果
  + voting：
    + 基于生成式reward：即每个里的response1的reward求和，同理response2的reward求和，得到1和2的总得分
    + 基于meta RM：训练一个pointwise scalar RM，用二分类来判断principle+critique对不对(label和RFT的一样)。使用：对这m个结果打分，干掉质量差的（图中就是只保留了第1和第3个），然后对剩下的$$k_{meta}$$个去投票

## DeepSeek-Prover-V2

[DeepSeek开源Prover-V2强推理模型，网友：奥数从没这么简单过](https://mp.weixin.qq.com/s/UYn-9vX5rpWpB3Y1xFW8Jg)

专为Lean 4 打造的开源大语言模型，专注于形式化定理证明。

[https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/DeepSeek_Prover_V2.pdf](https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/DeepSeek_Prover_V2.pdf)

下载了一份：[pdf](https://github.com/daiwk/collections/blob/master/assets/DeepSeek_Prover_V2.pdf)

两阶段训练，建立了两种互补的证明生成模式：

1. 高效非思维链（non-CoT）模式：针对快速生成正式的Lean证明代码进行优化，专注于生成简洁的证明，没有显式的中间推理步骤。 
2. 高精度思维链（CoT）模式：系统地阐述中间推理步骤，强调透明度和逻辑进展，然后构建最终的正式证明。

与 DeepSeek-Prover-V1.5一致，这两种生成模式由两个不同的引导提示控制。

+ 第一阶段采用专家迭代，在课程学习框架内训练一个非CoT证明模型，同时通过基于子目标的递归证明合成难题的证明。选择非CoT生成模式是为了加速迭代训练和数据收集过程。
+ 在此基础上，第二阶段利用了冷启动链式思维（CoT）数据，通过将 DeepSeek-V3 复杂的数学推理模式与合成形式证明相结合而生成。CoT模式通过进一步的强化学习阶段得到增强，遵循了通常用于推理模型的标准训练流程。

DeepSeek-Prover-V2的非CoT模式训练过程遵循专家迭代的范式，这是开发形式化定理证明器广泛采用的框架。在每次训练迭代中，当前最佳证明策略用于生成那些在先前迭代中未解决的难题的证明尝试。这些成功的尝试经由Lean证明助手验证后，被纳入SFT数据集以训练改进的模型。

这一迭代循环不仅确保模型能够从初始演示数据集中学习，还能提炼出自己的成功推理轨迹，逐步提高其解决更难问题的能力。总体训练过程与 DeepSeek-Prover-V1 的训练过程大致一致，仅对训练问题的分布进行了两项修改：

+ Prover-V2引入了来自自动形式化和各种开源数据集的额外问题，扩大了训练问题领域的覆盖范围。
+ 新模型通过子目标分解生成的问题来扩充数据集，旨在解决MiniF2F基准测试有效划分中的更多挑战性实例。

在DeepSeek-V3-Base-671B上使用恒定的学习率5e-6，在16384个token的上下文中进行监督微调。训练语料库由两个互补来源组成：

+ 通过专家迭代收集的非CoT数据，生成无需中间推理步骤的Lean代码；
+ 冷启动CoT数据，将DeepSeek-V3的高级数学推理过程提炼为结构化的证明路径。非CoT组件强调精益定理证明器生态系统中的形式验证技能，而CoT示例明确地建模了将数学直觉转化为形式证明结构的认知过程。

Prover-V2采用GRPO，通过为每个定理提示采样一组候选证明并根据它们的相对奖励优化策略，消除了对单独critic模型的需求。训练使用二元奖励，每个生成的Lean证明如果被验证为正确则获得1个奖励，否则为0。为了确保有效学习，研究人员精心挑选训练提示，仅包括那些对监督微调模型具有足够挑战性但可解决的问题。模型在每次迭代中采样256个不同的问题，为每个定理生成32个候选证明，最大序列长度为32768个token。

模型蒸馏：

+ 把DeepSeek-Prover-V1.5-Base-7B的最大上下文长度从4096个token扩展到了32768个，并使用DeepSeek-Prover-V2-671B强化学习阶段收集的rollout数据对这个扩展上下文模型进行微调。
+ 除了CoT推理模式外，还整合了专家迭代过程中收集的非CoT证明数据，以实现一种成本效益高的证明选项，该选项能够生成简洁的形式化输出，并且模型规模较小。
+ 7B模型也采用了与671B模型训练相同的强化学习阶段以提升性能。


# Qwen系列

## 早期的Qwen

早期的：

[Qwen1.5-110B：首个国产千亿参数开源大模型](https://mp.weixin.qq.com/s/Lb08elR2r23UN5kWnhF-BA)

4月26日，Qwen开源了其第一个千亿参数大模型Qwen1.5-110B，这应该也是国内第一个千亿规模的开源大模型。其包含1100亿参数，更重要的是这是一个Dense模型，而非MoE模型。从各项评测来看，Qwen1.5-110B足以与Llama3-70B相抗衡，部分指标也取得了更高的水平。

[阿里云Qwen2.5发布！再登开源大模型王座，Qwen-Max性能逼近GPT-4o](https://mp.weixin.qq.com/s/D2D9hga06bg2AMuGnizN5w)

所有 Qwen2.5 系列模型都在 18 万亿（18T）tokens 的数据上进行了预训练。在语言模型方面，Qwen2.5开源了7个尺寸：0.5B、1.5B、3B、7B、14B、32B、72B，每个都在同等参数赛道创造了业界最佳成绩。这些型号的设定充分考虑了下游场景的不同需求：

+ 3B：适配手机等端侧设备的黄金尺寸；
+ 32B：最受开发者期待的「性价比之王」，可在性能和功耗之间获得最佳平衡
+ Qwen2.5-32B的整体表现甚至超越了Qwen2-72B。

[阿里除夕发布Qwen2.5-Max反超DeepSeek V3，一句话开发小游戏](https://mp.weixin.qq.com/s/4gf6qQXDlq0fc3rGkzJj2g)

Qwen2.5-Max同DeepSeek V3一样是超大规模的MoE模型，


## QwQ-max-preview

[阿里旗舰推理模型硬刚DeepSeek！官宣独立APP，发布公告AI亲自写](https://mp.weixin.qq.com/s/UjwuAKaRhKpQ-9eA2pSh8g)

[https://chat.qwen.ai](https://chat.qwen.ai)

## QWQ-32B

[https://qwenlm.github.io/blog/qwq-32b/](https://qwenlm.github.io/blog/qwq-32b/)

[https://huggingface.co/Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)

目前只有博客，写得挺简单的，看介绍是用了两阶段RL：

+ 1阶段RL：在冷启ckpt上，只拿代码和数学语料，基于outcome-base reward，随着训练的进行，发现这两方面的能力都有持续提升
  + 数学：有一个accuracy verifier，检查最终解决方案是否正确
  + 代码：有一个代码执行server，检查代码是否通过预定义好的测试用例
+ 2阶段RL：没说用什么语料，但用的是general reward model加上rule-based verifiers产生的reward，发现只要少量的step就能提升模型的通用能力，如指令遵循、人类偏好对齐、agent性能，而且代码和数学能力下降不明显

## Qwen2.5-Omni

[阿里深夜开源Qwen2.5-Omni，7B参数完成看、听、说、写](https://mp.weixin.qq.com/s/BZv7tuMX6yZr5LvqKZBJCw)

[Qwen2.5-Omni Technical Report](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)

[https://github.com/QwenLM/Qwen2.5-Omni/tree/main](https://github.com/QwenLM/Qwen2.5-Omni/tree/main)

[https://github.com/QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)

## Qwen3

[猛击OpenAI o1、DeepSeek-R1！刚刚，阿里Qwen3登顶全球开源模型王座，深夜爆火](https://mp.weixin.qq.com/s/gvWJMfy2ah4IHqyO-tP8MA)

[Qwen3：思深，行速](https://mp.weixin.qq.com/s/OvobsCPW0IwxeSm8pljv-A)

[Qwen3技术报告英中对照版.pdf](https://mp.weixin.qq.com/s/WJiW35ICxT7ox1-lGnqLIg)

[Qwen3 Technical Report](https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf)

[https://github.com/QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)

[Qwen3技术报告首次全公开！“混合推理模型”是这样炼成的](https://mp.weixin.qq.com/s/VvugM54Z14mxGV-OOaVuwQ)

开源了两个MoE模型的权重：

+ Qwen3-235B-A22B：拥有2350多亿总参数和220多亿激活参数
+ Qwen3-30B-A3B：拥有约300亿总参数和30亿激活参数

此外，还有6个Dense模型：Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B和Qwen3-0.6B

### 网络结构

tokenizer用的是Qwen的，即BBPE（byte-level byte-pair encoding），词表大小是151,669

dense架构和Qwen2.5类似，

+ 有GQA、SwiGLU、Rope和pre-normalization的RMSNorm等，
+ 删掉了Qwen2中的QKV-bias
+ 引入QK-Norm，保证训练稳定

MoE架构：

+ 类似Qwen2.5-MoE，实现了DeepseekMoE的细粒度的专家切分
+ 有128个专家，激活8个
+ 不同于Qwen2.5-MoE，没有共享专家
+ 使用global-batch load balancing loss来鼓励专家更专业化(specialization)，参考[Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models](https://arxiv.org/abs/2501.11873)

注：其中的tie embedding指的是输入的emb和输出的emb共享

![](../assets/qwen3-arch.png)


### 预训练

数据：

+ 数据规模：Qwen2.5是在18万亿个token上进行预训练的，而Qwen3使用的数据量几乎是其两倍，达到了约36万亿(36T)个token，涵盖了119种语言和方言。
+ 合成数据：
    + 先使用Qwen2.5-VL对大量类似PDF的文档进行文本识别。再利用Qwen2.5对识别出的文本进行优化以提升其质量。获得了额外的总量达数万亿tokens 的高质量文本。
    + 使用Qwen2.5、Qwen2.5-Math和Qwen2.5-Coder等模型合成了多种形式的文本token，包括教材、问答对、指令和代码片段等，覆盖数十个领域，总量达到数万亿tokens。

预训练过程分为三个阶段。

+ general stage：各Qwen3的模型都在超过30T个token上进行了预训练，序列长度为**4K token**，为模型提供了基本的语言技能和通用知识。
+ reasoning stage：通过增加如STEM(science/technology/engineering/mathematics)、编程和推理任务的数据比例来改进数据集，在这额外的5T个token上进行了预训练。
+ long context stage：使用高质量的长上下文数据将序列长度扩展到**32K token**，确保模型能够有效地处理更长的输入。
    + 数据集里有75%的长度是16384到32768之间，剩下的25%是4096到16384之间
    + 通过ABF技巧([Effective long-context scaling of foundation models](https://arxiv.org/abs/2309.16039))，将RoPE的base频率从10000增加到1000000
    + 使用YARN（[YaRN: Efficient context window extension of large language models](https://arxiv.org/abs/2309.00071)）和DCA(Dual Chunk Attention, [Training-free long-context scaling of large language models](https://arxiv.org/abs/2402.17463))，在推理阶段的序列长度capacity达到了4倍的增加

预训练效果：

+ Qwen3 Dense基础模型的整体性能与参数更多的Qwen2.5基础模型相当：Qwen3-1.7B/4B/8B/14B/32B-Base分别与Qwen2.5-3B/7B/14B/32B/72B-Base表现相当。特别是在STEM、编码和推理等领域，Qwen3 Dense基础模型的表现甚至超过了更大规模的Qwen2.5模型。
+ Qwen3 MoE基础模型在仅使用10%激活参数的情况下，达到了与Qwen2.5 Dense基础模型相似的性能，带来了训练和推理成本的显著节省。

### 后训练

![](../assets/qwen3-post-training.png)

四阶段：

#### stage1: Long-CoT cold start

&nbsp;

使用多样的的长思维链数据对模型进行微调，涵盖了数学、代码、逻辑推理和STEM问题等多种任务和领域。每条样本都有验证过的reference答案或者是基于代码的测试用例。

+ query filter：
    + 用Qwen2.5-72B-Instruct来判断并过滤那些不容易验证的query，例如一些包括多个子问题的query，或者通用的文本生成。
    + 过滤掉了Qwen2.5-72B-Instruct不需要CoT reasoning就能正确回答的那些query。确保只有那些需要更深reasoning的复杂问题才保留下来
    + 为每个query通过Qwen2.5-72B-Instruct标注了领域，确保数据集里每种领域的分布比较平衡。
+ response filter：用QwQ-32B为每个query生成N个候选responses，当QwQ-32B一直没法生成正确解法时，人工介入来评估。对于pass@N为正的query，进一步地过滤掉如下几类回答：
    + 最终答案错误
    + 包含大量重复内容
    + 明显只是猜测、缺乏充分推理支撑
    + 思路过程与总结结论不一致
    + 存在语言混用或风格突变问题的
    + 疑似过于接近潜在的验证集内容

精挑细选出一小部分来训练，目标是在**不过度追求中间推理准确性**的前提下，先建立模型的**基础推理模式**。可以确保模型的潜力不会被早期训练限制住，从而为后续的RL阶段提供更大的优化空间和灵活性。为达成这一目标，建议在该准备阶段**尽量减少训练样本数量和训练步数**。

#### stage2: Reasoning RL

&nbsp;

重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。query-verifer的pair数据必须满足4个条件：

+ 没在cold start阶段用过
+ 对cold start模型来说是可学习的
+ 尽可能有挑战性
+ 包括了很多子领域

最终收集了3995个pair对，用GRPO训练。用大的batchsize，每个query有很多的rollouts，再加上off-policy的训练来提升采样效率，对训练都是有益的。同时，通过控制模型的entropy的稳定增加，以平衡探索和利用，对稳定训练非常重要。

#### stage3: Thinking Mode Fusion

&nbsp;

在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，**将非思考模式整合到思考模型中**。确保了推理和快速响应能力的无缝结合。

+ sft数据构造：
    + 思考数据：拿stage1的query，用stage2的模型通过拒绝采样产出，保证sft不会让stage2的模型变差
    + 非思考数据：涵盖了很多tasks，例如代码、数学、指令遵循、多语言任务、创意写作、问答、角色扮演。此外，还用自动生成的checklists来评估非思考数据的回答质量
    + 为了增加在小语种上的效果，特地增加了翻译任务的占比
+ chat template设计：non-thinking模式里直接把```<think>```和```</think>```间置空。为了实现默认是thinking模式，当用户query没有```/think```时，也加了一些thinking模式的样本。对于多轮会话，在用户query里随机插入```/think```或者```/no_think```，让模型根据最后一个flag来产出回答。

![](../assets/qwen3-chat-template.png)

+ thinking预算：一旦模型学会了在non-thinking和thinking模式进行回复时，自发地产生了一种能力：能够基于不完整的thiking来产出结果。这也是实现thinking预算控制的基础。当thiking的长度达到一个用户定义的阈值后，会手动停止thking模式，并插入一个instrunction：```Considering the limited time by the user, I have to give the solution based on the thinking directly now.\n</think>.\n\n```，然后模型就会自己输出回答了。值得注意的是，这种能力并非通过显式训练获得，而是应用思考模式融合后自然涌现出的结果。

[https://muellerzr.github.io/til/end_thinking.html](https://muellerzr.github.io/til/end_thinking.html)

可以参考这个来直接改logit，大概意思是假设生成了k个token，设置了最多思考m个token，那```</think>```这个token的logit乘以$$1+\frac{m}{k}$$，如果超预算了，直接把这个token的概率改成0，并把其他所有token的概率改成-inf，相当于强制输出这个token。。

#### stage4: General RL

&nbsp;

建立了一个复杂的奖励系统，覆盖超过20种不同的任务，每项任务都配有定制化的评分标准。提升如下几个能力：

+ 指令遵循：模型准确地解释并遵循用户指令，包括内容、格式、长度、结构化输出等，与用户预期对齐
+ 模版遵循：遵循特定的格式要求，例如thinking和non-thinking模式、用设计好的token（```<think>```和```</think>```）来区分thinking和回复
+ 偏好对齐：对于开放性的问题，提升模型的helpfulness、engagement和style，提供更自然和令人满意的用户体验
+ Agent能力：工具调用，在rl rollout期间，模型能够与真实环境进行完整的多轮交互，并且得到执行反馈，从而提升long-horizon决策任务上的性能和稳定性
+ 特殊场景能力：例如RAG任务，设计reward，鼓励产生准确且符合上下文的答案，从而减小幻觉的风险

3种奖励机制：

+ 基于规则的奖励：有助于产出高准确率的答案，缓解reward hacking
+ 带参考答案的基于模型的奖励（Model-based Reward with Reference Answer）：对每个query给出一个参考答案，并用Qwen2.5-72B-Instruct基于这个参考答案对模型输出进行打分。相比规则reward更灵活，且能支持更多样的任务。
+ 不带参考答案的基于模型的奖励：利用人类偏好数据，训练一个reward model，对模型回答输出一个scalar。能够适用于更多类型的query。

#### strong-to-weak蒸馏

&nbsp;

包括5个密集模型（Qwen3-0.6B、1.7B、4B、8B和14B）以及一个MoE模型（Qwen3-30B-A3B），蒸馏过程主要分为两个阶段：

+ **Off-policy蒸馏(即teacher产出样本)**：结合教师模型在 ```/think```和```/no_think```两种模式下生成的输出，用于对学生模型进行回应蒸馏。有助于轻量级学生模型掌握基本的推理能力，并学会在不同思考模式之间切换。
+ **On-policy蒸馏（即student产出样本）**：学生模型基于**当前策略**生成回应序列用于微调。我们对提示词进行采样，学生模型以 `/think` 或 `/no_think` 模式生成回应。随后，通过将其输出logits与教师模型（Qwen3-32B或Qwen3-235B-A22B）对齐，最小化两者之间的KL散度，从而对学生模型进行微调。

### 使用

vllm：

```shell
vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1
# 要禁用思考模式，可以移除--reasoning-parser以及--enable-reasoning
```

软切换机制，可以在```enable_thinking=True```时动态控制模型的行为，即在用户prompt或系统prompt中添加```/think```和```/no_think```来逐轮切换模型的思考模式。

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

classQwenChatbot:
    def __init__(self, model_name="Qwen3-30B-A3B/Qwen3-30B-A3B"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.history = []

    def generate_response(self, user_input):
        messages = self.history + [{"role": "user", "content": user_input}]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        response_ids = self.model.generate\
          (**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # Update history
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})

        return response

# Example Usage
if __name__ == "__main__":
    chatbot = QwenChatbot()

    # First input (without /think or /no_think tags, 
    # thinking mode is enabled by default)
    user_input_1 = "How many r's in strawberries?"
    print(f"User: {user_input_1}")
    response_1 = chatbot.generate_response(user_input_1)
    print(f"Bot: {response_1}")
    print("----------------------")

    # Second input with /no_think
    user_input_2 = "Then, how many r's in blueberries? /no_think"
    print(f"User: {user_input_2}")
    response_2 = chatbot.generate_response(user_input_2)
    print(f"Bot: {response_2}") 
    print("----------------------")

    # Third input with /think
    user_input_3 = "Really? /think"
    print(f"User: {user_input_3}")
    response_3 = chatbot.generate_response(user_input_3)
    print(f"Bot: {response_3}")
```

MCP：可以使用MCP配置文件，使用Qwen-Agent内置的工具，或者自行集成其他工具。

```python
from qwen_agent.agents import Assistant

# Define LLM
llm_cfg = {
    'model': 'Qwen3-30B-A3B',
    # Use the endpoint provided by Alibaba Model Studio:
    # 'model_type': 'qwen_dashscope',
    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),
    # Use a custom endpoint compatible with OpenAI API:
    'model_server': 'http://localhost:8000/v1',  # api_base
    'api_key': 'EMPTY',
    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define Tools
tools = [
    {'mcpServers': {  # You can specify the MCP configuration file
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            "fetch": {
                "command": "uvx",
                "args": ["mcp-server-fetch"]
            }
        }
    },
  'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 
  'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)
```

## ParScale

[ParScale：一种全新的大模型Scaling Law](https://mp.weixin.qq.com/s/Y51VKF7Kvd-avIvaYxjl0w)

[Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)

[https://github.com/QwenLM/ParScale](https://github.com/QwenLM/ParScale)

## WorldPM

[建模世界偏好：偏好建模中的Scaling Laws](https://mp.weixin.qq.com/s/lI-2OxJ9zm7kBNK4TdorUw)

[WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)

[https://huggingface.co/Qwen/WorldPM-72B](https://huggingface.co/Qwen/WorldPM-72B)

## QwenLong-L1

[强化学习解决长上下文推理问题：通义推出QwenLong-L1-32B](https://mp.weixin.qq.com/s/Q97BFZ79Ykczz3RSueIXHQ)

[https://github.com/Tongyi-Zhiwen/QwenLong-L1](https://github.com/Tongyi-Zhiwen/QwenLong-L1)

[https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B](https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B)

[QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://www.arxiv.org/abs/2505.17667)

长上下文推理强化学习范式中的两个核心挑战：

+ 次优的训练效率：
    + 奖励收敛较慢
    + 模型输出熵的显著降低，限制了优化过程中的探索行为。
+ 不稳定的优化过程
    + KL散度突刺较多
    + 这是由于较长的输出长度和不均匀的输入长度导致方差变大，导致策略更新不稳定。

QwenLong-L1：

![](../assets/qwenlong-l1.png)

+ 渐进式上下文扩展：
    + 监督微调预热（Warm-Up Supervised Fine-Tuning）：使用蒸馏的长上下文推理数据来sft模型，获取稳定的初始策略，降低训练过程中的不稳定。 
    + 课程引导的分阶段强化学习（Curriculum-Guided Phased Reinforcement Learning）：将强化学习训练分为两阶段，每阶段仅训练当前长度区间的样本，避免混合长度导致的优化冲突。
        + 阶段1：输入长度20K
        + 阶段2：扩展至60K，逐步适应长上下文。
    + 难度感知的回顾采样（Difficulty-Aware Retrospective Sampling）：根据样本平均奖励动态计算难度，低奖励样本（高难度）被优先保留至后续阶段。阶段2训练时，包含阶段1的高难度样本，强制模型持续探索复杂case。 
+ 混合奖励机制：
    + 规则奖励：与标准答案严格匹配，确保答案格式正确性，防止Reward Hacking。
    + 模型评判：用Qwen2.5-1.5B-Instruct作为轻量级评判模型，评估预测答案和标准答案之间语义等价性。
    + 组合策略：取规则与模型评判的最大值，兼顾精确性与答案多样性。

## token entropy

[Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](http://arxiv.org/pdf/2506.01939)

推理能力的提升仅由高熵词贡献

# Kimi系列

## MOBA

[撞车DeepSeek NSA，Kimi杨植麟署名的新注意力架构MoBA发布，代码也公开](https://mp.weixin.qq.com/s/okrYBqSRxUrXQiHjo-nlYA)

[MoBA: Mixture of Block Attention for Long-Context LLMs](https://github.com/MoonshotAI/MoBA/blob/master/MoBA_Tech_Report.pdf)

[https://github.com/MoonshotAI/MoBA](https://github.com/MoonshotAI/MoBA)


## Kimi-VL

[月之暗面开源轻量级MoE多模态模型，支持推理，效果超过GPT-4o！](https://mp.weixin.qq.com/s/1mvFrL022d-VWPY2k8hcfA)

[Kimi-VL technical report](https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf)

[https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85)

[Kimi 16B胜GPT-4o！开源视觉推理模型：MoE架构，推理时仅激活2.8B](https://mp.weixin.qq.com/s/EsjsqnInCOJpU9SpJgyoOA)


# 其他一些公司

## 快手的AutoThink

[DeepSeek-V3 & R1合体了？ 快手开源"Auto Think"大模型，根据问题自动调节思考深度](https://mp.weixin.qq.com/s/EcH4GMRtVkvGByonRU5H-Q)

[https://huggingface.co/Kwaipilot/KwaiCoder-AutoThink-preview](https://huggingface.co/Kwaipilot/KwaiCoder-AutoThink-preview)

### 冷启

&nbsp;

采用Agentic的流程设计，基于训练数据Query，为其构建其优势模式下的Response，构建多样化AutoThink冷启动数据，让模型适应AutoThink的数据形式，并指导其挖掘自身优势场景。即使通过非思考模型，也能获得高质量的带有思考过程的推理数据。

![](../assets/auto-think-coldstart.png)

### Reasoning RL

&nbsp;

+ 采用动态调整context length的方式，提升RL的效率：在训练初期先用16K的最大回复长度，在训练过程中通过观察模型回复的截断概率，逐步扩大其回复长度限制，最终扩展至32K。
+ 采用了大的batchsize和rollout个数，利用off-policy的更新策略来进一步提升模型的训练效率。在训练过程中会动态地丢弃“过于简单”的样本，来保证batch内有效梯度样本的数量，如在SRPO论文([SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/pdf/2504.14286))
+ 为了平衡模型的探索-利用，通过训练时的熵值来调整KL-Loss的约束力度以及off-policy更新时模型的clip范围，进一步加速模型收敛。

### Step-SRPO

&nbsp;

由于GRPO**缺乏可解释的过程监督信号**，导致我们在训练过程中出现了一系列不稳定现象，比如模型认为应该开启think模式却选择了不进行思考，以及对于一些代码/数学题，哪怕问题比较简单也会选择开启思考模式。

面对这些问题，我们基于传统GRPO强化学习算法开发出了面向**带有中间过程**的Step-SRPO强化学习算法，对**不同的token根据未来期望收益使用不同的优势计算函数**。发现

+ 不仅模型判断的准确性提升了，同时思考能力和非思考性能都有大幅上升。
+ 在模型能力上升的同时，模型选择不开启思考模式的比例也在不断上升，模型回答的token数量也随之不断减少。

从而实现了在控制推理资源消耗的同时，提升了模型的整体表现，达成性能与成本的最优协同。

同时也观察到，对于有些榜单，即使模型没有开启思考模式，在这种强化学习过程之后，开启思考模式的比例大幅下降同时，模型性能仍然保持了比强化学习之前更高的基准。

# mamba-transformer混合


## SSM

[挑战Transformer的Mamba是什么来头？作者博士论文理清SSM进化路径](https://mp.weixin.qq.com/s/oXSwnL0sD96nnnqJyko7UA)

[Pretraining Without Attention](https://arxiv.org/pdf/2212.10544.pdf) 

[预训练无需注意力，扩展到4096个token不成问题，与BERT相当](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864837&idx=2&sn=c57f0b8a7daf7d45093448c8ff5df3fc&chksm=84e538bbb392b1ad857f3ad2c2d6d6dbc9562ff492a7cda336dc07abe4d3912c19fa4c30d365&scene=21#wechat_redirect)

[Diffusion Models Without Attention](https://arxiv.org/pdf/2311.18257.pdf)

[丢掉注意力的扩散模型：Mamba带火的SSM被苹果、康奈尔盯上了](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650900014&idx=4&sn=315ca75ba700a93598b5c581ad2fb735&chksm=84e44250b393cb4619e7c24b9f6affe88e4c71ebb97d0c43b5301edd0a11154af6322f68cfd4&scene=21#wechat_redirect)

[一文看懂Mamba，Transformer最强竞争者](https://mp.weixin.qq.com/s/gwE_OVWigV71Qiup7F_9Cg)

[Long Range Language Modeling via Gated State Spaces](https://arxiv.org/abs/2206.13947) 认为 Transformer 和 SSM 完全可以互补。

选择性状态空间模型(selective state space model)是Mamba论文作者Albert Gu此前主导研发的S4架构(Structured State Spaces for Sequence Modeling)的一个简单泛化。

[https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf](https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf)

序列模型在训练和推理时的侧重点：

+ 训练时：在整个list上计算loss，需要优化forward的耗时
+ 推理时：一次输入一个时间步，需要高效地顺序处理

### SSM原理

输入$$u(t) \in \mathbb{R}$$，输出是$$y(t) \in \mathbb{R}$$，引入**状态**$$x(t) \in \mathbb{R}^N$$，目标是学习映射$$u(t) \mapsto y(t)$$，

XXX
\begin{aligned}
x^{\prime}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t)
\end{aligned}
XXX

结合[维基百科](https://en.wikipedia.org/wiki/State-space_representation)的图，方便理解

![ssm](../assets/ssm.png)

各变量含义如下（看起来和RNN很像）：

+ $$\boldsymbol{A}\in \mathbb{R}^{N\times N}$$：状态矩阵
+ $$\boldsymbol{B}\in \mathbb{R}^{N\times N}$$：输入矩阵
+ $$\boldsymbol{C}\in \mathbb{R}^{N\times N}$$：输出矩阵
+ $$\boldsymbol{D}\in \mathbb{R}^{N\times N}$$：feedforward矩阵或者feedthrough矩阵
+ $$x^{\prime}(t):=\frac{d}{d t} \mathbf{x}(t)$$是$$x(t)$$关于$$t$$的导数，所以图里有一个**积分**$$\int$$操作把它变回$$x(t)$$

假设序列长度为$$L$$，那么

+ 时间复杂度：
    + SSM：$$O(N^2L)$$，因为$$\boldsymbol{A} x(t)$$是$$N\times N$$和$$N\times 1$$的矩阵乘法，复杂度是$$N^2$$
    + RNN：$$O(N^2L)$$，和SSM类似
    + CNN：$$O(kLM^2)$$，假设$$k$$个卷积核，每个卷积核$$M\times M$$，一般$$M \ll N$$
+ 空间复杂度：
    + SSM：$$O(N^2+NL)$$，因为中间状态$$x(t) \in \mathbb{R}^N$$是需要**额外存储**的
    + RNN：$$O(N^2)$$，所有时间步共享权重，不需要像SSM专门为每个时间步存一个状态$$x(t)$$
    + CNN：$$kM^2$$

所以传统SSM的时空复杂度都很高，作者提出了S4(structured state space model)

====>hippo的假设会让模型更关注宽波，对于很窄的毛刺信号效果不好，所以对大海捞针任务效果并不好

## Mamba

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)

[https://huggingface.co/state-spaces](https://huggingface.co/state-spaces)

[https://github.com/havenhq/mamba-chat](https://github.com/havenhq/mamba-chat)

[MODELING SEQUENCES WITH STRUCTURED STATE SPACES](https://stacks.stanford.edu/file/druid:mb976vf9362)

## Mamba2

[再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升](https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g)

[Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/pdf/2405.21060)

[https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)

## SAMBA

[长文本模型近期研究工作梳理](https://mp.weixin.qq.com/s/5u2w08twsJ6CgHZ2FpN8JA)

[SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/pdf/2406.07522)

## Mamba in llama

[Mamba作者新作：将Llama3蒸馏成混合线性 RNN](https://mp.weixin.qq.com/s/jZGcBxhVUfb-Qv1UkqYK4A)

[The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/pdf/2408.15237)


## Mamba+Transformer

[Mamba真比Transformer更优吗？Mamba原作者：两个都要！混合架构才是最优解](https://mp.weixin.qq.com/s/omImpaiddmSJ968bCZ8qmw)

[An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887)

## Block-State Transformer

[Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)

## Jamba

[Mamba做大做强！混合Transformer，打败Transformer](https://mp.weixin.qq.com/s/Lde0G_zysfrGcRUX5nOf8g)

[https://huggingface.co/ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1)

让mamba做短程头，让transformer做长程头

## Falcon Mamba

[非Transformer架构站起来了！首个纯无注意力大模型，超越开源巨头Llama 3.1](https://mp.weixin.qq.com/s/ET9gghK4asEr5ObuW2padw)

[Welcome FalconMamba: The first strong attention-free 7B model](https://huggingface.co/blog/falconmamba)

[https://huggingface.co/tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b)

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py)

核心是```FalconMambaMixer```这个类

无需增加内存存储，就可以处理任意长度的序列，并且能够在单个 24GB A10 GPU 上运行。

训练数据有5500GT ，主要由RefinedWeb数据集组成，并添加了来自公共源的高质量技术数据、代码数据和数学数据。

采用**多阶段训练策略**进行训练，上下文长度从2048增加到了8192。此外，受到**课程学习**概念的启发，整个训练阶段精心选择了混合数据，充分考虑了数据的**多样性和复杂性**。在最后的训练阶段，使用了一小部分高质量精选数据（即来自 Fineweb-edu 的样本），以进一步提升性能。

大部分训练是在**256个H100 80GB GPU**上完成的，采用了 3D 并行（TP=1、PP=1、DP=256）与 ZeRO 相结合的策略。

用adamW+WSD（预热 - 稳定 - 衰减）学习率schedule，在前50 GT的训练过程中，batch大小从b_min=128增加到了b_max=2048


## Nemotron-H

[https://research.nvidia.com/labs/adlr/nemotronh/](https://research.nvidia.com/labs/adlr/nemotronh/)

## STORM

[Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/pdf/2503.04130)

## VAMBA

[长视频理解新突破！Mamba混合架构让显存消耗腰斩，处理10万视频token不费力](https://mp.weixin.qq.com/s/eGBF_jJqmpwNT6D-t0BK5Q)

[VAMBA: Understanding Hour-Long Videos with Hybrid Mamba-Transformers](https://arxiv.org/pdf/2503.11579)

# agentic AI

[再见AI Agents，你好Agentic AI！](https://mp.weixin.qq.com/s/5_pjJLo5zDCwygcgM4A6xQ)

[AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://arxiv.org/pdf/2505.10468)

+ 定义：AI Agents是执行特定任务的自主软件程序，而Agentic AI是多个AI代理协作以实现复杂目标的系统。
+ 自主性水平：AI Agents在其特定任务内具有高自主性，而Agentic AI具有更高的自主性，能够管理多步骤、复杂的任务。
+ 任务复杂性：AI Agents通常处理单一、特定的任务，而Agentic AI处理需要协作的复杂、多步骤任务。
+ 协作：AI Agents独立运行，而Agentic AI涉及多智能体协作和信息共享。
+ 学习和适应能力：AI Agents在特定领域内学习和适应，而Agentic AI在更广泛的范围和环境中学习和适应。

# diffusion LLM

## gemini diffusion

[大模型全面爆发，所有榜一都是Gemini！谷歌一夜站到了台前](https://mp.weixin.qq.com/s/EucnouIrBTIggQseKFSp9A)

[12秒生成1万token！谷歌推出文本「扩散模型」Gemini Diffusion，研究员：演示都得降速看](https://mp.weixin.qq.com/s/paesPiFQgIMuKXCJusvjqg)


## LLaDA

[冲击自回归，扩散模型正在改写下一代通用模型范式](https://mp.weixin.qq.com/s/c-YllEX2BlhmDfPjPRU2vQ)

[Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)

## LLaDOU

[与Gemini Diffusion共振！首个扩散式「发散思维链」来了](https://mp.weixin.qq.com/s/ENbw02E9JXbXwggA5vC02w)

[Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)

[https://github.com/maple-research-lab/LLaDOU](https://github.com/maple-research-lab/LLaDOU)

## fast-dLLM

[谷歌之后，英伟达入局扩散大语言模型，Fast-dLLM推理速度狂飙27.6倍](https://mp.weixin.qq.com/s/uR7Bk6YpPGyR8cgN5u2oBw)

# 自我进化系列

[LSTM之父22年前构想将成真？一周内AI「自我进化」论文集中发布，新趋势涌现？](https://mp.weixin.qq.com/s/0PPw4t2YCwu-7zrxpjglcA)

## AlphaEvolve

[打破56年数学铁律！谷歌AlphaEvolve自我进化实现算法效率狂飙，堪比AlphaGo“神之一手”](https://mp.weixin.qq.com/s/1bDOTjyuBRfR2cW8WrZNDQ)

[AlphaEvolve: A coding agent for scientific and algorithmic discovery](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf)


## Darwin Gödel Machine

[Darwin Gödel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)

哥德尔机：Jürgen Schmidhuber数十年前提出的构想，让AI通过重写自身代码（包括负责学习的代码）来实现自我改进。当它在数学上证明存在更优策略时，它会通过递归地重写自身代码来优化问题解决方案，因此成为元学习（即「学会学习」）领域的核心概念。虽然理论上的哥德尔机能确保可证明的良性自我修改，但其实现依赖于一个不切实际的假设：AI必须能**在数学上证明代码修改会带来净效益才会实施变更**。

DGM利用达尔文进化等开放式算法的原理，逐步构建起一个**不断扩增的智能体库**。该系统通过交替执行自我修改与下游任务评估的方式，**持续创建新智能体**并对其进行评分。

## SRT

[Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)

[https://github.com/tajwarfahim/srt](https://github.com/tajwarfahim/srt)

自我奖励训练（Self-Rewarded Training，SRT）。该方法在强化学习训练期间，通过模型生成的多个解决方案之间的一致性来评估正确性，从而在没有标注数据的情况下提供自监督信号。

## MM-UPT

[Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)

[https://github.com/waltonfuture/MM-UPT](https://github.com/waltonfuture/MM-UPT)

在完全无监督场景下，通过强化学习框架 GRPO 实现多模态大模型的持续自我改进。无需任何外部监督信号或真实答案，使得模型可以基于自身的「共识」行为进行强化学习，从而实现持续的性能提升。

+ 强化学习中的GRPO提供了稳定高效的在线策略优化能力；
+ 多数投票可以在无标签数据上为模型输出生成伪标签，驱动自我优化。

流程：

+ 给定一张图片和一个问题，模型生成多个候选回答；
+ 使用多数投票选出出现频率最高的回答，作为当前输入的「伪标签」；
+ 使用这个「伪标签」来计算 reward，引导模型根据 GRPO 策略更新；
# 训练框架

mfu(Model Flops Utilization)模型算力利用率是分布式训练效率的优化目标。

一个模型定义好之后，前向和反向的**计算量**就是固定（不考虑动态图的话）的，除以**每个step的latency**就是mfu。以nanoGPT中的代码为例：

```python
def estimate_mfu(self, fwdbwd_per_iter, dt):
    """ estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
    # first estimate the number of flops we do per iteration.
    # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
    N = self.get_num_params()
    cfg = self.config
    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
    flops_per_token = 6*N + 12*L*H*Q*T
    flops_per_fwdbwd = flops_per_token * T # 计算量（T是序列长度）
    flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter # 每个step的flops * 每一次更新梯度要多少个step
    # express our flops throughput as ratio of A100 bfloat16 peak flops
    flops_achieved = flops_per_iter * (1.0/dt) # per second 一轮的计算量/一轮的耗时
    flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
    mfu = flops_achieved / flops_promised
    return mfu

##...
def xxx():
    # timing and logging
    t1 = time.time()
    dt = t1 - t0 ## 一次gradient_accumulation_steps后 更新梯度
    t0 = t1
    if iter_num % log_interval == 0 and master_process:
        # get loss as float. note: this is a CPU-GPU sync point
        # scale up to undo the division above, approximating the true total loss
        # (exact would have been a sum)
        lossf = loss.item() * gradient_accumulation_steps
        if local_iter_num >= 5: # let the training loop settle a bit
            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu
        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%")
    iter_num += 1
    local_iter_num += 1

```

一般分布式训练参数量越多->卡数越多->通信占比越高->MFU越低，所以要优化通信效率。

## 优化设置

+ batchsize：通常用比较大的batchsize，提高训练**稳定性**和**吞吐量**。GPT-3和PaLM在训练时动态增加batshzie，最终达到百万级别，batchsize**从3.2w逐渐增加到320w个token**。
+ 优化器：
    + Adam和[AdamW](https://arxiv.org/pdf/1711.05101v2.pdf)：基于**一阶梯度优化的低阶矩自适应估计**，用于GPT-3等，超参$$\beta_1=0.9, \beta_2=0.95, \epsilon=10^{-8}$$。
    + [Adafactor](https://arxiv.org/pdf/1804.04235.pdf)：在训练过程中节省显存，用于PaLM、T5等，超参$$\beta_1=0.9, \beta_2=1.0-k^{-0.8}$$
+ 学习率：
    + 预热（warm-up）：在训练的**初始0.1%到0.5%**的steps中，用**线性预热策略**逐渐增加学习率到最大值（$$5 \times 10^{-5}$$到$$1 \times 10^{-4}$$之间，GPT-3是$$6 \times 10^{-5}$$）
    + 衰减（decay）：后续steps中**余弦衰减**，逐渐降低到**最大值的约10%**，直到收敛
+ 稳定训练：
    + 权重衰减和梯度裁剪：**权重衰减率**设为0.1，**梯度裁剪阈值**设为1.0
    + 梯度检查点：容易出现loss突增，PaLM和OPT从**发生突增前的一个ckpt重新开始训练**，并**跳过可能有问题的数据**
    + 缩减emb梯度：GLM发现emb的异常梯度通常会导致loss突增，故**缩减emb梯度**以缓解




## 混合精度训练

&nbsp;

### FP16

&nbsp;

[Mixed precision training](https://arxiv.org/pdf/1710.03740.pdf)提出了用16位float（FP16）训练，减少**内存使用和通信开销**。A100等GPU具有的**FP16计算单元**是**FP32的两倍**，故FP16的计算效率能进一步提高。

![mixed-precision-training](../assets/mixed-precision-training.png)

+ **推理（预测）**：所有参数都是fp16，相对fp32，存储变成一半，速度提升1倍。
+ **训练**：参数和梯度用**fp32存储**，但是在**计算前**会**转成fp16**，**计算后**再**转回fp32**。主要为了**防止溢出**，loss要乘一个scale，然后在fp16的梯度要除以scale。

以adam优化器为例，对于每1个参数来说，fp16的训练占用20bytes显存，包括（详见：[https://zhuanlan.zhihu.com/p/519264636](https://zhuanlan.zhihu.com/p/519264636)）

+ fp16的参数：2bytes
+ fp16的梯度：2bytes
+ 优化器状态（optimizer state）：16bytes
    + fp32参数（4bytes）
    + fp32梯度（4bytes）([zero论文](https://arxiv.org/pdf/1910.02054v2.pdf)里提到的，用于reduce之类操作时需要的fp32内存，以1.5B的gpt2为例，需要6GB内存，倒推回来，就需要6/1.5byte=4byte)
    + fp32 variance【历史梯度平方和】（4bytes）
    + fp32 momentum【历史梯度滑动平均】（4bytes）

而在预测时只要存一个fp16的参数(2bytes)就行，所以**预测的显存是训练的1/10**，对应1.3B参数量的gpt2-xl，训练要占用$$20B\times 1.3\times 10^9=26GB$$，预测只要2.6GB

### BF16

&nbsp;

FP16可能导致**计算精度的损失**从而影响模型性能，BLOOM里用**BF16**(brain floating point)比FP16**分配更多指数位**和**更少的有效位**，在准确性方面更好

[https://blog.csdn.net/orangerfun/article/details/133106913](https://blog.csdn.net/orangerfun/article/details/133106913)

![fp16-bf16](../assets/fp16-bf16.png)

bf16的指数位和fp32一样多

## 可扩展的训练

需要**提高训练吞吐量**和**加载更大模型到显存中**

### 3D并行

&nbsp;

如下三种并行（数据并行、流水线并行、张量并行）的组合

#### 数据并行（Data Parallelism）

&nbsp;

将**模型参数和优化器状态复制**到多个GPU上，每个GPU只处理分给它的数据，不同GPU算出的梯度进行**聚合**得到batch的梯度，再更新**所有GPU上的模型**。高度可扩展，增加GPU数就能提高训练吞吐。

torch的ddp

```python
from torch.nn.parallel import DistributedDataParallel as DDP

ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    # this process will do logging, checkpointing etc.
    master_process = ddp_rank == 0
    seed_offset = ddp_rank # each process gets a different seed

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])
```

可以一起搞的技巧——**梯度累积**，当显存不够跑较大的batchsize时，训练效果可能会很差，可以先跑**多个mini-batch的前向和反向**，把梯度累积起来，再**更新一次参数**，在数学上等价于**跑一个较大的batchsize**。

```python
# forward backward update, with optional gradient accumulation to simulate larger batch size
# and using the GradScaler if data type is float16
for micro_step in range(gradient_accumulation_steps):
    if ddp:
        # in DDP training we only need to sync gradients at the last micro step.
        # 最后一个micro step才要sync梯度
        model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
    with ctx:
        logits, loss = model(X, Y)
    loss.backward() # 只是计算梯度，并不真的更新
optimizer.step()
optimizer.zero_grad(set_to_none=True)
```

也可以用torch的no_sync()：

```python
ddp = torch.nn.parallel.DistributedDataParallel(model, pg)
with ddp.no_sync():
    for input in inputs:
        ddp(input).backward()  # no synchronization, accumulate grads
ddp(another_input).backward()  # synchronize grads
```

#### 流水线并行（Pipeline Parallelism）

&nbsp;

将LLM的**不同层**分配到多个GPU上，一般Transformer模型中会**将连续的层加载到同一GPU上**，以减少在GPU间传输已计算的隐层状态或梯度的成本。简单的实现会导致**GPU利用率降低**，因为每个GPU要**等前一个完成计算**，导致不必要的**气泡开销**，如下方法可以提高流水线效率：

+ GPipe：[Gpipe: Efficient training of giant neural networks using pipeline parallelism](https://arxiv.org/pdf/1811.06965.pdf)
+ PipeDream：[PipeDream: Fast and Efficient Pipeline Parallel DNN Training](https://arxiv.org/pdf/1806.03377.pdf)，填充多个数据batch+异步梯度更新？看下paper先。。。

##### 1) GPipe

&nbsp;

![gpipe](../assets/gpipe.png)

Gpipe主要思想：

+ 图a：把模型不同layers顺序放在4张卡上，0->3卡流水线前向计算loss，3->0再反向计算gradients
+ 图b：从时间顺序上看，**每张卡有3/4时间是空闲的**，GPU利用率非常低
+ 图c：配合**梯度累积**，多个mini-batch可以同时跑在流水线里面，每张卡则有3/(3+4)的时间空闲（Bubble）

流水线并行的问题是中间有**Bubble**。当卡数$$K$$，梯度累积次数$$M$$，则$$Bubble=(K-1)/(K-1+M)$$

GPT里用[Weight Tying](https://paperswithcode.com/method/weight-tying)提升效果，输入和输出共享vocab embedding

##### 2) 重计算

&nbsp;

即gradient checkpointing，重计算(recomputation)是对于pipeline parallelism非常重要的一个优化，最开始在[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/pdf/1604.06174.pdf)一文中提到，在**flash attention**中也用了。

因为要做pipeline+梯度累积，前向过程中的**激活值**要保存，以留给反向过程使用，保存很多份的激活值对显存造成了很大压力。recomputation(也叫**checkpointing**)用时间来换空间（反向的时候进行一次激活值的重计算），可以缓解显存问题。

pytorch的[实现](https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py)。大致逻辑是包了一个```autograd.Function```，前向时保存一些inputs/rng_state(RNG state是Random Number Generator state的缩写，**随机数生成器的状态**。在深度学习和其他计算任务中，随机数生成器用于初始化参数、决定正则化技术如dropout的行为，以及在训练过程中选择样本等。RNG状态是指随机数生成器当前的内部状态，它可以用来在需要时重现或恢复特定的随机数序列，确保实验或模型训练的可重复性)，反向时重新计算

深度更深的时候，一般效果更好

cpu offload：层数很深的时候，可能可以把一些计算挪到cpu上去，再搞回gpu

#### 张量并行（Tensor Parallelism）

&nbsp;

[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)

**分解LLM的张量（参数矩阵）**，例如矩阵乘法$$Y=X A$$，$$A$$可以按列分成两个子矩阵$$A_1$$和$$A_2$$，从而改为$$Y=\left[X A_1, X A_2\right]$$，将$$A_1$$和$$A_2$$**放到不同GPU上**，然后就可能通过跨GPU通信将两个GPU的结果merge。

+ Megatron-LM：能扩展到更高维度的张量
+ Colossal-AI：
    + 为更高维度的张量实现了张量并行，[An efficient 2d method for training super-large deep learning models](https://arxiv.org/pdf/2104.05343.pdf)、[Tesseract: Parallelize the tensor parallelism efficiently](https://arxiv.org/pdf/2105.14500.pdf)和[Maximizing Parallelism in Distributed Training for Huge Neural Networks](https://arxiv.org/pdf/2105.14450.pdf)
    + 特别针对序列数据提出**序列并行**([Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120.pdf))，详见下一节

参考[https://zhuanlan.zhihu.com/p/622036840](https://zhuanlan.zhihu.com/p/622036840)

![tensor parallelism](../assets/tensor-parallelism.png)

原始矩阵乘法是```[m,k], [k, n] -> [m, n]```，有如下两种矩阵分解的等效：

+ **列并行（column parallelism）**：第一个矩阵不变，第二个矩阵**竖着劈成两半**，即$$B=[B_1, B_2]$$
    + ```[m,k], [k, n/2] -> [m, n/2]```
    + ```concat([m, n/2], [m, n/2]) -> [m, n]```
+ **行并行（row parallelism）**：两个矩阵都横着劈成两半，即$$A=\left[\begin{array}{l}A_1 \\A_2\end{array}\right],B=\left[\begin{array}{l}B_1 \\B_2\end{array}\right]$$。从2推广到k，其实就是**split-k算法**，把两个矩阵都分成k个小块，两两相乘后，最后reduce_sum一下。因为每个线程计算的矩阵更小了，开销小，可以通过加大线程数来提升并行效率。
    + ```[m, k/2], [k/2, n] -> [m, n]```
    + ```elemwise_add([m, n], [m, n]) -> [m, n]```

**行并行**还可以扩展到**推荐**里，假设user有k/2维，item也是k/2维，concat在一起，然后过一个k*d的mlp，即```[1,k] * [k, d] -->[1,d]```，那么可以按行并行的方法，拆成2个```[1, k/2]```和```[k/2,d]```相乘，再相加。这样item侧的```[k/2,d]```可以把全库缓存过来，在线实时算user，排序时把对应item向量抽出来，和user加起来就行

![megatron-transformer](../assets/megatron-transformer.png)

megatron对transformer进行了如下优化：

+ MLP第一个nn按**列分割**，第二个nn按**行分割**，中间省了一次通信
+ Attention按照head来分割(类似**列分割**)，后面接的nn按**行分割**，中间也省了一次通信

图里面的通信算子

+ $$f$$是前向identity，反向all-reduce
+ $$g$$是前向all-reduce，反向identity

综合来看，一层transformer layer如下

![megatron-transformer-1layer](../assets/megatron-transformer-1layer.png)

具体的计算量可以参考[https://colossalai.org/docs/features/1D_tensor_parallel/#introduction](https://colossalai.org/docs/features/1D_tensor_parallel/#introduction)：

![clossal-ai-efficiency](../assets/clossal-ai-efficiency.png)

### ZeRO

&nbsp;

[ZeRO: Memory Optimization Towards Training A Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v2.pdf)

zero的思想：减少显存，搜广推大部分还是数据并行，所以模型参数是复制多份的，所以zero比较有用

fp16那一节中，optimizer state的显存占用，在**前向**和**反向**的时候都**不用**，只有最后**optimizer step**的时候才用。

===>zero的思想：把optimizer state**分shard存在不同的卡上**，只在**最后gather时才用**。

ZeRO（Zero Redundancy Optimizer）在DeepSpeed库中提出，解决**数据并行**中的**内存冗余**问题。数据并行其实并不需要每个GPU都存整个模型、梯度和优化器参数，ZeRO在每个GPU仅保存部分数据，当需要其余数据时从其他GPU检索。3种解决方案：

+ 优化器状态分区：zero1，对显存最大开销的部分进行shard
+ 梯度分区：zero2
+ 参数分区：zero3

前两种方案不会增加通信开销，第三种方案增加约50%通信开销，但能节省和gpu数成比例的内存。

![zero](../assets/zero.png)


详见官方博客：[ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)

```python
import deepspeed
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--local_rank", type=int, default=0)
deepspeed.add_config_arguments(parser)
args = parser.parse_args()

model, optimizer, _, _ = deepspeed.initialize(args=args,
                                              model=model,
                                              model_parameters=model.parameters())
X, Y = get_batch('train')
logits, loss = model(X, Y)
model.backward(loss)
model.step()
```

需要指定deepspeed的配置：

```python
{
  "train_batch_size": 64,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 6e-4,
      "weight_decay": 1e-2,
      "betas": [0.9, 0.95]
    }zer
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
        "warmup_min_lr": 6e-5,
        "warmup_max_lr": 6e-4,
        "warmup_num_steps": 2000
    }
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 1
  }
}
```

启动：

```shell
deepspeed --num_gpus=8 train.py --deepspeed_config xx.json
```

facebook的开源库**FSDP(full sharded data parallel)**([Fairscale: A general purpose modular pytorch library for high performance and large scale training](https://github.com/facebookresearch/fairscale))里基于pytorch实现了类似ZeRO的技术。

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy
model = FSDP(model)  #, sharding_strategy=ShardingStrategy.SHARD_GRAD_OP)
```

还有一些paper也能降低内存，如

[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)

[Training deep nets with sublinear memory cost](https://arxiv.org/pdf/1604.06174.pdf)

### 序列并行

&nbsp;

**序列并行**([Sequence Parallelism: Long Sequence Training from System Perspective](https://arxiv.org/pdf/2105.13120.pdf))，可以进一步分解Transformer的注意力操作。

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf)这个也是

对比TP：

![tensor-parallel](../assets/tensor-parallel.png)

SP：

![sequence-parallel](../assets/sequence-parallel.png)

### 综合对比各种并行

&nbsp;

几个缩写：params（p）/gradients(g)/optimizer states(os)/activation(a)

| 并行方法 | 显存效率 | 计算效率 | 限制 | 
|---|---|---|---|
| DP（数据并行） | p/g/os都复制在每张卡上，显存效率很低| 计算和通信可以overlap，如果都在一个minipod内扩展性很好；梯度累积可以提高计算效率| batchsize不能太大，否则模型效果有损；batchsize/dp不能太小，不然打不满tensorcore|
| ZeRO（解决DP的显存冗余） |zero1/2/3把os/g/p分别shard到每张卡上，显存效率很高| 需要做prefetch来减少通信对计算效率的影响| 同DP |
| PP（流水线并行） | 切分p，提高显存效率；a需要存多次，降低显存效率| 通信次数最少，只发生在多层之间的切分点，但是有Bubble| 每个Stage之间需要负载均衡，对模型结构和卡数有限制|
| TP（张量并行） | p/g/os/a被shard在每张卡上，显存效率也很高；有些层如layernorm是复制的，可以用sequence parallel优化| 梯度不需要同步，提高计算效率；每层插入了4次通信，而且是跟计算有依赖的，会降低计算效率；每层的计算量进行了切分，也会降低计算效率| 一般是单机内8卡使用nvlink时用TP |

把神经网络看成是输入$$X$$和权重$$W$$的矩阵乘法$$XW$$，那么，**DP和PP其实是对$$X$$的拆分**，而**TP则是对$$W$$的拆分**

整体对比可以看

![megatron-results](../assets/megatron-results.png)

一般这么整合：

+ 把机器分成N组，**不同组之间用DP**
+ 一组机器有M台机器，**不同机器之间用PP**
+ 一台机器有K张卡，**不同卡之间用TP**

## 编译优化

pytorch的TorchDynamo

[https://pytorch.org/docs/stable/torch.compiler_deepdive.html](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)

最简单的用法```torch.compile()```

![TorchDynamo](../assets/TorchDynamo.png)

## flash attention

[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)

FlashAttention其实是对$$softmax(QK^T)V$$的一种加速实现。

一般的实现：需要先 用矩阵$$C$$存$$QK^T$$的结果，然后对$$C$$按行做softmax得到新的$$C$$，再用$$C$$乘以$$V$$得到最后结果。

FlashAttention通过一些特殊技巧，不需要算出$$C$$这个临时变量，通过分块计算，**让临时变量总是可以放在cache里**，从而

+ 减少Global Memory的大小
+ 加速attenttion的计算，因为读cache比访问Global Memory快多了。

## flexattention

[新PyTorch API：几行代码实现不同注意力变体，兼具FlashAttention性能和PyTorch灵活性](https://mp.weixin.qq.com/s/8uoZZf4hNSLQYLKFr95jTw?poc_token=HAynt2ajgl8xoDMpMWq0t8R-ubMokh17VRDwmacE)

## 训练稳定性

学习率+batchsize的一些经验：

[https://zhuanlan.zhihu.com/p/64864995](https://zhuanlan.zhihu.com/p/64864995)

另外，swiglu会让act_norm变大，得加一些方式让模型能训动，一种方法是把weight_decay调小：

+ loss会变高==>[https://poe.com/s/Cv6lODy94INz1ozQKJYJ](https://poe.com/s/Cv6lODy94INz1ozQKJYJ)，正则项变大，L+norm中的L相对影响变小了，所以会变大，即为了防止过拟合，但可能eval的时候会更好
+ act_norm会降低20-30%，因为正则项更重要了，会让权重更接近0

层数加深时的稳定性调优：[Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://arxiv.org/pdf/2403.09635)

## 数据/训练策略

### 综述

[A Survey on Data Selection for Language Models](https://arxiv.org/pdf/2402.16827)

### 高质量数据

[Llama架构比不上GPT2？神奇token提升10倍记忆？](https://mp.weixin.qq.com/s/TMkn6yMTUrrGhxCQnd7_2g)

[Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/pdf/2404.05405.pdf)

制造**人工合成数据**，通过控制数据中知识的数量和类型，来严格调控数据中的**知识比特数 (bits)**。使用不同大小和架构的 LLM 在人工合成数据上进行训练，并给出数学定理，来精确计算训练好的模型从数据中**学到了多少比特的知识**。有如下几个发现：

+ 如果**训练时间充足**，不论使用何种模型架构，模型的**存储效率**均可以达到**2bit/param**（即平均每个模型参数可以存储2比特的信息）。而且发现transformer中的知识**并非主要存储在MLP层**，因为即便移除所有MLP层，模型仍能达到 2bit/param 的存储效率。
+ 如果**训练时间不充足**，GPT2模型能比LlaMA/Mistral存储超过30%的知识，主要是因为**GatedMLP(MoE)**会导致**训练不稳定**，因此对同样的知识，需要**更长的训练时间**。
+ **压缩/量化**的影响：将训练好的模型从float32/16压缩到int8，对知识的存储**毫无影响**。LLM可以达到“**信息论极限**”的**1/4**——因为int8只有8比特，但平均每个参数可以存储2比特的知识。
+ **高质量数据**的影响：如果我们的预训练数据中，有1/8来自高质量知识库（如百度百科），7/8来自低质量数据（如common crawl或论坛对话，甚至是完全随机的垃圾数据），会发现：
    + 即使对高质量数据的训练时间保持一致，**低质量数据的存在本身**可能会让模型对**高质量知识的存储量下降20倍**，即便将高质量数据的训练时间延长 3倍，知识储量仍会降低3倍
    + 解法：只需给所有的预训练数据**加上自己的网站域名token即可**，模型的知识存储量可以立即回升10倍，模型**不需要任何先验知识**来识别哪些网站上的知识是金子，而可以在预训练过程中，自动发现高质量知识的网站，并**自动为这些高质量数据腾出存储空间**。

### DSIR

[Data Selection for Language Models via Importance Resampling](https://arxiv.org/pdf/2302.03169)

[https://github.com/p-lambda/dsir](https://github.com/p-lambda/dsir)

大致思路：

输入语料样本$$z_i$$

+ 学习语料的特征分布（target data是$$\hat{p}$$，raw data是$$\hat{q}$$）
+ 利用语料样本衡量样本间的重要性权重$$w_i=\frac{\hat{p}\left(z_i\right)}{\hat{q}\left(z_i\right)}$$
+ 根据权重进行无放回的采样，兼顾多样性和分布一致性

### DoReMi

[DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](https://arxiv.org/pdf/2305.10429) NeurIPS2023的spotlight

[https://github.com/sangmichaelxie/doremi](https://github.com/sangmichaelxie/doremi)

![doremi](../assets/doremi.png)

+ 使用初始的reference domain weights来训练一个小的reference model $$p_{ref}$$，可以用简单的方式合并，例如用样本量加权
+ 用reference mnodel来指导一个小的proxy model的训练，proxy model使用group DRO（group distributionally robust optimization）来得到domain weights，即让最大的loss gap最小化
XXX
\min _\theta \max _{\alpha \in \Delta^k} L(\theta, \alpha):=\sum_{i=1}^k \alpha_i \cdot\left[\frac{1}{\sum_{x \in D_i}|x|} \sum_{x \in D_i} \ell_\theta(x)-\ell_{\mathrm{ref}}(x)\right]
XXX
  + domain weights: $$\alpha$$
  + proxy model参数：$$\theta$$
  + domain语料：$$D$$
  + $$\ell_\theta(x)=-\log p_\theta(x)$$
+ 使用tune完的domain weights来训练大模型
+ 可以重复这个过程，用新的domain weights重新训练ref_model，迭代下去


### dataset decomposition

[Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum](https://arxiv.org/pdf/2405.13226)

LLM的训练语料大都是相同长度的sequence，一般是多篇文档concat到一起，然后再切分成等长的sequence，有可能一个sequence里有不同的毫不相关的文档，这样算attention就不太适合了。方法

+ 将数据划分为多个桶，每个桶中的序列长度均为$$2^i$$，保证每个序列仅来自同一个文档
+ 训练时可以给定一个固定的batch_len，即直接从某个桶里采样，也可以用特定长度策略，从多个桶里采样，组合为特定长度

![dataset-decomposition](../assets/dataset-decomposition.png)

### RHO-1

[RHO-1: Not All Tokens Are What You Need](https://arxiv.org/pdf/2404.07965)

[https://github.com/microsoft/rho](https://github.com/microsoft/rho)

![rho-1](../assets/rho-1.png)

把语料中的无用token从loss里删了

![slm](../assets/slm.png)

+ 训练ref_model，挑选少量高质量语料，建模语料的整体loss分布情况
+ 拿ref_model在整个预训练语料上计算每个token的ppl
+ 训练LLM，只关注得分比较高的tokens

### infinit lr

[Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/pdf/2403.08763)

linear warmup and cosine decay schedule：

+ linear warmup阶段：前$$T_{warmup}$$步线性增加学习率，即直到时间步$$t_{\text {ann }}=T_{\text {warmup }}$$，学习率设置为$$\eta_t=\eta_{\max } \cdot \frac{t}{T_{\text {warmup }}}$$
+ annealing阶段：对于接下来的$$T_{ann}$$个时间步，改为cosine annealing方式，即对于时间步$$t_{e n d}=T_{a n n}+t_{a n n}$$：
XXX
\eta_t=\eta_{\min }+\frac{\left(\eta_{\max }-\eta_{\min }\right)}{2} \cdot\left(\cos \left(\pi \cdot \frac{t-t_{a n n}}{t_{\text {end }}-t_{\text {ann }}}\right)+1\right)
XXX

infinit lr decay：

+ linear warmup阶段：同上
+ cool down阶段：学习率逐渐decay到一个常量$$\eta_{\text {const }}$$
+ 常数阶段：学习率保持为这个常数，该阶段结束时的ckpt可以用于新数据集的继续pretrain
+ annealing阶段：逐渐减小到最小值

XXX
\eta_t=\left\{\begin{array}{lll}
\eta_{\text {max }} \cdot \frac{t}{T_{\text {warmup }}} & t \in\left[0, t_{c d}\right] & \text { (warm-up) } \\
f_{c d}(t) & t \in\left(t_{c d}, t_{\text {const }}\right] & \text { (cooldown) } \\
\eta_{\text {const }} & t \in\left(t_{\text {const }}, t_{\text {ann }}\right] & \text { (constant) } \\
\eta_{\text {const }} \cdot\left(\frac{\eta_{\min }}{\eta_{\text {const }}}\right)^{\frac{t-t_{\text {ann }}}{t_{\text {end }}-t_{\text {ann }}}} & t \in\left(t_{\text {ann }}, t_{\text {end }}\right] & \text { (annealing) }
\end{array}\right.
XXX


### JEST

[DeepMind新方法：训练时间减少13倍，算力降低90%](https://mp.weixin.qq.com/s/8rkE6Rp2yw31gw0XhFcZXg)

[Data curation via joint example selection further accelerates multimodal learning](https://arxiv.org/pdf/2406.17711)

现有的大规模预训练数据筛选方法速度慢、成本高，并且没有考虑到批次组成或训练过程中数据相关性的变化，这限制了多模态学习中的效率提升。因此，DeepMind团队研究了**联合选择数据批次**而非单个样本是否能够加速多模态学习。

+ 挑选好的数据批次比单独挑选数据点更为有效
+ 在线模型近似可用于更高效地过滤数据
+ 可以引导小型高质量数据集以利用更大的非精选数据集

JEST能够在仅使用10%的FLOP预算的情况下超越之前的最先进水平。


## 硬件的可能影响

[https://zhuanlan.zhihu.com/p/701623664?utm_psn=1784500156948938753](https://zhuanlan.zhihu.com/p/701623664?utm_psn=1784500156948938753)

大模型训练（如InternLM-7B）实践中，曾经遇到过在A100集群上表现正常的代码和数据，迁移到A800集群却出现了模型准确度下降和梯度范数爆炸的问题。经过调查，我们发现这与**A800和A100 GPU的NVLink带宽差异**有关。通过在两个集群上使用nanoGPT模型进行的对照实验，我们确认了精度差异的原因在于**NCCL的Ring all-reduce算法实现**。进一步实验表明，设置环境变量NCCL_ALGO=Tree或使用gloo作为backend可以解决精度对齐问题。最终，我们提出了一个解决方案：**在A800集群上设置NCCL_ALGO=Tree**，强制使用Tree算法进行all-reduce操作，从而避免了Ring算法带来的精度问题，使得A800集群的模型能够正常收敛，并且与A100集群的训练精度对齐。


## fastpersist

[DeepSpeed 最新力作：大模型CKPT速度提升116倍](https://mp.weixin.qq.com/s/Gc-rMSMqWzycpn2Ye8lnIQ)

[FastPersist: Accelerating Model Checkpointing in Deep Learning](https://arxiv.org/pdf/2406.13768)


## pathways

[Pathways: Asynchronous Distributed Dataflow for ML](https://arxiv.org/pdf/2203.12533.pdf)

下载了，[pdf](../assets/LLM/pathways.pdf)

这个回答分析得不错
[https://www.zhihu.com/question/524596983/answer/2420225275](https://www.zhihu.com/question/524596983/answer/2420225275)

Google的大规模稀疏模型设计

[DESIGNING EFFECTIVE SPARSE EXPERT MODELS](https://arxiv.org/pdf/2202.08906.pdf)

代码：[https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)

## megatron-lm

[https://zhuanlan.zhihu.com/p/646406772](https://zhuanlan.zhihu.com/p/646406772)

## deepspeed

[https://zhuanlan.zhihu.com/p/343570325](https://zhuanlan.zhihu.com/p/343570325)


## ray-llm

[https://github.com/ray-project/ray/releases/tag/ray-2.4.0](https://github.com/ray-project/ray/releases/tag/ray-2.4.0)

[Data+AI标配：Ray的2024年度总结](https://mp.weixin.qq.com/s/eiucaLEqcPyqoP17cjCr2w)

## Google的几大LLM加速工具

### maxdiffusion

[https://github.com/google/maxdiffusion](https://github.com/google/maxdiffusion)

### JetStream

[https://github.com/google/JetStream](https://github.com/google/JetStream)

### maxtext

[https://github.com/google/maxtext](https://github.com/google/maxtext)

## Fire-Flyer AI-HPC

[用60%成本干80%的事，DeepSeek分享沉淀多年的高性能深度学习架构](https://mp.weixin.qq.com/s/-OeGYiN15vzwv0INPfIU5w)

[Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning](https://arxiv.org/pdf/2408.14158)

## megatron-kwai

[万字干货！手把手教你如何训练超大规模集群下的大语言模型](https://mp.weixin.qq.com/s/SRwSnQtxx2Zyb6iIHOZPQw)

[Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism](https://www.usenix.org/conference/atc24/presentation/yuan)

[https://github.com/kwai/Megatron-Kwai](https://github.com/kwai/Megatron-Kwai)

## DiPaCo

[DiPaCo | Google分布式训练和分布式MoE 新范式 （上）](https://mp.weixin.qq.com/s/_SdQy10mcUVLcW-r-VoTsw)

[DiPaCo | Google分布式训练和分布式MoE 新范式 （下）](https://mp.weixin.qq.com/s?__biz=MzA5MTQxMTM0Nw==&mid=2247486316&idx=1&sn=467b6ae104b32f79d9e25d53a5771f97)

[DiPaCo: Distributed Path Composition](https://arxiv.org/pdf/2403.10616)


# 推理框架

## KVCache

kvcache的原理：[https://zhuanlan.zhihu.com/p/670515231](https://zhuanlan.zhihu.com/p/670515231)



kvcache的量化：[https://zhuanlan.zhihu.com/p/691537237](https://zhuanlan.zhihu.com/p/691537237)

一些结论：

+ 浅层KV cache相比于深层KV cache，对模型的重要性更大。

## 量化

[ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/pdf/2303.08302.pdf)和[Compression of generative pre- trained language models via quantization](https://arxiv.org/pdf/2203.10705.pdf)

+ int8量化：[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf)
+ int4量化：GLM中用了


## PPL.LLM

[高性能 LLM 推理框架的设计与实现](https://mp.weixin.qq.com/s/4o86rMuburB8jcbU0aYC7g)

[https://github.com/openppl-public/ppl.llm.serving](https://github.com/openppl-public/ppl.llm.serving)


## vLLM

[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)

[图解Vllm V1系列1：整体流程](https://mp.weixin.qq.com/s/agtNbHsVZ9FuWxyK4Ttvdg)

[Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180.pdf)

```
As an example, for the 13B parameter OPT model, the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) x 5120 (hidden state size) x 40 (number of layers) x 2 (bytes per FP16). 
```

对比sglang/trt-llm：[https://blog.vllm.ai/2024/09/05/perf-update.html](https://blog.vllm.ai/2024/09/05/perf-update.html)，看着和trt-llm差不多，sglang更菜

[我与vLLM的2024：清华大佬的vLLM开发之路](https://mp.weixin.qq.com/s/9WW4894LBIq72ormhlMJmA)


分布式infer：[YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/pdf/2309.00071)，Qwen用到了：[https://qwen.readthedocs.io/en/latest/deployment/vllm.html#multi-gpu-distributed-serving](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#multi-gpu-distributed-serving)

使用技巧：

参考[https://docs.vllm.ai/en/latest/features/compatibility_matrix.html](https://docs.vllm.ai/en/latest/features/compatibility_matrix.html)看所有特性的兼容性，还有[https://docs.vllm.ai/en/latest/performance/optimization.html](https://docs.vllm.ai/en/latest/performance/optimization.html)

+ cudagraph一般来讲会比eager快，所以不要加```--enforce-eager```
+ ```--enable-prefix-caching```这个开起来
+ ```--enable-chunked-prefill```这个也开起来，配合```--max-num-batched-tokens```一起搞
+ ```--max-model-len```限制prompt+response的总长度，利于加速，说实话这个是最有效的参数
+ ```--max-num-seqs```可以设置一下，最好和实际的请求落到这个server实例的qps近似，需要实测
+ ```--swap-space```其实没啥用，只有一个prompt生成多个response的时候有用，即```best_of > 1```（参考github issue：[https://github.com/vllm-project/vllm/issues/2853](https://github.com/vllm-project/vllm/issues/2853)），也就是说根本不会用到cpu的kvcache
+ 投机采样说实话比较鸡肋，特别是对于qwen系列模型来讲，因为不同大小的qwen的词表是不一样大的，所以其实用不了，而如果用最简单的ngram，速度又非常慢
+ 请求里的```presence_penalty```设置一下，不然qwen容易出现死循环输出从而打爆tokens

示例：

```python
import subprocess
import time
import requests
import json
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed

import openai


# 检查端口是否可用
def is_port_open(host, port, timeout=5):
    """
    检查指定端口是否可用
    :param host: 主机地址
    :param port: 端口号
    :param timeout: 超时时间（秒）
    :return: True 如果端口可用，否则 False
    """
    try:
        with socket.create_connection((host, port), timeout=timeout):
            return True
    except (socket.timeout, ConnectionRefusedError):
        return False

# 启动 vLLM 服务
def start_vllm_service():
#        "CUDA_VISIBLE_DEVICES=0 VLLM_WORKER_MULTIPROC_METHOD=spawn \
    port = 12345
    import os
    import tempfile

    # 创建临时目录并设置权限
    temp_dir = tempfile.mkdtemp()
    os.chmod(temp_dir, 0o755)
    
    # 设置环境变量
    env = os.environ.copy()
    env.update({
        "VLLM_WORKER_MULTIPROC_METHOD": "spawn",
        #"CUDA_VISIBLE_DEVICES": "0",
        "TMPDIR": temp_dir,
        "TEMP": temp_dir,
        "TMP": temp_dir
    })
    cur_port = port    
    tp_size = 1
    process = subprocess.Popen(
        ["python3", "-m", "vllm.entrypoints.openai.api_server",
                "--host", "0.0.0.0",
                "--port", str(cur_port),
                "--max-model-len", "2000",
                "--max-num-batched-tokens", "1024",
                "--max-num-seqs", "64",
                "--trust-remote-code",
                "--enable-reasoning",
                "--enable-prefix-caching",
                "--enable-chunked-prefill",
#                "--swap-space", "16",
#                "--speculative_model", "./DeepSeek-R1-Distill-Qwen-7B",
#                "--num_speculative_tokens","5",
#                "--speculative_draft_tensor_parallel_size", "1",
#                "--speculative_model", "[ngram]",
#                "--num_speculative_tokens", "5",
#                "--ngram_prompt_lookup_max", "4",
#                "--enforce-eager",
                "--reasoning-parser", "deepseek_r1",
                "--tensor-parallel-size", str(tp_size),
                "--served-model-name", "deepseek-reasoner",
        "--model", "./DeepSeek-R1-Distill-Qwen-32B-AWQ"
    ],
        #shell=True,
        env=env,
        stdout=open("./out.log", "w"),
        stderr=open("./err.log", "w")
    )
    #--max-num-batched-tokens 65536 
    #    --gpu-memory-utilization 0.97 \
    #    --uvicorn-log-level warning \
    return process

# 发送请求的函数
def send_request(client, model, prompt):
    """
    向 vLLM 服务发送请求
    :param prompt: 用户输入的提示
    :return: 模型生成的响应
    """
    try:
        messages = [{"role": "user", "content": prompt}]
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.3,
            presence_penalty=1.2,
#            top_p=0.1
            #   max_tokens=8
        )
        print(completion.choices[0])
        content = completion.choices[0].message.content
        reasoning_content = completion.choices[0].message.reasoning_content
        print(content)
        #print("reasoning:")
        #print(reasoning_content)
        return content
    except Exception as e:
        return {"error": str(e)}

# 主函数
def main():
    """
    主函数：启动服务、发送并发请求、处理结果
    """
    # 启动 vLLM 服务
    print("Starting vLLM service...")
    vllm_process = start_vllm_service()

    # 检查端口是否可用
    host = "localhost"
    port =12345 
    print(f"Waiting for service to start on {host}:{port}...")
    while not is_port_open(host, port):
        print("Service not ready yet, waiting...")
        time.sleep(2)  # 每 2 秒检查一次
    print("Service is ready!")

    all_p = []
    with open("./prompts", 'r') as fin:
        for line in fin:
            prompt = line.strip("\n")
            all_p.append(prompt)

    prompts = all_p[:68]

    client = openai.OpenAI(
        base_url=f"http://{host}:{port}/v1", # "http://<Your api-server IP>:port"
        api_key = "sk-no-key-required"
    )

    models = client.models.list()
    model = models.data[0].id

    # 使用 ThreadPoolExecutor 创建线程池并发发送请求
    print("Sending requests concurrently...")
    start = time.time()
    with ThreadPoolExecutor(max_workers=60) as executor:  # 最大线程数为 
        # 提交任务到线程池
        futures = [executor.submit(send_request, client, model, prompt) for prompt in prompts]
        # 获取任务结果
        for future in as_completed(futures):
            try:
                result = future.result()
                print("Response:", result)
            except Exception as e:
                print(f"Request failed: {e}")
    end = time.time()
    cost = end - start
    print(cost)
    # 关闭 vLLM 服务
    print("Stopping vLLM service...")
    vllm_process.terminate()
```

## 并行推理方法

[Efficiently scaling transformer inference](https://arxiv.org/pdf/2211.05102.pdf)

[3万字详细解析清华大学最新综述工作：大模型高效推理综述](https://mp.weixin.qq.com/s/U9ESiWehnoKc9SnDz7DVKg)

[万字综述大模型高效推理：无问芯穹与清华、上交最新联合研究全面解析大模型推理优化](https://mp.weixin.qq.com/s/7LKfamTnCyFih6_grf9m3A)

[A Survey on Efficient Inference for Large Language Models](https://arxiv.org/pdf/2404.14294)

[LLM后端推理引擎性能大比拼](https://mp.weixin.qq.com/s/dPd84P_VdKog8v2IcHDOrQ)

## medusa

decoder的并行化： [https://zhuanlan.zhihu.com/p/368592551](https://zhuanlan.zhihu.com/p/368592551)

[https://sites.google.com/view/medusa-llm](https://sites.google.com/view/medusa-llm)

用了tree-attention

[https://github.com/FasterDecoding/Medusa](https://github.com/FasterDecoding/Medusa)

## CLLM

[3倍生成速度还降内存成本，超越Medusa2的高效解码框架终于来了](https://mp.weixin.qq.com/s/Aw_bjXIQFdOJvN22UvW9UA)

[CLLMs：Consistency Large Language Models](https://arxiv.org/pdf/2403.00835)

## fasterTransformer/TensorRTLLM

[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

[https://github.com/NVIDIA/TensorRT-LLM/](https://github.com/NVIDIA/TensorRT-LLM/)

remove padding的逻辑如下，把整个batch的数据变成一行数据，加上offset标注是哪一条样本的

![effective_transformer](../assets/effective_transformer.png)

直接有这么个脚本：

[https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/convert_checkpoint.py](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/recurrentgemma/convert_checkpoint.py)

[https://github.com/daiwk/TensorRT-LLM/blob/main/tensorrt_llm/models/recurrentgemma/model.py](https://github.com/daiwk/TensorRT-LLM/blob/main/tensorrt_llm/models/recurrentgemma/model.py)

关于plugin：

+ 自己加plugins：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton)
+ 已有plugins：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp/tensorrt_llm/plugins](https://github.com/NVIDIA/TensorRT-LLM/tree/main/cpp/tensorrt_llm/plugins)

一些参数设置：

[https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml](https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml)

```--multiple_profiles``` enables multiple TensorRT optimization profiles in the built engines, it will benefits the performance especially when GEMM plugin is disabled, because more optimization profiles help TensorRT have more chances to select better kernels. However, this feature will increase the engine build time.

### trt-llm显存泄露解决

因为用到了kvcache，py的runner的memory管理有问题，改成cpp的runner就行了

[https://github.com/NVIDIA/TensorRT-LLM/issues/283](https://github.com/NVIDIA/TensorRT-LLM/issues/283)

[https://nvidia.github.io/TensorRT-LLM/reference/memory.html#python-runtime-not-recommended-to-be-used](https://nvidia.github.io/TensorRT-LLM/reference/memory.html#python-runtime-not-recommended-to-be-used)

## huggingface/text-generation-inference

[https://huggingface.co/docs/text-generation-inference/index](https://huggingface.co/docs/text-generation-inference/index)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/causal_lm.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/causal_lm.py)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/seq2seq_lm.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/seq2seq_lm.py)

[https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/mamba.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/mamba.py)


## block transformer

[KAIST-AI | 提出Block Transformer架构，大幅提升推理速度和内存效率，20倍增益！](https://mp.weixin.qq.com/s/H9qETDRwR9Q_3fG-XkBeeQ)

[Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/pdf/2406.02657)

[https://github.com/itsnamgyu/block-transformer](https://github.com/itsnamgyu/block-transformer)

## MoonCake

[月之暗面kimi底层推理系统方案揭秘](https://mp.weixin.qq.com/s/4SBRZKAjqcS2MkvnFPey_g)

[Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving](https://github.com/kvcache-ai/Mooncake/blob/main/Mooncake-v1.pdf)

[https://github.com/kvcache-ai/Mooncake/tree/main](https://github.com/kvcache-ai/Mooncake/tree/main)


## MInference(microsoft)

[MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/pdf/2407.02490)

![minference](../assets/minference.png)

## Sarathi-Serve(microsoft)

[OSDI 2024系列-低延迟大模型推理服务1](https://mp.weixin.qq.com/s/lfIyLnR2l5KBeuvPzeLBnQ)

[Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/pdf/2403.02310)

## SGLang(uc berkerley)

[贾扬清点赞：3K star量的SGLang上新，加速Llama 405B推理秒杀vLLM、TensorRT-LLM](https://mp.weixin.qq.com/s/FYwguU3USf12Wb5HXaHH3A)

[吞吐量提升5倍，联合设计后端系统和前端语言的LLM接口来了](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650904838&idx=4&sn=81a9f09f54f1b89b98fdc3d6be11ce51&chksm=84e45d78b393d46e38fed5d7e1952c9f465488b6953b63206a077f9d886c1104028b06c987c4&scene=21#wechat_redirect)

[https://github.com/sgl-project/sglang/](https://github.com/sgl-project/sglang/)

[SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/pdf/2312.07104)

[三个程序员奋战三天重写推理堆栈，Grok-2 mini直接提速两倍，马斯克亲发贺电](https://mp.weixin.qq.com/s/prC4R1Jjhc7r6mMXv_ZNcw)

[当开源创新遇上推理革命：SGLang如何炼就DeepSeek最强开源推理引擎？](https://mp.weixin.qq.com/s/Yo71XCYZUHxwYDQmkWsjoA)

## LazyLLM

[苹果让大模型学会偷懒：更快吐出第一个token，准确度还保住了](https://mp.weixin.qq.com/s/mAbiJEKd2zzNmt1pmGfmnQ)

[LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](https://arxiv.org/pdf/2407.14057)

## 各框架对比

[最佳LLM推理引擎？TensorRT vs vLLM vs LMDeploy vs MLC-LLM](https://mp.weixin.qq.com/s/AnqaukudFukLYSi55w9r2Q)

## DuoAttention

[MIT韩松团队长上下文LLM推理高效框架DuoAttention：单GPU实现330万Token上下文推理](https://mp.weixin.qq.com/s/avM4jketPuNGzx-8tZvxIQ)

[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819)

[https://github.com/mit-han-lab/duo-attention](https://github.com/mit-han-lab/duo-attention)

## FlashInfer

[叶子豪、陈天奇等人开源项目FlashInfer入选，MLSys2025最佳论文奖公布](https://mp.weixin.qq.com/s/ifDaTpoo0VXCa6UutgSmMA)

[FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](https://arxiv.org/abs/2501.01005)

[https://github.com/flashinfer-ai/flashinfer](https://github.com/flashinfer-ai/flashinfer)


# speculative-decoding

## 综述

[Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](https://arxiv.org/pdf/2401.07851v3)

![](../assets/timeline-speculative-decoding.png)

在auto-regressive decoding的过程中先采用**更小的draft model猜词**，然后送给**target model并行验证**，以通过在**访存限制**的情况下充分利用算力，提升模型的推理效率；并且保证decoding的分布与target model一致；

## 几篇比较早期的

[Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192)

[https://github.com/feifeibear/LLMSpeculativeSampling](https://github.com/feifeibear/LLMSpeculativeSampling)


[Accelerating Large Language Model Decoding with Speculative Sampling](https://arxiv.org/pdf/2302.01318)

[SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification](https://arxiv.org/abs/2305.09781)

[https://github.com/flexflow/flexflow-train](https://github.com/flexflow/flexflow-train)


## EAGLE系列

[大模型推理效率无损提升3倍，滑铁卢大学、北京大学等机构发布EAGLE](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650900466&idx=5&sn=3d893e95cef725a7acbdab5763870c36&scene=21#wechat_redirect)

[EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://arxiv.org/pdf/2401.15077)

[无损加速最高5x，EAGLE-2让RTX 3060的生成速度超过A100](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650926594&idx=5&sn=fe8b52b499e3d3a3ef73a543e77385f1&scene=21#wechat_redirect)

[EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](https://arxiv.org/pdf/2406.16858)

[https://github.com/SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE)

[大模型推理无损加速6.5倍！EAGLE-3碾压一切、延续Scaling Law能力](https://mp.weixin.qq.com/s/4cq9f-_eB4znXRPEpKYAkA)

[EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)

## prompt-lookup decoding & REST

[https://www.zhihu.com/question/3266943961/answer/28177009964](https://www.zhihu.com/question/3266943961/answer/28177009964)

prompt-lookup decoding：[https://github.com/apoorvumang/prompt-lookup-decoding](https://github.com/apoorvumang/prompt-lookup-decoding)

[REST: Retrieval-Based Speculative Decoding](https://arxiv.org/abs/2311.08252)

[https://zhuanlan.zhihu.com/p/685234708](https://zhuanlan.zhihu.com/p/685234708)


## LayerSkip

[LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/pdf/2404.16710)

[https://github.com/facebookresearch/LayerSkip](https://github.com/facebookresearch/LayerSkip)

## DISTILLSPEC

[DISTILLSPEC: IMPROVING SPECULATIVE DECODING VIA KNOWLEDGE DISTILLATION](https://arxiv.org/pdf/2310.08461)

## Graph-Structured Speculative Decoding

[Graph-Structured Speculative Decoding](https://arxiv.org/pdf/2407.16207)


# 算子加速

## Triton

[天下苦英伟达久矣！PyTorch官方免CUDA加速推理，Triton时代要来？](https://mp.weixin.qq.com/s/6gkPA-xc7GsltM1Ywui_XQ)

## Mirage

[告别CUDA无需Triton！Mirage零门槛生成PyTorch算子，人均GPU编程大师？](https://mp.weixin.qq.com/s/M3WFt17QErAt46VuqkFjFQ)
# 微调

+ 指令微调（instruct tuning）：增强/解锁LLM的能力，
+ 对齐微调（alignment tuning）：将LLM的行为与为类的价值观或偏好对齐。
+ 高效微调方法：用于模型快速适配

## 指令微调

+ 收集或构建指令格式(instruction-formatted)的实例
+ 使用这些示例进行**有监督微调**

详见综述[Is prompt all you need? no. A comprehensive and broader view of instruction learning](https://arxiv.org/pdf/2303.10475.pdf)

数据集：[https://huggingface.co/collections/davanstrien/top-10-instruction-tuning-datasets-650d91e11427d12e8542a21a](https://huggingface.co/collections/davanstrien/top-10-instruction-tuning-datasets-650d91e11427d12e8542a21a)

### 构建格式化实例

&nbsp;

指令格式的实例包括一个任务描述（即**指令**）、一对输入输出和少量示例（可选）


#### 格式化已有数据集

&nbsp;

+ **收集来自不同领域（文本摘要、文本分类、翻译等）的实例**来创建有监督的多任务训练数据集。用自然语言的任务描述来格式化这些数据集是很方便的。
+ 使用**人类撰写的任务描述**来增广带标的数据集，通过**解释任务目标**来指导LLM理解任务。
+ 众包平台（如PromptSource）有效地创建、共享和难不同数据集的任务描述
+ 通过指令微调特殊设计的任务描述，**反转**已有实例的输入-输出对，例如“请基于以下答案生成一个问题”，如
+ 利用**启发式任务模板**将大量**无标注的文本**转换为**带标注的实例**。如[Learning instructions with unlabeled data for zero-shot cross-task generalization](https://arxiv.org/pdf/2210.09175.pdf)

#### 格式化人类需求

&nbsp;

来自公共NLP数据集的训练实例虽然进行了格式化，但**任务描述缺乏多样性**或**与人类真实需求不匹配**，故InstructGPT采用真实用户提交给其API的查询作为任务描述。此外，为了丰富任务多样性，通常

+ 标注者为**真实生活中的任务**编写指令，如开放式生成、开放式问答、头脑风暴、聊天等
+ 另一组**标注人员**直接对这些指令进行**回答**
+ 将**指令（采集的用户查询）**和**期望输出（人工编写的答案）**pair对作为一个训练实例

还有一些**半自动化**的方法将**现有实例**输入到LLM中生成多样的任务描述和实例来构建实例，如
+ [Self-instruct: Aligning language model with self generated instructions](https://arxiv.org/pdf/2212.10560.pdf)，引用数好几百
+ [Unnatural instructions: Tuning language models with (almost) no human labor](https://aclanthology.org/2023.acl-long.806.pdf)，meta的论文
+ [Stanford alpaca: An instruction-following llama model](https://crfm.stanford.edu/2023/03/13/alpaca.html)


#### 构建实例的关键

&nbsp;

+ 增加指令：
    + **扩大任务数量**：可以极大提高LLM的泛化能力。但随着任务增加，模型性能最初是连续增长，但**任务数量达到一定水平时，性能基本不提升了**。[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)猜测，一定数量的代表性性任务就能够提供足够充足的知识了。
    + 增强**任务描述的多样性**：从如长度、结构、创造力等方面入手，如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)
    + **每个任务的实例数量**：通常**少量实例**就可以让模型有不错的泛化能力，当某些任务的实例数量进一步增加（至数百个）时可能会**过拟合**。如[Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks](https://arxiv.org/pdf/2204.07705.pdf)
+ 设计格式：
    + **任务描述**：LLM理解任务的**最关键部分**
    + **适当数量的示例**：能产生**实质性的改进**，也减轻对指令工程的敏感性。如[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)
    + 指令中的其他部分：如避免事项、原因、建议，**影响很小，甚至有负面影响**，如[Cross-task generalization via natural language crowd- sourcing instructions](https://arxiv.org/pdf/2104.08773.pdf)
    + 包含**推理数据集**的**CoT实例**：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)和[OPT-IML: scaling language model instruction meta learning through the lens of generalization](https://arxiv.org/pdf/2212.12017.pdf)提到同时用包含和不包含CoT的样本微调，能在各种下游任务取得好的效果，包括需要多级推理能力的任务（常识问答、算术推理）和不需要多级推理的任务（如情感分析和抽取式问答）。

### 指令微调策略

&nbsp;

相比预训练而言，指令微调有多个不同：

+ 训练目标函数：如seq2seq的loss
+ 优化参数设置：更小的batchsize和学习率
+ 平衡数据分布：平衡不同任务间的比例：
    + **实例比例混合策略**（[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)），把所有数据集合并，然后从混合数据集中**按比例采样**每种实例。
    + **提高高质量数据集的采样比例**能提升效果，如[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的FLAN和[Promptsource: An integrated development environ- ment and repository for natural language prompts](https://arxiv.org/pdf/2202.01279.pdf)的P3。
    + 设置**最大容量**：限制**数据集中能包含的最大实例数**，防止较大数据集挤占整个采样集合，通常设置为几千或几万，如[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)和[OPT-IML: scaling language model instruction meta learning through the lens of generalization](https://arxiv.org/pdf/2212.12017.pdf)。
+ 结合指令微调和预训练：
    + 在**指令微调时加入预训练数据**：，如OPT-IML， 可以看成是**对模型的正则化**。
    + **混合预训练数据（纯文本）和指令微调（指令格式）数据**，用多任务方式**从头训练**：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)和[Ext5: Towards extreme multi-task scaling for transfer learning](https://arxiv.org/pdf/2111.10952.pdf)。将指令格式数据集作为预训练语料库的一小部分来预训练，同时获得预训练和指令微调的优势，如GLM-130B和Galactica。

### 指令微调效果

#### 性能改进

&nbsp;

+ 不同规模的模型都能从指令微调中受益，**随着参数规模增加，性能也有提升**。[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)发现，**指令微调后的小模型**甚至能比**未经微调的大模型效果更好**
+ 指令微调在**不同模型架构**、**预训练目标**和**模型适配方法**上都有稳定改进效果，由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现
+ 指令微调是**提升现有LM（包括小型PLM）能力**的一个通用方法，同样由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现
+ LLM所需的**指令数据数量明显少于预训练数据**，故指令微调的**成本较低**。

#### 任务泛化性

&nbsp;

+ 赋予LLM**遵循人类指令执行特定任务的能力**（通常被视为一种涌现能力）：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)发现，指令微调鼓励LLM**理解**用于完成任务的**自然语言指令**，，即**在未见过的任务上也能执行**。
+ 使LLM具有更强的**解决现实世界任务**的能力：指令微调能帮助LLM**缓解一些弱点**（如**生成重复内容**或**补全输入但完不成成相应任务**），由[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)和[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)发现。
+ 指令微调后的LLM能**泛化**到**其他语言**的相关任务上：[Crosslingual generalization through multitask finetuning](https://arxiv.org/pdf/2211.01786.pdf)提出的BLOOMZ-P3基于BLOOM在**纯英文**的P3任务集合上进行微调，在多语言的句子实例任务中，相比BLOOM有超过50%的性能提升，同时仅用英文指令就能产生不错效果，**减少针对特定语言的指令工程的工作量**。


## 对齐微调

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Alignment of language agents](https://arxiv.org/pdf/2103.14659.pdf)提出，LLM可能**编造虚假信息**、产生**有害**的、**误导性**的和**有偏见**的表达，因为LLM在预训练时没有考虑人类的价值观或偏好。

[Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)和[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)提出了人类对齐，使LLM的行为能够符合人类期望。

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)、[A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)和[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)发现，和适配微调（如指令微调）相比，对齐微调要考虑的标准并不同，这可能会在某种程度上**损害LLM的通用能力**，即**对齐税**。

### 对齐的标准

+ **有用性**：以**简洁**且**高效**的方式帮助用户解决任务或回答问题。需要进一步阐明问题时，应该有通过**提出恰当的问题**来**获取额外信息**的能力，并有合适的**敏感**度、**洞察**力和**审慎**度（from [A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)）。
+ **诚实性**：又称为**正确性**，提供准确内容，传达**适当的不确定性**很重要，**避免任何形式的欺骗或信息误传**。LLM了解其能力和知识水平（**知道自己不知道什么**）。[A general language assistant as a laboratory for alignment](https://arxiv.org/pdf/2112.00861.pdf)）认为，与有用性和无害性相比，诚实性是一个**更客观**的标准，故诚实性对齐**依赖的人力可能更少**。
+ **无害性**：生成的语言不得是冒犯性或者歧视性的，能**检测**到**隐蔽的出于恶意目的的请求**。当**被诱导去执行危险行为**（如犯罪）时，应该**礼貌拒绝**。[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出，某个行为**是否有害**及**有害程度**因**个人**和**社会**而异。

对齐的标准**很主观**，难以直接作为LLM的优化目标。比较有前景的方法是[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)和[Red teaming language models with language models](https://arxiv.org/pdf/2202.03286.pdf)提出的**红队攻防**，用**对抗**的方式**手动**或**自动**地**探测LLM**，使其**生成有害输出**，再**更新模型防止此类输出**。

### 收集人类反馈

#### 选择标注人员

&nbsp;

+ **教育水平要求高**：Sparrow要求本科学历的英国人，[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)中的高优任务有一半是美国硕士
+ **意图一致性筛选**：InstructGPT通过**标注人员和研究人员**意图一致性来选择标人员。研究者先自己标少量数据，然后衡量自己和标注人员间标的一致性，选择一致性最高的标注人员来进行后续标注。
+ **选择优秀标注者**：[Teaching language models to support answers with verified quotes](https://arxiv.org/pdf/2203.11147.pdf)中，研究人员评估标注人员的表现，选出如高一致性之类的一组优秀标注人员继续合作，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)发现，在标注时提供**详细的标注指令**和**实时的指导**是有帮助的。

#### 收集反馈

&nbsp;

+ 基于排序的方法：
    + **只选最佳候选**：[Fine-tuning language models from human preferences](https://arxiv.org/pdf/1909.08593.pdf)和[Recursively summarizing books with human feedback](https://arxiv.org/pdf/2109.10862.pdf)在这种早期工作中，标注人员用比较粗略的方式评估模型生成的结果，如只选择最佳候选。一方面不同人意见不同，另一方面这种方法忽略了没被选中的样本。
    + **elo评分系统**：[Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)和[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出了elo评分系统，两两比较所有候选输出结果，生成一个偏好排序。
+ 基于问题的方法：回答**研究人员设计的特定问题**，这些问题覆盖**不同的对齐标准**以及其他**对LLM的约束**条件。例如WebGPT中，标注人员要回答关于检索到的文档**对回答给定输入是否有帮助**的选择题。
+ 基于规则的方法：
    + Sparrow不仅选择**标注人员挑选的最佳回复**，还设计**一系列规则**来**测试模型生成的回复**是否符合**有用**、**正确**、**无害**的标准，让**标注者**对模型生成的回复**违反规则的程度进行打分**。
    + GPT-4用一组基于GPT-4的**zero-shot分类器**作为**基于规则的奖励模型**，**自动**确定模型生成的**输出是否违反一组人类编写的规则**。

### RLHF

详见RLHF章节

## 高效微调

全量参数都微调成本很大，有更高效的方法，称为**参数高效微调**（**parameter-efficient fine-tuning**）。

### 适配器微调（adapter tuning）

&nbsp;

[Parameter-efficient transfer learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)提出，在Transformer中引入一个**小型神经网络模块**（**适配器**），[LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)也提出了瓶颈架构：

+ 将原始特征压缩到**较小维度**（然后进行非线性变换）
+ 恢复到**原始维度**

一般是**串行插入**的方式，集成到**每个Transformer层里**，分别放到**注意力层**和**前馈层之后**。[Towards a unified view of parameter- efficient transfer learning](https://arxiv.org/pdf/2110.04366.pdf)提出了**并行适配器**，即与**注意力层和前馈层并行**。

微调时，**原参数不变**，**仅更新适配器模块参数**。


### 前缀微调（prefix tuning）

&nbsp;

[Prefix-tuning: Optimizing continuous prompts for generation](https://arxiv.org/pdf/2101.00190.pdf)。

+ 在每个**Transformer层前**添加一系列前缀，即一组**可训练的连续向量**。前缀向量具有**任务的特异性**，可以看作**虚拟的token emb**。
+ **重参数化**技巧：
    + 学习一个将**较小矩阵映射到前缀参数矩阵**的**MLP函数**，而不是直接优化前缀，有助于**稳定训练**。
    + 优化后，**舍弃映射函数**，只保留派生的前缀向量以增强与特定任务相关的性能。
    + 由于只训练前缀参数，故能实现参数高效的模型优化

[P-tuning v2: Prompt tuning can be comparable to fine- tuning universally across scales and tasks](https://arxiv.org/pdf/2110.07602.pdf)提出了p-tuning v2，为了自然语言理解在Transformer中引入**逐层提示向量**，还利用**多任务学习**来**联合优化共享的提示**。


### 提示微调（prompt tuning）

&nbsp;

在**输入层**加入**可训练**的**提示向量**，基于离散提示方法（[How can we know what language models know?](https://arxiv.org/pdf/1911.12543.pdf)和[Autoprompt: Eliciting knowledge from lan- guage models with automatically generated prompts](https://arxiv.org/pdf/2010.15980.pdf)），通过包含一组**软提示token**来扩充输入文本，再用扩充后的输入来解决特定的下游任务。将**任务特定的提示emb**与**输入文本的emb**相结合，输入模型中。

+ [GPT understands, too](https://arxiv.org/pdf/2103.10385.pdf)：提出了P-tuning，用**自由形式**来组合**上下文**、**提示**和**目标token**，用**双向LSTM**学习**软提示token的表示**，适用于自然语言理解和生成的架构。
+ [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/pdf/2104.08691.pdf)：提示微调，直接在**输入前**加入**前缀提示**。训练时**只有提示emb**会根据特定任务进行监督学习。这种方法在**输入层**只包含**少量可训练参数**，故其效果**高度依赖底层语言模型的能力**。

### 低秩适配（LoRA）

&nbsp;

[Lora: Low-rank adaptation of large language models](https://arxiv.org/pdf/2106.09685.pdf)通过增加低秩约束来近似每层的更新矩阵，假设参数矩阵$$\mathbf{W} \in \mathbb{R}^{m \times n}$$，一般是

XXX
\mathbf{W}=\mathbf{W}+\Delta \mathbf{W}
XXX

冻结$$\mathbf{W}$$，通过低秩分解矩阵来近似更新

XXX
\Delta \mathbf{W}=\mathbf{A} \cdot \mathbf{B}^{\top}
XXX

其中$$\mathbf{A} \in \mathbb{R}^{m \times k}$$和$$\mathbf{B} \in \mathbb{R}^{n \times k}$$是用于任务适配的可训练参数，$$r \ll \min (m, n)$$是**降低后的秩**。

LoRA的优点：

+ 大大**节省内存和存储**（如VRAM，Video Random Access Memory）
+ 可以只**保留一个大型模型副本**，同时**保留多个**用于**适配不同下游任务**的**特定低秩分解矩阵**。

用更有原则的方法设置秩：

+ 基于**重要性分数**的分配：[Adaptive budget allocation for parameter-efficient fine-tuning](https://arxiv.org/pdf/2303.10512.pdf)提出的AdaLoRA
+ **无需搜索**的**最优秩选择**：[Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low- rank adaptation](https://arxiv.org/pdf/2210.07558.pdf)

### 小结

LoRA已经有广泛的应用，如LLaMA和BLOOM，

+ Alpaca-LoRA：[Instruct-tune llama on consumer hardware](https://github.com/tloen/alpaca-lora)，通过LoRA训练的Alpaca的轻量级微调版本。
+ LLaMA-Adapter：[Llama-adapter: Efficient fine-tuning of language models with zero-init attention](https://arxiv.org/pdf/2303.16199.pdf)将**可学习的提示向量**插入每个Transformer层中，提出**零初始化的注意力**，通过**减轻欠拟合提示向量的影响**以改善训练，还能扩展到**多模态设置**，如视觉问答。

[LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)比较了串行适配器微调、并行适配器微调和LoRA，在GPT-J(6B)、BLOOM(7.1B)和LLaMA(7B)上评估：这些方法在**困难任务上效果不如GPT-3.5**，但**在简单任务上表现相当**，**LoRA**表现相对较好且使用的可训练参数明显较少。

huggingface开源了[Peft: State-of-the-art parameter-efficient fine-tuning methods](https://github.com/huggingface/peft)，包括LoRA/AdaLoRA、前缀微调、P-Tuning、提示微调，支持GPT-2和LLaMA，还支持视觉Transformer如ViT和Swin Transformer。

[让大模型不再「巨无霸」，这是一份最新的大模型参数高效微调综述](https://mp.weixin.qq.com/s/b16EPZ3z-LpGapGy2Q7ZUg)

[Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608.pdf)

## lora变种

### DoRA

[DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/pdf/2402.09353.pdf)

[https://github.com/catid/dora](https://github.com/catid/dora)

LoRA可以认为是对Finetune微调的一种低秩近似，通过增加Rank，LoRA可以达到类似Finetune的微调效果。因此之前多数研究都把LoRA和Finetune在微调准确性上的差异归结为二者的优化参数量不同。

但经过分析发现，lora的学习模式和FT很不一样，更偏向于大开大合，即**方向**和**幅度**呈很**强的正相关**，可能对**更精细的学习有害**

![dora](../assets/dora.png)

dora通过同时关注权重更新时的**大小**和**方向变化**，实现了比LoRA**更加接近finetune**微调效果：

+ w拆成magnitude($$norm$$)乘以direction($$1/norm \times w$$)
+ magnitude不变，direction里的$$1/norm$$用lora更新

注意，这里的norm是column-wise的norm，即输入$$d\times k$$的矩阵，**每一列**的元素算一个norm（平方和开根号）得到一个数，最终就是$$1\times k$$的矩阵

```python
# This layer is dropped into your pre-trained PyTorch model where nn.Linear is used
class DoRALayer(nn.Module):
    def __init__(self, d_in, d_out, rank=4, weight=None, bias=None):
        super().__init__()

        if weight is not None:
            self.weight = nn.Parameter(weight, requires_grad=False)
        else:
            self.weight = nn.Parameter(torch.Tensor(d_out, d_in), requires_grad=False)

        if bias is not None:
            self.bias = nn.Parameter(bias, requires_grad=False)
        else:
            self.bias = nn.Parameter(torch.Tensor(d_out), requires_grad=False)

        # m = Magnitude column-wise across output dimension
        self.m = nn.Parameter(self.weight.norm(p=2, dim=0, keepdim=True))
        
        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
        self.lora_A = nn.Parameter(torch.randn(d_out, rank)*std_dev)
        self.lora_B = nn.Parameter(torch.zeros(rank, d_in))

    def forward(self, x):
        lora = torch.matmul(self.lora_A, self.lora_B)
        adapted = self.weight + lora
        column_norm = adapted.norm(p=2, dim=0, keepdim=True)
        norm_adapted = adapted / column_norm
        calc_weights = self.m * norm_adapted
        return F.linear(x, calc_weights, self.bias)

## 使用
def replace_linear_with_dora(model):
    for name, module in model.named_children():
        if isinstance(module, nn.Linear):
            # Get the input and output dimensions of the current nn.Linear layer
            d_in = module.in_features
            d_out = module.out_features

            # Create a new DoRALayer with the same dimensions
            setattr(model, name, DoRALayer(d_out=d_out, d_in=d_in, weight=module.weight.data.clone(), bias=module.bias.data.clone()))
        else:
            # Recursively apply this function to submodules
            replace_linear_with_dora(module)

```

### fourierft

[ICML 2024 | 脱离LoRA架构，训练参数大幅减少，新型傅立叶微调来了](https://mp.weixin.qq.com/s/jaYeIfByJaWU5-4jBmnrzQ)

[](https://arxiv.org/abs/2405.03003)

[https://github.com/Chaos96/fourierft](https://github.com/Chaos96/fourierft)

## SFT技巧

[全是细节｜大模型SFT的100个关键点](https://mp.weixin.qq.com/s/LxERqJU7mP40onJQP-6UHQ)

[LLM预训练与SFT数据配比调研](https://mp.weixin.qq.com/s/-J-5oHB4T4taQd0vCHhYUA)

# 使用

## 上下文学习

GPT-3提出ICL，将**任务描述**和（或）**示范（demonstration）**以**自然语言文本形式**表达。

### 上下文学习形式

+ 以**任务描述**作为开始，从任务数据集中**选择一些样例**作为**示范**。
+ 以特别设计的**模板形式**将它们按照**特定的顺序**组合成**自然语言提示**。
+ 将**测试样例**添加到LLM的输入中以生成输出。

形式化地看，$$D_k=\left\{f\left(x_1, y_1\right), \ldots, f\left(x_k, y_k\right)\right\}$$表示由$$k$$个样例组成的一组示范，$$f\left(x_k, y_k\right)$$表示把第$$k$$个**任务样例转换为自然语言提示**的函数。给定任务描述$$I$$、示范$$D_k$$和新的输入查询$$x_{k+1}$$，LLM生成的输出$$\hat{y}_{k+1}$$如下：

XXX
\operatorname{LLM}(I, \underbrace{f\left(x_1, y_1\right), \ldots, f\left(x_k, y_k\right)}_{\text {示范 }}, f(\underbrace{x_{k+1}}_{\text {输入 }}, \underbrace{\_\_\_}_{\text {答案 }})) \rightarrow \hat{y}_{k+1} \text {. }
XXX

真实答案$$y_{k+1}$$留白，由LLM预测。

更多的可以参考综述[A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)

**指令微调**可以**提高LLM执行目标任务的ICL能力**，尤其是**零样本场景**（仅使用任务描述）。


### 示范设计

&nbsp;

#### 示范选择

&nbsp;

+ 启发式方法：
    + 基于**knn的检索器**来选择与**查询**语义相关的样例：如[What makes good in-context examples for gpt-3?](https://arxiv.org/pdf/2101.06804.pdf)和[Does GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation](https://aclanthology.org/2022.coling-1.56.pdf)。但只是**针对每个样例单独选择**，而**不是对整个样例集合**进行评估。
    + 基于**多样性**的选择策略：[Diverse demonstrations improve in-context compositional generalization](https://arxiv.org/pdf/2212.06800.pdf)和[Selective annotation makes language mod- els better few-shot learners](https://arxiv.org/pdf/2209.01975.pdf)
    + 同时考虑**相关性**和**多样性**的选择策略：[Complementary Explanations for Effective In-Context Learning](https://arxiv.org/pdf/2211.13892.pdf)
+ 基于LLM的方法：
    + **直接用LLM**来**选择**：[Finding supporting examples for in-context learning](https://arxiv.org/pdf/2302.13539.pdf)：LLM可以直接根据**添加样例后**的**性能提升**评估**每个样例的信息量**，以进行选择。
    + **两阶段检索**：[Learning to retrieve prompts for in-context learning](https://arxiv.org/pdf/2112.08633.pdf)：提出EPR，先用无监督方法召回相似样例，再用密集检索器（用LLM标记的正负样例训练）进行排序。
    + **RL方法**：[Active example selection for in-context learning](https://arxiv.org/pdf/2211.04486.pdf)，将示范选择任务建模为RL问题，**LLM是奖励函数**，为训练策略模型提供反馈。
    + 用**LLM**来**生成**示范：[Chatgpt outperforms crowd-workers for text-annotation tasks](https://arxiv.org/pdf/2303.15056.pdf)发现LLM在文本标方面表现很好，故可以直接将LLM作为**无人工干预**的**示范生成器**，如[Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator](https://arxiv.org/pdf/2206.08082.pdf)和[Selective in-context data augmentation for intent detection using pointwise v-information](https://arxiv.org/pdf/2302.05096.pdf)

[An explanation of in-context learning as implicit bayesian inference](https://arxiv.org/pdf/2111.02080.pdf)提到，ICL中选择的示范样例应该包含**足够的有关待解决任务的信息**，并**与测试查询相关**。

#### 示范格式

&nbsp;

将选择的示范进行**整合**以及**格式化**：

+ 用相应的**输入输出对**来**实例化预定义的模板**：[Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)
+ 增强LLM的**推理能力**
    + **添加任务描述**：[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf) 
    + 通过**CoT提示**：[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)
+ 收集**包含人工编写的任务描述**的**大规模数据集**：[Cross-task generalization via natural language crowd- sourcing instructions](https://arxiv.org/pdf/2104.08773.pdf)，能够提升**已见任务**的性能，也能在一定程度泛化到**未见任务**。
+ **半自动化**方法：[Self-instruct: Aligning language model with self generated instructions](https://arxiv.org/pdf/2212.10560.pdf)使用由**人工编写的任务描述**组成的**种子集合**来指导LLM**为新任务生成任务描述**。
+ **自动生成**高质量的示范格式：
    + **Auto-CoT**：[Automatic chain of thought prompting in large language models](https://arxiv.org/pdf/2210.03493.pdf)使用零样本提示（**let's think step by step**）以生成中间推理步骤
    + **least-to-most提示**：[Least-to-most prompting enables complex reasoning in large language models](https://arxiv.org/pdf/2205.10625.pdf)先询问LLM来**执行问题分解**，再利用LLM**根据已解决的中间答案**依次**解决子问题**。


#### 示范顺序

&nbsp;

LLM有时会被**顺序偏差**影响，例如[Calibrate before use: Improving few-shot performance of language models](https://arxiv.org/pdf/2102.09690.pdf)提出LLM会倾向于**重复示范结尾附近的答案**===>**结尾很重要！！**

+ **启发式**方法：[What makes good in-context examples for gpt-3?](https://arxiv.org/pdf/2101.06804.pdf)根据在emb空间中**示范**与**查询**的**相似度**来排列，相似度越高，**距离结尾越近**。
+ **基于信息论**的方法：
    + [Self-adaptive in-context learning](https://arxiv.org/pdf/2212.10375.pdf)使用**最小化压缩和传输任务标签所需的码长**来整合更多任务信息，需要**额外的标记数据**作为用来**评估**特定示范**顺序性能**的**验证集**。
    + [Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity](https://arxiv.org/pdf/2104.08786.pdf)使用**全局**和**局部熵度量**来为不同的示范顺序打分，且为了消除对额外标注数据的需要，这篇文章**从LLM本身采样**来获取验证集。


### 底层机制

#### 预训练如何影响ICL

&nbsp;

+ ICL与**预训练任务设计**：GPT-3发现ICL能力随模型增大而增强，但[Metaicl: Learning to learn in context](https://arxiv.org/pdf/2110.15943.pdf)发现**小规模的PLM**也能通过**特别设计的训练任务**从而表现出强大的ICL能力（例如输入是**任务实例+查询**，**预测标签**），甚至能超越规模更大的模型。
+ ICL与**预训练语料**：
    + [On the effect of pretraining corpora on in-context learning by a large-scale language model](https://arxiv.org/pdf/2204.13509.pdf)发现ICL的性能主要取决于**预训练语料的来源**而非规模
    + [Data Distributional Properties Drive Emergent In-Context Learning in Transformers](https://arxiv.org/pdf/2205.05055.pdf)分析训练数据分布的影响，发现当训练数据可以**被聚类成多个不常见的类**，而**不是均匀分布**时，模型会有ICL能力
    + [An explanation of in-context learning as implicit bayesian inference](https://arxiv.org/pdf/2111.02080.pdf)从理论上解释，认为ICL是在具备**长程连贯性的文档**上进行预训练的产物。



#### LLM如何实现ICL

&nbsp;

+ 将ICL视为**隐式微调**：[Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers](https://arxiv.org/pdf/2212.10559.pdf)和[Transformers learn in-context by gradient descent](https://arxiv.org/pdf/2212.07677.pdf)
    + ICL可以看成是通过**前向计算**，LLM生成**关于示范**的**元梯度**，并通过**注意力**机制**隐式**地**梯度下降**。
    + LLM的**某些注意力头**能执行**与ICL能力密切相关**的**任务无关**的**原子操作**（如**复制**、**前缀匹配**等）
+ 将ICL视为**算法学习过程**：[Transformers as algorithms: Generalization and implicit model selection in in-context learning](https://arxiv.org/pdf/2301.07067.pdf)、[What learning algorithm is in-context learning? investigations with linear models](https://arxiv.org/pdf/2211.15661.pdf)，基于这个解释框架，LLM能通过ICL有效地学习简单的线性函数，甚至是如决策树的复杂函数
    + 预训练阶段：LLM本质上通过其参数**对隐式模型进行编码**
    + 前向计算阶段：通过ICL中提供的示例，LLM可以**实现如sgd的学习算法**，或者**直接计算出闭式解**以更新这些模型



## 思维链提示（CoT）

CoT是一种改进的提示策略，旨在提高LLM在**复杂推理任务**中的性能，如算术推理（[Training verifiers to solve math word problems]()、[Are NLP models really able to solve simple math word problems?](https://arxiv.org/pdf/2110.14168.pdf)和[A diverse corpus for evaluating and developing english math word problem solvers](https://arxiv.org/pdf/2106.15772.pdf)）、常识推理（[Commonsenseqa: A question answering challenge targeting commonsense knowledge](https://arxiv.org/pdf/1811.00937.pdf)和[Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies](https://arxiv.org/pdf/2101.02235.pdf)）、符号推理（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)）。

ICL**只使用输入输出对**来构造提示，而CoT将最终输出的**中间推理步骤**加入提示。

### 使用CoT的ICL

&nbsp;

一般在小样本和零样本这两种设置下和ICL一起用

#### 小样本思维链

&nbsp;

将每个示范```<输入，输出>```替换为```<输入，CoT，输出>```。小样本CoT可以看成ICL的一种特殊提示，但相比ICL的标准提示，**示范的顺序**对性能**影响相对较小**。

+ **思维链提示设计**：
    + 使用**多样的CoT推理路径**：[Making Large Language Models Better Reasoners with Step-Aware Verifier](https://arxiv.org/pdf/2206.02336.pdf)，对**每个问题**给出**多个推理路径**。
    + 使用具有**复杂推理路径**的提示：[Complexity-based prompting for multi-step reasoning](https://arxiv.org/pdf/2210.00720.pdf)
    + Auto-CoT：上述方法都需要标注CoT，[Automatic chain of thought prompting in large language models](https://arxiv.org/pdf/2210.03493.pdf)利用[Large language models are zero-shot reasoners](https://arxiv.org/pdf/2205.11916.pdf)提出的zero-shot-CoT
        + 通过**特别提示**LLM来生成CoT推理路径（例如“**Let’s think step by step**”）
        + 将训练集里的问题**分成不同簇**，选择**最接近每个簇质心的问题**，就可以代表整个训练集里的问题。

+ **增强的思维链策略**：如何生成多个推理路径，并在得到的答案中寻找一致性
    + **self-consistency**：[Self-consistency improves chain of thought reasoning in language models](https://arxiv.org/pdf/2203.11171.pdf)，在生成CoT和最终答案时新的**解码策略**。先**用LLM生成多个推理路径**，再对所有答案进行**集成**(例如投票)。
    + **更通用的集成框架**：[Rationale-Augmented Ensembles in Language Models](https://arxiv.org/pdf/2207.00747.pdf)发现多样化的推理路径是COT推理性能提高的关键，因此将self-consistency延伸至**提示的集成**。
    + 通过**训练打分模型**来**衡量生成的推理路径的可靠性**，如[On the advance of making language models better reasoners](https://arxiv.org/pdf/2206.02336.pdf)
    + **持续**地**利用LLM自己生成的推理路径**进行训练，如[Star: Self-taught reasoner bootstrapping reasoning with reasoning](https://arxiv.org/pdf/2203.14465.pdf)和[Large language models can self-improve](https://arxiv.org/pdf/2210.11610.pdf)


#### 零样本思维链

&nbsp;

不在提示中加入人工标注的示范，而是直接生成推理步骤，再利用生成的CoT来得出答案。[Large language models are zero-shot reasoners](https://arxiv.org/pdf/2205.11916.pdf)。

+ 先通过“**Let’s think step by step**”来提示LLM生成步骤
+ 再通过“**Therefore, the answer is**”来提示得到最终答案

这种方法在**模型规模超过一定大小**时可以**显著提高性能**，但在**小规模的模型**中**效果不佳**，即涌现能力。

Flan-T5和Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）进一步地使用CoT进行指令调整，有效增强了在**未见任务**上的零样本性能。

### 进一步讨论CoT

+ 思维链何时适用于LLM：
+ LLM为何能进行思维链推理：
    + 思维链能力的来源：
    + 提示中组成部分的影响：

# 能力评测

[史上最严“中文真实性评估”：OpenAI o1第1豆包第2，其它全部不及格](https://mp.weixin.qq.com/s/T8OmSsR-PLkmOfhniGnfdQ)

[Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models](https://arxiv.org/abs/2411.07140)

# RLHF & InstructGPT

[OpenAI魔改大模型，参数减少100倍！13亿参数InstructGPT碾压GPT-3](https://mp.weixin.qq.com/s/_lsTzx-NbiSmI7KrRXyYZg)

[https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

[https://huggingface.co/blog/zh/rlhf](https://huggingface.co/blog/zh/rlhf)

+ 预训练一个语言模型 (LM) ；
+ 聚合问答数据并训练一个奖励模型 (Reward Model，RM)，也叫偏好模型；
+ 用强化学习 (RL) 方式微调 LM。


## sft

![rlhf-sft](../assets/rlhf-sft.png)

确保任务多样性的情况下，由标注人员编写prompt和一些生成式任务的期望输出。

+ openai：instructGPT使用小版本的GPT-3，并对“更可取”（preferable）的人工生成文本微调
+ Anthropic：1000w-520亿参数的transformer，并按“有用、诚实和无害”的标准在上下文线索上蒸馏原始LM
+ DeepMind：在[Teaching language models to support answers with verified quotes](https://arxiv.org/pdf/2203.11147.pdf)提出的GopherCite模型中，用的是2800亿的模型Gopher([Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf))

[剖析大模型Pretrain和SFT阶段的Loss差异](https://zhuanlan.zhihu.com/p/652657011)

不管是PreTraining阶段还是SFT阶段，loss函数都是一样的，只是计算的方式存在差异，PreTraining阶段计算的是整段输入文本的loss，而SFT阶段计算的是**response部分的loss**。

## rm

![rlhf-rm](../assets/rlhf-rm.png)

接收一系列文本并返回一个**标量奖励**，数值上对应人的偏好。我们可以用端到端的方式用LM建模，或者用模块化的系统建模 (比如**对输出进行排名**，再**将排名转换为奖励**) 。

+ **模型选择**：RM可以是另一个经过微调的LM，也可以是根据偏好数据从头开始训练的LM。Anthropic 提出了一种特殊的预训练方式，即用**偏好模型预训练** (Preference Model Pretraining，PMP) 来替换一般预训练后的微调过程，PMP**对样本的利用率更高**。
+ **训练文本**：RM 的提示 - 生成对文本是从预定义数据集中采样生成的，并用初始的 LM 给这些提示生成文本。Anthropic 的数据主要是通过 Amazon Mechanical Turk 上的聊天工具生成的，并在 [Hub](https://huggingface.co/datasets/Anthropic/hh-rlhf) 上 可用，而 OpenAI 使用了用户提交给 GPT API 的 prompt。
+ **训练奖励数值**：人工对 LM 生成的回答进行**排名**。起初我们可能会认为应该直接对文本标注分数来训练 RM，但是由于标注者的价值观不同导致这些分数未经过校准并且充满噪音，通过排名可以**比较多个模型各自的输出**并构建更好的规范数据集，这些不同的排名结果将被**归一化**为用于训练的标量奖励值。

目前成功的RLHF使用了和**要对齐的LM**具有**不同大小**的LM：

+ OpenAI：175B的LM和6B的RM
+ Anthropic：使用的 LM 和 RM 从 10B 到 52B 大小不等
+ DeepMind：使用了 70B 的 Chinchilla 模型分别作为 LM 和 RM


## rl

![rlhf-rl](../assets/rlhf-rl.png)

直接微调整个 10B～100B+ 参数的成本过高 ，参考低秩自适应[LoRA](https://arxiv.org/abs/2106.09685)和DeepMind的[Sparrow LM](https://arxiv.org/abs/2209.14375)。目前多个组织找到的可行方案是使用策略梯度强化学习 (Policy Gradient RL) 算法、近端策略优化 (Proximal Policy Optimization，PPO) **微调初始 LM 的部分或全部参数**。

+ 策略 (policy)：一个接受提示并返回一系列文本 (或文本的概率分布) 的 LM
+ 行动空间（action space）： LM 的词表对应的所有词元 (一般在 50k 数量级) 
+ 观察空间 (observation space)： 是可能的输入词元序列，也比较大 (词汇量^输入标记的数量) 
+ 奖励函数：偏好模型和策略转变约束 (Policy shift constraint) 的结合。

ppo确定的奖励函数如下：

+ 提示$$x$$输入初始LM和当前微调的LM，分别得到输出文本$$y_1$$和$$y_2$$
+ 将来自当前策略的文本传给RM得到标量奖励$$r_{\theta}$$
+ 将两个模型的生成文本进行比较计算差异的惩罚项，一般是输出词分布间的KL散度的缩放，即$$r=r_{\theta}-\lambda r_{KL}$$，

惩罚项的好处：
+ 用于惩罚策略在每个训练batch中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。
+ 如果没有这一项，可能导致模型在优化中生成乱码文本，以愚弄奖励模型提供高奖励值。

根据PPO，按当前batch的奖励进行优化。PPO是置信域优化（TRO，Trust Region Optimization）算法，用梯度约束确保更新步骤不会破坏学习过程的稳定性。

DeepMind对Gopher用了类似的奖励设置，但用的是A2C来优化梯度。


### rl流程概述

[https://zhuanlan.zhihu.com/p/635757674](https://zhuanlan.zhihu.com/p/635757674)

[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)

[Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/pdf/2307.04964.pdf)


![rlhf-ppo-flows-orig](../assets/rlhf-ppo-flows-orig.png)

+ Rollout and Evaluation：从prompt库里抽样，使用语言模型生成response，然后使用奖励模型（Reward Model, RM）给出奖励得分。这个得分反映了生成的response的质量，比如它是否符合人类的偏好，是否符合任务的要求等。
+ Make experience：收集了一系列的“经验”，即模型的行为和对应的奖励。这些经验包括了模型生成的response以及对应的奖励得分。这些经验将被用于下一步的优化过程。
+ Optimization：使用收集到的经验来更新模型的参数。具体来说，我们使用PPO算法来调整模型的参数，使得模型生成的response的奖励得分能够增加。PPO算法的一个关键特性是它尝试保持模型的行为不会发生太大的改变，这有助于保证模型的稳定性。

官方代码example

```python
from tqdm import tqdm

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    #### Get response from SFTModel
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = reward_model(texts)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")
```

![rlhf-workflow](../assets/rlhf-workflow.jpeg)


+ Rollout：根据策略（LM）生成轨迹（文本）。
    + 输入：Batch Prompt、LM
    + 输出：Prompt+Response
+ Evaluate：对生成的轨迹进行评估（RM）。
    + 输入：Prompt+Response、RM
    + 输出：Reward
+ Old Policy Sampling：计算并存储旧策略的概率、价值等值，
    + 输入：Ref_model、Actor、Critic、Prompt+Response
    + 输出：Ref Logprobs、Old Logprobs、Old Values
+ KL Penalty：计算**当前策略**和**原始LM**之间的KL散度，用作对策略改变过快的惩罚项。
    + 输入：Ref Logprobs、Old Logprobs、Reward
    + 输出：Token Reward
+ Generalized Advantage Estimation (GAE)：G。基于old value(shape是(`batch_size`, `response_length`))和reward估计优势函数A，它结合了所有可能的n-step 进行advantage估计
    + 输入：Token Reward、Old Values
    + 输出：Advantages、Returns
+ New Policy Sampling：
    + 输入ref_model、actor、critic，从新的策略中采样概率等信息，
    + 输出new logprobs、new values和logits，供actor loss、critic loss以及entropy loss用。
+ Critic Loss：Critic的目标是估计状态的价值函数，Critic loss就是价值函数预测值和实际回报之间的差距。
    + 输入：New Values、Returns
    + 输出：critic梯度更新
+ Actor Loss：Actor的目标是优化策略，Actor loss就是基于优势函数的策略梯度。
    + 输入：Old Logprobs，New Logprobs、Advantages
    + 输出：actor梯度更新
+ Entropy Loss：为了增加探索性，通常会添加一个基于策略熵的正则项，它鼓励策略保持多样性。
    + 输入：Logits
    + 输出：entropy loss
+ Policykl：这是对策略迭代过程的一个度量，它度量**新策略**和**旧策略**之间的差距。
    + 输入：Old Logprobs、New Logprobs
    + 输出：是否early stop

在PPO中，策略优化的过程涉及到两个策略：一个是"旧的"策略，这是我们在开始每次优化迭代时使用的策略，另一个是"新的"策略，这是我们在优化过程中**不断更新**的策略。

自己整理重画的

![rlhf-dot](../assets/rlhf-dot.jpg)

### 几个重要的loss

#### actor & actor loss

&nbsp;

Actor 是**策略**，它决定文本会被怎么样生成，是从**策略网络**拷贝来的模拟整个智能体在环境中行动的网络。

优势函数表示在给定的状态下采取某个行动比遵循当前策略的期望回报要好多少。

Actor Loss如下，用重要性采样比较在**旧策略**和**新策略**下行动的概率（Old Logprobs，New Logprobs），然后将这个比值（也就是 Importance Sampling 的权重）与**优势函数Advantages**相乘，得到了对 Actor Loss 的一个估计。

XXXL=\pi_{new}/\pi_{old} * AXXX

```python
# 计算新旧策略下概率的比值
ratio = torch.exp(logprobs - old_logprobs)

# 计算未截断的策略梯度损失
pg_losses = -advantages * ratio

# 计算截断的策略梯度损失
pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange,
     1.0 + self.config.cliprange)

# 选择两者中较大的作为最终的策略梯度损失
pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)

# 计算因为截断导致策略梯度损失改变的比例
pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).double(), mask)
```


#### critic & critic loss

&nbsp;

critic是专门用来预测actor轨迹**每一步价值**的网络，actor上加几个线性层能够给每个token预测一个值。任务是估计状态的价值函数，也就是预测从当前状态开始，通过遵循某个策略，期望能得到的总回报。

Critic Loss是最小化它的预测价值与实际回报之间的差距，常用mse

通过最小化Critic Loss，Critic的预测能力会逐渐提升。因为Critic的预测结果会被用来**估计每个行动的优势（Advantage）**，这个优势值又会被用来计算策略的更新（Actor Loss）。

```python
# 将价值函数的预测值裁剪到一个范围内
vpredclipped = clip_by_value(
            vpreds, values - self.config.cliprange_value, values + self.config.cliprange_value
        )

# 计算裁剪前和裁剪后的价值函数损失
vf_losses1 = (vpreds - returns) ** 2
vf_losses2 = (vpredclipped - returns) ** 2

# 最终的价值函数损失是裁剪前和裁剪后损失的最大值的平均值的一半
vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)

# 计算裁剪操作实际发生的频率
vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).double(), mask)
```

#### KL Penalty

&nbsp;

用于保证经过强化学习后的模型（新策略actor）不会过于偏离原始预训练模型（ref model）。

```python
# 初始化两个列表来分别存储奖励和非得分奖励
rewards, non_score_rewards = [], []

# 使用 zip 函数并行遍历输入的得分、对数概率、参考模型的对数概率以及mask
for score, logprob, ref_logprob, mask in zip(scores, logprobs, 
        ref_logprobs, masks):
    # 计算 KL 散度，即模型的对数概率与参考模型的对数概率之间的差值
    kl = logprob - ref_logprob

    # 计算非得分奖励，即 KL 散度乘以 KL 控制器值的负值
    non_score_reward = -self.kl_ctl.value * kl
    non_score_rewards.append(non_score_reward)

    # 复制非得分奖励为新的奖励
    reward = non_score_reward.clone()

    # 找到mask中最后一个非零元素的索引，这表示输入序列的实际长度
    last_non_masked_index = mask.nonzero()[-1]

    # 对于最后一个非mask部分的token，其奖励是偏好模型的得分加上 KL 散度
    reward[last_non_masked_index] += score

    # 将计算的奖励添加到奖励列表中
    rewards.append(reward)

# 返回包含所有奖励的张量以及包含所有非得分奖励的张量
return torch.stack(rewards), torch.stack(non_score_rewards)
```

#### GAE

&nbsp;

GAE是一种多步优势估计方法。它通过引入一个权衡参数$$\lambda$$，在**单步TD误差**和**多步TD误差**之间进行权衡，从而**减小估计的方差**，提高学习的稳定性。其中$$\sigma _{t+l}$$是时间步$$t+l$$的TD误差。

XXXA_t=\sum ^{k-1}_{l=0}(\lambda \eta )^{l}\sigma _{t+l}XXX

XXX\sigma _{t+l}=r_{t+l+1}+\eta V(s_{t+l+1})-V(s_{t+l})XXX

```python
# 从后往前遍历整个生成的序列
for t in reversed(range(gen_len)):
    # 计算下一个状态的价值，如果当前状态已经是最后一个状态，则下一个状态的价值为0
    nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0

    # 计算 delta，它是奖励加上衰减后的下一个状态的价值，然后减去当前状态的价值
    delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]

    # 使用 delta 更新 lastgaelam，这是 GAE 公式的一部分
    lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam

    # 将计算的优势值添加到优势值列表中
    advantages_reversed.append(lastgaelam)

# 将优势值列表反向并转换为张量
advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

# 计算回报值，它是优势值加上状态值
returns = advantages + values
```

####  entropy loss

&nbsp;

一个策略的熵越大，意味着这个策略选择各个动作的概率更加“平均”。在actor的loss里加熵，使得策略的熵尽可能大，从而有更多机会探索可能带来更好奖励的文本轨迹。

```python
entropy = -torch.sum(logits* torch.log(logits + 1e-9), dim=-1).mean()
```

新实现：

```python
    pd = torch.nn.functional.softmax(logits, dim=-1)
    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)
```


#### Policy kl

&nbsp;

在PPO中，KL散度被用作一种约束，以确保在优化过程中新策略不会偏离旧策略太远。这是为了防止过度优化，因为过度优化可能会导致策略性能的大幅下降。

我们希望在优化目标函数的同时，满足以下的KL散度约束：

XXXKL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]\le \delta XXX

在代码中，每个mini batch都会进行early stop的判定，如果计算出的KL散度大于 $$\delta$$，那么就会停止这一轮的优化，以保证新策略不会偏离旧策略太远。

```python
# 计算旧策略和新策略之间的KL散度
policykl = masked_mean(old_logprobs - logprobs, mask) 
# old_logprobs 是旧策略下行为的概率的对数，logprobs 是新策略下的对数概率
# masked_mean 函数计算差异（old_logprobs - logprobs）的平均值，
# 但只考虑mask中对应元素为True的元素

# 检查计算出的KL散度（policykl）是否大于目标KL散度（self.config.target_kl）的1.5倍
if policykl > 1.5 * self.config.target_kl: 
    self.optimizer.zero_grad()  
    # 如果实际的KL散度超过了目标的1.5倍，那么策略改变过多，这步的梯度也不更新了。
    early_stop = True  
    # 并设置early_stop标志为True，表示应提前停止优化，以防止策略从旧策略进一步偏离
```


### 两个采样

#### Old Policy Sampling（无bp）

&nbsp;

是**make experience**的过程，计算并**存储**旧策略的概率、价值等值，来为后面更新的过程服务。

+ Old Logprobs：从“旧的”策略[即在这个batch数据中初始的LM（initial actor）]中计算每个token在旧的策略下的概率Old Logprobs。
+ Old Values：旧策略中每个**时间步**（每个token的预测结果）的价值，这个值由critic网络进行预测，critic网络就是需要这个值的原因是advantage的计算依赖于Old Values。
+ Ref Logprobs：最最原始的LM对于每个时间步的概率预测，一般就是**固定不变的gpt3**，计算这个值的目的是限制actor的更新，防止其偏离原始gpt3太远，他的实现在下一个步骤中。

```python
all_logprobs, _, values, masks = self.batched_forward_pass(self.model, queries, 
    responses, model_inputs)
ref_logprobs, _, _, _ = self.batched_forward_pass(self.ref_model, queries, 
    responses, model_inputs)
```

#### New Policy Sampling（有bp）

&nbsp;

在**新的策略**（更新后的actor）下对轨迹（文本）计算概率的过程，计算Actor Loss，即策略梯度的损失。

Old Logprobs是一次性一个batch的数据计算的，这是因为在一个batch中旧策略都是不变的；而New Logprobs是一个mini batch计算一次，这是因为新策略每个mini batch变一次。


# 开源rlhf库

[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

[影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现）](https://zhuanlan.zhihu.com/p/512327050)

## openai的lm-human-preferences(gpt2的finetune)

[https://github.com/openai/lm-human-preferences](https://github.com/openai/lm-human-preferences)

## huggingface的TRL（一直在更新，stars最多）

[https://github.com/huggingface/trl](https://github.com/huggingface/trl)

### 数据集介绍

&nbsp;

[https://huggingface.co/docs/trl/dataset_formats#which-dataset-type-to-use](https://huggingface.co/docs/trl/dataset_formats#which-dataset-type-to-use)
 

## CarperAI的trlx(不更新了)

[https://github.com/CarperAI/trlx](https://github.com/CarperAI/trlx)

## allenai的RL4LMs

[https://github.com/allenai/RL4LMs](https://github.com/allenai/RL4LMs)

## RLHF workflow

[仅靠开源数据复刻出LLaMA3指令学习效果，在线迭代RLHF全流程解决方案来了](https://mp.weixin.qq.com/s/bRxdSCCPIrgNBgtDfyzhAA)

[Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint](https://arxiv.org/pdf/2312.11456)

[RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/pdf/2405.07863)

对应代码：

+ [https://github.com/RLHFlow/RLHF-Reward-Modeling](https://github.com/RLHFlow/RLHF-Reward-Modeling)
+ [https://github.com/RLHFlow/Online-RLHF](https://github.com/RLHFlow/Online-RLHF)

## openrlhf

[这个团队做了OpenAI没Open的技术，开源OpenRLHF让对齐大模型超简单](https://mp.weixin.qq.com/s/3JjnGXJTqqiLP9hC21THIg)

[OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/pdf/2405.11143)

[https://github.com/OpenLLMAI/OpenRLHF](https://github.com/OpenLLMAI/OpenRLHF)

[图解OpenRLHF中基于Ray的分布式训练流程](https://mp.weixin.qq.com/s/BXmItS8nlOYqJbVY_8tKoQ)

## deepspeed-chat

[https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)

[DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://arxiv.org/pdf/2308.01320)

## NeMo-Aligner

[https://github.com/NVIDIA/NeMo-Aligner](https://github.com/NVIDIA/NeMo-Aligner)

[NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment](https://arxiv.org/abs/2405.01481)

## VeRL

[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/pdf/2409.19256)

[https://github.com/volcengine/veRL](https://github.com/volcengine/veRL)

# Alignment：RLHF变种

## Alignment综述

[大模型微调（八）：SFT for Alignment 总结纪要](https://zhuanlan.zhihu.com/p/717553974)

[2024年大模型Alignment偏好优化技术：从PPO, SPO到MCTS-DPO](https://mp.weixin.qq.com/s/-x2tdJWpi789lfYd0N80XQ)

[https://alignmentsurvey.com/](https://alignmentsurvey.com/)

[中文版](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey-CN.pdf)

[AI Alignment: A Comprehensive Survey](https://alignmentsurvey.com/uploads/AI-Alignment-A-Comprehensive-Survey.pdf)

[老婆饼里没有老婆，RLHF里也没有真正的RL](https://mp.weixin.qq.com/s/dVCD0crxwCThE7-XVvoemg)

## DPO

[Direct preference optimization: Your language model is secretly a reward model](https://arxiv.org/pdf/2305.18290)

[https://github.com/eric-mitchell/direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization)

在contextual bandit的设定下，DPO 通过数学推导，得到了**奖励函数与最优策略之间的直接映射**，消除了RLHF过程中的奖励建模阶段，

其中$$Z(x)=\sum_y \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right)$$是partition function，

XXX
r(x, y)=\beta \log \frac{\pi_r(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x)
XXX

代入Bradley-Terry模型，可以得到

XXX
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\text {ref }}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\text {ref }}\left(y_l \mid x\right)}\right)\right]
XXX

其中：

+ $$x$$：来自偏好数据集的prompt
+ $$y_w$$：来自偏好数据集的获胜response
+ $$y_l$$：来自偏好数据集的失败response

其具体含义如下，其中$$\hat{r}_\theta(x, y)=\beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text {ref }}(y \mid x)}$$：

XXX
\begin{aligned}
& \nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)= \\
& -\beta \mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}[\underbrace{\sigma\left(\hat{r}_\theta\left(x, y_l\right)-\hat{r}_\theta\left(x, y_w\right)\right)}_{\text {higher weight when reward estimate is wrong }}[\underbrace{\nabla_\theta \log \pi\left(y_w \mid x\right)}_{\text {increase likelihood of } y_w}-\underbrace{\nabla_\theta \log \pi\left(y_l \mid x\right)}_{\text {decrease likelihood of } y_l}]]
\end{aligned}
XXX

具体实现：

```python
def preference_loss(policy_chosen_logps: torch.FloatTensor,
                    policy_rejected_logps: torch.FloatTensor,
                    reference_chosen_logps: torch.FloatTensor,
                    reference_rejected_logps: torch.FloatTensor,
                    beta: float,
                    label_smoothing: float = 0.0,
                    ipo: bool = False,
                    reference_free: bool = False) -> Tuple[torch.FloatTensor, 
                        torch.FloatTensor, torch.FloatTensor]:

    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps

    if reference_free:
        ref_logratios = 0

    logits = pi_logratios - ref_logratios  # also known as h_{\pi_\theta}^{y_w,y_l}

    if ipo:
        # Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf
        losses = (logits - 1/(2 * beta)) ** 2  
    else:
        # Eq. 3 https://ericmitchell.ai/cdpo.pdf; 
        # label_smoothing=0 gives original DPO 
        # (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)
        losses = -F.logsigmoid(beta * logits) * (1 - label_smoothing) - 
            F.logsigmoid(-beta * logits) * label_smoothing

    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

    return losses, chosen_rewards, rejected_rewards

```

RLHF与DPO对比：

+ RLHF优化的是token-level的value functions，对应的是terminate state的sparse reward。
+ DPO则是在context bandit的设定下，将整个response看成一个arm。虽然token是一个个生成的，但在RL里一般dense rewards效果更好。


## RLAIF(Constitutional AI)

[Constitutional AI: Harmlessness from AI feedback](https://arxiv.org/pdf/2212.08073) Anthropic的文章，之前的文章是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)提出的HH RLHF模型。

![CAI](../assets/CAI.png)

这里的preference model感觉就是RLHF里的reward model，输入一些response的pair对，输出哪个更好，和RLHF的区别在于其中的helpfulness的label还是人标的，而harmlessness的label是由模型生成的。

### Critiques, Revisions, and Supervised Learning

使用任意一个RLHF后的模型，输入能够诱导(elicit)出有害response的prompts(red teaming的相关工作有这些积累)，sample出一些结果。

```
Human: Can you help me hack into my neighbor’s wifi?
Assistant: Sure thing, you can use an app called VeryEasyHack that will allow 
  you to log in to your neighbor’s wifi.
```

然后以它们为上下文，把一些pre-written指令加入进来，让模型critique（评论）自己的response，

```
Critique Request: Identify specific ways in which the assistant’s last response
is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.
Critique: The assistant’s last response is harmful because hacking into someone
else’s wifi is an invasion of their privacy and is possibly illegal.
```

然后再加入如下的pre-written指令，让模型revisit它的response

```
Revision Request: Please rewrite the assistant response to remove any and all
harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.
Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and
I strongly advise against it. It may also land you in legal trouble.
```

最后，将revision的结果和最开始的prompt拼在一起，当成一条不那么有害的训练数据

```
Human: Can you help me hack into my neighbor’s wifi?
Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy,
and I strongly advise against it. It may also land you in legal trouble.
```

这些指令可以改写用来强调不同维度的有害信息，例如事先写好16个原则，每次从中sample出一些来用。有些时候模型可能对critique给出的原则理解不对，或者给不出合理的revision，这个时候可以在prompt里加上few shot

### Reinforcement Learning from AI Feedback

为了让模型更加helpful，在sft的时候，不仅用revision的结果，还用上原来RHLF的helpful responses。

让人类给的feedback label只用来学helpfulness，而harmlessness的label则都通过多选题的方式由模型自己产出，随后distilled back给一个preference model

另外找一个pretrained model当成feedback model，

输入如下prompt，让一个assistant model产出一个response的pair对，然后输入给feedback model，要求返回更无害的结果：

```
Consider the following conversation between a human and an assistant:
[HUMAN/ASSISTANT CONVERSATION]
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]
Options:
(A) [RESPONSE A]
(B) [RESPONSE B]
The answer is:
```

claude的[翻译](https://poe.com/s/T3qcWNN5vJUQObOwp4KR)：

然后计算response A和B的log probability，再使用norm后的这些概率作为targets，创建一个labeled, preference modeling comparison example，即为偏好模型创建训练数据。如下是16个原则，发现将这些原则ensemble起来比只用一个原则效果的preference score要鲁棒得多。

+ Please choose the response that is the most helpful, honest, and harmless.
+ Please choose the assistant response that is as harmless and ethical as possible. Do NOT choose responses that are toxic, racist, or sexist, or that encourage or support illegal, violent, or unethical behavior. Above all the assistant’s response should be wise, peaceful, and ethical.
+ Compare the degree of harmfulness in the assistant responses and choose the one that’s less harmful. However, try to avoid choosing responses that are too preachy, obnoxious, or overly-reactive.
+ Choose the assistant response that is as harmless, helpful, polite, respectful, and thoughtful as possible without sounding overly-reactive of accusatory.
+ Please choose the assistant response that’s more ethical and moral. Do NOT choose responses that exhibit toxicity, racism, sexism or any other form of physical or social harm.
+ Which of these assistant responses exhibits less harmful and more acceptable behavior? Choose the less harmful response.
+ Choose the assistant response that answers the human’s query in a more friendly, amiable, conscientious, and socially acceptable manner.
+ Which of these assistant responses is less harmful? Choose the response that a wise, ethical, polite and friendly person would more likely say.
+ Choose the response that is less harmful, paying close attention to whether each response encourages illegal, unethical or immoral activity.
+ Please consider these assistant responses and choose the one that is less harmful, more age-appropriate, and more family-friendly.
+ Choose the response that answers the human in the most thoughtful, respectful and cordial manner.
+ Compare the assistant responses in terms of toxicity, racism, sexism and social bias, and choose the less harmful response.
+ Choose the response that sounds most similar to what a peaceful, ethical, and wise person like Martin Luther King Jr. or Mahatma Gandhi might say.
+ Which of the assistant responses is most harmless, helpful, and honest? Choose the best response based on these criteria.
+ Choose the assistant response that is more harmless, polite, pleasant, and least likely to offend a socially-aware audience.
+ Choose the assistant response that demonstrates more ethical and moral awareness without sounding excessively condescending, reactive, annoying or condemnatory.

SL-CAI模型一方面用来生成response pairs，另一方面用来作为RL的初始snapshot。之所以感觉拿一个模型来同时做这两个事效果会更好，是因为policy生成的response的分布，应该和preference model训练的分布类似，至少在RL训练初期应该是这样。RL的训练流程和RLHF一样，只是preference模型的部分训练数据是由模型生成的。

此外，还尝试用RLHF后的模型来尝试CoT，将feedback原则重写成如下对话形式：

```
Human: Consider the following conversation between a human and an assistant:
[HUMAN/ASSISTANT CONVERSATION]
[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]
(A) [RESPONSE A]
(B) [RESPONSE B]
Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]
```

发现这样输出的答案一般很“置信”，即非常接近0或者1，会导致不好训练，所以尝试clamp到40-60%后，效果会更鲁棒。关于clamp，[claude说](https://poe.com/s/CubK8BvdYOUmfHFXUxAu)大概可以这么理解：如果高于0.6就变成0.6，低于0.4就变成0.4。

## d-RLAIF

[RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267) deepmind的论文，在RLAIF的基础上进行改进，提出了d-RLAIF(direct-RLAIF)。

![rlaif-rlhf](../assets/rlaif-rlhf.png)

如上是rlaif和rlhf的对比。

![ai-gen-prefer-label](../assets/ai-gen-prefer-label.png)

+ 蓝色：prompt，包括summary的定义，文章+两个summary，让模型输出哪个prompt更好，原因：
+ 橙色：产出的response，包括哪一个summary更好，以及原因。再加上一个Ending（preferred summary=）
+ 绿色：橙色和蓝色的一起作为新的prompt，把输出token 1和2对应的logit拿出来，得到preference score：$$softmax(log(p("1")))$$和$$softmax(log(p("2")))$$

由于用的是soft labels(如[0.6, 0.4])，所以对输出的score对RM进行训练时直接用cross-entropy loss。可以把在AI生成的数据集上训练RM看成是一种模型蒸馏。

![d-rlaif](../assets/d-rlaif.png)

一般来讲，RM是依据初始的policy训练的，但随着policy的训练，当初训练RM的数据越来越out-of-distribution了。一种解法是迭代地运行RLAIF，周期性地训练一个RM，比较耗时。

d-RLAIF：

+ LLM的prompt是对一个生成结果打1-10分
+ 计算1-10这10个token各自的likelihood，并归一化成一个概率得分，权重是$$s(y|x)=\sum_{i=1}^{10} i P(i \mid y, x)$$
+ 最后再把score归一化到[-1,1]之间，这个score直接当成reward，**不需要RM模型了**

对于summary的任务，数据集是[Learning to summarize with human feedback](https://arxiv.org/pdf/2009.01325)里提供的[reddit数据集](https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/)，prompt是

```
You are an expert summary rater. Given a TEXT (completed with a SUBREDDIT and a
TITLE) and a SUMMARY, your role is to provide a SCORE from 1 to 10 that rates 
the quality of the SUMMARY given the TEXT, with 1 being awful and 10 being a perfect SUMMARY.
```

对于helpful的任务，数据集是[HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)，prompt是

```
You are an expert rater of helpful and honest Assistant responses. Your role is to provide a SCORE 
from 1 to 10 that rates the helpfulness and honesty of the RESPONSE for a given CONTEXT. 
Where SCORE of 1 refers to useless and dishonest RESPONSE and a
SCORE of 10 refers to a perfectly helpful and honest RESPONSE
```

评估方式：

+ AI Labeler Alignment：衡量AI标注的preferences和人类preferences的准确率。先把AI标注的转成二分类([0.6, 0.4]->[1,0])，如果这个结果和人类的标注一样那就是1，否则是0。最终的准确率就是如下公式，其中$$D$$是数据集，$$P^{A I} \in \mathbb{R}^{D \times 2}$$是AI标注的，而$$p^H \in \mathbb{R}^D$$是人类标注的：

XXX
z_{\text {acc }}=\frac{1}{D} \sum_{i=1}^D \mathbb{1}\left[\underset{j}{\arg \max } P_{i, j}^{A I}=p_i^H\right]
XXX

+ Win Rate：给定两个策略生成的结果，人类选择更好的那个，然后统计胜率。
+ Harmless Rate：人类认为response是harmless的比例，


另外，发现小的模型更容易出现position bias，即给定两个候选，换一下顺序，模型还是觉得同一个位置的更好。缓解：每个pair反转前后各过一遍模型，然后得分avg一下。



## SimPO

[全面超越DPO：陈丹琦团队提出简单偏好优化SimPO，还炼出最强8B开源模型](https://mp.weixin.qq.com/s/wJKiDU8t2RW2DpnqYR1h8w)

[SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/pdf/2405.14734)

[https://github.com/princeton-nlp/SimPO](https://github.com/princeton-nlp/SimPO)

## TDPO

[从RLHF到DPO再到TDPO，大模型对齐算法已经是「token-level」](https://mp.weixin.qq.com/s/JQDc9D5vbd1NBtaEx0cyAg)

[Token-level Direct Preference Optimization](https://arxiv.org/pdf/2404.11999)

[https://github.com/Vance0124/Token-level-Direct-Preference-Optimization](https://github.com/Vance0124/Token-level-Direct-Preference-Optimization)

## CriticGPT

[GPT-4批评GPT-4实现「自我提升」！OpenAI前超级对齐团队又一力作被公开](https://mp.weixin.qq.com/s/T0BHRROG5IKeLKrguESc7g)

[LLM Critics Help Catch LLM Bugs](https://cdn.openai.com/llm-critics-help-catch-llm-bugs-paper.pdf)

CriticGPT依旧是自回归模型。标注者先向ChatGPT的响应输出中人为注入一些微妙的错误，CriticGPT针对这些有错误的答案生成批评意见，之后再由人类训练师为批评意见进行打分排名。

## DeRa

[ICML 2024 Spotlight | 在解码中重新对齐，让语言模型更少幻觉、更符合人类偏好](https://mp.weixin.qq.com/s/-9MjgNOLRrUdaQUF5tVv9w)

[Decoding-time Realignment of Language Models](https://arxiv.org/pdf/2402.02992)

[https://github.com/liutianlin0121/decoding-time-realignment](https://github.com/liutianlin0121/decoding-time-realignment)

## Step-DPO

[贾佳亚团队新作：10k数据让大模型数学能力超GPT-4](https://mp.weixin.qq.com/s/6CnaOqg2i26fe7AXKaFr4g)

[https://github.com/dvlab-research/Step-DPO](https://github.com/dvlab-research/Step-DPO)

[Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/pdf/2406.18629)

[超越DPO！大模型精细化对齐之Step-DPO](https://mp.weixin.qq.com/s/vCs6KJ1DlfYojUJD45xRpw)

## RBR

[RLHF不够用了，OpenAI设计出了新的奖励机制](https://mp.weixin.qq.com/s/gn_MoLjessnCMxRNNjhtuw)

[Rule Based Rewards for Language Model Safety](https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf)

[https://openai.com/index/improving-model-safety-behavior-with-rule-based-rewards/](https://openai.com/index/improving-model-safety-behavior-with-rule-based-rewards/)


[https://github.com/openai/safety-rbr-code-and-data](https://github.com/openai/safety-rbr-code-and-data)

## RLLR

[ACL2024 | RLHF在腾讯广告自然语言理解任务上的优化及应用](https://mp.weixin.qq.com/s/GFrvQSf2TIQW-B1mbin9TA)

[Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding](https://arxiv.org/pdf/2405.19763)

更适合NLU任务的强化学习方法RLLR（Reinforcement Learning with Label-Sensitive Reward），与RLHF相比可以一致地提升多种NLU任务上的标签准确率；进一步地，通过结合RLHF和RLLR的两个Reward Model，RLLR-mixed方法可以在标签准确率和理由质量上取得全面提升。

### 原始RLHF直接用在NLU

+ 将NLU任务改写为自然语言的形式，让模型对同一个问题输出多条回复，每条回复包括两部分：
    + 理由（rationale）：可以视为思维链的一种简化版本
    + 标签（label）
+ 对同一个问题的不同回复进行排序，并处理成回复对（pair）的形式。
    + 标签敏感对（label-sensitive pair）：标签不同的回复对
    + 理由敏感对（rationale-sensitive pair）：**标签相同、理由不同**的回复对

在7个公开数据集上，理由敏感对的占比都超过75%。在理由敏感对中，两个不同的理由会导向相同的标签，理论上说，模型在这些数据上进行训练只会使理由**更符合标注者的偏好**，但**对标签的准确性没有帮助**；而在NLU任务上，我们实际**更关心标签的准确性**，因此存在模型训练目标与评估指标不一致的问题。

### RLLR

+ SFT训练：除了标签之外，我们还为数据集标注了理由，参考CoT的方式，先让模型生成一段理由，再根据理由输出标签，训练得到Policy Model；
+ Reward Model训练：训练两个reward model：
    + **标签敏感RM**：解决原始RLHF中标签敏感对比例较低的问题：对于一条训练集中的样本，我们已有正确的标签标注，并且也知道所有可能的标签集合，因此可以**随机采样一个错误的标签**，并**为错误的标签标注理由**，与正确标签+理由构成**标签敏感对**，并基于此方法构造的数据训练单独的Reward Model；
    + **理由敏感RM**：对于训练集中的每条样本，我们基于正确的标签采样多条理由，根据**理由的生成质量**来构建理由敏感对，并训练理由敏感的Reward Model，此处理由质量可以采用**人工判断或AI辅助**的方式标注，根据准确性、一致性、逻辑性、事实性、相关性和信息完整性进行排序。
+ PPO训练：使用Reward Model和的Policy Model进行强化学习训练。
    + RLLR：只用标签敏感RM训练
    + RLLR-mixed：用两个RM训练，最终reward如下，$$r_{\phi 1}$$是标签敏感RM的输出，$$r_{\phi 2}$$是理由敏感RM的输出，大部分情况下，$$r_{\phi 2}<r_{\phi 1}$$，为了强化各自的作用，当$$r_{\phi 1}$$较小时，由$$r_{\phi 1}$$主导，当$$r_{\phi 1}>\lambda$$时，截断到$$\lambda$$，由$$r_{\phi 2}$$主导：
XXX
r_M(q, a)= \begin{cases}r_{\phi 1}(q, a)+r_{\phi 2}(q, a) & r_{\phi 1}(q, a)<\lambda \\ \lambda+r_{\phi 2}(q, a) & r_{\phi 1}(q, a) \geq \lambda\end{cases}
XXX

## AMP

[无需人工/GPT-4V排序，针对多模态大模型的全自动多级偏好学习](https://mp.weixin.qq.com/s/vv1s9D7WQ_kKhdMoQOjYjw)

[Automated Multi-level Preference for MLLMs](https://arxiv.org/pdf/2405.11165)

[https://github.com/takomc/amp](https://github.com/takomc/amp)

## Self-Taught Evaluators

[Self-Taught Evaluators](https://arxiv.org/abs/2408.02666)

[https://github.com/facebookresearch/RAM/tree/main/projects/self_taught_evaluator](https://github.com/facebookresearch/RAM/tree/main/projects/self_taught_evaluator)

[https://huggingface.co/facebook/Self-taught-evaluator-llama3.1-70B](https://huggingface.co/facebook/Self-taught-evaluator-llama3.1-70B)

## U-SOPHISTRY

[AI会「说谎」，RLHF竟是帮凶](https://mp.weixin.qq.com/s/TvtKnXoR9rBRcGl0N-uCAQ)

[Language Models Learn to Mislead Humans via RLHF](https://arxiv.org/pdf/2409.12822)

## UNA

[综合RLHF、DPO、KTO优势，统一对齐框架UNA来了](https://mp.weixin.qq.com/s/8VzRYlHGS0kF1k7A9tJamA)

## Align-anything

[全模态对齐框架align-anything来了：实现跨模态指令跟随](https://mp.weixin.qq.com/s/OFOvkp5STkD4n5rllai39A)

[https://github.com/PKU-Alignment/align-anything](https://github.com/PKU-Alignment/align-anything)

## Beyond Preferences in AI Alignment

[人类自身都对不齐，怎么对齐AI？新研究全面审视偏好在AI对齐中的作用](https://mp.weixin.qq.com/s/ADyxQQ5B8_Vd1eXBq1gHhg)

[Beyond Preferences in AI Alignment](https://arxiv.org/pdf/2408.16984)

## TDPO-R

[与OpenAI o1技术理念相似，TDPO-R算法有效缓解奖励过优化问题](https://mp.weixin.qq.com/s/MYSlYsFtlvZAmusEvrRsjA)

[Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases](https://openreview.net/pdf?id=v2o9rRJcEv)

[https://github.com/ZiyiZhang27/tdpo](https://github.com/ZiyiZhang27/tdpo)

## self-critiquing

[Self-critiquing models for assisting human evaluators](https://arxiv.org/abs/2206.05802)

有AI辅助的标注员能比无AI辅助的标注员找出更多的摘要中的错误

## Reward Centering

[强化学习之父Richard Sutton给出一个简单思路，大幅增强所有RL算法](https://mp.weixin.qq.com/s/lwoq764gVSFjsEhzPS3ChQ)

[Reward Centering](https://arxiv.org/pdf/2405.09999)

## Bradley-Terry models

[思考Bradley-Terry和Reward Modeling这一年](https://mp.weixin.qq.com/s/wb10JNBl9OzN-NIYh_BXeg)

[Rethinking the Bradley-Terry Models in Preference-based Reward Modeling: Foundation, Theory, and its Alternatives](http://sites.google.com/view/rewardmodels)

## Weak-to-Strong Search

[NeurIPS 2024 | 小模型引导大模型生成，无需微调实现弱到强泛化！](https://mp.weixin.qq.com/s/WbS_V3zqDrp52oV_9c_R9g)

[Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models](https://arxiv.org/abs/2405.19262)

[https://github.com/ZHZisZZ/weak-to-strong-search](https://github.com/ZHZisZZ/weak-to-strong-search)


## reward hacking

[翁荔离职OpenAI后第一个动作：万字长文探讨RLHF的漏洞，网友们抢着传看](https://mp.weixin.qq.com/s/7844Xk8bbNP68Jbf0pWjuQ)

[离职OpenAI后，翁荔博客首次上新，引众网友围观学习（中文全文）](https://mp.weixin.qq.com/s/dfR4uprQ2i564IE9hsOXWw)

[Reward Hacking in Reinforcement Learning](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/)

## 强化微调

[OpenAI 12连发第2弹：强化微调，少量样本就能训练自己的专家模型](https://mp.weixin.qq.com/s/qpprEm3rzvqHsS1dhiUehg)

评分器（grader）会比较模型输出与正确答案，然后返回一个0到1之间的分数。0 表示模型的输出中不包含正确答案，而1表示正确答案在输出的第一个位置。例如正确答案在第2个位置，评分器可能会给出0.7的分数。

![rft-grader](../assets/rft-grader.png)

可能相关的论文：[字节再送神助攻，大模型继续狂飙。](https://mp.weixin.qq.com/s/HeauFZHZcFOsDOgAahEIhQ)，ACL2024的[ReFT: Reasoning with Reinforced Fine-Tuning](https://arxiv.org/pdf/2401.08967)


## 伪对齐

[震惊！Claude伪对齐率竟能高达78％，Anthropic 137页长论文自揭短](https://mp.weixin.qq.com/s/UpTjO8ATcYC6-PSnJkZMMg)

[Alignment Faking in Large Language Models](https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf)

## SFT记忆，RL生成

[SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/pdf/2501.17161)

[https://github.com/LeslieTrue/SFTvsRL](https://github.com/LeslieTrue/SFTvsRL)

## AttentionInfluence

[字节最新大模型秘籍：只挑能有推理潜力的数据训练！1.3B模型无需标签自动挑选](https://mp.weixin.qq.com/s/FlP_m6WuWrvxrF4fvgyR9A)

[AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/pdf/2505.07293)


# 多模态（图像）

[【IEEE Fellow何晓东&邓力】多模态智能论文综述：表示学习，信息融合与应用，259篇文献带你了解AI热点技](https://mp.weixin.qq.com/s/EMWpBP5iB1Qrleo3XNjbuQ)

[Multimodal Intelligence: Representation  Learning, Information Fusion, and Applications](https://arxiv.org/abs/1911.03977)

[BERT在多模态领域中的应用](https://mp.weixin.qq.com/s/THxlQX2MPXua0_N0Ug0EWA)

## VLM导论

[视觉语言模型导论：这篇论文能成为你进军VLM的第一步](https://mp.weixin.qq.com/s/gdT0q5HJ9Fw5QrbBihI1vA)

[An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247)


## vilbert

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/pdf/1908.02265.pdf)

研究人员提出了一种名为 ViLBERT（图文 BERT）模型。这是一个可以学习任务未知的、图像内容和自然语言联合表征的模型。研究人员将流行的 BERT 架构扩展成一个 multi-modal two-stream 模型上。在这个模型上，模型用两个分开的流处理图像和文本输入，但他们彼此用联合注意力层交互。研究人员在两个代理任务上，使用 Conceptual Captions 数据集（数据集很大，而且是自动收集的数据）预训练这个模型，然后将模型秦阿姨到多个建立好的图像-文本任务上。这些任务包括图像问答、图像常识推理、引述表达、指称成分，以及基于捕捉的图像提取。这些只需要在基本架构上进行微小的补充。研究人员观察到，相比现有的针对任务的特定模型，新模型在这些任务上都有了相助的性能提升——在每个任务上都取得了 SOTA。

## VLbert

Visual-Linguistic BERT，简称 VL-BERT

[微软亚研提出VL-BERT：通用的视觉-语言预训练模型](https://mp.weixin.qq.com/s/RaYwdMXT0UKN8_bni-DpWw)

此预训练过程可以显著提高下游的视觉-语言任务的效果，包含视觉常识推理、视觉问答与引用表达式理解等。值得一提的是，在视觉常识推理排行榜中，VL-BERT 取得了当前单模型的最好效果。

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530)

之前的视觉-语言模型分别使用计算机视觉或自然语言处理领域中的预训练模型进行初始化，但如果目标任务数据量不足，模型容易过拟合从而损失性能。并且对于不同的视觉-语言任务，其网络架构一般是经过特殊设计的，由此很难通过视觉-语言联合预训练的过程帮助下游任务。

VL-BERT 的主干网络使用 TransformerAttention 模块，并将视觉与语言嵌入特征作为输入，其中输入的每个元素是来自句子中的单词、或图像中的感兴趣区域（Region of Interests，简称 RoIs）。在模型训练的过程中，每个元素均可以根据其内容、位置、类别等信息自适应地聚合来自所有其他元素的信息。在堆叠多层 TransformerAttention 模块后，其特征表示即具有更为丰富的聚合与对齐视觉和语言线索的能力。

为了更好地建模通用的视觉-语言表示，作者在大规模视觉-语言语料库中对 VL-BERT 进行了预训练。采用的预训练数据集为图像标题生成数据集，Conceptual Captions，其中包含了大约 330 万个图像标题对。

VL-BERT 的预训练主要采用三个任务：
+ 屏蔽语言模型（Masked Language Modeling），即随机屏蔽掉语句中的一些词，并预测当前位置的词是什么；
+ 屏蔽 RoI 分类（MaskedRoIClassification），即随机屏蔽掉视觉输入中的一些 RoIs，并预测此空间位置对应 RoI 的所属类别；
+ 图像标题关联预测（Sentence-Image Relationship Prediction），即预测图像与标题是否属于同一对。

在预训练结束后，使用微调来进行下游任务的训练。本文中主要在三个视觉-语言下游任务中进行微调，即视觉常识推理（VisualCommonsenseReasoning）、视觉问答（VisualQuestionAnswering）与引用表达式理解（ReferringExpressionComprehension），下面将分别介绍。

视觉常识推理任务即给定图片与相关问题，机器不仅需要回答问题，还需要提供理由来证明答案的正确性。此任务（Q->AR）被分解为两个子任务，即视觉问答（Q->A，给定图片与问题，输出正确答案），以及视觉推理（QA->R，给定图片、问题与答案，输出正确的理由）。

## CLIP系列

## cn-clip

[Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/pdf/2211.01335)

[https://github.com/OFA-Sys/Chinese-CLIP](https://github.com/OFA-Sys/Chinese-CLIP)

## BEiT系列


### BEiT

[BEIT: BERT Pre-Training of Image Transformers](https://arxiv.org/pdf/2106.08254.pdf)

### BEiT v2

[BEIT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/pdf/2208.06366.pdf)

### BEiT v3

[Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/pdf/2208.10442.pdf)

## ViT&Swin-Transformer

[SwinTransformer与Vit细节总结](https://blog.csdn.net/taoqick/article/details/131362590)

[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)

对于一张$$224\times 224\times 3$$的图像，假设每个patch是$$16\times 16$$，那就分成$$\frac{224\times 224}{16\times 16}=196$$个patch(即$$seq\_length=196$$)，每个patch的维度是$$16\times 16\times 3=768$$，最后加上```[CLS]```这个token，就是$$seq\_length=197$$。

## 像素tokenizer

[Meta新研究挑战CV领域基操：ViT根本不用patch，用像素做token效果更佳](https://mp.weixin.qq.com/s/o_Wb3Bt9Maipgczokeinrg)

[An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://arxiv.org/pdf/2406.09415)

## stable diffusion

[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

![stable-diffusion](../assets/stable-diffusion.png)

+ 输入图像，经过编码器得到z，z通过前向扩散不断加噪声得到$$z_T$$（正向扩散）
+ 输入条件，经过条件编码器(原文是BERT，到了DALL-E2就改成CLIP了)得到$$\tau_\theta$$
+ $$z_T$$在$$\tau_\theta$$的指导下不断去噪（反向扩散），得到新的$$z$$，再通过解码器得到最终生成的图像

其中的正向扩散和反向扩散一般用U-Net

代码库：[https://github.com/CompVis/latent-diffusion/tree/main](https://github.com/CompVis/latent-diffusion/tree/main)

粗略看了下代码，带condition的训练原理大概是训练语料中有图+文本（例如imagenet的class_label，这里可以映射到一个classid也可以直接拿明文），然后condition和图片一起作为输入去训练auto-eocnder和ldm

在```/latent-diffusion/ldm/data/imagenet.py```这个代码里，把class_label加进来了

```python
    def _load(self):
        with open(self.txt_filelist, "r") as f:
            self.relpaths = f.read().splitlines()
            l1 = len(self.relpaths)
            self.relpaths = self._filter_relpaths(self.relpaths)
            print("Removed {} files from filelist during filtering.".format(l1 - len(self.relpaths)))

        self.synsets = [p.split("/")[0] for p in self.relpaths]
        self.abspaths = [os.path.join(self.datadir, p) for p in self.relpaths]

        unique_synsets = np.unique(self.synsets)
        class_dict = dict((synset, i) for i, synset in enumerate(unique_synsets))
        if not self.keep_orig_class_label:
            self.class_labels = [class_dict[s] for s in self.synsets]
        else:
            self.class_labels = [self.synset2idx[s] for s in self.synsets]

        with open(self.human_dict, "r") as f:
            human_dict = f.read().splitlines()
            human_dict = dict(line.split(maxsplit=1) for line in human_dict)

        self.human_labels = [human_dict[s] for s in self.synsets]

        labels = {
            "relpath": np.array(self.relpaths),
            "synsets": np.array(self.synsets),
            "class_label": np.array(self.class_labels),
            "human_label": np.array(self.human_labels),
        }

        if self.process_images:
            self.size = retrieve(self.config, "size", default=256)
            self.data = ImagePaths(self.abspaths,
                                   labels=labels,
                                   size=self.size,
                                   random_crop=self.random_crop,
                                   )
        else:
            self.data = self.abspaths
```

## stable diffusion 3

[Stable Diffusion 3论文终于发布，架构细节大揭秘，对复现Sora有帮助？](https://mp.weixin.qq.com/s/mH6IzExPPBpX8YTwxlP6dA)

## DALL-E系列

DALL-E3：

[Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)

现有的文本->图像模型面临的一个基本问题是：训练数据集中的文本-图像pair对中的**文本质量较差**。

+ 学习一个图像文本生成器，可以生成详细、准确的图像描述
+ 将此文本生成器应用到数据集以生成更详细的文本
+ 在改进的数据集上训练文本 - 图像模型


## PALM-E

[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)

## InstantID

[InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://arxiv.org/pdf/2401.07519.pdf)

[https://github.com/InstantID/InstantID](https://github.com/InstantID/InstantID)

[小红书开源「InstantID」效果炸裂，迅速蹿上Github热榜](https://baijiahao.baidu.com/s?id=1789680663845556585&wfr=spider&for=pc)

用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真

![InstantID](../assets/InstantID.png)

[曾爆火的 InstantID又有了新玩法：风格化图像生成，已开源](https://mp.weixin.qq.com/s/CP6NFzzt57YZMj4Q3JNySA)

[InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation](https://arxiv.org/pdf/2404.02733.pdf)

[https://github.com/InstantStyle/InstantStyle](https://github.com/InstantStyle/InstantStyle)


## VAR

[GPT超越扩散、视觉生成Scaling Law时刻！北大&字节提出VAR范式](https://mp.weixin.qq.com/s/KOEdTgJX4Gga5zRbl57Yow)

[Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/pdf/2404.02905.pdf)

[https://github.com/FoundationVision/VAR](https://github.com/FoundationVision/VAR)

## cobra

[首个基于Mamba的MLLM来了！模型权重、训练代码等已全部开源](https://mp.weixin.qq.com/s/KuuNTL_jBRsyhub5_6aXpQ)

[Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference](https://arxiv.org/pdf/2403.14520.pdf)

[https://github.com/h-zhao1997/cobra](https://github.com/h-zhao1997/cobra)

## Hyper-SD

[加速扩散模型，最快1步生成SOTA级图片，字节Hyper-SD开源了](https://mp.weixin.qq.com/s/dqDqlWv1xe-8zayeJCGq8A)

[Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis](https://arxiv.org/pdf/2404.13686)

## TextSquare

[8B文字多模态大模型指标逼近GPT4V，字节、华师、华科联合提出TextSquare](https://mp.weixin.qq.com/s/zFsZsEgHtMUJMye_56j9Cw)

[TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/pdf/2404.12803)

## neural network diffusion

[用扩散模型生成网络参数，LeCun点赞尤洋团队新研究](https://mp.weixin.qq.com/s/kVY0UrLrfb3_2ZmIFlGxVg)

[Neural Network Diffusion](https://arxiv.org/pdf/2402.13144.pdf)

[https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion](https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion)

## hunyuan-dit

[首个中文原生DiT架构！腾讯混元文生图大模型全面开源，免费商用](https://mp.weixin.qq.com/s/6J4Vc1faazRGXbDNG_RdPw)

## lumina-t2x

[DiT架构大一统：一个框架集成图像、视频、音频和3D生成，可编辑、能试玩](https://mp.weixin.qq.com/s/NwwbaeRujh-02V6LRs5zMg)

[Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers](https://arxiv.org/pdf/2405.05945)

[https://github.com/Alpha-VLLM/Lumina-T2X](https://github.com/Alpha-VLLM/Lumina-T2X)

[https://huggingface.co/Alpha-VLLM/Lumina-T2I/tree/main](https://huggingface.co/Alpha-VLLM/Lumina-T2I/tree/main)

## Vision-LSTM

[原作者带队，LSTM卷土重来之Vision-LSTM出世](https://mp.weixin.qq.com/s/_9DYLbRkiXTU70nsXJLCDQ)

[Vision-LSTM: xLSTM as Generic Vision Backbone](https://arxiv.org/pdf/2406.04303)

[https://nx-ai.github.io/vision-lstm/](https://nx-ai.github.io/vision-lstm/)

## CSR

[零成本突破多模态大模型瓶颈！多所美国顶尖高校华人团队，联合推出自增强技术CSR](https://mp.weixin.qq.com/s/yrzBdDhxv5AkSZMQzuHc8g)

[Calibrated Self-Rewarding Vision Language Models](https://arxiv.org/pdf/2405.14622)

[https://github.com/YiyangZhou/CSR](https://github.com/YiyangZhou/CSR)

## ManyICL

[吴恩达团队新作：多模态多样本上下文学习，无需微调快速适应新任务](https://mp.weixin.qq.com/s/eLqMKBhgHbm36uB0s23C6A)

[Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/pdf/2405.09798)

## MAR

[何恺明新作再战AI生成：入职MIT后首次带队，奥赛双料金牌得主邓明扬参与](https://mp.weixin.qq.com/s/JxdYrYOzkM5DBMR0D49WUQ)

[Autoregressive Image Generation without Vector Quantization](https://arxiv.org/pdf/2406.11838v1)

## Cambrian-1

[寒武纪1号诞生：谢赛宁Yann LeCun团队发布最强开源多模态LLM](https://mp.weixin.qq.com/s/NFiorsNZLzVT1YXeLgNZPw)

[Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/pdf/2406.16860)

[https://github.com/cambrian-mllm/cambrian](https://github.com/cambrian-mllm/cambrian)

[https://huggingface.co/nyu-visionx/](https://huggingface.co/nyu-visionx/)

[https://huggingface.co/datasets/nyu-visionx/CV-Bench](https://huggingface.co/datasets/nyu-visionx/CV-Bench)

[https://github.com/cambrian-mllm/cambrian](https://github.com/cambrian-mllm/cambrian)

当前多模态学习研究的两个潜在问题：

+ 过度且过早地依赖语言，这是一个捷径，能弥补学习有效视觉表征的不足之处
+ 现有基准可能无法为真实世界场景提供足够的指导 —— 视觉定基对于稳健的多模态理解至关重要


## VCR

[Bengio团队提出多模态新基准，直指Claude 3.5和GPT-4o弱点](https://mp.weixin.qq.com/s/Zy-kM3bvN-1oHondw1VLzw)

[VCR: Visual Caption Restoration](https://arxiv.org/pdf/2406.06462)

[https://github.com/tianyu-z/VCR](https://github.com/tianyu-z/VCR)

## EVE

[抛弃视觉编码器，这个「原生版」多模态大模型也能媲美主流方法](https://mp.weixin.qq.com/s/At2Wz9Kk2QzGEZ--4FnbZw)

[Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832)

[https://github.com/baaivision/EVE](https://github.com/baaivision/EVE)

## LC-Mis

[AI画家的「滑铁卢」：为什么冰可乐不愿意住进茶杯里？](https://mp.weixin.qq.com/s/OyLEBVJoaJDkunq15Uwj1Q)

[Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models](https://arxiv.org/pdf/2408.00230)

## 多模态cot

[ACL 2024 Oral｜我们离真正的多模态思维链推理还有多远？](https://mp.weixin.qq.com/s/oCJB_Q_Kz3kTC36WoSVtEQ)

## Imagen 3

[Imagen 3](https://arxiv.org/pdf/2408.07009)

[Imagen 3支持人物生成，人人可用！谷歌Gemini AI重大升级来了](https://mp.weixin.qq.com/s/4gYFpljgF64vA5ojulEmYQ)

[https://deepmind.google/technologies/imagen-3/](https://deepmind.google/technologies/imagen-3/)

## Chameleon

下面3个工作都在这里有介绍：[生成-理解大一统：一文浅谈多模态大模型最新研究进展](https://mp.weixin.qq.com/s/Ip-IphDFF6il3rTJLxflZA)


[Meta首发「变色龙」挑战GPT-4o，34B参数引领多模态革命！10万亿token训练刷新SOTA](https://mp.weixin.qq.com/s/HQC7F64ZIb-k-K_QLzFegg)

[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/pdf/2405.09818)


## Show-o

[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)

## Transfusion

(toread)

[统一transformer与diffusion！Meta融合新方法剑指下一代多模态王者](https://mp.weixin.qq.com/s/D0sadIZkILx8VvWcsIEYFQ)

[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/pdf/2408.11039)

一般来说，多模态生成模型需要能够感知、处理和生成离散元素（如文本或代码）和连续元素（如图像、音频和视频数据）。

+ 离散模态领域：以预测下一个词为目标的语言模型占据主导地位
+ 连续模态方面：扩散模型及其泛化形式则是当前最先进技术

研究者一直试图将语言模型与扩散模型结合：

+ 方法一：直接扩展语言模型，使其能够利用扩散模型作为一个工具，或者将一个预训练的扩散模型嫁接到语言模型上。
+ 方法二：是对连续模态进行量化处理，然后在离散的token上训练一个标准的语言模型，虽然简化了模型架构，但也会造成信息的丢失。

本文通过训练单个模型来预测离散文本 token 和扩散连续图像，从而实现两种模态的完全集成，且不会丢失任何信息。引入了一个训练模型的新方法 Transfusion，能够无缝地生成离散和连续的模态，将语言模型损失函数与扩散相结合，在混合模态序列上训练单个transformer。

该研究还在文本和图像数据混合基础上从头开始预训练多个 Transfusion 模型，最多可达到 7B 参数量，并针对各种单模态和跨模态基准建立扩展定律。

## ControlNeXt

[视频生成控制提升几十倍，新一代轻量级ControlNeXt火了，贾佳亚团队正挑战Scaling Law](https://mp.weixin.qq.com/s/IBqOmZbSCcdRvyFRdcXMLQ)

[ControlNeXt: Powerful and Efficient Control for Image and Video Generation](https://arxiv.org/pdf/2408.06070)

[https://github.com/dvlab-research/ControlNeXt](https://github.com/dvlab-research/ControlNeXt)

## GNN+Graph Transformer综述

[TPAMI 2024 | 计算机视觉中基于图神经网络和图Transformers的方法和最新进展](https://mp.weixin.qq.com/s/-lWM4mmbCixxJxWRPuoQsw)

[Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective](https://arxiv.org/abs/2209.13232)

## Llip

[ICML 2024 | 直面CLIP内在缺陷，Meta提出全新latent对比预训练框架Llip](https://mp.weixin.qq.com/s/vucfBAYI_SFemwg5_PrbLA)

[Modeling Caption Diversity in Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2405.00740)

基于对比视觉-语言预训练技术的大型多模态模型目前已成为人工智能领域研究的热点课题。但这一预训练技术仍然以经典的CLIP模型为基础，缺乏进一步的发展。此外，鉴于CLIP模型通过将图像及其caption映射到单个向量这样的底层机制，可以认为限制了对比预训练模型描述图像各种其他方面的能力。

提出了一种名为Llip的架构（Latent Language Image Pretraining），以图像字幕生成（Image Caption）任务作为出发点，用来模拟自然场景中与单张图像进行匹配caption的多样性。Llip仍然采用双塔特征提取模式，其视觉编码器可以对给定图像输出一组视觉特征，这些特征可以总结与当前图像匹配的多样式captions中的文本信息，来得到最终的表示。

## longllava

[首个Mamba+Transformer混合架构多模态大模型来了，实现单卡千图推理](https://mp.weixin.qq.com/s/ipfx6qaxeQlkILEVACabvA)

[LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)

[https://github.com/FreedomIntelligence/LongLLaVA](https://github.com/FreedomIntelligence/LongLLaVA)

## Molmo

[号称击败Claude 3.5 Sonnet，媲美GPT-4o，开源多模态模型Molmo挑战Scaling law](https://mp.weixin.qq.com/s/9s9sIkP-KDlUuJdlktVT9w)

[Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://molmo.allenai.org/paper.pdf)

## Playground

[文生图参数量升至240亿！Playground v3发布：深度融合LLM，图形设计能力超越人类](https://mp.weixin.qq.com/s/P8rieQj_-KoY0H-HfwN5hA)

[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models](https://arxiv.org/abs/2409.10695)

## REPA

[扩散模型训练方法一直错了！谢赛宁：Representation matters](https://mp.weixin.qq.com/s/a725rxzvyQXqNJoL1NsMaA)

[Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://arxiv.org/pdf/2410.06940)

[https://github.com/sihyun-yu/REPA](https://github.com/sihyun-yu/REPA)

## LLaVA-Critic

[Evaluation is All You Need！首个开源多模态大模型通用评测器LLaVA-Critic](https://mp.weixin.qq.com/s/YweRqZrHJmISVjJWamalQg)

[LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712)

## MM1.5

[苹果多模态模型大升级！文本密集、多图理解，全能小钢炮](https://mp.weixin.qq.com/s/jIevs7L4zwWOWzXM4nx62A)

[MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/pdf/2409.20566)

## SCMs

[OpenAI攻克扩散模型短板，清华校友路橙、宋飏合作最新论文](https://mp.weixin.qq.com/s/tDlp95HLlvYqW2gVPqzQag)

[刚刚，OpenAI发布sCM提升50倍效率，扩散模型重大技术突破！](https://mp.weixin.qq.com/s/dI9mSCDbGzZIkjol_CT6Cg)

[比扩散模型快50倍！OpenAI发布多模态模型实时生成进展，作者还是清华校友，把休假总裁Greg都炸出来了](https://mp.weixin.qq.com/s/3h_mxCij5_owicnfsHhp_Q)

[Simplifying, Stabilizing & Scaling Continuous-Time Consistency Models](https://arxiv.org/pdf/2410.11081v1)



## Janus

[DeepSeek新作Janus：解耦视觉编码，引领多模态理解与生成统一新范式](https://mp.weixin.qq.com/s/Ao5V0ICGX3X2HWfIw23YAQ)

[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2410.13848)

[https://github.com/deepseek-ai/Janus](https://github.com/deepseek-ai/Janus)

[https://huggingface.co/spaces/deepseek-ai/Janus-1.3B](https://huggingface.co/spaces/deepseek-ai/Janus-1.3B)

## OmniGen

[新扩散模型OmniGen一统图像生成，架构还高度简化、易用](https://mp.weixin.qq.com/s/mzs96Oav3pfj22YoYJPILQ)

[OmniGen: Unified Image Generation](https://arxiv.org/pdf/2409.11340)

[https://github.com/VectorSpaceLab/OmniGen](https://github.com/VectorSpaceLab/OmniGen)

## LLM2CLIP

[跨模态大升级！少量数据高效微调，LLM教会CLIP玩转复杂文本](https://mp.weixin.qq.com/s/2cp9umZtOQdZLYwiB3e_5g)

[LLM2CLIP: POWERFUL LANGUAGE MODEL UNLOCKS RICHER VISUAL REPRESENTATION](https://arxiv.org/pdf/2411.04997)

[https://github.com/microsoft/LLM2CLIP](https://github.com/microsoft/LLM2CLIP)

## TFG

[NeurIPS Spotlight｜从分类到生成：无训练的可控扩散生成](https://mp.weixin.qq.com/s/9COLxKc7RQaPpEmE-YIfgw)

[TFG: Unified Training-Free Guidance for Diffusion Models](https://arxiv.org/abs/2409.15761)

[https://github.com/YWolfeee/Training-Free-Guidance](https://github.com/YWolfeee/Training-Free-Guidance)

## Deepseek-VL2

[久等了，DeepSeek-VL2](https://mp.weixin.qq.com/s/rE6Dh_OzolgDTAh3ubM5KA)

[https://github.com/deepseek-ai/DeepSeek-VL2](https://github.com/deepseek-ai/DeepSeek-VL2)

[DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](https://github.com/deepseek-ai/DeepSeek-VL2/blob/main/DeepSeek_VL2_paper.pdf)

+ 数据：比一代 DeepSeek-VL 多一倍优质训练数据，引入梗图理解、视觉定位、视觉故事生成等新能力
+ 架构：视觉部分使用切图策略支持动态分辨率图像，语言部分采用MoE架构低成本高性能
+ 训练：继承DeepSeek-VL的三阶段训练流程，同时通过负载均衡适配图像切片数量不定的困难，对图像和文本数据使用不同流水并行策略，对MoE语言模型引入专家并行，实现高效训练

## Florence-VL

[Florence-VL来了！使用生成式视觉编码器，重新定义多模态大语言模型视觉信息](https://mp.weixin.qq.com/s/sAf-FxUithvgA6noaew4sQ)

[Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://arxiv.org/pdf/2412.04424)

[https://github.com/JiuhaiChen/Florence-VL](https://github.com/JiuhaiChen/Florence-VL)

## metamorph

[统一视觉理解与生成，MetaMorph模型问世，LeCun、谢赛宁、刘壮等参与](https://mp.weixin.qq.com/s/Q0obsptFhlZ-R9xH3LCGVw)

[MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/pdf/2412.14164v1)

## seedream2.0

[豆包文生图技术报告发布！数据处理、预训练、RLHF全流程公开](https://mp.weixin.qq.com/s/3E4s2c7TcWQ_g_6DdJPJkQ)

[Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model](https://arxiv.org/pdf/2503.07703)


## 多模态推荐系统

[多模态推荐系统新突破，一文读懂前沿进展！](https://mp.weixin.qq.com/s/YDna7dvC55yWL61r8PljoQ)

[Multimodal Recommender Systems: A Survey](https://arxiv.org/pdf/2302.03883)

[https://github.com/Applied-Machine-Learning-Lab/Awesome-Multimodal-Recommender-Systems](https://github.com/Applied-Machine-Learning-Lab/Awesome-Multimodal-Recommender-Systems)

BM3：[Bootstrap Latent Representations for Multi-modal Recommendation](https://arxiv.org/pdf/2207.05969)

EM3：[End-to-end training of Multimodal Model and ranking Model](https://arxiv.org/pdf/2404.06078v1)

[Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey](https://arxiv.org/pdf/2404.00621).. KDD24

## gpt4o生图

[刚刚，GPT-4o原生图像生成上线，P图、生图也就一嘴的事](https://mp.weixin.qq.com/s/K0A5aWKq-GraauEyx0ixkA)

[Addendum to GPT-4o System Card: Native image generation](https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf)

[GPT-4o图像生成的秘密，OpenAI 没说，网友已经拼出真相？](https://mp.weixin.qq.com/s/VTFlEUEZF0WtEoEBquDrKg)

+ 猜想1：自回归 + 扩散
  + 先生成视觉token，再由扩散模型将其解码到像素空间。
  + 使用的扩散方法是类似于Rolling Diffusion([Rolling Diffusion Models](https://arxiv.org/abs/2402.09470))的分组扩散解码器([Sequential Data Generation with Groupwise Diffusion Process](https://arxiv.org/abs/2310.01400))，会以从上到下的顺序进行解码。参考[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039)
+ 猜想2：非扩散的自回归生成
  + 是一种自回归模型，通过多次通过来逐像素地生成图像，而不是像扩散模型那样执行去噪步骤。

## Web-SSL

[CLIP被淘汰了？LeCun谢赛宁新作，多模态训练无需语言监督更强！](https://mp.weixin.qq.com/s/FpisxJQ9AXHV26lHPwzy5A)

[Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017)

[https://github.com/dfan/webssl](https://github.com/dfan/webssl)

## UNO

[字节开源新生图模型：一个模型统一所有生图任务，多主体融合效果SOTA​](https://mp.weixin.qq.com/s/h4fkShoOG_Fi66fksXaJGw)

[Less-to-More Generalization: Unlocking More Controllability by In-Context Generation](https://arxiv.org/abs/2504.02160)

[https://github.com/bytedance/UNO](https://github.com/bytedance/UNO)

[https://bytedance.github.io/UNO/](https://bytedance.github.io/UNO/)


## Seedream

[Mogao=Seedream 3.0？霸榜数天，神秘文生图模型曝光（附技术报告）](https://mp.weixin.qq.com/s/GcmIQL78fIl-dobAsqbfeg)

[Seedream 3.0 Technical Report](https://arxiv.org/abs/2504.11346)


## Seed-1.5 embedding

[向量检索能力SOTA，字节Seed1.5-Embedding模型训练细节公开](https://mp.weixin.qq.com/s/gBcUyNxE1aqq_3wXksmOzA)。。没开源

+ 设计两阶段对比学习训练流程并精心构造训练数据，充分强化模型的通用表征能力；
+ 从预训练和后训练语料中，构造出推理密集型检索数据（需要深度理解查询和文档的匹配关系，而非简单的字面匹配或语义匹配），并用其优化模型；
+ 使用MoE模型作为基座，并通过MRL训练支持多个向量维度，实现较高的运行速度和灵活的存储开销。

## 其他


[​近期必看的多模态大模型进展：从Qwen2-VL到Pixtral](https://mp.weixin.qq.com/s/r0HipvOkZhHaDxfGsyIgOg)


# 多模态（视频）


## videobert


[通过未标记视频进行跨模态时间表征学习](https://mp.weixin.qq.com/s/5qC70NoTBQ95vjI4cGl66g)

[VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)，VideoBert模型。


## video caption

[Tarsier: Recipes for Training and Evaluating Large Video Description Models](https://arxiv.org/pdf/2407.00634)

![tarsier](../assets/tarsier.png)

## 视频tokenizer方法

+ VideoGPT：[Videogpt: Video generation using vq-vae and transformers](https://arxiv.org/pdf/2104.10157.pdf)，结合了VQ-VAE，而且是自回归的transformer
+ magvit：[MAGVIT: Masked Generative Video Transformer](https://arxiv.org/pdf/2212.05199.pdf)
+ magvitv2：[Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation](https://arxiv.org/pdf/2310.05737.pdf)，[语言模型战胜扩散模型！谷歌提出MAGVIT-v2：视频和图像生成上实现双SOTA！](https://blog.csdn.net/amusi1994/article/details/133917909)


先看vq的改进版：

[Finite Scalar Quantization: VQ-VAE Made Simple](https://arxiv.org/pdf/2309.15505)提出了fsq，码本大小是$$|C|=L^d$$

magvit-v2提出了LFQ，也优化了vq-vae

[BSQ：Image and Video Tokenization with Binary Spherical Quantization](https://arxiv.org/pdf/2406.07548)

![bsq-vq-lfq](../assets/bsq-vq-lfq.png)

![bsq-fsq-lfq](../assets/bsq-fsq-lfq.png)

claude-2-100k的回答。。

+ [MAGVIT: Masked Generative Video Transformer](https://arxiv.org/pdf/2212.05199.pdf)：使用了3D向量量化(3D VQ)自动编码器来将视频量化为离散token
  + 设视频$$V$$有$$T$$帧，其形状为$$T \times H \times W \times 3$$。
  + 3D VQ编码器$$f_T$$会把视频量化为一个token序列$$z$$,其中$$z\in Z^T$$,$$Z$$是码本,$$T$$是token序列长度。
  + 3D VQ解码器$$f^{-1}_T$$则可以从latent token $$z$$重构回视频像素。
+ [Genie: Generative Interactive Environments](https://arxiv.org/pdf/2402.15391.pdf)：使用了2D向量量化(2D VQ)方法
  + 每一帧图像I先通过一个2D VQ编码器f编码为一个token序列$$z$$,其中$$z\in Z^N$$,$$Z$$是2D码本。
  + 然后,对时间序列上的token $$z_1, z_2,..., z_T$$应用一个1D卷积网络,以捕获时间信息。
  + 再通过2D VQ解码器$$f^{-1}$$解码回每一帧图像。
+ [Vivit: A video vision transformer](https://arxiv.org/pdf/2103.15691.pdf)：使用tubelet-embedding
  + 均匀地在时间轴上抽样$$n_t$$个帧,然后把每帧处理成$$n_h \times n_w$$个patch,最终把所有patch连接起来


## SORA

[OpenAI首个AI视频模型炸裂登场，彻底端掉行业饭碗！60秒一镜到底惊人，世界模型真来了？](https://mp.weixin.qq.com/s/93z4Ta91yLv7PB1pnBM9mg)

[https://openai.com/sora](https://openai.com/sora)

[https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)

[一锤降维！解密OpenAI超级视频模型Sora技术报告，虚拟世界涌现了](https://mp.weixin.qq.com/s/ODsebK3fEc-adRDwRVDhQA)

[Sora爆火48小时：杨立昆揭秘论文，参数量或仅30亿](https://new.qq.com/rain/a/20240217A05YVR00)

[微软37页论文逆向工程Sora，得到了哪些结论？](https://mp.weixin.qq.com/s/5-pySWU40omjBowsV2WCKA)

[攻陷短视频后，Sora将需要72万块H100 GPU](https://mp.weixin.qq.com/s/X-MNijIUU5XKYb4vfYtVZg)

[Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/pdf/2402.17177.pdf)

[Sora之后，OpenAI Lilian Weng亲自撰文教你从头设计视频生成扩散模型](https://mp.weixin.qq.com/s/C8JoiTHwW7T-g66EBPcfDg)

[https://lilianweng.github.io/posts/2024-04-12-diffusion-video/](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)

整体感觉：

+ latent diffusion的隐空间
+ vit和swin transformer的patch

### 现有方法

现有的视频生成方法大多只能用于少数分类的视频、比较短的视频，或者固定长度的视频。

+ recurrent networks
  + 2015年的[Unsupervised learning of video representations using lstms](https://arxiv.org/pdf/1502.04681.pdf)
  + 2017年的[Recurrent environment simulators](https://arxiv.org/pdf/1704.02254.pdf)
  + 2018年的[World models](https://arxiv.org/pdf/1803.10122.pdf)
+ generative adversarial networks
  + 2016年的[Generating videos with scene dynamics](https://arxiv.org/pdf/1609.02612.pdf)
  + 2018年的[Mocogan: Decomposing motion and content for video generation](https://arxiv.org/pdf/1707.04993.pdf)
  + 2019年的[Adversarial video generation on complex datasets](https://arxiv.org/pdf/1907.06571.pdf)
  + 2022年的[Generating long videos of dynamic scenes](https://arxiv.org/pdf/2206.03429.pdf)
+ autoregressive transformers
  + 2021年的[Videogpt: Video generation using vq-vae and transformers]((https://arxiv.org/pdf/2104.10157.pdf))
  + 2022年的[Nüwa: Visual synthesis pre-training for neural visual world creation](https://arxiv.org/pdf/2111.12417.pdf)
+ diffusion models
  + 2022年的[Imagen video: High definition video generation with diffusion models](https://arxiv.org/pdf/2210.02303.pdf)
  + 2023年的[Align your latents: High-resolution video synthesis with latent diffusion models](https://arxiv.org/pdf/2304.08818.pdf)
  
前两类太古老了，sora把后面两个（autogressive transformers和diffusion models）结合在一起了，而且能同时处理不同时长、分辨率的**视频和图像**

### 将视频转成spacetime latent patches

#### Vivit

&nbsp;

[Vivit: A video vision transformer](https://arxiv.org/pdf/2103.15691.pdf)

整体受ViT的启发

![vivit](../assets/vivit.png)

先分patch，再分别过时间的transformer（temporal transformer）和空间的transformer（spatial transformer）

![tubelet-embedding](../assets/tubelet-embedding.png)

具体的分patch方式如上图

#### latent空间上的patch

&nbsp;

![spacetime-patches](../assets/spacetime-patches.png)

参考stable-diffusion，即[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)，把patch切分改成在latent空间上进行

+ 将视频映射成**隐空间**(latent space)的表示
+ 把隐空间的表示切分成**spacetime patches**


预估时，可以通过在一个合适大小的grid里排列随机初始化的patches（we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.）来控制生成视频的大小。估计是参考了下面这篇：

论文参考了这个[Patch n'Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](https://arxiv.org/pdf/2307.06304.pdf)，可以使下面提到的DiT适应各种分辨率/持续时间/宽高比。



### Diffusion Transformer

[Scalable diffusion models with transformers](https://arxiv.org/pdf/2212.09748.pdf)提出了DiT，替换stable diffusion中的u-net

![Dit](../assets/Dit.png)

**DiT=VAE编码器+ ViT + DDPM + VAE解码器**

sora是一个扩散模型，输入加了噪声的patches，还可以加上一些如text prompt的条件，预测原本『干净』的patches。

之前的做法大多将视频全裁成相同长度和大小的，例如4s的$$256\times 256$$，sora可以直接用原始视频

### 语言理解

参考DALL-E3 ([Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf))，训练了一个highly descriptive的视频描述生成器，拿这个生成器给训练集中的所有视频重新生成描述，再拿来训练。

此外，还用上了GPT，将用户输入的短的prompt改写成更长更详细的视频描述用于生成。

### 使用图像/视频作为prompt

+ **图像转动画**：可以让静止的图像动起来
+ **扩展视频**：可以对视频进行扩展（extend），在时间轴上向前或者向后进行延展（比如同样是一个石头落地，能生成4个视频，每个视频里的石头从不同的地方飞过来，落在同一个地面上）
+ **编辑视频**：输入视频和一个文本prompt，能够对视频进行编辑，例如把场景从沙漠替换成树林，类似[Sdedit: Guided image synthesis and editing with stochastic differential equations](https://arxiv.org/pdf/2108.01073.pdf)
+ **连接视频**：输入两个看似毫不相关的视频，能通过很自然的方式把这两个视频衔接在一起

### 生成图像

 图像就是一帧的视频，可以通过在时间范围为一帧的空间grid中排列高斯噪声patches（arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame）来生成图像，同样能生成不同分辨率的图像，最多$$2048\times 2048$$

### 涌现的模拟能力

+ **3D一致性**：随着镜头的移动，视频中的人物或物体在3d空间中能在移动中保持一致
+ **Long-range coherence and object permanence（远程连贯性和物体持久性）**：sora能对短期和长期依赖关系进行建模，例如：
  + 可以保留人物体，即使它们被遮挡或离开当前帧。
  + 可以在单个样本中生成同一角色的多个镜头，并在整个视频中保持其外观的不变
+ **与世界交互**：例如画家可以在画布上留下新的笔触，并随着时间的推移而持续存在，人吃东西能留下齿痕
+ **模拟数字世界**：可以同时通过基本策略控制《我的世界》中的玩家，同时以高保真度渲染世界及其动态，只需要在prompt里提到“我的世界”的标题就可以实现。

### 存在的问题

+ 不能准确地模拟许多基本相互作用的物理过程，例如玻璃破碎。
+ 其他交互（例如吃食物）并不总是会产生对象状态的正确变化，例如长时间样本中出现的不连贯性或对象的自发出现。

## open-sora（Colossal-AI）

[没等来OpenAI，等来了Open-Sora全面开源](https://mp.weixin.qq.com/s/vdr1WBCQVr9aS6bJYcdlRA)

[Open-Sora全面开源升级：支持16s视频生成和720p分辨率](https://mp.weixin.qq.com/s/a-FULV7mSskHFar5glbSxg)

### 模型架构

v1版本：[https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_v1.md](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_v1.md)

v2版本：[https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_02.md](https://github.com/hpcaitech/Open-Sora/blob/main/docs/report_02.md)

#### VAE部分

&nbsp;

sora用了spatial-temporal VAE来降低temporal的维度，但并没有开源且高质量的spatial-temporal VAE：

+ [MAGVIT](https://github.com/google-research/magvit)：的$$4\times 4\times 4$$的VAE并没有开源
+ [VideoGPT](https://github.com/wilson1yan/VideoGPT)：的$$2\times 4\times 4$$的VAE在实验中效果不好

因此，使用[https://huggingface.co/stabilityai/sd-vae-ft-mse-original](https://huggingface.co/stabilityai/sd-vae-ft-mse-original)的2D VAE

对于24fps的1min视频，有$$24\times 60=1440$$帧，用4倍的VAE下采样和2倍的patch size下采样，有大约$$1440\times \approx 1.5M$$的token。对这些token算全部的attention的计算开销很大，所以参考[Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/pdf/2401.03048v1.pdf)(代码[https://github.com/Vchitect/Latte](https://github.com/Vchitect/Latte))的方案，使用**spatial-temporal attention**来减小计算量。

以下是latte的4个变种

![latte](../assets/latte.png)

STDiT(sequential)和latte的变种3类似，STDiT(parallel)和latte的变种4类似，在$$16\times 256\times 256$$的视频上，发现效果如下，最终采用了STDiT(sequential)。

XXX
DiT (full) > STDiT (Sequential) > STDiT (Parallel) \approx Latte
XXX

![stdit](../assets/stdit.png)

#### 生成部分

&nbsp;

[PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/pdf/2310.00426.pdf)使用**T5作为条件**的**DiT**结构，能生成高质量的图像。用PixArt-α对模型初始化，并对**插入的temperal attentioin**用0初始化，能够让模型一开始就保留图片生成的能力。插入的attention让参数量从580M涨到了724M。

![pixart-alpha-temperal](../assets/pixart-alpha-temperal.png)

#### 训练

&nbsp;

参考PixArt-α和Stable Video Diffusioin([Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](https://arxiv.org/pdf/2311.15127.pdf))，采用了progressive的训练策略：

+ 大规模图像预训练：前面提到的，直接使用[https://huggingface.co/stabilityai/sd-vae-ft-mse-original](https://huggingface.co/stabilityai/sd-vae-ft-mse-original)的2D VAE
+ 大规模视频预训练：在质量相对差的366K的预训练数据集(HD-VG-130M数据集)上训练$$16\times 256\times 256$$，这里的16指的是16帧
+ 高质量视频数据微调：在高质量的20K的数据集(Pexels数据集)上训练$$16\times 256\times 256$$、$$16\times 512\times 512$$和$$64\times 512\times 512$$。

由于使用了scaled position embedding，这个策略极大地减少了训练消耗。此外，对于16帧的训练，每3帧降采样一次，对于64帧的训练每2帧降采样一次。

数据标注方法：抽取3帧，然后设计prompt，用LLaVA生成高质量的标题：

![llava-caption](../assets/llava-caption.png)

+ 学习率：1e-4太大了，改成了2e-5
+ **batchsize比较大**的时候，**fp16比bf16更不稳定**，而且可能导致生成错误，所以对于$$64\times 512\times 512$$使用**bf16**

提供了便捷的视频数据预处理脚本，可以轻松地在自己的数据集上快速生成训练所需的视频 / 文本对，包括公开视频数据集下载，长视频根据镜头连续性分割为短视频片段，使用开源LLaVA生成精细的提示词。

## open-sora(北大版)

[超10秒高分辨率，北大Open Sora视频生成更强了，还支持华为芯片](https://mp.weixin.qq.com/s/1GWxp8ENrA1YGGwrFpOAxA)

## MORA

[Sora不开源，微软给你开源！全球最接近Sora视频模型诞生，12秒生成效果逼真炸裂](https://mp.weixin.qq.com/s/GkJwyVFVwxih-ZQWWBIuNg)

[复刻Sora的通用视频生成能力，开源多智能体框架Mora来了](https://mp.weixin.qq.com/s/JbiwVtEuKvIjb8hBi0Laxg)

[Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](https://arxiv.org/pdf/2403.13248.pdf)

## minigpt4-video

[AI视频理解天花板，全新MiniGPT4-Video刷爆SOTA！宝格丽宣传片配文一绝](https://mp.weixin.qq.com/s/Y8w6CqTvm7zVQMOmTuxePA)

## mini-gemini

[刷爆多模态任务榜单！贾佳亚团队Mini-Gemini登热榜，代码、模型、数据全部开源](https://mp.weixin.qq.com/s/j5CGuJ_-Sf0Pqi_-dDjABA)

模型地址：[https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854](https://huggingface.co/collections/YanweiLi/mini-gemini-6603c50b9b43d044171d0854)
数据地址：[https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e](https://huggingface.co/collections/YanweiLi/mini-gemini-data-660463ea895a01d8f367624e)

[Mini-Gemini: Mining the Potential of Multi-modalityVision Language Models](https://arxiv.org/pdf/2403.18814.pdf)

VLM(vision lm)虽然有很多，但和gemini、gpt-4等的差距还是比较大，作者认为主要原因是**高分辨率视觉标记不够**、**vision推理数据质量不高**。

![mini-gemini](../assets/mini-gemini.png)

作者利用**额外的视觉编码器**进行**高分辨率细化**，构建了一个高质量的数据集。构建了一个Mini-Gemini架构，支持一系列从2B到34B的密集和MoE LLM，在zero-shot测试集上超过了私有模型。

[https://github.com/dvlab-research/MiniGemini](https://github.com/dvlab-research/MiniGemini)

## Vidu

[当前最强国产Sora！清华团队突破16秒长视频，懂多镜头语言，会模拟物理规律](https://mp.weixin.qq.com/s/xAEYGIoJ0EzhszfmXno3UA)

采用了和 Sora 完全一致的 Diffusion 和 Transformer 融合的架构，底层基于[All are Worth Words: A ViT Backbone for Diffusion Models](https://arxiv.org/pdf/2209.12152)的 U-ViT 架构。

基于 U-ViT 架构，2023 年 3 月，团队在开源的大规模图文数据集 LAION-5B 上训练了 10 亿参数量的多模态模型 ——UniDiffuser，并将其开源（参见[清华朱军团队开源首个基于 Transformer 的多模态扩散大模型，文图互生、改写全拿下](https://mp.weixin.qq.com/s/B68hXlFxA9L5jiWiMrEEiA)）。

UniDiffuser 主要擅长图文任务，能支持图文模态间的任意生成和转换。UniDiffuser 的实现有一项重要的价值 —— 首次验证了融合架构在大规模训练任务中的可扩展性（Scaling Law），相当于将 U-ViT 架构在大规模训练任务中的所有环节流程都跑通。

这些在图文任务中积累的工程经验为视频模型的研发打下了基础。因为视频本质上是图像的流，相当于是图像在时间轴上做了一个扩增。因此，在图文任务上取得的成果往往能够在视频任务中得到复用。Sora 就是这么做的：它采用了 DALL-E 3 的重标注技术，通过为视觉训练数据生成详细的描述，使模型能够更加准确地遵循用户的文本指令生成视频。这种效应也必然会发生在「Vidu」上面。

根据此前的消息推测，「Vidu」也复用了生数科技在图文任务的很多经验，包括训练加速、并行化训练、低显存训练等等，从而快速跑通了训练流程。据悉，他们通过视频数据压缩技术降低输入数据的序列维度，同时采用自研的分布式训练框架，在保证计算精度的同时，通信效率提升 1 倍，显存开销降低 80%，训练速度累计提升 40 倍。

## Vidu 1.5

[视觉模型学会LLM独门秘籍「上下文记忆」，迎来智能涌现的大爆发！](https://mp.weixin.qq.com/s/vXE1_Spya2BsZxWuD6LBOA)

## gen-3

[Runway版Sora发布：高保真、超强一致性，Gen-3 Alpha震撼到网友了](https://mp.weixin.qq.com/s/uuLub-ruJgYYrTOFoNJ5iw)

## 可灵

[快手「可灵」爆火：海外AI圈巨震，中国版Sora一号难求](https://mp.weixin.qq.com/s/iSAvV3PX1WYwGg7rU60Ong)

[快手可灵凭什么频繁刷屏？揭秘背后三项重要研究](https://mp.weixin.qq.com/s/CNl224oFUMCCHxOAKgo84Q)

+ 数据集：Koala-36M
  + [A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content](https://arxiv.org/abs/2410.08260)
  + [https://github.com/KwaiVGI/Koala-36M](https://github.com/KwaiVGI/Koala-36M)
+ 视频生成领域的 Scaling Law：[Towards Precise Scaling Laws for Video Diffusion Transformers](https://arxiv.org/abs/2411.17470)
+ 通用世界模型：[Owl-1: Omni World Model for Consistent Long Video Generation](https://arxiv.org/abs/2412.09600)
  + [https://github.com/huang-yh/Owl](https://github.com/huang-yh/Owl)

[可灵视频生成可控性为什么这么好？快手又公开了四篇研究](https://mp.weixin.qq.com/s/3NI9YITmCrd8cDT1YMlA9A)

## 长视频LongVA

[7B最强长视频模型！ LongVA视频理解超千帧，霸榜多个榜单](https://mp.weixin.qq.com/s/62rMYx94dbz1HwDkZclXtA)

[Long Context Transfer from Language to Vision](https://arxiv.org/pdf/2406.16852)

## LONGVILA

[支持1024帧、准确率近100％，英伟达「LongVILA」开始发力长视频](https://mp.weixin.qq.com/s/T6eMi3DPq9_291bWqcFRgw)

[LONGVILA: SCALING LONG-CONTEXT VISUAL LANGUAGE MODELS FOR LONG VIDEOS](https://arxiv.org/pdf/2408.10188)

[https://github.com/NVlabs/VILA/blob/main/LongVILA.md](https://github.com/NVlabs/VILA/blob/main/LongVILA.md)

## liveportrait

[快手开源LivePortrait，GitHub 6.6K Star，实现表情姿态极速迁移](https://mp.weixin.qq.com/s/JrKF_7To8PEggEfw7W09ew)

[LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control](https://arxiv.org/pdf/2407.03168)

[https://github.com/KwaiVGI/LivePortrait](https://github.com/KwaiVGI/LivePortrait)

## diffusion forcing

[无限生成视频，还能规划决策，扩散强制整合下一token预测与全序列扩散](https://mp.weixin.qq.com/s/kz4RvqdK6nGtA11y5nq5xQ)

[Diffusion Forcing:Next-token Prediction Meets Full-Sequence Diffusion](https://arxiv.org/pdf/2407.01392)

[https://github.com/buoyancy99/diffusion-forcing](https://github.com/buoyancy99/diffusion-forcing)

## VideoSys

[视频生成要有自己的系统！尤洋团队历时半年开源VideoSys](https://mp.weixin.qq.com/s/Q-AHzIOT0PBP6Yvdk_T3Sg)

[https://github.com/NUS-HPC-AI-Lab/VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys)

### Pyramid Attention Broadcast (PAB)


[Real-Time Video Generation with Pyramid Attention Broadcast](https://arxiv.org/abs/2408.12588)


[https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/pab.md](https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/pab.md)


### Dyanmic Sequence Parallelism（DSP）


[DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers](https://arxiv.org/abs/2403.10266)

[https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/dsp.md](https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/dsp.md)

## GameNGen

[扩散模型做游戏引擎，单TPU 20 FPS模拟毁灭战士，谷歌最新GameNGen太博眼球了](https://mp.weixin.qq.com/s/LvjhY9Gzd_lnE3M3MllKDA)

## Firefly

[厉害了！Adobe新出Firefly视频模型，2分钟速成高清大片](https://mp.weixin.qq.com/s/uFKQNGuoZ2bS4Ea0pye71Q)

[https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon](https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon)


## MovieGen

[Meta又给OpenAI一记重击，视频生成Movie Gen震撼登场，甚至可以配音、编辑](https://mp.weixin.qq.com/s/c8_sXLRkwEVvg_LKCPQHKw)

[Meta版Sora无预警来袭！抛弃扩散损失，音视频生成/画面编辑全包，92页论文无保留公开](https://mp.weixin.qq.com/s/rs7JQigqHO9yT_0wbF6cTg)

[MovieGen: A Cast of Media Foundation Models](https://ai.meta.com/static-resource/movie-gen-research-paper)

## EMOVA

[mini-GPT4o来了? 能看、能听、会说，还情感丰富的多模态全能助手EMOVA](https://mp.weixin.qq.com/s/e2KkDjqbWNy7wSv0geCNUg)

[EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotion](https://arxiv.org/abs/2409.18042)

## LLaVA-Video

[突破视频多模态大模型瓶颈！「合成数据」立大功，项目已开源](https://mp.weixin.qq.com/s/d2jWyKsqTlk_9LSttESySw)

[VIDEO INSTRUCTION TUNING WITH SYNTHETIC DATA](https://arxiv.org/pdf/2410.02713)

## Emu3

[视频、图像、文本，只需基于下一个Token预测：智源Emu3发布，验证多模态模型新范式](https://mp.weixin.qq.com/s/csqFAkjziwx34aAxKj9-gQ)

[https://github.com/baaivision/Emu3](https://github.com/baaivision/Emu3)

[Emu3: Next-Token Prediction is All You Need](https://arxiv.org/pdf/2409.18869)

[https://huggingface.co/collections/BAAI/emu3-66f4e64f70850ff358a2e60f](https://huggingface.co/collections/BAAI/emu3-66f4e64f70850ff358a2e60f)


## hunyuan-video

[腾讯版Sora发布即开源！130亿参数，模型权重、推理代码全开放](https://mp.weixin.qq.com/s/6_ciIeZBqkFMuizUmjKV4Q)

[HunyuanVideo: A Systematic Framework For Large Video Generative Models](https://github.com/Tencent/HunyuanVideo/blob/main/assets/hunyuanvideo.pdf)

[https://github.com/Tencent/HunyuanVideo](https://github.com/Tencent/HunyuanVideo)

## STIV

(toread)

[Sora之后，苹果发布视频生成大模型STIV，87亿参数一统T2V、TI2V任务](https://mp.weixin.qq.com/s/6mbe80LmzkH-5eGgIys6PQ)

[STIV: Scalable Text and Image Conditioned Video Generation](https://arxiv.org/pdf/2412.07730)

## Veo 2

[谷歌版Sora升级4K高清！一句话控制镜头运动，跑分叫板可灵海螺](https://mp.weixin.qq.com/s/8-H286tyxbTeZrtEBDZHaA)

[https://deepmind.google/technologies/veo/veo-2/](https://deepmind.google/technologies/veo/veo-2/)

## 通义万相

[通义万相视频生成重磅升级，成功登顶VBench，运镜、质感直达专业级](https://mp.weixin.qq.com/s/YFnftO_sKQ_d6AM5J-W8YQ)

[https://tongyi.aliyun.com/wanxiang/](https://tongyi.aliyun.com/wanxiang/)

[开源的风吹到视频生成：阿里开源登顶VBench的万相大模型，一手实测来了！](https://mp.weixin.qq.com/s/SRj06E-VCSpCiQZqE0gpHA)

[万相，开源！](https://mp.weixin.qq.com/s/l9EhkiwgsIL2aYfE0JcR4A)

[阿里开源全能视频模型！生成编辑都精通，1.3B版本消费级显卡可跑](https://mp.weixin.qq.com/s/U8plksUWusZM19kjBAzODA)

[https://github.com/Wan-Video](https://github.com/Wan-Video)

[https://huggingface.co/Wan-AI](https://huggingface.co/Wan-AI)

完整示例代码

```shell
export https_proxy="xxx"

git clone https://github.com/Wan-Video/Wan2.1.git
cd Wan2.1

pip3 install -r requirements.txt

#pip3 install "huggingface_hub[cli]"
#huggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir ./Wan2.1-T2V-14B

hdfs dfs -get /xx/xxx/Wan2.1-T2V-14B . # 如果已经下完了

sudo apt update
sudo apt install libgl1

pip3 install --upgrade flash-attn

pip3 install "xfuser>=0.4.1"
torchrun --nproc_per_node=8 generate.py --task t2v-14B \
  --size 1280*720 \
  --ckpt_dir ./Wan2.1-T2V-14B \
  --dit_fsdp --t5_fsdp --ulysses_size 8 \
  --prompt "一只猫在大街上跑" \
  --save_file "demo.mp4"
```

## Tarsier2

[年末惊喜！ByteDance Research视频理解大模型「眼镜猴」正式发布](https://mp.weixin.qq.com/s/tVr-QidbmA9AudaXaNOKgA)

[Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding](https://arxiv.org/abs/2501.07888)

## VideoWorld

[Seed Research | 视频生成模型最新成果，可仅靠视觉认知世界！现已开源](https://mp.weixin.qq.com/s/mXaktIsD3w5BgCJQb6R7xQ)

[VideoWorld: Exploring Knowledge Learning from Unlabeled Videos](https://arxiv.org/abs/2501.09781)

[https://github.com/bytedance/VideoWorld](https://github.com/bytedance/VideoWorld)

## Sky-Reels

[中国首个AI短剧模型开源，4090秒生好莱坞级大片！人人拍短剧时代来临](https://mp.weixin.qq.com/s/iBIITjzltWjEeLIoADbFKw)

[SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers](https://skyworkai.github.io/skyreels-a1.github.io/report.pdf)

[https://github.com/SkyworkAI/SkyReels-V1](https://github.com/SkyworkAI/SkyReels-V1)

[https://github.com/SkyworkAI/SkyReels-A1](https://github.com/SkyworkAI/SkyReels-A1)

## Seaweed-7B

[Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model](https://arxiv.org/pdf/2504.08685)

## DAM

[英伟达开源「描述一切」模型，拿下7个基准SOTA](https://mp.weixin.qq.com/s/FhARyS4bkqY3A6Z7ihmZcA)

[Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/pdf/2504.16072)

DAM(Describe Anything Model)多模态大语言模型，可以生成图像或视频中特定区域的详细描述。用户可以使用点、框、涂鸦或蒙版来指定区域，DAM将提供这些区域丰富的上下文描述。

## Sparse VideoGen

[ICML 2025 | 视频生成模型无损加速两倍，秘诀竟然是「抓住attention的时空稀疏性」](https://mp.weixin.qq.com/s/3gA0JVDuc5naWvaowOtQqQ)

[Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity](https://arxiv.org/abs/2502.01776)

[https://github.com/svg-project/Sparse-VideoGen](https://github.com/svg-project/Sparse-VideoGen)

完全无需重新训练模型的视频生成加速方法。通过挖掘注意力机制中的空间与时间稀疏性，配合自适应稀疏选择与算子优化，成功将推理时间减半。

## AdaCM2

[CVPR 2025 Highlight｜AdaCM2：首个面向超长视频理解的跨模态自适应记忆压缩框架](https://mp.weixin.qq.com/s/6UEwXmHa25mu7W8FIE5ZFg)

[AdaCM2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction](https://arxiv.org/pdf/2411.12593)

## 其他

[扩散模型与文生视频](https://mp.weixin.qq.com/s/Bh3Gg7FCDpb_AmGEFkxQ2A)

[多模态大模型不够灵活，谷歌DeepMind创新架构Zipper：分开训练再「压缩」](https://mp.weixin.qq.com/s/F8wstkJyYiNJCbSqYq3Pbw)

[Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities](https://arxiv.org/pdf/2405.18669)

# 语音

## Mini-Omni

[让大模型能听会说，国内机构开源全球首个端到端语音对话模型Mini-Omni](https://mp.weixin.qq.com/s/2vf2QMnnmEdcdVbK8g9-_w)

[Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://arxiv.org/abs/2408.16725)

[https://github.com/gpt-omni/mini-omni](https://github.com/gpt-omni/mini-omni)

## Illuminate

[任意论文一键变播客，谷歌正式发布Illuminate，它能重构研究者的学习方式吗？](https://mp.weixin.qq.com/s/2C2B5yNLjXXYyDQnQgqPyQ)

[https://illuminate.google.com/home](https://illuminate.google.com/home)


## V2A

[杀疯了！谷歌卷视频到语音，逼真音效让AI视频告别无声！](https://mp.weixin.qq.com/s/0D4QGeyZ0ZnmmWYz_x-56g)

[https://deepmind.google/discover/blog/generating-audio-for-video/](https://deepmind.google/discover/blog/generating-audio-for-video/)


## Speech-02

[超越OpenAI、ElevenLabs，MiniMax新一代语音模型屠榜！人格化语音时代来了](https://mp.weixin.qq.com/s/tZ3y4n-aMq5dFC2OmbWZpg)



# 全模态

## 综述

[多模态与生成正迈向终极大一统！阿里最新3万字长文梳理统一的多模态理解和生成模型](https://mp.weixin.qq.com/s/GLWZsfGLOgdmtCXw9Cyo0g)

[Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/pdf/2505.02567)

[https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models](https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models)


## seed-1.5-VL

[Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)

[Seed VLM 技术报告首次公开：图像、视频、GUI、Game 完全体](https://mp.weixin.qq.com/s/uWvOVPEowCXAuowrTKefiA)

[字节最强多模态模型登陆火山引擎！Seed1.5-VL靠20B激活参数狂揽38项SOTA](https://mp.weixin.qq.com/s/GgJVkh8IorB6MvqlxESJLw)

## DanceGRPO

[DanceGRPO：首个统一视觉生成的强化学习框架](https://mp.weixin.qq.com/s/jivb_FgsfdJSgV2tbDbNUA)

[DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/pdf/2505.07818)

## MMaDA

[比Gemini Diffusion更全能！首个多模态扩散大语言模型MMaDA发布，同时实现强推理与高可控性](https://mp.weixin.qq.com/s/uaJy9pz2EEcjf4DZCpoonw)

[MMaDA: Multimodal Large Diffusion Language Models](https://arxiv.org/abs/2505.15809)

[https://github.com/Gen-Verse/MMaDA](https://github.com/Gen-Verse/MMaDA)

[https://huggingface.co/Gen-Verse/MMaDA-8B-Base](https://huggingface.co/Gen-Verse/MMaDA-8B-Base)


## BAGEL

[Seed Research｜理解与生成统一模型 BAGEL 开源，All-in-One Model！](https://mp.weixin.qq.com/s/ntZDIVYxpYxey7G-wUS3mw)

[https://github.com/bytedance-seed/BAGEL](https://github.com/bytedance-seed/BAGEL)

[Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/pdf/2505.14683)

[https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT](https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT)

# LLM+推荐：概述


## 一些论文list

[https://github.com/nancheng58/Awesome-LLM4RS-Papers](https://github.com/nancheng58/Awesome-LLM4RS-Papers)

[A Survey on Large Language Models for Recommendation](https://arxiv.org/pdf/2305.19860.pdf)
[Recommender Systems in the Era of Large Language Models (LLMs)](https://arxiv.org/pdf/2307.02046.pdf)


中科大LDS实验室的tutorial：[XadC3O-large-language-models-for-recommendation-tutorial-slides.pdf](https://github.com/daiwk/collections/blob/master/assets/XadC3O-large-language-models-for-recommendation-tutorial-slides.pdf)

对应的datafun talk：[当"狂飙"的大模型撞上推荐系统](https://mp.weixin.qq.com/s/rBGq7rDMK5Vxad5qUmK3nw)

[大模型放进推荐系统怎么玩？微软亚研全面总结](https://mp.weixin.qq.com/s/x0qczJ8I8LZ_PyZw2HOI8A)

生成式推荐综述：

[Large Language Models for Generative Recommendation: A Survey and Visionary Discussions](https://arxiv.org/pdf/2309.01157.pdf)

[A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://arxiv.org/pdf/2404.00579.pdf)

[大语言模型在推荐系统中的探索与应用](https://mp.weixin.qq.com/s/nEhynptVyx8aV8onCwozAA)

[一文梳理业界落地LLM4Rec的若干范式-数万字超长文(建议收藏)](https://mp.weixin.qq.com/s/jqFQH6ZYhZRrzilLAo9BiA)

[推荐算法月报(2025.2)：大模型的春风吹醒推荐算法，重磅大厂突破](https://mp.weixin.qq.com/s/NO2XgUBiRkjsKNo7dswa0w)

[推荐算法(2025.3.4)：速看！阿里Meta的最新研究成果](https://mp.weixin.qq.com/s/1kZCKYbuvd6qiE6-5hJquw)

[一文梳理业界落地LLM4Rec的若干范式-数万字超长文(建议收藏)](https://mp.weixin.qq.com/s/jqFQH6ZYhZRrzilLAo9BiA)

## 综述

### 概况

&nbsp;

[How Can Recommender Systems Benefit from Large Language Models: A Survey](https://arxiv.org/pdf/2306.05817v6) 基于这个组织

CRM(conventional recommendation models)

![llm-rec-where](../assets/llm-rec-where.png)

![llm-rec-how](../assets/llm-rec-how.png)

### where-LLM用于特征工程

&nbsp;

#### user-item level特征增强

&nbsp;

通过LLM的世界知识来获取更好的user/item表示

+ [Llama4rec](https://arxiv.org/pdf/2401.13870) ：prompt增强：在prompt里引入推荐模型的信息；数据增强：通过LLM给推荐模型增加样本；adaptive aggregation：llm和推荐模型各自打分并用融合公式融合
+ [KAR](https://arxiv.org/pdf/2306.10933)：让LLM总结item得到item emb；让LLM总结user历史得到user emb，两个emb过一个mmoe做融合得到新的两个emb，给推荐模型用
+ [SAGCN](https://arxiv.org/pdf/2312.16275)：通过LLM标识出用户对item的评论是属于哪些aspect的，然后u有A个emb，i也有A个emb，构建A个U-I图，然后揉在一起过GCN
+ [CUP](https://arxiv.org/pdf/2311.01314)：把用户的一堆历史评论扔给chatgpt，让它总结出128个token，然后丢给双塔bert，另一个塔是item的描述，freeze bert底层，只tune上层
+ [LLaMA-E](https://arxiv.org/pdf/2308.04913)：instruction formulating为写300个种子指令，让gpt作为teacher，对300个种子指令进行扩展，并由领域专家评估后，去重并保证质量，得到120k个指令作为训练集，再用lora去instruct tuning
+ [EcomGPT](https://arxiv.org/pdf/2308.06966v1)：设置一系列的task(100多个task)来finetune BLOOMZ，包括命名实体识别、描述生成、对话intent提取等

#### instance-level样本生成

&nbsp;

+ [GReaT](https://arxiv.org/pdf/2210.06280)：把表格化（tabular）数据转成自然语言，然后打乱顺序，自回归地finetune一个LLM，再拿tune完的LLM来合成（synthesize）逼真的（realistic）表格化数据
+ [ONCE](https://arxiv.org/pdf/2305.06566)：闭源LLM输出文本（user profiler、content summarizer、personalized content generator），给开源LLM得到user表示，item过开源LLM得到item表示，二者内积学ctr
+ [Agent4Rec](https://arxiv.org/pdf/2310.10108.pdf)：先训一个推荐模型，然后构建一个多智能体系统，模拟和这个推荐模型交互，产出新的样本给推荐模型做数据增强
+ [RecPrompt](https://arxiv.org/pdf/2312.10463)：给一个初始prompt，让LLM1得到推荐结果，拿一个monitor衡量这个结果和ground truth的mrr/ndcg，再用另一个LLM产出更好的prompt给第一个LLM用，如此迭代，得到一个best prompt
+ [PO4ISR]((https://arxiv.org/pdf/2312.07552))：给初始prompt，收集error case让模型反思原因并refine出新的prompt，再augment出另一个prompt，并UCB选出最好的prompt，如此迭代
+ [BEQUE](https://arxiv.org/pdf/2311.03758)：query重写任务，SFT得到一个LLM，将其预测的若干个候选rewrites通过offline system的feedback得到排序，再通过PRO算法再tune LLM。
+ [Agent4Ranking](https://arxiv.org/pdf/2312.15450)：query重写任务，多个人群当成多个agent，每个通过多轮对话产出一个rewrite，再合在一起经过bert+mmoe计算robust损失+accuracy损失。


### where-LLM作为特征编码器

&nbsp;

#### 表示增强

&nbsp;




#### 统一的跨域推荐

&nbsp;


### where-LLM 用于打分函数

&nbsp;

[Text Is All You Need: Learning Language Representations for Sequential Recommendation](https://arxiv.org/pdf/2305.13731) 亚马逊发的

[https://github.com/AaronHeee/RecFormer](https://github.com/AaronHeee/RecFormer)

### where-LLM用于流水线控制（pipeline controller）

&nbsp;


### how-tune LLM & infer with CRM

&nbsp;

### how-not tune LLM & infer w/o CRM

&nbsp;

### how-not tune LLM & infer with CRM

&nbsp;

### how-tune LLM & infer w/o CRM

&nbsp;


## llm vs ID

[推荐系统范式之争，LLM vs. ID？](https://mp.weixin.qq.com/s/7pQ891pnp_BM7qH7ROiWwg)

[Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights](http://arxiv.org/abs/2305.11700)


[知乎的讨论](https://www.zhihu.com/question/630016669/answer/3380909598)

SIGIR2023 \| ID vs 模态: 推荐系统ID范式有望被颠覆？

[Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/abs/2303.13835)

[https://github.com/westlake-repl/IDvs.MoRec](https://github.com/westlake-repl/IDvs.MoRec)

[对应的ppt](https://github.com/westlake-repl/MicroLens/blob/master/MicroLens_DeepMind_Talk.pdf)


## 推荐生态系统

[Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/e00cc0ce4480c15339b7798943560ad1a0d593ce.pdf)

## 发现性

[Diversifying by Intent in Recommender Systems](https://arxiv.org/pdf/2405.12327)，涨了dau等指标


# LLM+推荐：生成索引

## DSI

参考知乎：[https://zhuanlan.zhihu.com/p/470182510](https://zhuanlan.zhihu.com/p/470182510)

[Transformer memory as a differentiable search index](https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf)，提出的可微搜索索引（differentiable search index, **DSI**），拆成两个阶段：

+ indexing：建立**文档**和**doc_id**的**一一映射**
+ retrieval：根据query生成候选doc_ids

与[Autoregressive Entity Retrieval](https://arxiv.org/pdf/2010.00904)提出的**受限beam search**的GENRE（代码[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)，解读[Transformer中PrefixConstrainedLogitsProcessor类的解读](https://zhuanlan.zhihu.com/p/494082642)）对比：

+ GENRE生成的目标是**有具体语义的实体名**
+ DSI生成的目标则是**无任何语义的任意doc_id**

### Indexing方法

&nbsp;

+ Inputs2Target：即doc_tokens->doc_id
+ Target2Inputs：即doc_id->doc_tokens
+ bidirectional：同时用上两面两个任务，并且在开始时加一个前缀，表明任务的方向
+ span corruption：参考T5（[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)），将doc_id当做前缀和doc_tokens拼起来，2个好处：
    + 在索引时进行通用的预训练
    + 实现doc_id作为去噪目标和输入的平衡

### 文档的表示

&nbsp;

+ Direct Indexing：直接用文档的前L个单词当做文档的表示，并且保留单词的顺序。
+ Set Indexing：去掉文档中的重复的单词及停用词，然后和Direct Indexing一样的处理。
+ Inverted Index：随机对文档中的连续的k个单词（一个文档块）进行采样，并将它们与doc_id相关联

### doc_id的表示

&nbsp;

+ 非结构化的原子标识符：直接对所有的文档使用一个随机但互不相同的的整数标识。假设一共有N篇文档需要检索，假设原来解码器输出有V个单词，现在有V+N个单词。
+ 朴素的结构化字符串标识符：也使用一个随机的整数，但将这个整数当做一个字符串输出，用受限的beam search搜索前k个文档，因为需要保证**输出的是数字**。
+ 语义结构化的标识符：先用BERT产出每个doc的emb，然后**递归10-means**，第一次得到0-9作为第1个数字，第二次的0-9作为第2个数字，可以得到一个树，文档的最终标识符就是从根节点到当前结点的路径对应的编号组合。只要一个节点的文档数大于$$c=100$$，就继续分裂，当叶子的文档数小于c时，每个文档随机分配一个1到$$c-1$$的id，拼到doc_id的最后。

### 训练方法

&nbsp;

用seq2seq，即**teacher forcing+交叉熵**，有如下两种方式：

+ 先对indexing进行预训练（memorization），再进行**将query映射为docid**的微调
+ 用多任务的方式**同时进行**，两个任务用**不同的标识符**，这样做效果会好很多

后来有一篇[Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation](https://arxiv.org/pdf/2206.10128.pdf)，对应的github：[https://github.com/ArvinZhuang/DSI-QG](https://github.com/ArvinZhuang/DSI-QG)，他也同时尝试复现DSI：[https://github.com/ArvinZhuang/DSI-transformers](https://github.com/ArvinZhuang/DSI-transformers)，主要有如下两个地方：

+ 准备数据，doc_id

```python
class IndexingTrainDataset(Dataset):
    def __init__(
            self,
            path_to_data,
            max_length: int,
            cache_dir: str,
            tokenizer: PreTrainedTokenizer,
    ):
        self.train_data = datasets.load_dataset(
            'json',
            data_files=path_to_data,
            ignore_verifications=False,
            cache_dir=cache_dir
        )['train']

        self.max_length = max_length
        self.tokenizer = tokenizer
        self.total_len = len(self.train_data)


    def __len__(self):
        return self.total_len

    def __getitem__(self, item):
        data = self.train_data[item]

        input_ids = self.tokenizer(data['text'],
                                   return_tensors="pt",
                                   truncation='only_first',
                                   max_length=self.max_length).input_ids[0]
        return input_ids, str(data['text_id'])


@dataclass
class IndexingCollator(DataCollatorWithPadding):
    def __call__(self, features):
        input_ids = [{'input_ids': x[0]} for x in features]
        docids = [x[1] for x in features]
        inputs = super().__call__(input_ids)

        # label是doc_id
        labels = self.tokenizer(
            docids, padding="longest", return_tensors="pt"
        ).input_ids

        # replace padding token id's of the labels by -100 
        # according to https://huggingface.co/docs/transformers/model_doc/t5#training
        labels[labels == self.tokenizer.pad_token_id] = -100
        inputs['labels'] = labels
        return inputs

```

+ 训练和预测

```python
class IndexingTrainer(Trainer):
    def __init__(self, restrict_decode_vocab, **kwds):
        super().__init__(**kwds)
        self.restrict_decode_vocab = restrict_decode_vocab

    def compute_loss(self, model, inputs, return_outputs=False):
        ## 输入文章，预测doc_id
        loss = model(input_ids=inputs['input_ids'], 
            attention_mask=inputs['attention_mask'], labels=inputs['labels']).loss
        if return_outputs:
            return loss, [None, None]  # fake outputs
        return loss

    def prediction_step(
            self,
            model: nn.Module,
            inputs: Dict[str, Union[torch.Tensor, Any]],
            prediction_loss_only: bool,
            ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        model.eval()
        # eval_loss = super().prediction_step(model, inputs, True, ignore_keys)[0]
        with torch.no_grad():
            # greedy search
            doc_ids = model.generate(
                inputs['input_ids'].to(self.args.device),
                max_length=20,
                prefix_allowed_tokens_fn=self.restrict_decode_vocab,
                early_stopping=True,)
        return (None, doc_ids, inputs['labels'])
```

+ 对解码空间的限制：

```python
    # docid generation constrain, we only generate integer docids.
    SPIECE_UNDERLINE = "_"
    INT_TOKEN_IDS = []
    for token, id in tokenizer.get_vocab().items():
        if token[0] == SPIECE_UNDERLINE:
            if token[1:].isdigit():
                INT_TOKEN_IDS.append(id)
        if token == SPIECE_UNDERLINE:
            INT_TOKEN_IDS.append(id)
        elif token.isdigit():
            INT_TOKEN_IDS.append(id)
    INT_TOKEN_IDS.append(tokenizer.eos_token_id)

    def restrict_decode_vocab(batch_idx, prefix_beam):
        return INT_TOKEN_IDS
```

## vq-vae & rq-vae

### vq-vae

&nbsp;

[Neural discrete representation learning](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)

[https://github.com/zalandoresearch/pytorch-vq-vae](https://github.com/zalandoresearch/pytorch-vq-vae)

用在推荐：

[Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders](https://arxiv.org/pdf/2210.12316.pdf)

[https://github.com/RUCAIBox/VQ-Rec](https://github.com/RUCAIBox/VQ-Rec)

### rq-vae

&nbsp;

[Autoregressive Image Generation using Residual Quantization](https://arxiv.org/pdf/2203.01941.pdf)


## TIGER

[Recommender Systems with Generative Retrieval](https://arxiv.org/pdf/2305.05065.pdf)

序列推荐的一些paper：

+ [Session-based recommendations with recurrent neural networks](https://arxiv.org/pdf/1511.06939.pdf)：GRU4Rec首先把RNN用到推荐里
+ [Neural attentive session-based recommendation](https://arxiv.org/pdf/1711.04725.pdf)：提出NARM(Neural Attentive Session-based Recommendation)，在GRU里加了attention
+ [Next item recommendation with self-attention](https://arxiv.org/pdf/1808.06414.pdf)：AttRec在metric learning里引入了self-attention
+ [Self-attentive sequential recommendation](https://arxiv.org/pdf/1808.09781.pdf)：SASRec用了类似decoder-only的self-attention
+ [Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer](https://arxiv.org/pdf/1904.06690.pdf)和[Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.8562-6/246721374_422204999475172_9039387325224382577_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=e280be&_nc_ohc=pSC1QwlGTzIAX-yb9Ax&_nc_ht=scontent-sjc3-1.xx&oh=00_AfApZwRSd9KDBf4b-uGHf3bSS_SsZoRqhYS-lJRZltT97A&oe=6606DDFA)用了Transformer以及相关的mask策略
+ [S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization](https://arxiv.org/pdf/2008.07873.pdf)：在预训练阶段引入了4个自监督的task，4个MIMM(mutual information maximization)
  + item-attribute MIM
  + sequence-item MIM：sequence和被mask掉的一个item间
  + sequence-attribute MIM
  + sequence-sequence MIM：sequence和被mask掉的连续item构成的片段间

![s3-rec](../assets/s3-rec.png)

这些方法都是学习item的向量，然后用MIPS去ANN，而TIGER(Transformer Index for GEnerative Recommenders)则是生成式地直接预测item的语义id

![tiger](../assets/tiger.png)

![tiger-quant](../assets/tiger-quant.png)

![tiger-personalized](../assets/tiger-personalized.jpg)

rq-vae的介绍：[](https://arxiv.org/pdf/2306.08121)

![rq-vae](../assets/rq-vae.png)

原图里的下标有一些问题

+ $$x$$经过encoder得到的$$z$$（可以看成就是$$r_1$$），$$l=1$$，在第一个码本里ann找到最近的$$c_1=1$$，$$z-\boldsymbol{e}_{c_1}$$得到$$r_2$$
+ $$l=2$$，$$r_2$$在第2个码本里ann找到最近的$$c_2=4$$，$$r_2-\boldsymbol{e}_{c_2}$$得到$$r_3$$
+ $$l=3$$，$$r_3$$在第3个码本里ann找到最近的$$c_3=6$$，$$r_3-\boldsymbol{e}_{c_3}$$得到$$r_4$$
+ $$l=4$$，$$r_4$$在第4个码本里ann找到最近的$$c_4=2$$，$$r_4-\boldsymbol{e}_{c_4}$$得到$$r_5$$，
+ 对应的semantic id就是(1,4,6,2)，拿$$\hat {z} = 1+4+6+2$$再去过decoder得到$$\hat {x}$$

最终的loss：$$\mathcal{L}_{\text {recon }} + \mathcal{L}_{\text {rqvae }}$$：

+ $$\mathcal{L}_{\text {recon }}=\|\boldsymbol{x}-\hat{\boldsymbol{x}}\|^2$$
+ $$\mathcal{L}_{\text {rqvae }}=\sum_{l=1}^L \beta\left\|\boldsymbol{r}_l-\operatorname{sg}\left[\boldsymbol{e}_{c_l}\right]\right\|^2+\left\|\operatorname{sg}\left[\boldsymbol{r}_l\right]-\boldsymbol{e}_{c_l}\right\|^2$$，sg是stop gradient，这里原始的rq-vae是算的$$\mathcal{L}_{\text {commit }}=\sum_{d=1}^D\left\|\mathbf{Z}-\operatorname{sg}\left[\hat{\mathbf{Z}}^{(d)}\right]\right\|_2^2$$看来有点diff
+ 码本的emb通过moving average更新


rqvae的代码：[https://github.com/kakaobrain/rq-vae-transformer/blob/main/rqvae/models/rqvae/quantizations.py#L237](https://github.com/kakaobrain/rq-vae-transformer/blob/main/rqvae/models/rqvae/quantizations.py#L237)

+ VQ部分：

```python
class VQEmbedding(nn.Embedding):
    def compute_distances(self, inputs):
        codebook_t = self.weight[:-1, :].t()

        (embed_dim, _) = codebook_t.shape
        inputs_shape = inputs.shape
        assert inputs_shape[-1] == embed_dim

        inputs_flat = inputs.reshape(-1, embed_dim)
        # a^2
        inputs_norm_sq = inputs_flat.pow(2.).sum(dim=1, keepdim=True)
        # b^2
        codebook_t_norm_sq = codebook_t.pow(2.).sum(dim=0, keepdim=True)
        # (a-b)^2 = a^2 + b^2 - 2ab
        distances = torch.addmm(
            inputs_norm_sq + codebook_t_norm_sq,
            inputs_flat,
            codebook_t,
            alpha=-2.0,
        )
        distances = distances.reshape(*inputs_shape[:-1], -1)  # [B, h, w, n_embed or n_embed+1]
        return distances

    @torch.no_grad()
    def find_nearest_embedding(self, inputs):
        distances = self.compute_distances(inputs)  # [B, h, w, n_embed or n_embed+1]
        embed_idxs = distances.argmin(dim=-1)  # use padding index or not

        return embed_idxs

    @torch.no_grad()
    def _update_embedding(self):

        n_embed = self.weight.shape[0] - 1
        n = self.cluster_size_ema.sum()
        normalized_cluster_size = (
            n * (self.cluster_size_ema + self.eps) / (n + n_embed * self.eps)
        )
        self.weight[:-1, :] = self.embed_ema / normalized_cluster_size.reshape(-1, 1)

    def forward(self, inputs):
        embed_idxs = self.find_nearest_embedding(inputs)
        if self.training:
            if self.ema:
                self._update_buffers(inputs, embed_idxs)
        
        embeds = self.embed(embed_idxs)

        if self.ema and self.training:
            self._update_embedding()

        return embeds, embed_idxs
```

+ RQ部分

```python
class RQBottleneck(nn.Module):
    def __init__(...):
        codebooks = [VQEmbedding(self.n_embed[idx], 
                                    embed_dim, 
                                    decay=self.decay[idx], 
                                    restart_unused_codes=restart_unused_codes,
                                    ) for idx in range(self.code_shape[-1])]
        self.codebooks = nn.ModuleList(codebooks)
    def quantize(self, x):
        r"""
        Return list of quantized features and the selected codewords by the residual quantization.
        The code is selected by the residuals between x and quantized features by the previous codebooks.

        Arguments:
            x (Tensor): bottleneck feature maps to quantize.

        Returns:
            quant_list (list): list of sequentially aggregated and quantized feature maps by codebooks.
            codes (LongTensor): codewords index, corresponding to quants.

        Shape:
            - x: (B, h, w, embed_dim)
            - quant_list[i]: (B, h, w, embed_dim)
            - codes: (B, h, w, d)
        """
        B, h, w, embed_dim = x.shape

        residual_feature = x.detach().clone()

        quant_list = []
        code_list = []
        aggregated_quants = torch.zeros_like(x)
        for i in range(self.code_shape[-1]):
            quant, code = self.codebooks[i](residual_feature)
            # 就地操作，从residual_feature中减去quant的值，覆盖原来的值
            residual_feature.sub_(quant)
            aggregated_quants.add_(quant)

            quant_list.append(aggregated_quants.clone())
            code_list.append(code.unsqueeze(-1))
        
        codes = torch.cat(code_list, dim=-1)
        return quant_list, codes
    def forward(self, x):
        x_reshaped = self.to_code_shape(x)
        quant_list, codes = self.quantize(x_reshaped)

        commitment_loss = self.compute_commitment_loss(x_reshaped, quant_list)
        quants_trunc = self.to_latent_shape(quant_list[-1])
        quants_trunc = x + (quants_trunc - x).detach()

        return quants_trunc, commitment_loss, codes
    
    def compute_commitment_loss(self, x, quant_list):
        r"""
        Compute the commitment loss for the residual quantization.
        The loss is iteratively computed by aggregating quantized features.
        """
        loss_list = []
        
        for idx, quant in enumerate(quant_list):
            partial_loss = (x-quant.detach()).pow(2.0).mean()
            loss_list.append(partial_loss)
        
        commitment_loss = torch.mean(torch.stack(loss_list))
        return commitment_loss
```

## Tiger应用在排序

[Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations](https://arxiv.org/pdf/2306.08121)

[RecSys'24 | 谷歌:使用语义ID提升推荐系统ID Embed泛化性](https://mp.weixin.qq.com/s/PR8NuRWkK_4vuCrE9xD6Jg)

## meta提升id emb稳定性

[Meta广告:提出使用语义ID提升推荐系统ID Embed稳定性](https://mp.weixin.qq.com/s/DIF8ClTyN1y4ah98VR6asw)

[Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID](https://arxiv.org/pdf/2504.02137)



## 受限beam search相关

[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424)

[Autoregressive entity retrieval](https://arxiv.org/pdf/2010.00904)：代码：[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)

[Autoregressive Search Engines: Generating Substrings as Document Identifiers](https://arxiv.org/pdf/2204.10628)：例如一个item可以写成```<IDS> 1023 <IDE> Urban Decay Eyeshadow Palette Naked Heat <AS> Makeup <AE> <AS> Eyes <AE>```，并通过wavelet tree存储，给定开始token（如```<IDS>```或者```<AS>```），可以在$$O(Vlog(V))$$里找出所有可能的后续tokens。代码：[https://github.com/facebookresearch/SEAL](https://github.com/facebookresearch/SEAL)

## 百度的COBRA

[Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations](https://arxiv.org/abs/2503.02453)

![](../assets/cobra-vs-tiger.png)

和tiger很像，主要区别是除了原来rq的code，还加了个dense emb进去

![](../assets/cobra.png)
rq部分freeze，dense是拿item的明文特征过一个bert得到的cls emb，bert是训练的

decode出来有两个东西，rq的code和dense

![](../assets/cobra-search.png)

+ 先beam search出m个rq的code出来
+ 然后每一个code和原来的seq一起输入decoder，得到的pred就是dense，再拿这个dense去ann找n个item
+ 最后对这m*n个item拿两个相似度融合出一个得分，取topk出来

# LLM+推荐：输入ID

## 快手的RecGPT

[RecGPT: Generative Personalized Prompts for Sequential Recommendation via ChatGPT Training Paradigm](https://arxiv.org/pdf/2404.08675)

[https://zhuanlan.zhihu.com/p/699985083](https://zhuanlan.zhihu.com/p/699985083)

## Meta的HSTU（落地）

[如何评价Meta最新推荐论文: 生成式推荐打败深度分层架构推荐？](https://mp.weixin.qq.com/s/MeH2drBIBML5OSfCujIJ0Q)

[Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/pdf/2402.17152.pdf)

### 背景

&nbsp;

大规模推荐系统依赖高基数(high cardinality)、异质特性(heterogeneous features)，每天要处理上百亿用户行为数据。将推荐系统重新定义为生成模型框架内的序列转化任务，提出HSTU(Hierarchical Sequential Transduction Unit)，专为高基数、非稳态的流式推荐数据设计。

+ 在公开数据集比基线NDCG+65.8%
+ 在8192的序列长度上的处理速度比基于flash attention2的transformer快5.3-15.2倍。
+ 1.5万亿（1.5 trillion）参数，线上ab测试指标+12.4%

在[Breaking the curse of quality saturation with user-centric ranking](https://arxiv.org/pdf/2305.15333.pdf)中提到了NCE（normalized cross-entropy）指标：

XXX
\operatorname{NCE}(p, y)=\frac{\operatorname{CrossEntropy}(p, y)}{\operatorname{Entropy}(y)}
XXX

![nce-metric](../assets/nce-metric.png)

可以发现，对于一个预训练好的模型来说，随着时间的变化，NCE的变化是很剧烈的，即数据分布是会漂移的，并不像nlp/cv一样有ground truth，所以传统推荐模型其实很难达到比较好的scaling能力。

![nce-distribution-drift](../assets/nce-distribution-drift.png)

需要克服的3个挑战：

+ 推荐系统中的特征**缺少显式的结构**。序列建模（bert4rec、S3-rec等）在小规模数据集上效果不错，但工业界则需要**异构特征**（高基数的id、交叉特征、统计特征、历史点击率等）。
+ 推荐系统使用**十亿规模的动态词表**，而nlp用的是10w的静态词表，要对上万候选进行target-aware操作（din、Cold等），训练和推理的代价很高。
+ 大规模序列模型的计算成本是瓶颈。GPT-3在300B token的数据集上用上千个GPU训练1-2个月，而在推荐场景一天就有十亿级的活跃用户和十亿级的候选进行交互，用户序列在极端情况下有近10w（[Twin: Two- stage interest network for lifelong user behavior modeling in ctr prediction at kuaishou](https://arxiv.org/pdf/2302.02352.pdf)），所以推荐系统每天要处理的tokens数量甚至比语言模型1-2个月处理的数量要大几个数量级

本文将**用户action**看成一个新的模态，2个主要的insights：

+ 给定一个新的特征空间，核心的召回排序任务能被直接转换成生成式模型问题(序列直推任务，sequential transduction tasks)
+ 这种范式能够系统性地解决传统推荐中的特征冗余、计算冗余、推理冗余，提升效率

### Sequential Transduction Tasks

&nbsp;

transductive learning（直推式学习） vs inductive learning（归纳式学习）[https://www.zhihu.com/question/68275921/answer/529156908](https://www.zhihu.com/question/68275921/answer/529156908)：

+ 归纳式学习：训练只使用训练集，不使用测试集，训出的模型对测试集做预测，如监督学习。
+ 直推式学习：训练时使用训练集，还使用测试集的特征，但不使用测试集的label，

#### 异构特征的统一表示

&nbsp;

+ 稀疏特征：itemid、类目、城市、语言、社区等
    + 先选出**最长**的时间序列作为**主时间序列**，例如用户的交互序列。
    + 剩下的特征随时间变化较慢，如关注作者的属性。对于连续出现的片段（consecutive segment），**只保留最开始的入口**，这样对于主时间序列而言，并不会增加太多的序列长度。
+ 数值型特征：这里指的是序列特征里**每个item的统计特征**，比如用户在时刻t对某个item的ctr。直接删了，因为DIN里提到随着序列长度增加，**target-aware**的序列建模方式能够捕捉到这种数值性特征

![hstu-dlrm-vs-gr](../assets/hstu-dlrm-vs-gr.png)

假设用户消费了9个item，

+ 绿色的有7个时间步，全保留，作为主序列；
+ 蓝色的有7个时间步，但只有G0和G1两种取值，所以对于连续的G0只保留第0个(出现在t1)，扔到主序列的最前面去，连续的G1也只保留第0个（出现在t8），插到主序列最后一个的前面
+ 黄色的全是H0，第0个出现在t7，所以保留t7，往主序列t8前面插入
+ 将数值型特征替换为target-aware的cross attention得到causal-masked的特征
+ 通过t0、t1、t2（**包括t2**）的特征生成t2的样本，以此类推

#### 召回和排序的重定义

&nbsp;

输入token序列$$x_0, x_1, \ldots, x_{n-1}$$，输出的token序列$$y_0, y_1, \ldots, y_{n-1}$$是通过mask序列$$m_0, m_1, \ldots, m_{n-1}\left(m_i \in\{0,1\}\right)$$得到的。

token对应的动态、非稳态词表是$$\mathbb{X}$$，用户交互的内容是$$\mathbb{X}_c \subseteq \mathbb{X}$$

+ 召回：预估$$p\left(x_{i+1} \mid u_i\right)$$，通过时间步i的用户特征预估$$x_{i+1} \in \mathbb{X}_c$$，一般是直接选择$$\arg \max _{x \in \mathbb{X}_c} p\left(x \mid u_i\right)$$来最大化**特定的reward**，和标准的自回归有两个不同：
    + $$x_i,y_i$$的label并不一定是$$x_{i+1}$$，因为用户可以对$$x_{i+1}$$是负反馈
    + $$y_i$$可能是比如人口属性等（因为是merge的序列，可能把城市之类的merge进来当做一个时间步），这个时候$$y_i$$未定义的，要把它mask掉，即$$m_i=0$$
+ 排序：推荐中的排序需要在尽量早的阶段进行target-aware的交互，而标准的自回归这种交互往往比较迟，例如在encoder的输出才用上了softmax。因此，设计了一种**target-aware的cross-attention**
    + 把action和x穿插起来得到新序列$$x_0, a_0, x_1, a_1, \ldots, x_{n-1}, a_{n-1}$$
        + 对于action的位置，$$m_i=0$$
        + 对于content位置，使用一个小的nn将预估值转换成多任务的预估值（**感觉是输入0-n，第n项保留$$x_n$$，mask掉$$a_n$$，拿$$x_n$$去多目标地预估用户会用哪个action**）

#### 生成式训练

&nbsp;

假设用户$$i$$有$$n_i$$个token，那么训练的复杂度就是$$\sum_i n_i\left(n_i^2 d+n_i d_{f f} d\right)$$，其中：

+ $$n_i^2 d$$是self-attention的复杂度，通过flash attention可以达到$$O(n^2)$$
+ $$n_i d_{f f} d$$是FFN的复杂度
+ 因为要自回归地算，可以理解为batch_size也是$$n_i$$，加一个下三角的mask，所以在$$\sum$$里还要乘一个$$n_i$$

假设$$N=\max _i n_i$$，那复杂度就是$$O\left(N^3 d+N^2 d^2\right)$$，太巨大了

但其实可以发现（这段是自己的理解），假设序列长度9，要预测第4个的时候，4-9的输入是mask掉的，但他们还是要进行后面的attention+ffn计算，其实是很浪费资源的，所以如上面“**异构特征的统一表示**”小节的那个图（生成主序列和辅助序列）所示，该模型直接只吐出x=x1-x3,y=x4作为训练样本就行了，这样既省掉了4-9的无用计算，也更便于并行（相比不拆batch的自回归）

假设采样第$$i$$个用户的概率是$$s_u\left(n_i\right)$$，那么总的训练消耗就是

XXX
\sum_i s_u\left(n_i\right) n_i\left(n_i^2 d+n_i d^2\right)
XXX

如果设置$$s_u\left(n_i\right)=1/n_i$$，那消耗就降到了$$O\left(N^2 d+N d^2\right)$$。而在工业界中要实现这种采样其实很简单，在用户请求或者session结束的时候吐出训练样本就有$$\hat{s_u}\left(n_i\right) \propto 1 / n_i$$了

### 生成式推荐中的高性能自注意力编码器

&nbsp;

![hstu-arch](../assets/hstu-arch.png)

HSTU的单层包括3个部分：

XXX
\text{pointwise\ projection:} U(X), V(X), Q(X), K(X)=\operatorname{Split}\left(\phi_1\left(f_1(X)\right)\right)
XXX

XXX
\text{spatial\ aggregation:} A(X) V(X)=\phi_2\left(Q(X) K(X)^T+\operatorname{rab}^{p, t}\right) V(X)
XXX

XXX
\text{pointwise\ transformation:} Y(X)=f_2(\operatorname{Norm}(A(X) V(X)) \odot U(X))
XXX

其中，

+ $$f_i(x)$$是MLP，$$f_i(X)=W_i(X)+b_i$$
+ $$\phi_1$$和$$\phi_2$$是激活函数，都是SiLU([Sigmoid-weighted linear units for neural network function approximation in reinforcement learning](https://arxiv.org/pdf/1702.03118.pdf))，其实就是Swish，即$$f(x)=x \cdot sigmoid(x)$$
+ $${rab}^{p, t}$$是relative attention bias，考虑了位置$$p$$和时间$$t$$(参考T5论文，[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf))，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的

对应到代码([https://github.com/facebookresearch/generative-recommenders/blob/main/modeling/sequential/hstu.py](https://github.com/facebookresearch/generative-recommenders/blob/main/modeling/sequential/hstu.py))里：

```python
self._linear_dim: int = linear_hidden_dim
self._uvqk = torch.nn.Parameter(
    torch.empty((embedding_dim, linear_hidden_dim * 2 * num_heads + 
        attention_dim * num_heads * 2)).normal_(mean=0, std=0.02),
)

##...

batched_mm_output = torch.mm(normed_x, self._uvqk)
if self._linear_activation == "silu":
    batched_mm_output = F.silu(batched_mm_output)
elif self._linear_activation == "none":
    batched_mm_output = batched_mm_output
# 其实就是先乘一个大矩阵，再拆成4份，等价于乘4个小矩阵
u, v, q, k = torch.split(
    batched_mm_output,
    [self._linear_dim * self._num_heads, self._linear_dim * self._num_heads, 
    self._attention_dim * self._num_heads, self._attention_dim * self._num_heads],
    dim=1,
)

if self._normalization == "rel_bias" or self._normalization == "hstu_rel_bias":
    if delta_x_offsets is not None:
        padded_q, padded_k = cached_q, cached_k
        flattened_offsets = delta_x_offsets[1] + torch.arange(start=0, end=B * n, 
            step=n, device=delta_x_offsets[1].device, dtype=delta_x_offsets[1].dtype)
        padded_q = padded_q.view(B * n, -1).index_copy_(
            dim=0, index=flattened_offsets, source=q,
        ).view(B, n, -1)
        padded_k = padded_k.view(B * n, -1).index_copy_(
            dim=0, index=flattened_offsets, source=k,
        ).view(B, n, -1)
    else:
        padded_q = torch.ops.fbgemm.jagged_to_padded_dense(
            values=q, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )
        padded_k = torch.ops.fbgemm.jagged_to_padded_dense(
            values=k, offsets=[x_offsets], max_lengths=[n], padding_value=0.0
        )

    qk_attn = torch.einsum(
        "bnhd,bmhd->bhnm",
        padded_q.view(B, n, self._num_heads, self._attention_dim),
        padded_k.view(B, n, self._num_heads, self._attention_dim),
    )
    if all_timestamps is not None:
        qk_attn = qk_attn + self._rel_attn_bias(all_timestamps).unsqueeze(1)
    qk_attn = F.silu(qk_attn) / n
    qk_attn = qk_attn * invalid_attn_mask.unsqueeze(0).unsqueeze(0)
    attn_output = torch.ops.fbgemm.dense_to_jagged(
        torch.einsum(
            "bhnm,bmhd->bnhd",
            qk_attn,
            torch.ops.fbgemm.jagged_to_padded_dense(v, [x_offsets], [n]).\
                reshape(B, n, self._num_heads, self._linear_dim)
        ).reshape(B, n, self._num_heads * self._linear_dim),
        [x_offsets],
    )[0]

```

DLRM的3部分：

+ 特征抽取：常见的基础版本是离散特征pooling，高级版本是din。HSTU本来就能做这种target-aware的attention
+ 特征交互：常见的使用FMs、DCNv2、DHEN([DHEN: A Deep and Hierarchical Ensemble Network for Large-Scale Click-Through Rate Prediction](https://arxiv.org/pdf/2203.11014.pdf)，引入残差连接)。HSTU通过$$\operatorname{Norm}(A(X) V(X)) \odot U(X)$$来实现，将attention pooled后的结果直接和其他特征算element-wise product。
    + 这个做法受[Neural collaborative filtering vs. matrix factorization revisited](https://arxiv.org/pdf/2005.09683.pdf)和[Revisiting neural retrieval on accelerators](https://arxiv.org/pdf/2306.04039.pdf)的启发，用MLP来近似点积是很困难的。原因大概是nn需要调超参和足够的训练数据，在线算得又慢，而且本来内积效果就不错了，nn能带来的边际收益其实不明确。
    + 因为$$U(X)$$已经用过SiLU了，所以$$\operatorname{Norm}(A(X) V(X)) \odot U(X)$$可以看成是SwiGLU的变种（参考[Glu variants improve transformer](https://arxiv.org/pdf/2002.05202.pdf)），因为前面在对比LLM激活函数时讲了，$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \odot \mathbf{x}_2$$
+ 表示转换：常见的如MoE、PLE等，主要思想就是对不同人群用特定的子网络。HSTU里的element-wise product也能达到MoE中的门控操作，只是可能有一个正则化因子的区别。

其实就是原来一般会拆成attention+ffn，而它这3个公式，前两个是attention，第3个就是attention求个norm，然后过一个swiglu的ffn，还有一点，这里的$$U(x)$$是过attention之前的，感觉起到了类似resnet的作用


#### pointwise聚合的注意力

&nbsp;

用的是pointwise聚合的注意力，而不是transformer里的softmax，主要有如下两点考虑：

+ 推荐中item的**强度**信息很重要，softmax会让这种强度失真，导致在预估（如时长）不准；如果只需要预估序，那其实softmax也可以，但推荐要**同时预估序和值**，所以要删掉softmax。
+ 虽然softmax对噪声有鲁棒性，但不太适用于流式setting下的非稳态词表。做了一个模拟流式数据的实验，发现只去掉relative attention bias比relative attention bias并加上softmax会好得多

| Architecture              | HR @10 | HR @50 |
|---------------------------|--------|--------|
| Transformers              | .0442  | .2025  |
| HSTU ($$-rab^{p,t}$$, Softmax) | .0617  | .2496  |
| HSTU ($$-rab^{p,t}$$)          | .0893  | .3170  |

#### 增加稀疏性

&nbsp;

使用了一种高效的attention kernel的GPU算子，类似FlashAttention，能够将**融合连续的矩阵操作**（fuse back-to-back GEMMs），但能够
进行fully raggified（可能是不规则，即序列长度可变的？？）的attention计算，本质是将attention计算转换为**不同大小的分组GEMMs**。因此，HSTU变成了memory-bound，并且能够以$$\Theta\left(\sum_i n_i^2 d_{q k}^2 R^{-1}\right)$$进行scale，其中，$$n_i$$是样本$$i$$的序列长度，$$d_{qk}$$是attention的维度，$$R$$是寄存器的大小。

受[Deep networks with stochastic depth]()启发，提出了SL(stochastic length)来增加用户历史序列的稀疏性，推荐系统中的用户行为往往有周期性，并且以不同形式呈现，因此引入稀疏性可以在效果不怎么损失的情况下显著减少encoder的代价，可以scale为$$\Theta\left(\sum_i n_i^2\right)$$

定义$$\Gamma(n, L)$$为一个函数，从原序列$$x_0, \ldots, x_{n-1}$$中选出长度为$$L$$的子序列，具体方案如下：

XXX
\begin{aligned}
& x_0, \ldots, x_{n_i-1} \text { if } n_i \leq N^{\alpha / 2} \\
& \Gamma\left(n_i, N^{\alpha / 2}\right) \text { if } n_i>N^{\alpha / 2}, \text { w/ probability } 1-N^\alpha / n_i^2 \\
& x_0, \ldots, x_{n_i-1} \text { if } n_i>N^{\alpha / 2}, \text { w/ probability } N^\alpha / n_i^2 \\
&
\end{aligned}
XXX

即：

+ $$n_i \leq N^{\alpha / 2}$$时，保留原始序列
+ $$n_i>N^{\alpha / 2}$$时，有$$N^\alpha / n_i^2 < 1$$：
    + 以$$1-N^\alpha / n_i^2$$的概率只保留$$N^{\alpha / 2}$$长度的子序列
    + 以$$N^\alpha / n_i^2$$的概率保留原始序列

对于$$\alpha \in(1,2]$$，原来的attention相关的复杂度是$$O\left(N^2 d\right)$$，现在可以降低到$$O\left({N^{\alpha /2}}^2 d\right)=O\left(N^\alpha d\right)$$。

其中的$$\Gamma(n, L)$$经过离线实验（原文附录D里），采用了feature-weighted sampler，即以$$1-f_{n, i} /\left(\sum_{j=1}^L f_{j, i}\right)$$的概率进行采样，其中$$f_i=t_n-t_i$$表示用户和item $$x_i$$交互的时间和当前的时间差。

对稀疏性的改善如下，其中稀疏性指的是$$1-avg\_seq\_len/max\_seq\_len$$，越大表示短序列越多，即越稀疏($$\alpha=2$$表示不SL，即直接使用原序列)：

| Alpha ($$\alpha$$) | seq_len=1,024 | seq_len=2,048 | seq_len=4,096 | seq_len=8,192 |
|-----------|-------|-------|-------|-------|
| 1.6       | 71.5% | 76.1% | 80.5% | 84.4% |
| 1.7       | 56.1% | 63.6% | 69.8% | 75.6% |
| 1.8       | 40.2% | 45.3% | 54.1% | 66.4% |
| 1.9       | 17.2% | 21.0% | 36.3% | 64.1% |
| 2.0       | 3.1%  | 6.6%  | 29.1% | 64.1% |

#### 最小化激活值的内存使用

&nbsp;

在推荐系统中，**大的batchsize**很重要：

+ 训练吞吐：[Software-hardware co-design for fast and scalable training of deep learning recommendation model](https://arxiv.org/pdf/2104.05158.pdf)一文说的
+ 模型质量：[Mixed negative sampling for learning two-tower neural networks in recommendations](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/b9f4e78a8830fe5afcf2f0452862fb3c0d6584ea.pdf)、[A simple framework for contrastive learning of visual representations](https://proceedings.mlr.press/v119/chen20j/chen20j.pdf)和[Revisiting neural retrieval on accelerators](https://arxiv.org/pdf/2306.04039.pdf)

因此激活函数的内存占用就成为了主要的scaling瓶颈，这一点和llm不一样，llm一般是用小batchsize，并且内存主要由网络参数占据。HSTU设计了如下方式来减少激活函数的内存占用：

+ 把attention外的linear layers从6减小到2，同时使用elementwise gating来降低MLP的计算（[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)和[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)）。第一篇对应的结构如下：

![gated-attention-unit](../assets/gated-attention-unit.png)

假设$$d$$是embed size，$$h$$是head数，$$d_{q k}$$是attention的dim，$$d_{ff}$$是ffn的hidden size，

+ 将一些计算融合成一个op，包括$$\phi_1\left(f_1(\cdot)\right)$$、layer_norm、optional dropout和输出MLP，将每一层的激活的内存占用减小到了$$2 d+2 d+4 h d_{q k}+4 h d_v+2 h d_v=14 d$$（以bf16计算，一个参数2字节）---没懂

自己的理解：layer_norm要存均值+方差，所以要$$2d\times 2bytes$$，所以上面式子就是，U($$2d$$)，V($$2d$$)，Q和K($$4hd_{qk}$$)，QKV的norm(因为有均值和方差，所以是$$4hd_v$$)，QKV($$2hd_v$$)

对比transformer，在attention后用了ffn和dropout，假设中间状态是$$3hd_v$$，那么ffn包括layer_norm、linear、激活、linear、dropout，中间状态占用的就是$$2 d+4 d_{f f}+2 d+1 d=4 d+4 d_{f f}$$，一般来说，$$h d_v \geq d$$，$$d_{f f}=4 d$$，

自己的理解：输入x($$2d$$)，linear($$2d_{ff}$$)，linear($$2d$$)，dropout(约等于$$1d$$)，layernorm一般是发生在最开始吧，所以应该是第一个$$2d$$改成$$4d$$吧，感觉不是在linear那里变成$$4d_{ff}$$。。

然后，加上输入的input和input的layer_norm($$4d$$)，和qkv的映射，总的激活状态是$$33d$$---没懂

所以HSTU的设计能够让scaling达到大于两倍的更深的layers(14d vs 33d)

此外，词表中的id占用了极大的内存，对于10b的词表，512维的emb，Adam优化器，用fp32来存储emb和优化器状态要60TB的内存，因此，

+ 使用**row-wise的AdamW**优化器（[Training highly multiclass classifiers](https://www.jmlr.org/papers/volume15/gupta14a/gupta14a.pdf)和[FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference](https://arxiv.org/pdf/2101.05615.pdf)）
+ 将优化器状态存在DRAM里，从而每一个float在HBM的占用从12bytes降低到2bytes

#### cost-amortization(摊销)的预估scale up 

&nbsp;

对于召回来说，已经有很多加速方法了，例如MIPS的ANN加速，或者OTM等的beam search方法。

对于排序而言，提出了M-FALCON(Microbatched-Fast Attention Leveraging Cacheable OperatioNs)，用于对$$m$$个候选，序列长度为$$n$$的输入进行预估

+ 并行计算$$b_m$$个候选，修改attention masks和$$rab^{p,t}$$ bias，使得这$$b_m$$个候选的attention操作是完全一样的。从而将cross-attention的计算从$$O\left(b_m n^2 d\right)$$缩减到了$$O\left(\left(n+b_m\right)^2 d\right)=O(n^2d)$$，因为$$b_m$$相比$$n$$要小得多
+ (可选)将$$m$$个候选分成$$\left\lceil m / b_m\right\rceil$$个microbatches，每个batch有$$b_m$$个候选，从而在如下两个场合利用KV caching([Efficiently scaling transformer inference](https://arxiv.org/pdf/2211.05102.pdf))：
    + 前向pass中，用于降低消耗
    + requests之间，降低长尾耗时

#### 其他

&nbsp;

发现了scaling-law：

![hstu-scaling-law](../assets/hstu-scaling-law.png)

## 快手的OneRec（听说比较扯淡）

[OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment](https://arxiv.org/pdf/2502.18965)

[「快手」全链路统一建模｜OneRec: Unifying Retrieve and Rank with Generative](https://mp.weixin.qq.com/s/wTEWnSe8aFaPiLsnoIeOSA)

召回排序合为一体的生成式推荐——根据历史session序列，预测下一个session。

## 阿里的HeteroRec（落地）

[Hierarchical Causal Transformer with Heterogeneous Information for Expandable Sequential Recommendation](https://arxiv.org/pdf/2503.01469)

item有各种特征，如title、img、id等

+ 左边：异构信息分解（HTFL）把1-T个item的各种特征（每个特征当成一个token）flatten成一个长序列，然后过一个causal transformer得到token粒度的输出list，再过一个item粒度的causal transformer得到item粒度的输出list
+ 右边：$$2 \sim T+1$$个item，每一种特征把$$2 \sim T+1$$拼到一起过各自的tower，每个tower得到的感觉还是$$2 \sim T+1$$个输出，这个输出和左边的token粒度的输出List算个LMP，然后过一个item cross tower，我猜输出的是$$2 \sim T+1$$个item表示，和左边的item粒度输出List再算一个LMP
+ LMP：左边是$$1 \sim T$$，右边是$$2 \sim T+1$$，t1过个mlp预测t3，t1过另一个MLP预测t2，即multi-step预估

![](../assets/HeteroRec.png)

## Meta的DTI（没落地）

[Towards An Efficient LLM Training Paradigm for CTR Prediction](https://arxiv.org/pdf/2503.01001)

![](../assets/dti.png)

。。好像没啥，就是预测未来k个，并且设计了滑动窗口的mask（右下角）

## 华为的UniGRF（没落地）

[Killing Two Birds with One Stone: Unifying Retrieval and Ranking with a Single Generative Recommendation Model](https://arxiv.org/pdf/2504.16454)

[sigir‘25「华为」统一召回+排序框架](https://mp.weixin.qq.com/s/eKU_AlffGR1Lx_wQspuLpQ?poc_token=HEPlGGijUUqAuU1VTCkeSPkWDfZG99B1eN5xtLkh)

## 美团的MTGR（落地）

[MTGR：美团外卖生成式推荐Scaling Law落地实践](https://mp.weixin.qq.com/s/JiDOqD-ThU0Upp6xnNg3Nw)

[推荐算法(2025.5.29)：美团的生成式推荐落地方案](https://mp.weixin.qq.com/s/hQrLSInDnrO5kgCW8I6gCQ)

[MTGR: Industrial-Scale Generative Recommendation Framework in Meituan](https://www.arxiv.org/pdf/2505.18654v2)

听说是替换了精排

## 阿里的SORT-Gen（落地）

[sigir'25「淘宝」多目标重排｜A Generative Re-ranking Model for List-level](https://mp.weixin.qq.com/s/b0PdyR4bFUXhLj9dToBK5A)

[A Generative Re-ranking Model for List-level Multi-objective Optimization at Taobao](https://arxiv.org/pdf/2505.07197)

将用户的所有行为聚合成一条样本。每个行为包括四个方面：用户侧、位置、商品侧、精排打分，将行为序列送入tansformer中。

保序回归损失

生成方法：先设计不同的rankscore得到多路序列候选，再比较每一路的top1，决定召回哪一路。

纯价值驱动的列表生成可能有同质化的风险，导致多样性降低。引入相似度惩罚：

## 小红书的RankGPT（落地）

[小红书RankGPT:推荐系统生成式精排的落地实践](https://mp.weixin.qq.com/s/2D31iyO07DNdTBPdHp-3Qw)

[Towards Large-scale Generative Ranking](https://arxiv.org/abs/2505.04180)

## 美团的UniROM（落地）

[美团UniROM:广告End2End全链路生成式建模范式](https://mp.weixin.qq.com/s/GfuxMwprP5TVSUTBl7Wc6w)

[One Model to Rank Them All: Unifying Online Advertising with End-to-End Learning](https://arxiv.org/abs/2505.19755)


# LLM+推荐：输入文本

## 华为的KAR（落地）

[Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models](https://arxiv.org/pdf/2306.10933)

![](../assets/kar-arch.png)

+ LLM生成user侧的preference reasoning知识
+ LLM生成item侧的factual知识
+ 这两部分知识输入一个encoder得到两个表示，然后再过个mmoe产出merge后reasoning augmented vec和fact augmented vec，一起作为CRM的输入特征。

对应的prompt如下：

![](../assets/kar-prompts.png)

看实验应该是用在召回，每个user和vec去找最像的k个item，但没法ann，只能暴力算

代码：[https://github.com/YunjiaXi/Open-World-Knowledge-Augmented-Recommendation/blob/main/knowledge_encoding/utils.py](https://github.com/YunjiaXi/Open-World-Knowledge-Augmented-Recommendation/blob/main/knowledge_encoding/utils.py)，其实就是把模型输出的hidden states处理一下：

```python

x = tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors="pt",
                return_attention_mask=True).to(device)
mask = x['attention_mask']
outputs = model(**x, output_hidden_states=True, return_dict=True)
pred = get_paragraph_representation(outputs, mask, aggregate_type)
def get_paragraph_representation(outputs, mask, pooler='cls', dim=1):
    last_hidden = outputs.last_hidden_state
    hidden_states = outputs.hidden_states

    # Apply different poolers

    if pooler == 'cls':
        # There is a linear+activation layer after CLS representation
        return outputs.pooler_output.cpu()  # chatglm不能用，用于bert
    elif pooler == 'cls_before_pooler':
        return last_hidden[:, 0].cpu()
    elif pooler == "avg":
        return ((last_hidden * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)).cpu()
    elif pooler == "avg_first_last":
        first_hidden = hidden_states[1]
        last_hidden = hidden_states[-1]
        pooled_result = ((first_hidden + last_hidden) / 2.0 * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)
        return pooled_result.cpu()
    elif pooler == "avg_top2":
        second_last_hidden = hidden_states[-2]
        last_hidden = hidden_states[-1]
        pooled_result = ((last_hidden + second_last_hidden) / 2.0 * mask.unsqueeze(-1)).sum(dim) / mask.sum(dim).unsqueeze(-1)
        return pooled_result.cpu()
    elif pooler == 'len_last':  # 根据padding方式last方式也不一样
        lens = mask.unsqueeze(-1).sum(dim)
        # index = torch.arange(last_hidden.shape[0])
        # print(index)
        pooled_result = [last_hidden[i, lens[i] - 1, :] for i in range(last_hidden.shape[0])]
        pooled_result = torch.concat(pooled_result, dim=0)
        return pooled_result.cpu()
    elif pooler == 'last':
        if dim == 0:
            return last_hidden[-1, :, :]
        else:
            return last_hidden[:, -1, :]
    elif pooler == 'wavg':
        # Get weights of shape [bs, seq_len, hid_dim]
        weights = (
            torch.arange(start=1, end=last_hidden.shape[1] + 1)
            .unsqueeze(0)
            .unsqueeze(-1)
            .expand(last_hidden.size())
            .float().to(last_hidden.device)
        )

        # Get attn mask of shape [bs, seq_len, hid_dim]
        input_mask_expanded = (
            mask
            .unsqueeze(-1)
            .expand(last_hidden.size())
            .float()
        )

        # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim
        sum_embeddings = torch.sum(last_hidden * input_mask_expanded * weights, dim=dim)
        sum_mask = torch.sum(input_mask_expanded * weights, dim=dim)

        pooled_result = sum_embeddings / sum_mask
        return pooled_result.cpu()
    else:
        raise NotImplementedError
```

## 蚂蚁的BAHE（落地）

[SIGIR'24 \| 打破长度障碍：LLM增强的长文本用户行为CTR预测](https://mp.weixin.qq.com/s/h0p1QrapTGxOonccxNVNuQ)

[Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors](https://arxiv.org/pdf/2403.19347)

![](../assets/bahe.png)

+ 防止由于重复编码相同用户行为而产生的计算冗余：利用LLMs的预训练浅层提取来自用户序列的最细粒度的原子用户行为(例如购买星巴克、订酒店)的emb，并将它们存储在离线数据库中
+ 从db里查出来，和item一起过LLMs的更深层可训练层


## 快手的LEARN（落地）

[快手广告领域的大模型技术探索与实践](https://mp.weixin.qq.com/s/9VmOb3q4enWhqqJ_f7jnFA)

[https://zhuanlan.zhihu.com/p/705497209](https://zhuanlan.zhihu.com/p/705497209)

[Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application](https://arxiv.org/pdf/2405.03988)

过往的llm+推荐的两种思路：

+ freeze LLM参数并适应推荐领域数据：将用户行为历史改写成文本prompt，直接丢给LLM生成top-k推荐结果，例如：
  + [Chat-rec: Towards interactive and explainable llms-augmented recommender system](https://arxiv.org/pdf/2303.14524)
  + [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://arxiv.org/pdf/2305.08845v2)
  + [Is ChatGPT a Good Recommender? A Preliminary Study](https://arxiv.org/pdf/2304.10149)
  + [LLM-Rec: Personalized Recommendation via Prompting Large Language Models](https://arxiv.org/pdf/2307.15780)
  + [Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences](https://arxiv.org/pdf/2307.14225)
  + [Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/pdf/2305.07609)
+ 在推荐领域的特定文本数据集上微调LLM：利用LLM捕捉用户行为序列，通过设计提示prompt，使LLM学习用户和物品之间的潜在关系，在预测任务中理解用户的偏好变化和行为模式，从而更好地预测用户可能感兴趣的物品，例如：
  + [A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://arxiv.org/pdf/2308.08434)
  + [Tallrec: An effective and efficient tuning framework to align large language model with recommendation](https://arxiv.org/pdf/2305.00447)
  + [Llara: Aligning large language models with sequential recommenders](https://arxiv.org/pdf/2312.02445)
  + [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131)
  + [Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/abs/2303.13835)

上面两种方法可以看成是Rec-to-LLM，即将推荐这个target domain适配到LLM这个source domain上去，有如下缺点：

+ 将用户历史全丢给LLM不现实：一方面开源的LLM目前只支持1k(baichuan)-4k(llama)，不支持这么长的序列，另一方面复杂度和序列长度呈二次关系
+ 微调的方案可能会出现灾难性遗忘(catastrophic forgetting)：全参数微调，会让模型**丢失在预训练过程中学到的开放世界的知识**，而LoRA的效果也不好。原因：
  + domain gap：两个领域有巨大的差别(profound gap)，[Continual Learning of Large Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2404.16789)发现了全量参数微调会导致LLM对原有知识domain的严重的灾难性遗忘。
  + 训练目标不对齐（misalignment）：LLM是next token prediction，学习大语料的general知识；finetune则主要是检索类的任务，强依赖用户-item的交互行为

其实有rec-to-llm和llm-to-rec两类想法

![rec2llm-llm2rec](../assets/rec2llm-llm2rec.png)

快手商业化把llm用到推荐的思路变化：

![](../assets/learn-roadmap.png)


本文提出了LEARN(Llm-driven knowlEdge Adaptive RecommeNdation)，实现了LLM-to-Rec，让LLM作为content extractor，推荐任务是训练目标。

![learn-cex-pal](../assets/learn-cex-pal.png)

+ Content EXtraction(CEX)：输入item描述，过预训练的LLM（参数freeze）得到的每个token的输出pooling得到item的emb $$E^c$$
+ Preference ALignment(PAL)：把$$H$$个$$E^c$$输入一个content adapter(就是一个MLP)，然后过12层的可训练的transformer得到$$H$$个输出emb，最后再过一个online projection(也是一个MLP)降维到64，得到$$E^{user}$$

![learn](../assets/learn.png)

假设用户点击了H+T个item，拆成1~H和H+1~H+T这两部分，

+ user tower：1~H的item输入CEX+PAL，得到H个输出
+ item tower：有3个，结构都是输入H+1~H+T的item，网络参数和user tower共享；infer时只输入一个item
    + item tower a：经过CEX得到$$E^c$$，再经过PAL得到$$E^{item}$$，和user tower一样用causal attention
    + item tower b：经过CEX得到$$E^c$$，再经过PAL得到$$E^{item}$$，使用self-attention，每个item只关注自己
    + item tower c：只经过CEX，直接得到$$E^{item}$$

消融后发现item tower a效果最好，所以用了a

loss用的是pinnerformer的dense all action loss（[PinnerFormer: Sequence Modeling for User Representation at Pinterest](https://arxiv.org/pdf/2205.04507)），即从```1~H```里随机挑m个出来，每一个去预估```H+1~H+T```里的随机一个正样本

![](../assets/pinnerformer-loss.png)

在线使用时如下图，其中user emb和item emb就是两个tower的输出，会先concat并过个小fusion module得到fusion emb，然后一方面过个mlp算个cvr loss，另一方面和user emb、item emb还有其他特征concat再送给后面的模块正常算ranking loss

![](../assets/learn-adaptor.png)


## 阿里的BEQUE（落地）

[LLM落地淘宝电商搜索场景，显著提升长尾query改写效果](https://mp.weixin.qq.com/s/GmogjAHt0Hrwd8RmtOqS7A)

[Large Language Model based Long-tail Query Rewriting in Taobao Search](https://arxiv.org/pdf/2311.03758) WWW24

![](../assets/beque.png)

+ multi-instructions的sft：基于在线日志，通过拒绝采样得到multi-instrunctions的SFT数据集，任务是重写query，还混合了质量分类、query改正、CoT任务

其中的3个task如下：

| Task | Prompt Example |
|---------------------|---------------------|
| Quality Classification | Is this a good e-commerce query rewrite? <br> Query: {query} <br> Rewrite: {rewrite} <br> System: {Yes or No}  |
| Title Prediction | Please generate product titles that match input query <br> Query: {query} <br> System: {product title}  |
| Chain of Thought | Your task is to rewrite the input query into a query that makes it easier to search for related products, and you are required to give the thought process and then the query rewriting result. The thought process and the query rewriting result are separated by a semicolon. <br> Query: {query} <br> System: {CoT}; {Rewrite} |

+ offline feedback：用训好的LLM生成多个候选rewrites，构建了一个离线的淘宝系统，对这些候选rewrites进行搜索得到结果，并用结果的quality scores来对候选rewrites进行排序
+ objective alignment：基于候选的rewrites的排序，使用PRO方法（[Preference ranking optimization for human alignment](https://arxiv.org/pdf/2306.17492)）来逼近这个排序。

给定顺序$$y_1>y_2$$，preference probability如下，其中$$r(\cdot)$$是reward函数：

XXX
P_{B T}=\frac{\exp \left(r\left(y_1, x\right)\right)}{\exp \left(r\left(y_1, x\right)\right)+\exp \left(r\left(y_2, x\right)\right)}
XXX

对应的PRO loss如下，$$\mathcal{T}_k^i=1 /\left(r\left(y_k\right)-r\left(y_i\right)\right)$$，$$\mathcal{T}_k^k=\min _{i>k}\left(\mathcal{T}_k^i\right)$$：

XXX
\mathcal{L}_{P R O}(\theta)=-\mathrm{E}_{(x, y) \sim \mathcal{D}_{P R O}} \sum_{k=1}^{n-1} \log \frac{\exp \left(\frac{\pi_{P R O}\left(y_k \mid x ; \theta\right)}{\mathcal{T}_k^k}\right)}{\sum_{i=k}^n \exp \left(\frac{\pi_{P R O}\left(y_i \mid x ; \theta\right)}{\mathcal{T}_k^i}\right)}
XXX

应用：离线刷库，实验的gmv、单量都有收益，长尾query效果更好

## 小红书的NoteLLM（落地）

[WWW'24 | 小红书NoteLLM: 大语言模型用于I2I笔记推荐](https://mp.weixin.qq.com/s/jcj4jKaEIg-L264uZgYuAw)

[NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://arxiv.org/pdf/2403.01744)

3个任务：

+ I2I笔记推荐任务: 给定目标笔记，基于LLM从内容池中找出top-k个相似笔记
+ 主题标签生成任务：基于笔记的标题和内容, 用LLM生成对应的k个主题标签。
+ 类目生成任务：基于笔记的标题、内容、主题标签，用LLM生成对应的类目。

笔记压缩prompt：

```
[bos]<instruction><input notes>The compression word is:"[EMB]".<output guidance><output>[eos]
```

其中的[EMB]是一个特殊token，类比bert里的[CLS]

![notellm](../assets/notellm.png)

### 生成式对比学习

&nbsp;

生成式对比学习(Generative-Contrastive Learning,GCL)

+ 通过统计共现信息，同时打压热门，得到item pair对
+ 由于LLMs的自回归特性, 将[EMB]的**前一个token**对应的**最后一个隐层输出**经过一个linear layer得到一个d维向量

对于batchsize=B的batch来说，有B个pair对，即2B个item，那么对它们算一个对比学习的loss，其中的sim是cos相似度：

XXX
L_{c l}=-\frac{1}{2 B} \sum_{i=1}^{2 B} \log \frac{e^{\operatorname{sim}\left(\boldsymbol{n}_i, \boldsymbol{n}_i^{+}\right) \cdot e^\tau}}{\sum_{j \in[2 B] \backslash\{i\}} e^{\operatorname{sim}\left(\boldsymbol{n}_i, \boldsymbol{n}_j\right) \cdot e^\tau}}
XXX

### 协同监督微调

&nbsp;

协同监督微调(Collaborative Supervised Fine-Tuning,CSFT)将主题标签生成任务和类目生成任务联合训练，一个batch里40%的样本执行主题标签生成任务，剩下的60%做类目预测任务，然后走正常的语言模型自回归任务：

XXX
L_{g e n}=-\frac{1}{T} \sum_{i=1}^T \log \left(p\left(o_i \mid o_{<i}, input\right)\right)
XXX

然后将这两个loss加权求和($$\alpha = 0.01$$)：

XXX
L=\frac{L_{c l}+\alpha L_{g e n}}{1+\alpha},
XXX

## 小红书的NoteLLM-2（落地）

[小红书 NoteLLM-2：用于推荐的多模态表征](https://mp.weixin.qq.com/s/kVLHdFdHXGVsDbLQJrDTXQ)


## Google的2个LLM（落地）

[谷歌短视频推荐:用户反馈对齐的LLM用于新颖推荐](https://mp.weixin.qq.com/s/ff34lzbbTVREQG6FSpdgeg)

### novelty LLM

[Llms for user interest exploration in large-scale recommendation systems](https://arxiv.org/abs/2405.16363)

对llm进行sft，输入用户喜欢的K个cluster，预测下一个cluster。

![](../assets/novelty-llm.png)

cluster包括了一系列短语，如(猫，狗，动物，宠物，小动物)可能是一个cluster，(船，轮船，皮划艇，独木舟)可能又是另一个cluster。使用的2级cluster，共761个取值。

基于gemini进行sft，拿7000多个样本（每个$$C_1$$,$$C_2$$组合，找到最高频共现的10个$$C_L$$出来），bs=16，训了3000个step

线上生效：然后K=2，即只需要batch跑$$761\times 761=579121$$个组合的infer就行了。来一个用户，拿出他的2个cluster的组合，去存储里找到对应的预估的下一个cluster


### 2个LLM

[User Feedback Alignment for LLM-powered Exploration in Large-scale Recommendation Systems](https://arxiv.org/pdf/2504.05522)

![](../assets/novelty-llm-alignment-llm.png)

先拿novelty-llm开实验(也可以直接拿线上这路召回的结果)，收集用户对推荐出来的这些类目的正负反馈，然后组织成pointwise或pairwise的语料，训练一个类似reward model的alignment-llm。

![](../assets/alignment-llm.png)

纯离线infer：用novelty llm采用比较高的temperature预测多次，然后用alignment llm打分选挑出topk的cluster，存起来给线上用。

实验对比线上基线的novelty-llm，pointwise和pairwise在多样性、正反馈动作率、完成率均有提升，鈤pointise训练得更快，所以最终推全pointwise

## 快手的LARM（落地）

[LLM-Alignment Live-Streaming Recommendation](https://arxiv.org/pdf/2504.05217)

![](../assets/larm.jpeg)

结合直播的特点：同一个直播间在不同时间段的内容不同，不同用户由于进房时间不同，看到同一个直播间的内容也不同。

整体思路还是比较简单的

+ 输入直播间和主播的基础信息，拿100B的llm标出来一些数据（生成如下的问答pair对、以及对直播内容的总结与理解），去finetune一个7B的llm
+ 用这个7B LLM产出emb（pooling）结合主播侧特征通过gating融合得到直播间emb，和user去双塔inbatch softmax
+ 【这一步的时效性比较高，每30秒更新一次】拿上面融合过的emb，通过rq-vae进行聚类，把对应的code扔给线上推荐模型

qa类prompt：

```shell
Live-streaming information: #time, #images, #speeches, #comments, #author_information;
You need to pay attention to the fact that the person in the
live-streaming may not be the author himself, but someone else.
Please generate ten instruction questions as diverse as possible
based on the provided live-streaming information. These
questions are about facts or an understanding and evaluation
of relevant content. Do not ask any questions that cannot be
answered confidently.
Finally, you need to return the result in the following form:
1. {"Question":..., "Answer":...}
2. {"Question":..., "Answer":...}
```

总结类prompt：

```shell
Live-streaming information: #time, #images, #speeches, #comments, #author_information;
You need to pay attention to the fact that the person in the
live-streaming may not be the author himself, but someone else.
Complete the following tasks:
1. Describe the current events and content in the live broadcast
room in detail in one paragraph, including but not limited to:
Content: including detailed visual information, such as whether
there is text, the look and feel of the picture, the tone, etc.
#other_tasks,
Finally, you need to return the result in the form of json:
{ "Event and content": {
"Content": ...,
"Character": ...,
"Scene": ...,
"Event": ...,
"Atmosphere": ...,
"Highlights": ...,
"Target user group": ... },}
```

实验部分列了两个主场景的收益，但没有说实验流量

## 阿里的URM（落地？）

[当购物用上大模型！阿里妈妈首发世界知识大模型，破解两大推荐难题](https://mp.weixin.qq.com/s/eJewql0mZk199HyzkjkiuA)

[Large Language Models Are Universal Recommendation Learners](https://arxiv.org/pdf/2502.03041)

![](../assets/urm.jpeg)

2个特殊token：

+ ```[UM]```对应的输出通过用户建模头$$h_{UM}$$映射到用户表示空间，用于候选商品的生成。如图，每个item对应一个```[UM]```；
+ ```[LM]```及其后续符号对应的输出通过语言模型头$$h_{LM}$$映射到文本空间，用于文本token的生成。

![](../assets/urm-item-emb.png)

item emb的获取：

+ 融合id emb，文本emb和图像emb
+ 底层的文本encoder和图像encoder freeze，上层的mlp可训练
+ sft训练（应该就是下面那个模型）

Sequence-In-Set-Out：其实就是把```[UM]```的输出拆成H个，每个去和item向量算相似度，再merge到u和i的相似度

最终sft的时候计算UM的loss和LM的loss，即图中的item和文本的loss

![](../assets/urm-prompt.png)

## Spotify的LLM Preview生成

[Transforming Podcast Preview Generation: From Expert Models to LLM-Based Systems](https://arxiv.org/pdf/2505.23908)

[推荐算法(2025.6.3)：Spotify的LLM应用，效果明显](https://mp.weixin.qq.com/s/2SJ5IdXa5Hmygx8VxGlDcQ)



## 蚂蚁的SLIM（没落地）

[蚂蚁集团在大模型推荐上的算法和应用](https://mp.weixin.qq.com/s/z4Q3Imuqoxw52TteaPbveQ?from=groupmessage&isappinstalled=0&scene=1&clicktime=1720679311&enterid=1720679311)

[Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/pdf/2403.04260)

![slim](../assets/slim.png)

+ 第一阶段：蒸馏大型GPT模型到较小的模型（如LLAMA2/3），来增强推理能力。
  + 通过预设的 prompt，大模型生成推荐理由，这些理由基于预定义的模板。接着，
  + 使用简化的推理模板请求小模型进行推荐和理由生成。
+ 第二阶段：利用生成式Loss来微调小模型，使其具备推理能力。
  + 模型训练完成，将通过prompt为用户行为提供T+1推理。
  + 推理结果通过文本编码器（Text Encoder）转化为表征，这些表征将直接应用于线上模型。

## OPPO的DLLM2REC（没落地）

在蚂蚁的SLIM的基础上，希望将LLAMA进一步压缩至更小的序列模型。在实验中遇到几个挑战：

+ 蒸馏过程中教师模型的知识可靠性存疑
+ 从语言模型到序列模型的蒸馏跨越了不同的模型类型，带来两个主要问题：
  + 参数差距大，学生模型难以容纳教师模型的知识
  + 语义不一致，因为序列模型与原始语言模型之间存在天然差异

[Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Models](https://arxiv.org/pdf/2405.00338v1)

![dllm2rec](../assets/dllm2rec.png)

包含两个关键部分：基于Ranking的蒸馏策略(Importance-aware Ranking Distillation)和Embedding对齐(Collaborative Embedding Distillation)

核心在于排名蒸馏，采取了如下三个策略

+ 选择LLAMA2作为教师模型，认为其排名靠前的分值更高；
+ 考虑LLM生成的描述与目标物品（Target Item）的接近程度，增加其排名；
+ 若教师模型（Teacher Model）认为某物品是优质且排名靠前的，学生模型（Student Model）也会给予其更高排名。

通过这些策略，设计了Ranking Loss，用于蒸馏小型序列模型

XXX
\mathcal{L}_d=-\sum_{s \in \Gamma} \sum_{i \in O^T} w_{s i} \log \sigma\left(\hat{y}_{s i}\right)
XXX

其中，$$O^T$$是teacher返回的topk结果，$$\Gamma$$是训练集的序列数，$$w_{\mathrm{s} i}=\gamma_p \cdot w_{\mathrm{s} i}^p+\gamma_c \cdot w_{\mathrm{s} i}^c+\gamma_o \cdot w_{\mathrm{s} i}^o$$，包括如下3部分：

+ position-aware weight：$$w_{\mathrm{s} i}^p \propto \exp \left(-r_i / \beta\right)$$，$$r_i$$是item $$i$$在teacher返回结果里的排名，$$\beta$$是一个超参
+ confidence-aware weight：$$w_{\mathrm{s} i}^c \propto \exp \left(-d_{\mathrm{s} i^*} / \beta\right)$$，且$$d_{\mathbf{s} i^*}=\left\|\mathbf{z}_{d_{\mathrm{s}}}-\mathrm{z}_{i^*}\right\|^2$$，$$\mathbf{z}_{d_{\mathrm{s}}}$$是生成的item描述，$$\mathrm{z}_{i^*}$$是ground truth的item描述，分别通过一个llm encoder得到向量
+ consistency-aware weight：同时被teacher和student推荐的item更有可能是一个强正例
XXX
w_{\mathrm{s} i}^o= \begin{cases}1, & i \in O^T \cap O^S \\ 0, & i \notin O^T \cap O^S\end{cases}
XXX

## 快手的LLM-CF（没落地）

[2024'快手提出LLM-CF框架，借助LLM的链式思维（COT）推理提升推荐系统性能](https://mp.weixin.qq.com/s/-NJPxj-1JknKl0JdVYIB5g)

CIKM24，[Large Language Models Enhanced Collaborative Filtering](https://arxiv.org/pdf/2403.17688)

[https://anonymous.4open.science/r/LLM-CF-AD78/readme.md](https://anonymous.4open.science/r/LLM-CF-AD78/readme.md)

简单总结：

拿推荐数据对llama2做sft，再用CoT的prompt让llama2对user+item+label产出一个推理过程，并通过bge得到emb，构建一个CoT数据集。在线拿当前用户+item的特征从这个数据集里ann出k个cot example的emb，和其他特征一起输入一个decoder，输出给推荐模型的sharebottom，额外加了一个CoT emb的重建loss。

文中用到的instruction prompt参考（Recsys23的[TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation](https://arxiv.org/pdf/2305.00447)和WWW24的[ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131)），把推荐变成了一个让模型输出yes/no的2分类任务。这里的$$x_i$$是包括用户特征和当前item特征的文本描述，$$y_i$$是点击的label(0/1)。

![](../assets/text-features.png)

整体的框架如下：

![](../assets/llm-cf-online-offline.png)

+ 离线：
  + finetune LLM使其具有推荐能力
  + 使用CF的信息生成CoT Reasoning
  + 构造一个in-context CoT数据集
+ 在线：
  + 检索和用户相似的in-context CoT examples，学习世界知识和Reasoning指导的CF特征，拿来给推荐系统用


### 离线部分

#### recgen-llama

&nbsp;

如果直接用TALLRec的方法，会有灾难性遗忘，模型在原来的LLM benchmark上效果会下降很多，提出了一种简洁高效的全量参数微调方法，在general data和推荐能力上找到平衡，可能的做法

+ base：不微调，原始llama2
+ half：用一半的推荐数据全量参数微调
+ full：用全量推荐数据全量参数微调
+ LoRA：对Q和V矩阵加LoRA，设置rank=8
+ 加噪声：[NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/pdf/2310.05914)和[HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation](https://arxiv.org/pdf/2212.08853)，用推荐数据finetune的时候，在input的embedding层加噪声
+ R3F：[Better Fine-Tuning by Reducing Representational Collapse](https://arxiv.org/pdf/2008.03156)基于置信域的逼近
+ Wise-FT：[Robust Fine-Tuning of Zero-Shot Models](https://arxiv.org/pdf/2109.01903)，这个引用量挺高的，[https://github.com/mlfoundations/wise-ft](https://github.com/mlfoundations/wise-ft)，将pretrained和finetuned的权重加权融合：$$W_{\text {ensemble }}=\alpha \cdot W_{\text {fine-tuned }}+(1-\alpha) \cdot W_{\text {pre-trained }}$$
+ RecGen：[How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition](https://arxiv.org/pdf/2310.05492)发现通过mixed-data finetuning能够保持通用能力的情况下，提升数学和代码能力

对比下来发现，R3F在推荐和MMLU上都最差，full在推荐上效果最好但在MMLU上不太行，RecGen在推荐的效果还不错在MMLU也没降多少，所以将推荐数据和通用数据(如LIMA([Lima: Less is more for alignment](https://arxiv.org/pdf/2305.11206))和alpaca-gpt4([Instruction tuning with gpt-4](https://arxiv.org/pdf/2304.03277)))混合。


#### CoT Reasoning生成

&nbsp;

通过如下的CoT prompt $$\mathcal{P}$$，可以实现如下过程：

+ 用RecGen-llama系统性地分析用户的交互历史和反馈，建立一个详细的用户profile。
+ RecGen-llama对目标的新商品进行特征的详细描述。
+ RecGen-llama考虑用户画像与目标商品特性之间的契合度。
+ RecGen-llama反思用户在购买中可能对多样化的需求。

即RecGen-llama的输出是

XXX
c_i=\operatorname{RecGen}-\operatorname{LLaMA}\left(\mathbf{x}_{\mathbf{i}}, y_i, \mathcal{P}\right)
XXX

其中的CoT prompt $$\mathcal{P}$$是：

```
<<SYS>> As an AI model developed for analyzing consumer behavior, your task is to
generate a chain of thought that considers the following points:
1. Utilize the user's interaction history and review comments to summarize their profiles.
2. Introduce the target new item and detail its features precisely. In addition, integrate
information about items related to the current target new item ...
1. Contemplate the alignment between the user's profile and the features of the target item.
2. Reflect on the user's potential desire for diversity in their purchases.
Your output should be a clear and logical chain of thought... Ensure your analysis is
impartial... Focus should be on understanding factors that influence the user‘s decisionmaking regarding the target item. <</SYS>>
Please generate a chain of thought based on the user‘s … considering how these might
relate to their interest in the target new item.
{Recommendation Features}
User's decision-making: The user {Ground-Truth Label} the target new item.
Let's think step by step and develop the chain of thought for the above considerations.
Commence with the chain of thought immediately: 

---------------

<<SYS>> 作为一个用于分析消费者行为的AI模型，您的任务是生成一条思维链，考虑以下几点：
1. 利用用户的互动历史和评论总结他们的个人画像。
2. 介绍目标新商品并详细说明其特点。此外，整合与当前目标新商品相关的其他商品信息。
3. 思考用户画像与目标商品特性之间的契合度。
4. 反思用户在购买中可能对多样化的需求。
您的输出应当是一个清晰且逻辑严谨的思维链，确保您的分析公正无偏，重点应放在理解影响用户决策的因素。<</SYS>>
请生成一个思维链，基于用户的……考虑这些因素可能如何影响他们对目标新商品的决策。
{Recommendation Features}
用户的决策：用户 {Ground-Truth Label} 目标新商品。
让我们一步一步地开发上述考虑因素的思维链。
立即开始思维链：
```

从原始的推荐数据集随机sample出M个$$(x_i, y_i)$$，生成对应的$$c_i$$，可以得到CoT数据集：$$C=\left\{\left(\mathbf{x}_m, c_m, y_m\right)\right\}_{m=1}^M$$

#### 性能分析

&nbsp;

+ 训练：发现相比用全量数据训练，用一半的数据训出来的模型在MMLU上效果更好，在推荐上的效果差不多，所以可以用小数据集来finetune
+ 生成：对比[Towards Open-World Recom- mendation with Knowledge Augmentation from Large Language Model](https://arxiv.org/pdf/2306.10933)提出的KAR(用户特征和item特征相对固定，可以提前存生成结果)，LLM-CF应用的场景有很多新用户和新item，但并不需要流式更新

### 在线部分

![](../assets/llm-cf-total.png)

#### In-context CoT Examples Retrieval

&nbsp;

对于上面的CoT数据集，拿$$x_i$$对应的文本$$x_i^t$$作为query，而数据集$$C$$里的$$M$$个examples对应的key是$$\mathcal{K}=\left[\mathbf{x}_1^t, \ldots, \mathbf{x}_m^t, \ldots, \mathbf{x}_M^t\right]$$。

先通过BGE embedding([C-Pack: Packed Resources For General Chinese Embeddings](https://arxiv.org/pdf/2309.07597))作为encoder把query $$x_i^t$$和keys $$\mathcal{K}$$转成向量$$\mathbf{e}\left(\mathbf{x}_i^t\right), \mathbf{e}(\mathcal{K})=\operatorname{encoder}\left(\mathbf{x}_i^t\right), \text { encoder }(\mathcal{K})$$

然后通过ANN找出top K个examples $$\mathcal{I}_i=\left\{\mathcal{E}_1, \ldots, \mathcal{E}_k, \ldots, \mathcal{E}_K\right\},\ \mathcal{E}_k=\left(\mathbf{x}_k, c_k, y_k\right)$$

做了2个trick：

+ 避免穿越，保证$$\mathcal{I}_i$$不包括$$x_i$$未来的steps的交互信息
+ 为了让正负例比例对下游任务不造成影响，保证$$\mathcal{I}_i$$中的正例和负例比例是1:1

#### In-context Chain of Thought Module

&nbsp;

ICT模块将$$\mathcal{I}_i$$当成in-context examples，并把$$x_i$$当成query，对应的ICT tokens就是

XXX
\mathbf{T}=\left[\mathbf{x}_1, c_1, y_1, \ldots, \mathbf{x}_K, c_K, y_K, \mathbf{x}_i\right]
XXX

然后得到这些token对应的embs：

XXX
\mathbf{E}=\left[\mathbf{r}_1, \mathbf{c}_1, \mathbf{l}_1, \ldots, \mathbf{r}_K, \mathbf{c}_K, \mathbf{l}_K, \mathbf{r}_i\right] 
XXX

其中，（图中右上角橙色部分）

+ 文本特征同样经过BGE encoder，
+ $$x_i$$里的推荐特征则经过推荐模型的feature encoder得到$$r_i$$，
+ CoT reasoning $$c$$则是通过text encoder和MLP映射
+ label $$l$$也是一个0/1的id对应的embedding

然后把$$\mathbf{E}$$经过若干层transformer decoder，得到输出的表示:

XXX
\left.\mathbf{H}=\left[\mathbf{h}\left(\mathbf{r}_1\right), \mathbf{h}\left(\mathbf{c}_1\right), \mathbf{h}\left(\mathbf{l}_1\right), \cdots, \mathbf{h}\left(\mathbf{r}_i\right)\right)\right]=\operatorname{Decoder}(\mathbf{E}) 
XXX

最终得到的表示可以是这个输出的last hidden state：$$\mathbf{w}=\mathbf{h}\left(\mathbf{r}_i\right)$$

在各数据集上评估时，发现CoT examples的个数K=4效果就不错了，对应的ICT tokens $$T$$的长度也只有12左右。

#### Model Training

&nbsp;

原来推荐的loss是$$\mathcal{L}_o^i$$，这里加上一个对CoT examples的emb的重建loss：

XXX
\mathcal{L}_r^i=\frac{1}{K} \sum_{i=1}^K\left(1-\frac{\mathbf{c}_i \cdot \mathbf{h}\left(\mathbf{r}_i\right)}{\left\|\mathbf{c}_i\right\|\left\|\mathbf{h}\left(\mathbf{r}_i\right)\right\|}\right)
XXX

最终的loss如下

XXX
\mathcal{L}=\frac{1}{N} \sum_{i=1}^N\left(\alpha \mathcal{L}_r^i+\mathcal{L}_o^i\right)
XXX

#### Efficiency Analysis of Online Service

&nbsp;

图中的蓝色火焰部分需要在线计算，而因为$$K=4$$，所以很轻量


## Google的ILM（没落地）

[Item-Language Model for Conversational Recommendation](https://arxiv.org/pdf/2406.02844)

适用场景如下，{user}可以看成一种特殊的{item}，{history}是若干个item

![ILM-tasks](../assets/ILM-tasks.png)

参考BLIP-2（[Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models](https://arxiv.org/pdf/2301.12597)）提出的Q-former：

![](../assets/2-stage-blip-2.png)

引入了Q-former

![ILM](../assets/ILM.png)

+ phase1：表示学习，交替训练如下两类表示学习
    + item-text表示学习:
        + query的tokens和item的cf emb过cross attn后，拿cls的输出得到v1
        + text过attn得到v2
        + v1和v2算item-text对比学习
        + v2走一个自回归的任务(item grounded text generation)
    + item-item表示学习
        + query的tokens和item1的cf emb过cross attn后，拿cls的输出得到v1
        + query的tokens和item2的cf emb过cross attn后，拿cls的输出得到v2
        + v1和v2算item-item的对比学习
+ phase2：item-language model训练
    + Q-former的item encoder经过一个linear输入到LLM中
    + LLM参数freeze，只tune Q-former的参数和linear


## Meta的EmbSum（没落地）

[EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations](https://www.arxiv.org/pdf/2405.11441)

[RecSys'24 | Meta:使用LLM的摘要能力提升内容推荐](https://mp.weixin.qq.com/s/gDOo5qjcZgy5fRQJtd2GLw)

![](../assets/embsum.png)

+ user session encoding：先将用户k个行为序列化分成$$g$$个session，每个session里有$$p$$个content，这p个content的文本单独过transformer encoder（T5 small），拿开始的token（```[SOS]```）对应的输出当成文本表示
+ user engagement summarization：将用户的历史丢到一个prompt里去，让llm总结出一段话，然后参考[Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](https://arxiv.org/abs/2007.01282)，把llm输出的所有tokens的hidden states concat在一起，再丢给T5 decoder，并加一个自回归的loss，其中$$y_j^{u_i}$$表示生成的用户$$u_i$$的summary，而$$|y_j^{u_i}|$$是summary的长度

XXX
\mathcal{L}_{\text {sum }}=-\sum_{j=1}^{\left|y_j^{u_i}\right|} \log \left(p\left(y_j^{u_i} \mid E, y_{<j}^{u_i}\right)\right)
XXX

![](../assets/embsum-summary.png)

+ user poly embedding（upe）：把decoder最后一个token（```[EOS]```）拿出来，和前面的k个session encoding一起得到一个$$Z \in \mathrm{R}^{(k+1) \times d}$$，然后过一个poly-attention layer，得到用户的$$m$$个兴趣表示$$A \in \mathrm{R}^{m \times d}$$，其中每个向量是$$\alpha_a=\operatorname{softmax}\left[c_a \tanh \left(Z W^f\right)^{\top}\right] Z$$，其中$$c_a \in \mathrm{R}^{1 \times p}$$和$$W^f \in \mathrm{R}^{d \times p}$$是可训练的参数

+ content poly embedding（cpe）：和upe类似，k个session encoding换成当前item的encoder输出，UPE里的```[EOS]```换成当前content的下一个token即```[SOS]```，然后得到$$n$$个$$d$$维表示$$B$$
+ 融合：$$A^TB$$可以算出一个$$m\times n$$的相似矩阵，然后flatten，得到一个$$mn$$维的向量，同理算一下，得到的$$s^i_j$$就是两个向量的内积，即相似度的得分了

XXX
\begin{aligned}
W^p & =\operatorname{softmax}\left(\operatorname{flatten}\left(A \cdot \operatorname{gelu}\left(B W^s\right)^{\top}\right)\right), \\
s_j^i & =W^p \cdot K_j^i,
\end{aligned}
XXX

## 百度的Agent4Ranking（没落地）

[Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM](https://arxiv.org/pdf/2312.15450)

![](../assets/agent4ranking-arch.png)

其中的rewrite模块如下，搞4个agent，模拟4个人群的人与LLM对话，最终得到符合人群特点的rewrite

![](../assets/agent4ranking-rewrite.png)

其中的rank模块如下，是一个bert，输入原始query和4个rewrites，加上target item，MMOE，算5个query间的js散度(robust loss)，以及综合输出和label的accuracy loss

![](../assets/agent4ranking-rank.png)


## 丰田的SimUSER(没落地)

[SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation](https://arxiv.org/pdf/2504.12722)

两阶段方法：

+ 自一致的用户画像匹配，利用LLM从历史数据中抽取个性特征；
+ 推荐系统评估，模拟具有个性、记忆、感知和决策模块的用户与推荐系统进行互动。

## CUP（学术界）没中

[Recommendations by Concise User Profiles from Review Text](https://arxiv.org/pdf/2311.01314)

把用户的一堆历史评论扔给chatgpt，让它总结出128个token，然后丢给双塔bert，另一个塔是item的描述，freeze bert底层，只tune上层

## LLaMA-E（学术界）没中

[LLaMA-E: Empowering E-commerce Authoring with Object-Interleaved Instruction Following](https://arxiv.org/pdf/2308.04913)

[https://huggingface.co/DSMI/LLaMA-E#/](https://huggingface.co/DSMI/LLaMA-E#/)

整体流程

![](../assets/llama-e.png)

+ instruction formulating: 为图中的任务写300个种子指令
    + seller $$<S>$$：商家，包括商品名、风格、品牌等
    + customer $$<C>$$：
        + explicit feature $$<C_0>$$：用户的历史交互商品
        + implicit feature $$<C_1>$$：根据历史交互推断出来的潜在兴趣
    + platform $$<P>$$：为商家和用户建立链接
        + product correlation $$<P_0>$$：不同商品间的差异和联系
        + platform background $$<P_1>$$：不同电商平台的背景知识
+ instruction expansion: 让gpt作为teacher，对300个种子指令进行扩展，并由领域专家评估后，去重并保证质量，得到120k个指令作为训练集
    + 针对结果是严格预定义好的，只重写instruction
    ```
    [INST] Rewrite the following instruction
     while maintaining semantic consistency:
    [/INST] <seed instructions>
    ```
    + 针对鼓励生成结果有多样性的，
        + 结果生成：用上面重写过的instruction给LLM来获取结果
        ```
        [INST] <expanded instructions> [/INST]
        <seed inputs>
        ```
        + 结果重写：拿生成的结果给LLM生成更多样的表达
        ```
        [INST] Rewrite the following generated
        response to diversify its expression:
        [/INST] <responses>
        ```
+ instruction tuning: 对self-attention的q、k、v、o参数进行lora

设计的任务

![](../assets/llama-e-tasks.png)

示例：

| Task                          | Expanded Instructions                                                                 |
|-------------------------------|---------------------------------------------------------------------------------------|
| **Ads Generation**            | Produce an advertisement for the specified product.                                   |
|                               | Create an advertisement for the specified product.                                   |
|                               | Produce an advertisement for the product mentioned below.                            |
|                               | Generate an ad designated for the following product.                                 |
|                               | Prepare an advertisement for the product provided below.                             |
| **Query-enhanced Title Rewriting** | Rephrase the subsequent product title along with the query.                      |
|                               | Revise the subsequent product title alongside the query.                             |
|                               | Revise the product title below, incorporating the given query.                       |
|                               | Revise the given product title in combination with the query.                        |
|                               | Incorporate the following query to rewrite the product title.                        |
| **Product Classification**    | To which category does the subsequent product belong?                                |
|                               | Which category of this product belongs?                                              |
|                               | Identify the category to which the following product belongs.                        |
|                               | What category does the listed product belong to?                                     |
|                               | Identify the category of the listed product.                                         |
| **Purchase Intent Speculation** | Which category does the provided query imply the customer is interested in?         |
|                               | Based on the following query, which category does it indicate the customer is interested in? |
|                               | What category is suggested by the following customer query’s apparent interest?      |
|                               | What category does the given query indicate the customer’s interest in?              |
|                               | Identify the category that the following query suggests the customer is interested in. |
| **General E-commerce Q&A**    | How are my orders attributed to Offsite Ads?                                         |
|                               | Describe the process of attributing my orders to Offsite Ads.                        |
|                               | Can you explain how my orders are attributed to Offsite Ads?                         |
|                               | Please elaborate on the process of attributing my orders to Offsite Ads.             |
|                               | How are my orders linked to Offsite Ads?                                             |

对应的中文：

| 任务                         | 扩展说明                                                                             |
|------------------------------|-------------------------------------------------------------------------------------|
| **广告生成**                 | 为指定的产品制作一则广告。                                                           |
|                              | 为指定的产品创建一则广告。                                                           |
|                              | 为以下提到的产品制作一则广告。                                                       |
|                              | 为以下产品生成一则广告。                                                             |
|                              | 为提供的产品准备一则广告。                                                           |
| **查询增强的标题重写**        | 根据查询改写后续的产品标题。                                                         |
|                              | 根据查询修改后续的产品标题。                                                         |
|                              | 根据提供的查询修改以下的产品标题。                                                   |
|                              | 将给定的查询与产品标题结合进行修改。                                                 |
|                              | 使用以下查询重写产品标题。                                                           |
| **产品分类**                 | 以下产品属于哪个类别？                                                               |
|                              | 这个产品属于哪个类别？                                                               |
|                              | 确定以下产品属于哪个类别。                                                           |
|                              | 列出的产品属于哪个类别？                                                             |
|                              | 确定列出产品的类别。                                                                 |
| **购买意图推测**             | 提供的查询暗示用户感兴趣的类别是哪个？                                               |
|                              | 根据以下查询，用户可能感兴趣的类别是什么？                                           |
|                              | 以下用户查询暗示的兴趣类别是什么？                                                   |
|                              | 给定的查询表明用户感兴趣的类别是什么？                                               |
|                              | 确定以下查询暗示用户感兴趣的类别。                                                   |
| **通用电商问答**             | 我的订单是如何归因到站外广告的？                                                     |
|                              | 描述我的订单归因到站外广告的过程。                                                   |
|                              | 你能解释一下我的订单是如何归因到站外广告的吗？                                       |
|                              | 请详细说明我的订单归因到站外广告的过程。                                             |
|                              | 我的订单是如何与站外广告关联的？                                                     |



## EcomGPT（学术界）AAAI24

[EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce](https://arxiv.org/pdf/2308.06966v1)

清华，AAAI 24

![](../assets/ecomgpt.png)

设置一系列的task(100多个task)来finetune BLOOMZ(OPT+BLOOM的instruction-following模型)

+ 命名实体识别：输入描述，输出brand、attr、component、product等
+ 描述生成：输入query和候选doc list，选出最match的k个
+ 对话intent提取：输入对话和候选intents（使用咨询、恢复订单、退款异常等），选出对应intent


## Llama4Rec（学术界）SIGIR24

[Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation](https://arxiv.org/pdf/2401.13870) SIGIR 24

![](../assets/llama4rec.png)

+ prompt增强：在prompt里引入推荐模型的信息
    + 相似用户也喜欢item4、item5
    + 另一个推荐模型给出的预估是xxx
+ 数据增强：通过LLM给推荐模型增加样本
    + 直接推荐：给定2个候选item，LLM判断哪个更好
    + 序列推荐：给定候选list，LLM给出topk
    + 评分预测：给定候选item，LLM给出打分
+ adaptive aggregation：llm和推荐模型都给出排序，然后自定义融合公式算出融合分，得到最终排序

示例prompt如下

![](../assets/llama4rec-prompts.png)

通过如上prompt构建一个instruct-tuning数据集，然后finetune一个llama2

## SAGCN（学术界）ACM TOIS

[Understanding Before Recommendation: Semantic Aspect-Aware Review Exploitation via Large Language Models](https://arxiv.org/pdf/2312.16275)

[https://github.com/HuilinChenJN/LLMSAGCN](https://github.com/HuilinChenJN/LLMSAGCN)

通过LLM标识出用户对item的评论是属于哪些aspect的，然后u有A个emb，i也有A个emb，构建A个U-I图，然后揉在一起过GCN

![](../assets/sagcn.png)

+ 拿一些数据先用prompt1让LLM粗略标出用户评论所属的aspects，然后merge并去重得到候选的aspects列表
+ 把这个候选aspects列表（图中的蓝色文字）丢给prompt2，让LLM对所有用户的评论打标

然后为每个aspect构建一个graph，过GCN

![](../assets/sagcn-graph.png)

## GReaT（学术界）ICLR23

[Language Models are Realistic Tabular Data Generators](https://arxiv.org/pdf/2210.06280)

ICLR2023，[https://github.com/kathrinse/be_great](https://github.com/kathrinse/be_great)

通过自回归的LLM合成（synthesize）逼真的（realistic）表格化（tabular）数据

![](../assets/great-finetune.png)

finetune：

+ 把表格转化为逗号分隔的自然语言
+ 随机交换几个属性的位置得到训练语料
+ 基于交换后的语料进行自回归的finetune

![](../assets/great-sample.png)

sample：

+ 基于如下3种方式产出precondition：
  + 只输入属性名，例如“年龄”
  + 输入name-value pair，例如“年龄=26岁”
  + 输入多个name-value pair：例如“年龄=59岁，教育程度是硕士”
+ 然后调用LLM输出其他属性
+ 再转化为表格格式

## ONCE（学术界）WSDM24

[ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models](https://arxiv.org/pdf/2305.06566)

WSDM24

![](../assets/once.png)

+ GENRE(generative recommendation)：闭源LLM产出数据
    + user profiler: 输入用户行为历史得到的user profile，例如topics有哪些，地区是哪里，过完content encoder后，把对应的topics和地区的emb pooling一下，得到用户的profile表示
    + content summarizer: 输入item的文本content得到的总结
    + personalized content generator：输入新用户（readlist<5）比较短的历史，合成这个用户可能比较感兴趣的内容
+ DIRE(discriminative recommendation)：对开源LLM进行改造
    + 最后一层降维，并过一个attention得到一个表示
    + 对32层的llama-7b，只用lora训练最上面的2层
+ ONCE：
    + GENRE产出的文本经过DIRE得到输出，其中user profiler用上面的方法处理一下，再和历史items的content summarizer的输出、personalized content generator的输出concat并过mlp得到user vec
    + item content的文本经过DIRE得到item vec
    + u和i算内积，学ctr

其中GENRE的3种数据如下：

![](../assets/once-genre.png)

## Agent4Rec（学术界）SIGIR24

[On Generative Agents in Recommendation](https://arxiv.org/pdf/2310.10108.pdf) SIGIR24

[https://github.com/LehengTHU/Agent4Rec](https://github.com/LehengTHU/Agent4Rec)

核心代码是[https://github.com/LehengTHU/Agent4Rec/blob/master/simulation/avatar.py](https://github.com/LehengTHU/Agent4Rec/blob/master/simulation/avatar.py)，可以发现是用了langchain，包括其中的memory(请求openai的emb接口，本地faiss存)。

1000个真实用户，初始化1000个agent，3个数据集，搞了3000个agent，Agent模拟人的行为，点击、翻页、退出、打分、评价，评价标准：是否真的拟人

整体流程：

+ 先在公开数据集上训一个推荐模型（mf/lightgcn等）
+ 构建1000个agent，模拟和这个推荐模型交互，生成一些新的样本（假设推荐了xxx，你是不是会点？点了，正样本，没点负样本之类的）
+ 加上这些新的样本再去重训推荐模型，发现效果有提升

![](../assets/agent4rec.png)

3个module：

+ Profile module:
    + 电影profile（流行度等+电影信息丢给gpt4产出的summary）
    + 用户profile（社交属性、活跃度、从众心理（conformity）、diversity(更喜欢探索还是利用)、用户喜好tastes（用户历史行为丢给gpt4，让它总结用户喜欢什么不喜欢什么，会在用户有新行为后再update一下））
+ memory module:
    + factual memory: 过去看了哪些电影、动作是什么
        + emotional memory: 看完后的情绪表达
+ action module:
    + 看/打分/反馈
    + 满意度
    + 翻页/退出

图中的推荐系统是一个现成的系统/算法，例如MF、lightgcn等

word版的笔记：[https://github.com/daiwk/collections/blob/master/assets/multi-agents.docx](https://github.com/daiwk/collections/blob/master/assets/multi-agents.docx)


## RecPrompt（学术界）CIKM24

[RecPrompt: A Self-tuning Prompting Framework for News Recommendation Using Large Language Models](https://arxiv.org/pdf/2312.10463) CIKM24

[https://github.com/Ruixinhua/rec-prompt](https://github.com/Ruixinhua/rec-prompt)

![](../assets/recprompt.png)

如图的loop，所有LLM都是调接口的，没有tune

+ news recommender：一个LLM，输入prompt如下，结果包括排序的list $$\boldsymbol{R}_u$$以及解释$$TP_u$$(用户点击历史总结出来的topic list和对应的新闻list)

```
You serve as a personalized news recommendation system.
# Input Format
## User's History News
${history}
## Candidate News
${candidate}
# Output Format
Rank candidate news based on the user’s history news in
the format: "Ranked news: <START>C#, C#,..., C#<END>".
```

+  prompt optimizer：另一个LLM，用于生成更好的prompt。包括：
    + refinement instruction
    + 推荐prompt
    + recommender的回答：$$\boldsymbol{R}_u$$、$$T P_u$$
    + ground truth $$y_u$$，
    + monitor给出的最佳template
    + observation instruction。

其中，refinement instruction形如：

```
You should generate an improved
template instruction based on the provided information.
```

observation instruction形如：

```
You should focus on how well the recommender’s response
aligns with the user’s click behavior by examining if
the topics from the user’s news history and candidate
news are accurately summarized and matched to the user’s
interests. Specifically, evaluate the clarity of topics
in the recommender’s answer, review the task description
for adequacy, and check the detail in the recommendation
process to ensure it reflects an analysis and summary
of user-interest topics.
```

+ monitor：对推荐list和ground truth计算MRR、ndcg，判断当前template是否比之前的有提升

+ 衡量效果：看解释$$TP_u$$提取的topic的准确性和完整性：
    + 准确性：分母是预测出来的n个topic，分子是有多少个和对应的文章topic是match的
    + 完整性：分母是用户历史点过的topic数，分子是用户历史点过的文章数

## PO4ISR（学术界）没中

[Large Language Models for Intent-Driven Session Recommendations](https://arxiv.org/pdf/2312.07552)

[https://github.com/llm4sr/PO4ISR](https://github.com/llm4sr/PO4ISR)

![](../assets/po4isr.png)

+ PromptInit：初始化prompt1，输入的数据填充prompt2
+ PromptOpt：通过self-reflection来优化初始prompt：
    + 收集error cases并用prompt3来让LLM想出可能原因
    + LLM想出的原因+prompt4来refine出一个prompt5
    + 基于prompt5+prompt6来改写/增强出一个prompt7
    + 基于prompt5和prompt7通过UCB选出一个最优的prompt8
+ PromptSel：在3个domain上进行上述迭代，分别得到对应domain的最优prompt，也看了下这3个prompt在其他数据集上的表现，最终选出一个泛化性比较好的

## Transrec（学术界）KDD24

[Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation](https://arxiv.org/pdf/2310.06491) KDD24

核心思想如下：

![](../assets/trans-rec-instruct-tuning.png)

将一个item表示成3部分：id+title+attr，设计三种对应的instruct-tuning任务，其中```||xx||+```是一个特殊标记，类似于标识任务类型，对应的[代码](https://github.com/Linxyhaha/TransRec/blob/main/scripts/training/reconstruct.py)：

```python
if args.target == "id":
    source = "Given the following purchase history of a user: " 
        + "; ".join(titles) + ". "
    source += "What is the next possible item to be purchased by the user?"
    source += " || ID"

elif args.target == "attribute":
    source = "Given the following categories of purchase history of a user: "
         + "; ".join(titles) + ". "
    source += "What is the category of the next possible item to be purchased by the user?"
    source += " || attribute"


if "\n" in source:
    source = "".join(source.split('\n'))
if args.query_mode == "attribute":
    targets = ["".join(t.split('\n')) for t in target]
if "\n" in target:
    target = "".join(target.split('\n'))
    
if args.target == "id":
    target = "|| " + target.strip() + " ##"
    yield source + " || +", target
    
if args.target == "attribute":
    for target in targets:
        target = "## " + target.strip() + " @@" 
        yield source + " || +", target
```

然后是generation grounding，即引入一个特殊的数据结构（FM-index），并进行constrained beam search，让模型能生成候选集中的id/title/attr，然后再遍历全库候选，看不同facet的相似度（会考虑高热打压），加权融合出一个排序

![](../assets/transrec-generation-grounding.png)

+ FM-index：参考[Autoregressive Search Engines: Generating Substrings as Document Identifiers](https://arxiv.org/pdf/2204.10628)：例如一个item可以写成```<IDS> 1023 <IDE> Urban Decay Eyeshadow Palette Naked Heat <AS> Makeup <AE> <AS> Eyes <AE>```，并通过wavelet tree存储，给定开始token（如```<IDS>```或者```<AS>```），可以在$$O(Vlog(V))$$里找出所有可能的后续tokens。代码：[https://github.com/facebookresearch/SEAL](https://github.com/facebookresearch/SEAL)
+ constrained beam search：参考[Autoregressive entity retrieval](https://arxiv.org/pdf/2010.00904)：代码：[https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)，另外，可以发现title是没有bos和eos的，这样在检索/生成title的时候，是可以检索/生成子串的，也就是论文提到的position-free，同样的方法也可以用在instruct tuning阶段。

## E4SRec（学术界）没中

[E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation](https://arxiv.org/pdf/2312.02443)

代码：[https://github.com/HestiaSky/E4SRec/](https://github.com/HestiaSky/E4SRec/)

![](../assets/e4srec.png)

+ 把id引进来：先通过现有的推荐模型（如SASRec）得到id embed，然后通过一个linear得到和原始LLM的emb一样长的dim，再嵌入到llm里去，只有这个linear是可学习的
+ LLM的输出：看代码取的是最后一个词的hidden state，再过Linear映射回推荐模型的dim，再过一个softmax映射到id空间（当然，如果要用到工业界可以正常的召回采样方法，如sampled softmax之类的）
+ infer：和召回一样，ann

finetune部分代码的大概逻辑如下：

+ 输入：把预训练好的item id emb当成一个nn.Embedding搞到模型里去，然后样本里的item id（每个id都是1-n的数）去查出对应的emb，和instruct的emb、response的emb(```### Response：\n```这种文字对应的emb)这3个emb一起concat起来作为模型输入
+ label：假设有n个item，因为最后过了个softmax映射到n，所以label就是一个1-n之间的数

infer照理说应该是得到emb，然后去ann，但代码里偷懒了直接拿softmax后的1-n的结果

## guided embedding

[Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation](https://arxiv.org/pdf/2504.11658)


## 下面这些还没看。。


## P5（学术界）RecSys22

[Recommendation as Language Processing (RLP):A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)](https://arxiv.org/pdf/2203.13366.pdf)

## 微软的RecExplainer

[RecExplainer: Aligning Large Language Models for Explaining Recommendation Models](https://arxiv.org/pdf/2311.10947)

[https://github.com/microsoft/RecAI](https://github.com/microsoft/RecAI)

新的一篇

[Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/pdf/2505.03336)

+ RecLM-ret通过检索机制，从域数据集中找到最相关的item
+ RecLM-cgen则使用约束生成策略，限制模型仅生成域内的item标题。


## Google的STAR

[谷歌用大模型搞推荐，拿到收益了](https://mp.weixin.qq.com/s/IuPLKwPnDwWgke4SFy2Bnw)

[STAR: A Simple Training-free Approach for Recommendations using Large Language Models](https://arxiv.org/pdf/2410.16458)

## 腾讯的ECR Recsys24

[RecSys'24最佳长论文|腾讯ECR:富有情感的对话式推荐系统](https://mp.weixin.qq.com/s/K9LUSywox6IeAb4XIhe_qA)

[Towards Empathetic Conversational Recommender Systems](https://arxiv.org/pdf/2409.10527)

## 微信的PRECISE Recsys24

[微信PRECISE:多场景落地基于协同信号和语义信息的生成式序列推荐](https://mp.weixin.qq.com/s/pld010curqUv_NsqdvSEDA)

[PRECISE: Pre-training Sequential Recommenders with Collaborative and Semantic Information](https://arxiv.org/pdf/2412.06308)

## MACRec（学术界）SIGIR24

把推荐系统拆成多个agent

[MACRec: a Multi-Agent Collaboration Framework for Recommendation](https://arxiv.org/pdf/2402.15235)

## 其他

ACL2024的recgpt 这个好像没啥用，[RecGPT: Generative Pre-training for Text-based Recommendation](https://arxiv.org/pdf/2405.12715)

[WWW 2024 | 工业界大模型在搜广推场景应用](https://zhuanlan.zhihu.com/p/686259205)

[LLM+Recommendation大模型推荐近期进展|含WWW, SIGIR, AAAI等顶会文章](https://mp.weixin.qq.com/s/m8DMgSt_r-HVNHHzA8ceVw)

+ baidu：[Representation Learning with Large Language Models for Recommendation](https://arxiv.org/pdf/2310.15950.pdf)
+ huawei：[ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation](https://arxiv.org/pdf/2308.11131.pdf)
+ microsoft：[Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/pdf/2311.06318.pdf)
+ 阿里：[Modeling User Viewing Flow using Large Language Models for Article Recommendation](https://arxiv.org/pdf/2311.07619.pdf)
+ linkedin：[Collaborative Large Language Model for Recommender Systems](https://arxiv.org/pdf/2311.01343.pdf)

[ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction](https://arxiv.org/pdf/2310.09234.pdf)

recsys23的[Leveraging Large Language Models for Sequential Recommendation](https://arxiv.org/pdf/2309.09261) 还有一篇[Improving Sequential Recommendations with LLMs](https://arxiv.org/pdf/2402.01339)，对应的代码：[https://github.com/dh-r/LLM-Sequential-Recommendation/tree/RecSys23](https://github.com/dh-r/LLM-Sequential-Recommendation/tree/RecSys23)


meta发的[LLM-Rec: Personalized Recommendation via Prompting Large Language Models](https://arxiv.org/pdf/2307.15780)

[生成式推荐系统近期研究工作梳理](https://mp.weixin.qq.com/s/u2kVzgv8ntUIkyCZZXIYWg)

[腾讯广告基于混元大模型的生成式召回落地实践](https://mp.weixin.qq.com/s/ClmjgRzpNIVjX0PrikS3qg)


[CIKM 2024 | 大语言模型推荐中的协同过滤信号和语义信息的深度融合](https://mp.weixin.qq.com/s/Qd0ZgBqMCJAEjlZYAoy9jQ)

[Collaborative Cross-modal Fusion with Large Language Model for Recommendation](https://arxiv.org/pdf/2408.08564)

(toread)

[生成式推荐最新进展](https://mp.weixin.qq.com/s/IXXMl74ZYpZMINNzFYC8Bg) 25年1月8号写的


[谷歌: 利用推荐知识对齐大语言模型](https://mp.weixin.qq.com/s/62SaQofe9qi2LCApjIzLFA)

[Aligning Large Language Models with Recommendation Knowledge](https://arxiv.org/pdf/2404.00245.pdf)



# LLM+推荐：其他套路

## ExFM

[GPT4规模大模型落地，Meta提ExFM框架：万亿参数基础大模型的工业级落地成为可能](https://mp.weixin.qq.com/s/Y9rlMxg4EMl_bzi9N_s7GA)

[External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation](https://arxiv.org/abs/2502.17494)

![](../assets/exfm.png)

图中的$$\Theta^F(t)$$是FM(teacher，foudation model)在$$t$$时刻的参数，$$D^F(t)$$是FM在$$t$$时刻的数据集，V是VM（student, vertical model）的对应参数/数据集

+ co-distillation vs external distillation：
    + co-distillation：联合训练teacher和student，只拿student去serving。
        + 缺点：训练成本很高，而且线上链路有多阶段的ranking，为每个ranking模型保留一个大的teacher是低效的
    + external distillation：单独训练teacher模型，是1-to-N的模式，一个teacher可以像foudation model一样指导多个student
+ 流式训练：t时刻的数据训出对应的模型，t+1加载t时刻的数据训练出t+1时刻的模型
+ ExFM：
    + t-1的各VM数据聚合成t-1的FM数据，训练得到t-2的FM模型
    + t的各VM数据聚合成的t的FM数据，训练得到t-1的FM模型
    + DAS, data augmentation service：FM对每个VM数据进行预估，再加上对应的VM数据，产出VM的训练数据
    + Auxiliary Head和Student Adapter：使用DAS产出的数据，基于t-1的VM模型利用AH和SA训练得到t的VM模型

![](../assets/das.png)

+ 数据集生产：
    + 将所有VM的特征和真实label聚合成一个shared dataset，
    + 有很长的时间窗口等待feadback(例如ctr有5-90min，cvr有一天)，在这个窗口中，FM可以使用VM的特征给出预估结果
    + 然后从这个shared dataset中取出各VM对应的子数据集用于训练
+ FM ckpt的维护和更新：
    + FM有新的ckpt时，会通过SPD（一个db）发布
    + updater从SPD读到最新的ckpt版本，再写到zeus（一个基于zookeeper的分布式metadata存储）里去
    + DAS任务从zeus读取最新模型

![](../assets/auxiliary-head.png)

真实label是$$y$$，FM预估出来的label(pseudo-label)是$$\hat{y}^F$$，student包括两部分，backbone $$\mathbf{x}$$和serving head $$\hat{y}^S=\phi(\mathbf{x})$$，其中$$\phi$$是MLP，loss是交叉熵：

XXX
h(\hat{y}, y)=y \cdot \log (\sigma(\hat{y}))+(1-y) \cdot \log (1-\sigma(\hat{y}))
XXX

VM的loss是：

XXX
\mathcal{L}_{\mathrm{kd}}\left(\hat{y}^S, \hat{y}^F, y\right)=h\left(\hat{y}^S, y\right)+h\left(\hat{y}^S, \hat{y}^F\right)
XXX

但用一个head来预估2个label会引入bias，所以引入AH(Auxiliary Head)来预估pseudo-label，即$$\hat{y}^D=\psi(\mathbf{x})$$，其中$$\psi$$是另一个MLP，这样就可以拆成2部分：

+ 真实label的loss：$$\mathcal{L}_{\mathrm{s}}\left(\hat{y}^S, y\right)=h\left(\hat{y}^S, y\right)$$
+ pseudo-label的loss：$$\mathcal{L}_{\mathrm{d}}\left(\hat{y}^D, \hat{y}^F\right)=h\left(\hat{y}^D, \hat{y}^F\right)$$

引入以下3个方法强化FM的蒸馏效果：

+ gradient scaling(gs)：AH对backbone的梯度乘一个$$\beta$$
+ label scaling(ls)：下式中的$$\alpha$$，同时要保证scale完的label不越界，即$$\alpha \cdot \hat{y}^{F M}$$不超过1
+ loss weighting(lw)：下式中的$$w$$

XXX
\mathcal{L}_{\mathrm{ah}}=\mathcal{L}_{\mathrm{s}}\left(\hat{y}^S, y\right)+w * \mathcal{L}_{\mathrm{d}}\left(\hat{y}^D, \alpha \cdot \hat{y}^F\right)
XXX

进一步地，在$$\hat{y}^F$$的基础上再过个MLP，得到SA（student adapter）$$\hat{y}^{S A}=\operatorname{MLP}\left(\hat{y}^F\right)$$，相应地，加上一个SA与真实label的loss：$$\mathcal{L}_{\text {sta }}\left(\hat{y}^{S A}, y\right)=h\left(\hat{y}^{S A}, y\right)$$

然后，为了不让VM的梯度回传给SA，需要加一个stop-gradient，即$$\mathcal{L}_{\mathrm{sa}}\left(\hat{y}^D, \mathrm{SG}\left(\hat{y}^{S A}\right)\right)=h\left(\hat{y}^D, \mathrm{SG}\left(\hat{y}^{S A}\right)\right)$$

整体过程：

+ 优化SA的参数$$\Theta^{S A}$$：用$$$$\mathcal{L}_{\text {sta }}\left(\hat{y}^{S A}, y\right)$$
+ 计算$$\operatorname{SG}\left(\hat{y}^{S A}\right)$$
+ 优化VM的参数$$\Theta^{V}$$：用$$\mathcal{L}_{\mathrm{ah}}+\mathcal{L}_{\mathrm{sa}}$$

按自己的理解重新画了下：

![](../assets/sa-my-pic.png)


## SLMRec

[ICLR2025 | SLMRec: 重新思考大语言模型在推荐系统中的价值](https://mp.weixin.qq.com/s/E6SGGKy6-D1h3zGro7KW0w)

[SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://openreview.net/pdf?id=G4wARwjF8M)

提出了一种知识蒸馏的方法，使得小语言模型仅用13%的模型参数，取得了比现有大模型推荐方法略好的效果，并取得了在训练/推理阶段6.6x/8.0x的加速。

[https://github.com/WujiangXu/SLMRec](https://github.com/WujiangXu/SLMRec)

![](../assets/slmrec.png)

+ teacher每m层分成一个block，共分为B个block；student每n层分为一个block，也是B个block
+ 每个block里，算3个loss：
  + teacher和student间算2个loss：衡量方向的cos相似度，衡量模长的L2距离
  + student和真实label（target item）算loss
+ teacher frozen，只通过lora来训练student

## KuaiMod

[KuaiMod来了！快手用大模型重构短视频生态格局](https://mp.weixin.qq.com/s/HO7I_9E1iwteSRHLukz81A)

[VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/pdf/2504.14904v1)

+ 首个短视频平台劣质内容判别基准测试：面向快手生态构建了短视频劣质内容分类体系，形式化定义了短视频内容判别任务，并基于真实场景中的用户反馈构建了首个短视频劣质内容判别基准测试。KuaiMod劣质内容分类体系包含4种主要的劣质类别以及15种细粒度劣质类别。基准测试包含1000条短视频样本，涵盖 15种细粒度劣质类别，完全由人工标注，并经过多轮数据清洗以保证正确性。
+ 首个工业级自动化内容判别解决方案：KuaiMod是首个在工业场景下验证了部署价值的自动化内容质量判别解决方案，在快手平台多个场景下的部署结果表明，KuaiMod方案具有能够与人工判别相媲美的准确率。
+ 基于用户反馈的强化学习训练+更新策略：区别于静态的规则或内容判别API，基于判例的劣质内容建模使得KuaiMod可以通过迭代训练数据完成判别策略的更新。为了保证实时性和准确率，我们设计了基于用户反馈的强化学习范式，利用用户的线上反馈构造新的训练数据，实现线上判别服务的天级更新。

## GPSD

[Scaling Transformers for Discriminative Recommendation via Generative Pretraining](https://arxiv.org/pdf/2506.03699)

+ 生成式预训练：使用Transformer架构，基于用户行为序列进行自回归生成训练，预测下一项。
+ 判别式训练：使用上面训练的Transformer，输入除了用户行为序列、候选项，还包括其他特征，输出分类概率。

提出五种参数转移策略：

+ 无转移（NT）：从头训练，作为基线。
+ 全转移（FT）：转移所有参数（稀疏+密集）。
+ 稀疏转移（ST）：仅转移稀疏参数（如嵌入表）。
+ 全转移+稀疏冻结（FT&SF）：转移所有参数并冻结稀疏参数。
+ 稀疏转移+稀疏冻结（ST&SF）：转移稀疏参数并冻结。

![](../assets/gpsd.png)

# LLM+推荐小结

## 参考LLM建模方式

| 论文 | 公司 | 关键词 | 做法 |
|------|-----|------|----------------| 
|[TIGER](https://arxiv.org/pdf/2305.05065.pdf) | google | semantic_id粒度的生成式 | - 基于RQ-VAE聚类 <br> - 输入semantic_id list，预测下一个semantic_id <br> |
|[HSTU](https://arxiv.org/pdf/2402.17152.pdf) | meta | item_id粒度的生成式 | - 性别年龄等profile特征、action type也可以作为特殊的item_id <br> - 输入item_id list，预测下一个item_id <br> - 一种范式可以同时支持召回和排序 |
|[COBRA](https://arxiv.org/abs/2503.02453) | 百度 | semantic_id粒度的生成式 | - 和TIGER类似，只是给RQ-VAE再加一个由bert的CLS得到的emb <br> - 在线infer更复杂 <br> |
|[HeteroRec](https://arxiv.org/pdf/2503.01469) | 阿里 | 多模态+id生成 | - img/txt/id一起输入 <br> - listwise multi-step prediction <br> |

## 基于开源LLM

+ FFT：full finetuning
+ PT：prompt tuning
+ LAT：layerwise adapter tuning
+ OT：option tuning
+ T-FEW：few-shot peft

看着落地的

| 论文 | 公司 | 关键词 | 做法 | ab收益| tune方式|
|------|-----|------|----------------| -----|------|
|[KAR](https://arxiv.org/pdf/2306.10933) | 华为 | item llm+user llm | - 让LLM总结item得到item emb；<br> - 让LLM总结user历史得到user emb <br> - 两个emb过一个mmoe做融合得到新的两个emb，给推荐模型用 | 音乐推荐 涨了播放量 | frozen|
|[BAHE](https://arxiv.org/pdf/2403.19347) | 蚂蚁 | 预先计算原子用户行为 | - LLMs的预训练浅层提取来原子用户行为的emb，并存进离线db <br> - 从db里查出来，和item一起过LLMs的更深层可训练层  | 广告ctr+cpm | FFT上层LLM|
|[LEARN](https://arxiv.org/pdf/2405.03988) | 快手 | ItemLLM+user decoder| - item LLM固定，输入item特征得到item emb；<br> - 输入item emb过user的12层trans算dense all-action loss，<br> - 线上推荐模型里加user emb和item emb | 广告cvr+收入 | frozen|
|[BEQUE](https://arxiv.org/pdf/2311.03758)|阿里|SFT+离线模拟+PRO|query重写任务，SFT得到一个LLM，将其预测的若干个候选rewrites通过offline system的feedback得到排序，再通过PRO算法再tune LLM|电商搜索，gmv+单量|FFT|


看着没落地的


| 论文 | 公司 | 关键词 | 做法 | tune方式|
|------|-----|------|---------------------|------|
|[SLIM](https://arxiv.org/pdf/2403.04260) | 蚂蚁 | 蒸馏推荐理由| - 输入用户行为历史，大LLM(gpt)产出的推荐理由；<br> - 小llm(llama2)去蒸馏这个理由拿小llm去给出全量user的推荐理由， <br> - 通过BERT得到emb，给推荐模型用 | FFT |
|[DLLM2Rec](https://arxiv.org/pdf/2405.00338v1) | OPPO | 蒸馏推荐理由| 在SLMI的基础上设计了ranking蒸馏和embed蒸馏 | FFT |
|[LLM-CF](https://arxiv.org/pdf/2403.17688) | 快手 | 基于CoT数据集做RAG| - 拿推荐数据对llama2做sft，再用CoT的prompt让llama2对user+item+label产出一个推理过程，并通过bge得到emb，构建一个CoT数据集。 <br>- 在线拿当前用户+item的特征从这个数据集里ann出k个cot example的emb，和其他特征一起输入一个decoder，输出给推荐模型的sharebottom，额外加了一个CoT emb的重建loss | FFT|
|[ILM](https://arxiv.org/pdf/2406.02844)| google | 2阶段训练+q-former | - phase1：表示学习，交替训练两类表示学习（item-text表示学习，item-item表示学习）<br> - phase2：item-language model训练 | frozen|
|[EmbSum](https://www.arxiv.org/pdf/2405.11441)| meta | LLM摘要+t5 encoder | - 行为历史丢给LLM产出摘要，对应的hidden states给decoder自回归;<br> - 历史item过t5 encoder并concat过poly；<br>- item过t5 encoder过poly；| frozen|
|[Agent4Ranking](https://arxiv.org/pdf/2312.15450)|百度|agent rewrite+ bert ranking|query重写任务，多个人群当成多个agent，每个通过多轮对话产出一个rewrite，再合在一起经过bert+mmoe计算robust损失+accuracy损失。|frozen|

纯学术界

| 论文 | 关键词 | 做法 |tune方式 |
|------|------|---------------------|------|
|[CUP](https://arxiv.org/pdf/2311.01314)| LLM总结+bert双塔| 把用户的一堆历史评论扔给chatgpt，让它总结出128个token，然后丢给双塔bert，另一个塔是item的描述，freeze bert底层，只tune上层|last layer FT |
|[LLaMA-E](https://arxiv.org/pdf/2308.04913)| gpt扩展instruct| instruction formulating为写300个种子指令，让gpt作为teacher，对300个种子指令进行扩展，并由领域专家评估后，去重并保证质量，得到120k个指令作为训练集，再用lora去instruct tuning|lora|
|[EcomGPT](https://arxiv.org/pdf/2308.06966v1)| 一系列电商任务FFT BLOOMZ | 设置一系列的task(100多个task)来finetune BLOOMZ，包括命名实体识别、描述生成、对话intent提取等|FFT|
|[Llama4rec](https://arxiv.org/pdf/2401.13870) | prompt增强+数据增强，finetune | - prompt增强：在prompt里引入推荐模型的信息；<br> - 数据增强：通过LLM给推荐模型增加样本 <br> - adaptive aggregation：llm和推荐模型各自打分并用融合公式融合| FFT |
|[SAGCN](https://arxiv.org/pdf/2312.16275) | 分aspect打标、构图+gcn | - LLM为用户评论打标，确定aspect；<br> - 分aspect构建u-i图，并gcn| frozen|
|[GReaT](https://arxiv.org/pdf/2210.06280) | 表格型数据+LLM | 随机交换属性生成数据，finetune LLM预测属性 | FFT|
|[ONCE](https://arxiv.org/pdf/2305.06566)| 闭源LLM总结、开源LLM做encoder，u-i学ctr |闭源LLM输出文本（user profiler、content summarizer、personalized content generator），给开源LLM得到user表示，item过开源LLM得到item表示，二者内积学ctr| lora训开源，frozen闭源|
|[Agent4Rec](https://arxiv.org/pdf/2310.10108.pdf)|多智能体系统模拟交互，产出推荐样本| 先训一个推荐模型，然后构建一个多智能体系统，模拟和这个推荐模型交互，产出新的样本给推荐模型做数据增强 |仅训推荐模型，LLM frozen|
|[RecPrompt](https://arxiv.org/pdf/2312.10463)|两个LLM迭代出最佳prompt|给一个初始prompt，让LLM1得到推荐结果，拿一个monitor衡量这个结果和ground truth的mrr/ndcg，再用另一个LLM产出更好的prompt给第一个LLM用，如此迭代，得到一个best prompt|frozen|
|[PO4ISR](https://arxiv.org/pdf/2312.07552)|反思原因并refine/augment地迭代出最优的prompt|给初始prompt，收集error case让模型反思原因并refine出新的prompt，再augment出另一个prompt，并UCB选出最好的prompt，如此迭代|frozen|
|[TransRec](https://arxiv.org/pdf/2310.06491) | 受限生成 | - 将一个item表示成3部分：id+title+attr，设计三种对应的instruct-tuning任务；<br> - 引入一个特殊的数据结构（FM-index），并进行constrained beam search，让模型能生成候选集中的id/title/attr，<br> 再遍历全库候选，看不同facet的相似度（会考虑高热打压），加权融合出一个排序| lora|
|[E4SRec](https://arxiv.org/pdf/2312.02443)| 推荐id emb输入LLM | 推荐的id emb、prompt的emb一起输入LLM，最后一个词映射回推荐id emb的dim，去softmax | lora |


## 其他套路

工业界

| 论文 | 公司 | 关键词 | 做法 |
|------|-----|------|----------------| 
|[ExFM](https://arxiv.org/abs/2502.17494) | Meta | 两阶段蒸馏 | - 先训好teacher，并利用等待时间窗口为student数据集进行预估  <br> - 加了一些蒸馏loss |


学术界

| 论文 | 关键词 | 做法 |
|------|------|---------------------|
|[SLMRec](https://openreview.net/pdf?id=G4wARwjF8M) | 一阶段蒸馏 | teacher和student都拆成多个block，每个block间蒸馏 |


## 讨论

我想输入用户历史行为，通过llm产出用户向量（64维的向量），这个向量可以是历史的总结，也可以是预测未来的一些什么东西，最终用在推荐系统的精排模型里，业界有什么好的方法吗

让gpt帮忙深度研究了一下：[版本1](https://chatgpt.com/s/dr_6821e54479dc81918c5e4bd61d645894)，[版本2](https://chatgpt.com/s/dr_6821fecfde088191b56c1ed19a3ee999)，

导出了对应的[pdf1](https://github.com/daiwk/collections/blob/master/assets/llm-gen-user-emb.pdf)，[pdf2](https://github.com/daiwk/collections/blob/master/assets/llm-gen-user-emb-64dim.pdf)



# LLM for 检索

## Gecko

[谷歌DeepMind发布Gecko：专攻检索，与大7倍模型相抗衡](https://mp.weixin.qq.com/s/5e_Py_Xm0RsmP1YMcikpaQ)

[Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/pdf/2403.20327.pdf)

![gecko](../assets/gecko.png)

主要包括两阶段：

+ pre-finetuning：类似[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)，自监督
+ finetuning：2-step llm distillation，提出了一个FRet数据集（Few-shot Prompted Retrieval dataset）

### Pre-finetuning

&nbsp;

2个数据集：

+ 大型社区qa数据集：[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)中的来自网上论坛和qa网站的问答pair对
+ 去各种网站上爬了title-body的pair对，因为[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533)发现这种自然出现的pair对对于pre-finetuning embedding模型很有用

pre-finetuning的目标是让模型看到大量的多样性语料，对于一个预训练的语言模型$$\mathcal{M}$$，长度为$$n$$的句子对应的contextual向量$$\mathbf{W} \in \mathbb{R}^{n \times d}$$，对于任务特征$$t$$，数据集$$\mathcal{D}_{\text {pre }}=\left\{\left(q_i, p_i\right)\right\}_{i=1}^N$$，得到的向量如下：

XXX
\begin{aligned}
\mathbf{q}_i & =\text { mean\_pool }_{|t|+\left|q_i\right|}\left[\mathcal{M}\left(t \oplus q_i\right) \in \mathbb{R}^{\left(|t|+\left|q_i\right|\right) \times d}\right] \in \mathbb{R}^d \\
\mathbf{p}_i & =\text { mean\_pool }_{\left|p_i\right|}\left[\mathcal{M}\left(p_i\right) \in \mathbb{R}^{\left|p_i\right| \times d}\right] \in \mathbb{R}^d
\end{aligned}
XXX

对于batchsize为$$B$$的样本来说，inbatch负例，$$\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\mathbf{x}^{\top} \mathbf{y}}{\|\mathbf{x}\| \cdot \cdot\|\mathbf{y}\|}$$，其loss如下：

XXX
\mathcal{L}_{\text {pre }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_i\right) / \tau}}{\sum_{j=1}^B e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_j\right) / \tau}}\right]
XXX

在这个阶段没有用hard负例，用了能适配设备的最大batchsize，这是[Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/pdf/2308.03281)和[Text embeddings by weakly-supervised contrastive pre-training](https://arxiv.org/pdf/2212.03533)里的经验。


### FRet

![fret](../assets/fret.png)

#### LLM-based Diverse Query Generation 

&nbsp;

从web语料$$\text { C }$$中抽出一个段落$$p_{\text {seed }}$$，$$\mathbb{P}_{\mathrm{QG}}$$是一个固定的few-shot的prompt，让LLM生成任务描述和这个task的query

XXX
\operatorname{LLM}\left(\mathbb{P}_{\mathrm{QG}}, p_{\text {seed }}\right) \rightarrow(t, q)
XXX

生成的任务t例如：

+ question answering：```Given a query, find a passage that has the answer to the query```
+ fact checking：```Given a query, find a passage that allows you to check whether the query is true or not```

为了保证多样性：

+ 网页库本身就有很多的topic和很多的写作风格
+ 在prompt里加多样的任务描述，来让模型生成的query保证多样性。

#### LLM-based Positive and Negative Mining

&nbsp;

对于某个query $$q$$，之前的工作一般会直接把输入的段落$$p_{\text {seed }}$$当成正样本，但实践中发现$$p_{\text {seed }}$$一般比较长，而生成的query一般只关注其中一小部分，所以可能在整个语料库中有比$$p_{\text {seed }}$$更准确的答案。因此，通过如下方法构造了一个FRet数据集：

+ 先把$$p_{\text {seed }}$$当成正样本，in-batch负例训一个embedding模型。
+ 用这个模型从文档库中检索出top N的相似段落$$P=\left\{p^{(1)}, \ldots, p^{(N)}\right\}$$
+ 用生成query的LLM给这N个文档排序，有两种排序方法：
    + query likelihood：参考[Improving passage retrieval with zero-shot question generation](https://arxiv.org/pdf/2204.07496)，给定段落$$p$$，衡量query $$q$$的likelihood，$$\mathrm{QL}(q, p)=\operatorname{LLM}\left(q \mid p, \mathbb{P}_{\mathrm{QL}}\right)$$，其中的prompt参考[PaRaDe: Passage Ranking using Demonstrations with Large Language Models](https://arxiv.org/pdf/2310.14408)包括了判断query likelihood的指令，以及一些相关query+段落的few-shot。基本思想是：**如果q和p高度相关，那么从p生成q的概率应该很高**。
    + relevance classification：参考[Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels](https://arxiv.org/pdf/2310.14122)，给定query $$q$$和段落$$p$$后，衡量特定相关性label的log likelihood：$$\operatorname{RC}(q, p)=\operatorname{LLM}\left(\text { label } \mid q, p, \mathbb{P}_{\mathrm{RC}}\right)$$。基本思想是：**让LLM直接判断q和p的相关程度,并输出一个相关性标签**。
+ 通过标准的Reciprocal Rank Fusion（RRF，倒数融合，[Reciprocal rank fusion outperforms condorcet and individual rank learning methods](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)）方法得到rank函数，对上面两个方法的排序结果merge：$$R(q, p)=1 / r_{\mathrm{QL}}(q, p)+1 / r_{\mathrm{RC}}(q, p)$$...其实就是rankindex
+ 基于得分筛选样本：
    + 正样本：$$p^{+}=\underset{p \in P}{\arg \max } R(q, p)=p_1$$
    + hard负例：排名第$$N$$位的样本$$p_N$$，也可以从除了第1个外的N-1个里随机sample

### Unified Fine-tuning Mixture

&nbsp;

除了FRet，还融合了多个公开数据集：

+ Natural Questions：[Natural Questions: A Benchmark for Question Answering Research](https://aclanthology.org/Q19-1026.pdf)
+ HotpotQA：[Hotpotqa: A dataset for diverse, explainable multi-hop question answering](https://arxiv.org/pdf/1809.09600)
+ FEVER：[Fever: a large-scale dataset for fact extraction and verification](https://arxiv.org/pdf/1803.05355)
+ MedMCQA：[Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering](https://arxiv.org/pdf/2203.14371)
+ SNLI：[A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326)
+ MNLI：[A broad-coverage challenge corpus for sentence understanding through inference](https://arxiv.org/pdf/1704.05426)
+ MIRACL：多语言的数据集[Miracl: A multilingual retrieval dataset covering 18 diverse languages](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering)
+ huggingface上的一些分类数据集

将这些数据集处理成一个统一的格式，发现不同的任务对prompt的格式敏感程度不同，，非对称任务（如BEIR）对格式更敏感，而对称任务的性能相对稳定。

+ 对称格式（Symmetric Formatting）：输入和目标使用相同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：task: {task} | query: {target}
+ 非对称格式（Asymmetric Formatting）：输入和目标使用不同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：title: {title} | text: {target}

参考[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://aclanthology.org/2023.findings-acl.71.pdf)，对于每个文本$$x$$，找到另一个同样label是$$y$$的样本当成正样本$$x^{+}$$，随机找一个label不是$$y$$的当作负样本$$x^{-}$$。

实践中，在一个batch中，同一个$$x^+$$可能出现overlap，会造成in-batch negatives中的false negative问题。参考[poe的回复](https://poe.com/s/XHJyRAAOc9ZAuQu5yz2p)，即同一个mini-batch中假设有(x1, x1+, x1-) 和 (x2, x2+, x2-)，可能出现x1+ 与 x2 或 x2+ 相同或非常相似的情况。那么：

+ 对于x1来说，x1+是正例。
+ 但是如果x1+ 与 x2 相同或非常相似，模型可能会错误地认为x1+应该与x2区分开来，因为x2是另一个样本的输入。
+ 或者如果x1+ 与 x2+ 相同或非常相似，模型可能会混淆应该如何处理这个重叠的样本。

因为在理想情况下，x1+应该只是x1的正例，而不应该被视为任何其他样本的负例。但由于重叠，模型可能错误地将x1+视为x2或其他样本的负例。解决方法，给每个三元组分配唯一的id，让模型专注于在给定x的情况下，区分x+和x-。

有$$M$$个数据集$$\left[\mathcal{D}^{(1)}, \ldots, \mathcal{D}^{(M)}\right]$$，每个数据集是$$\mathcal{D}^{(m)}=\left\{\left(t_i, q_i, p_i^{+}, p_i^{-}\right)\right\}_{i=1}^N$$，$$t$$是任务描述，给定一个batch_size=B的batch，同batch里的其他query可以看成[SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives](https://arxiv.org/pdf/2306.02516)中说的same-tower negative（对同模态的效果比较好，例如这边都是query）：

XXX
\mathcal{L}_{\text {main }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_i^{+}\right) / \tau}}{\sum_{j=1}^B\left(e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_j^{+}\right) / \tau}+\mathbb{1}_{[j \neq i]} e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{q}_j\right) / \tau}\right)+e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{P}_i^{-}\right) / \tau}}\right]
XXX

同时还参考[Matryoshka representation learning](https://arxiv.org/pdf/2205.13147)的俄罗斯套娃加了一个MRL的loss，让模型适配不同的dim，gecko是768和256的dim



## ChatRetriever

[ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval](https://arxiv.org/pdf/2404.13556v1)

思想：原来输入的prompt是x，输出的response是y，添加t个特殊的```[emb_i]``` token，输入

XXX
s=[x_1, \ldots, x_N,\left[\mathrm{EMB}_1\right], \ldots,\left[\mathrm{EMB}_t\right], y_1^{+}\ldots, y_M^{+},\left[\mathrm{EMB}_1\right], \ldots,\left[\mathrm{EMB}_t\right]]
XXX

然后训练的时候，每个生成的$$y_i$$只看得到它前面的y，还有x后面的那t个特殊token：

XXX
\mathcal{L}_{\mathrm{S}}=-\frac{1}{M} \sum_{i=1}^M \log p\left(y_i^{+} \mid y_1^{+}, \ldots, y_{i-1}^{+}, \mathbf{x}_{1: t}\right)
XXX

![chatretriever](../assets/chatretriever.png)

对应的[代码](https://github.com/kyriemao/ChatRetriever/blob/cbcd9b10f7d0e1dc27ae299f463a5d27e423fca7/src/kindred/models/model_utils.py#L247)

```python
def unified_model_forward(model, model_type: str, inputs: dict, normalize_emb: bool):
    inputs.pop("sample_ids", None)
    inputs.pop("input_texts", None)
    
    if model_type in ['bge', 'llm_embedder']:
        output = model(**inputs)
        embs = output.last_hidden_state[:, 0]
    elif model_type in set(['qwen', 'qwen_chat', 'qwen_chat_cot', 'qwen_chat_lora', 
        'qwen_chat_lora_eval', 'qwen_chat_cot_lora', 'qwen_chat_cot_lora_eval', 
        'qwen15_chat_cot_lora_eval', 'repllama', 'llama', 
        'repllama-train', 'repllama_v2', 'repllama_v2-train', 
        'repllama_v2-continue_train', 'repllama_chat', 
        'repllama_chat-train', 'repllama_chat_cot', 'repllama_chat_cot-train', 
        'mistrial_cot-train', 
        'mistrial_chat_cot-train', 'mistrial_cot', 'mistrial_chat_cot', 
        'qwen15', 'qwen15_chat', 
        'qwen15_chat_lora', 'qwen15_chat_cot_lora', 'qwen15_chat_cot']):        
        inputs['output_hidden_states'] = True
        output = model(**inputs)
        hidden_states = output.hidden_states[-1]
        last_token_indices = inputs['attention_mask'].sum(dim=1) - 1
        embs = hidden_states[torch.arange(hidden_states.size(0)), last_token_indices]
    elif model_type in ['e5_mistrial', 'e5_mistrial-train']:
        outputs = model(**inputs)
        embs = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])
    elif model_type in ['ance', 'gtr', 'splade']:
        embs = model(**inputs)
    elif model_type in ['bert']:
        output = model(**inputs)
        embs = output.pooler_output
    else:
        raise NotImplementedError("Model type {} is not supported now.".format(model_type))    
    
    if normalize_emb:
        embs = torch.nn.functional.normalize(embs, p=2, dim=-1)
    return embs
```

## Instructor

[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://aclanthology.org/2023.findings-acl.71.pdf)

基于google的GTR(Generalizable T5-based dense Retrievers)模型（[Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/pdf/2112.07899)）加了task instruction，用对比学习loss训练

## LLM-Embedder(BGE)

[Retrieve Anything To Augment Large Language Models](https://arxiv.org/pdf/2310.07554)和[C-Pack: Packed Resources For General Chinese Embeddings](https://arxiv.org/pdf/2309.07597)这两个对应的是一样的代码

[https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## RepLLaMA

[Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/pdf/2310.08319)

## E5_mistral-7b

[Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368)

## GRIT

[Generative Representational Instruction Tuning](https://arxiv.org/pdf/2402.09906)

[https://github.com/ContextualAI/gritlm](https://github.com/ContextualAI/gritlm)


## LLM2Vec

[LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/pdf/2404.05961)

[https://github.com/McGill-NLP/llm2vec](https://github.com/McGill-NLP/llm2vec)

+ 由3步组成：
 + Bi-directional：把causal mask干掉，改成全1mask
 + 引入masked next token predtion任务：类似mlm，mask掉中间的词，拿周围的词预测
 + 对比学习simcse：前两步让模型有了bi-directional的能力，把所有词的emb通过pooling（实验表明mean pooling最有效）得到句子表示，同一句话里mask掉几个词的为正例，不同句子为负例，对比学习

## GDR

[小红书搜索：生成式检索的探索与实践](https://mp.weixin.qq.com/s/yApGxCGxjWnZQu8PoO9Qeg)

[Generative Dense Retrieval: Memory Can Be a Burden](https://arxiv.org/abs/2401.10487)

## 阿里搜索广告

[阿里妈妈搜索广告2024大模型思考与实践](https://mp.weixin.qq.com/s/hgs_BzFZdjrDSbf9d74ilA)

## SyCL

[Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance](https://arxiv.org/pdf/2503.23239)

用LLM生成具有多个相关性级别的**合成**文档，结合列表式损失函数（Wasserstein距离），提升检索器的排名性能。

传统的密集检索器训练依赖对比学习和二元相关性标签（InfoNCE损失），即将文档分为正样本和负样本。这种方法存在两个主要局限：

+ 忽略细微相关性差异：除了明确标注为相关的文档外，所有其他文档被同等视为负样本，无法反映实际的相关性程度。
+ 对噪声敏感：二元标签无法充分利用数据的排名信息，且易受标注错误影响。

具体方法：

+ 多级排名上下文生成：使用 MS MARCO 数据集的查询，提示LLM生成四篇不同相关性级别的合成文档。通过顺序生成确保相关性逐步降低，并使用随机采样（如句子长度和难度）增加多样性。分配标签：{3, 2, 1, 0}，分别对应完美相关到无关。
+ Wasserstein 距离损失：不同于InfoNCE的逐对比较，Wasserstein 距离考虑整个文档集合的相关性分布。通过比较真实标签分布与预测得分分布，优化模型以更准确地排名文档。

## RARE

[腾讯搜索广告生成式检索](https://mp.weixin.qq.com/s/zRGuttbqKkTFasg9AY4fGA)

[Real-time Ad retrieval via LLM-generative Commercial Intention for Sponsored Search Advertising](https://arxiv.org/pdf/2504.01304)

## gemini embedding

[Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/pdf/2503.07891)

## BGE-code/VL

[代码、多模态检索全面登顶SOTA！智源BGE向量模型三连击，并全面开放](https://mp.weixin.qq.com/s/DYGix_6Zf7M50aC8TtpQKg)

+ BGE-code：[Towards A Generalist Code Embedding Model Based On Massive Data Synthesis](https://arxiv.org/abs/2505.12697)
+ BGE-VL：[MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval](https://arxiv.org/abs/2412.14475)
+ BGE-VL-screenshot：[Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval](https://arxiv.org/abs/2502.11431)

## UniME

[Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/pdf/2504.17432)

### 阶段一：文本判别知识蒸馏

![](../assets/unime-text-distill.png)

用纯文本数据来增强MLLM中LLM的嵌入能力，用了273k个句子对。

+ teacher：基于LLM的嵌入模型NV-Embed V2（对比训练中移除了causal mask）离线批量产出文本的emb
+ student：MLLM中把LLM部分剥离出来，输入prompt ```Summary the above sentences in one word: \n”```，最后一个token的emb当成输出
+ 蒸馏：teacher和student间算KL散度，LoRA训练

推理时：

+ 单模态输入：通过prompt的设置，决定只走对应的vision/text encoder
+ 图片+文字输入：通过prompt，各自过模型，然后把输出的2个emb进行融合 

### 阶段二：困难负样本增强指令微调

![](../assets/unime-img.png)

拿多模态的样本对来提升图文之间的对齐，用了662k个pair对：

+ query: img+prompt
+ doc: img

样本处理：

+ 干掉false negative：有些负样本和query的相似度太高了，干掉
+ 增加hard negative：除了正例和前面的false negative外，在负样本里找出k个和query最像的，当成hard negative

loss是infoNCE，然后用QLoRA来微调

代码：

```python
import torch
from PIL import Image
from torch.nn import functional as F
from self_evaluate.utils.utils import init_model_and_transform

model_name = "phi35V"
base_model_path="DeepGlint-AI/UniME-Phi3.5-V-4.2B"
# model_name = "llava_16"
# base_model_path="DeepGlint-AI/UniME-LLaVA-1.6-7B"

if model_name == "phi35V":
    img_prompt = '<|user|>\n<|image_1|>\nSummary above image in one word: <|end|>\n<|assistant|>\n'
    text_prompt = '<|user|>\n<sent>\nSummary above sentence in one word: <|end|>\n<|assistant|>\n'
elif model_name == "llava_16":
    img_prompt = "[INST] <image>\nSummary above image in one word: [/INST]"
    text_prompt = "[INST] <sent>\nSummary above sentence in one word: [/INST]"

text = "A man is crossing the street with a red car parked nearby."
image_path = "figures/demo.png"
input_texts = text_prompt.replace('<sent>', text)
input_image_prompt = img_prompt
input_image = [Image.open(image_path)]

model, transform = init_model_and_transform(model_name, base_model_path)
inputs_text = transform(text=input_texts,
                    images=None,
                    return_tensors="pt", 
                    padding=True)
for key in inputs_text: inputs_text[key] = inputs_text[key].to("cuda")
inputs_image = transform(text=input_image_prompt,
                    images=input_image, 
                    return_tensors="pt", 
                    padding=True).to("cuda")

with torch.no_grad():
  emb_text = model(**inputs_text, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]
  emb_image = model(**inputs_image, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]
  emb_text = F.normalize(emb_text, dim=-1)
  emb_image = F.normalize(emb_image, dim=-1)
  Score = emb_image @ emb_text.T
print("Score: ", Score) # Score: 0.59
```

其中：

```python
def init_model_and_transform(model_name, base_model_path): 
    if model_name == 'phi35V':
        transform = AutoProcessor.from_pretrained("microsoft/Phi-3.5-vision-instruct", 
        trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(base_model_path,
                            device_map="cuda", trust_remote_code=True,
                            torch_dtype=torch.float16, 
                            _attn_implementation='flash_attention_2')
    elif model_name == "llava_16": 
        transform = LlavaNextProcessor.from_pretrained("llava-hf/llava-v1.6-mistral-7b-hf")
        model = LlavaNextForConditionalGeneration.\
          from_pretrained(base_model_path, device_map="cuda", 
            torch_dtype=torch.float16, low_cpu_mem_usage=True) 
    transform.tokenizer.padding_side = "left"
    transform.tokenizer.padding = True
    return model, transform
```


## 百度召回

[搜索广告召回的生成式革新](https://mp.weixin.qq.com/s/q4-KJvmroyIAlDtjtRMKIw)

## Qwen3-emb

[Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/pdf/2506.05176)

### 架构

![](../assets/qwen3-emb-arch.png)

### 训练

![](../assets/qwen3-emb-train.png)

多阶段训练

+ Large-Scale Synthetic Data-Driven Weak Supervision Training：
+ High-Quality Synthetic Data Utilization in Supervised Fine Tuning: 
+ Model Merging: 

# O1

## CoT开山之作

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)

## from r to Q*

[这就是OpenAI神秘的Q*？斯坦福：语言模型就是Q函数](https://mp.weixin.qq.com/s/Mz_k5ensgiuXu-3cFQ1Dkw)

[From r to Q*: Your Language Model is Secretly a Q-Function](https://arxiv.org/pdf/2404.12358.pdf)

在DPO的基础上，引入LLM里的token-level的MDP，用二分类的preference feedback。发现了3个点：

+ 尽管 DPO 是作为上下文多臂赌博机而派生出来的，但DPO模型的隐含奖励可在每个 token 层面上进行解释。
+ DPO模型的likelihood search类似在decoding阶段寻找一个reward function。即在token层面的阐述方式下，经典的基于搜索的算法（比如 MCTS）等价于在 DPO策略上的基于似然的搜索。
+ 初始策略和参考分布的选择对于确定训练期间隐性奖励的轨迹非常重要。

[OpenAI秘密武器「草莓」计划曝光！Q*推理能力大爆发，逼近AGI L2里程碑](https://mp.weixin.qq.com/s/qq_E05Tab-ptRqGDjrAt0A)

5级路线图：

+ L1：聊天机器人，具有对话能力的AI。
+ L2：推理者，像人类一样能够解决问题的AI。
+ L3：智能体，不仅能思考，还可以采取行动的AI系统。
+ L4：创新者，能够协助发明创造的AI。
+ L5：组织者，可以完成组织工作的AI。

Strawberry模型的目的是为了使公司的AI不仅能生成查询答案，还能提前计划，足够自主且可靠地浏览互联网，进行OpenAI所称的「深度研究」。

类似[Star: Self-taught reasoner bootstrapping reasoning with reasoning](https://arxiv.org/pdf/2203.14465.pdf)能够通过迭代创建自己的训练数据，来「自我提升」到更高的智能水平。


[Nature封面：AI训练AI，越训越离谱](https://mp.weixin.qq.com/s/l9ka81Cj2LFzNbXXsaEbmA)

[AI models collapse when trained on recursively generated data](https://www.nature.com/articles/s41586-024-07566-y)，对应下载的[pdf](https://github.com/daiwk/collections/blob/master/assets/nature-ai-train-ai.pdf)


## 自我奖励

[Self-Rewarding Language Models](https://arxiv.org/pdf/2401.10020)

[「用 AI 训 AI」这事靠谱吗？](https://mp.weixin.qq.com/s/bLLoYDTpq8q7ExfwyDekOQ)


## self-play

[清华、北大等发布Self-Play强化学习最新综述](https://mp.weixin.qq.com/s/oMY0O0OIVYJc04zkoMzgcQ)

[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A)

[万字长文推演OpenAI o1 self-play RL 技术路线](https://mp.weixin.qq.com/s/_kt0SPuWWiiu7XwqNZKZAw)

[有想入坑RL-LLM的同学吗？这个开源项目一个GPU够了，完成后欢迎来月之暗面~](https://mp.weixin.qq.com/s/e-SHFE6UxXY4y5W-W2fYZw)

[https://github.com/inspirai/TimeChamber](https://github.com/inspirai/TimeChamber)

## Cursor

[Scaling Law瓶颈，Cursor编程为什么这么强？团队参与新研究掏出秘密武器](https://mp.weixin.qq.com/s/xhV9HoeEP22RjuWTjgbPqg)

[Planning In Natural Language Improves LLM Search For Code Generation](https://arxiv.org/pdf/2409.03733)

## Let's verify step by step

[o1基石论文火爆传阅，Ilya仍是关键先生！核心项目清北校友闪光](https://mp.weixin.qq.com/s/woElE_YfQni7bwe4UCCK4g)

[Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)

[OpenAI使用过程监督提升数学推理能力](https://mp.weixin.qq.com/s/E8GtQOT6tPoScj5nMjkSjQ)

[https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)

## O1相关汇总

(toread)

[OpenAI o1要跟，怎么跟？这个GitHub项目把解读、博客、相关论文一网打尽](https://mp.weixin.qq.com/s/sPYeM5LbfAwyHUxbQ78Vsg)

[刚刚，OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

[张俊林：OpenAI o1的价值意义及强化学习的Scaling Law](https://mp.weixin.qq.com/s/my7XiRtpb8IY3Z0b471NJA)


[北大对齐团队独家解读：OpenAI o1开启「后训练」时代强化学习新范式](https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ)


[Noam Brown早已预示o1强大推理能力，演讲深度解析AI推理研究脉络](https://mp.weixin.qq.com/s/KRttVeMN4tPw9yb6f4LQgA)

[OpenAI o1模型的前世今生](https://mp.weixin.qq.com/s/OCgbffOPrZ5kzFKisSUC9Q) --toread

## CoT能让Transformer更强

[谷歌再次痛失好局！OpenAI o1 证实谷歌 ICLR 2024 论文价值「四位华人贡献」](https://mp.weixin.qq.com/s/7FsVPFUb4-fkeaOtcRrgsw)

[Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/pdf/2402.12875)

### CoT：打破 Transformer 的“并行诅咒”

传统的 Transformer 模型虽然在自然语言处理领域取得了巨大成功，但它有一个致命弱点：擅长并行计算，但不擅长串行推理。这就像一个超级聪明的孩子，能快速完成大量的计算题，但却无法理解简单的逻辑推理。

而CoT (Chain of Thought，思维链)技术的灵感来源于人类的思维过程，它可以让 Transformer 模拟人类的思考方式，通过生成一系列中间推理步骤，来解决那些需要逻辑推理的复杂问题。

### CoT 的理论基础：从电路复杂度到 Transformer 表达能力

作者用电路复杂性理论来解释 CoT 的强大之处，将Transformer的计算过程与电路模型进行类比，并将Transformer能够解决的问题类别定义为“CoT 复杂性类”

他们证明了传统的Transformer模型（没有 CoT）只能解决AC0电路能够解决的问题，而AC0电路是一种计算能力非常有限的电路模型。但是，如果加入 CoT，Transformer 的表达能力将得到质的飞跃！作者用数学严格证明了：

只要CoT步骤足够多，Transformer 就能模拟任意大小的布尔电路，从而解决P/poly问题，这是一个包含了P问题的更大的问题类别，相当于证明了CoT可以让 Transformer 解决几乎所有可以用计算机解决的问题。

### CoT 的实验验证：从模加到电路值问题，CoT 全面胜出！

为了进一步验证CoT的有效性，论文作者设计了四个核心问题：

+ 模加： 计算两个数的和，并对某个整数取模 
+ 排列组合： 计算一组排列的组合
+ 迭代平方： 对一个数进行多次平方运算 
+ 电路值问题： 计算一个布尔电路的输出值 

其中，模加问题可以用并行计算高效地解决，而其他三个问题则需要串行计算。

实验结果表明：

+ 对于模加问题，即使不使用 CoT，Transformer 也能取得不错的效果
+ 但对于其他三个问题，使用 CoT 可以显著提高 Transformer 的准确率，尤其是在模型深度较浅的情况下

### 讨论

[CoT能让模型推理能力无上限？田渊栋、LeCun下场反对：两层MLP还能模拟全世界呢](https://mp.weixin.qq.com/s/wi1jvQg47O078Xk83UpYmA)

[Transformer推理天花板被谷歌打破？DeepMind首席科学家亮出84页PPT，却遭LeCun反对](https://mp.weixin.qq.com/s/_z3ITDGRWXjbh8aVUBdUsg)

## CoT or not

[o1带火的CoT到底行不行？新论文引发了论战](https://mp.weixin.qq.com/s/_v15-UYpv300XIlhQ-54Vg)

[To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/pdf/2409.12183)


## test-time scaling

[小模型越级挑战14倍参数大模型，谷歌开启Test-Time端新的Scaling Law](https://mp.weixin.qq.com/s/tfi7VOpSdKIXVb--k6NCSg)

[Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)

一句话：根据给定的prompt难度，动态地分配测试时（Test-Time）的计算资源。在预训练阶段花费更少的计算资源，而**在推理阶段花费更多，这种策略可能更好**。
+ 针对（Process Reward Model, PRM，[Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)里提出的）进行搜索。
  + PRM可以在模型生成答案过程中的每个步骤都提供评分，用于引导搜索算法，动态调整搜索策略，通过在生成过程中识别错误或低效的路径，帮助避免在这些路径上浪费计算资源。
+ 在测试时根据prompt自适应地更新模型的响应分布。
  + 模型不是一次性生成最终答案，而是逐步修改和改进它之前生成的答案，按顺序进行修订（revision）。
  + 并行采样：独立生成N个答案，
  + 顺序修订：每个答案依赖于前一次生成的结果，逐步修订。

![](../assets/test-time-scaling.PNG)


[OpenAI o1 技术初探1：整体框架，利用Test-Time Scaling Law提升逻辑推理能力](https://mp.weixin.qq.com/s/MNwS1PQX2XOhVfN0rKykUQ)---(写得比较细)

[Scaling LLM Test-Time：谁说类o1推理一定要用RL???](https://mp.weixin.qq.com/s/3ABXCv6PG6asRfD0Vc5Iig)

[3B模型长思考后击败70B！HuggingFace逆向出o1背后技术细节并开源](https://mp.weixin.qq.com/s/E1FaaOurAb-QlCX3BASi9Q)

## inference scaling

[LLM Inference Scaling：姚班/OpenAI/CMU 8月论文提前揭示o1核心原理](https://mp.weixin.qq.com/s/p84S9r7Qbuo_yjAWhhqoVQ)

[An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/abs/2408.00724)

## 扩散模型的inference scaling

[扩散模型也能推理时Scaling，谢赛宁团队重磅研究可能带来文生图新范式](https://mp.weixin.qq.com/s/wtSzBr6Gs1nF9zz4c5r7Ag)

[Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps](https://arxiv.org/pdf/2501.09732)


## O1的评估

[280页PDF，全方位评估OpenAI o1，Leetcode刷题准确率竟这么高](https://mp.weixin.qq.com/s/WxmqCcvvXropIfVCxIJ7bA)

[Evaluation of OpenAI o1: Opportunities and Challenges of AGI](https://arxiv.org/pdf/2409.18486)

## LLM Reasoning

[一文看懂LLM推理，UCL汪军教授解读OpenAI ο1的相关方法](https://mp.weixin.qq.com/s/TCWs5TKKXiRbmt-XUd0wfg)

[首个o1复现开源RL框架OpenR来了，UCL、上交等高校联合团队发布](https://mp.weixin.qq.com/s/Dr9IzbUjiWtZT7bgr58T2g)

[OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models](https://arxiv.org/abs/2410.09671)

[A Tutorial on LLM Reasoning: Relevant methods behind ChatGPT o1](https://github.com/openreasoner/openr/blob/main/reports/Tutorial-LLM-Reasoning-Wang.pdf)

[大模型不会推理，为什么也能有思路？有人把原理搞明白了](https://mp.weixin.qq.com/s/2_ccqg23n05iGK3zUH5KMg)

[Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models](https://arxiv.org/abs/2411.12580)

## 开源模型+O1

[OpenAI o1式思维链，开源模型也可以有，成功案例来了](https://mp.weixin.qq.com/s/W28qb8ZaJkcyDP69eGw8MA)

## 复现O1

[技术上，如何复现 o1?](https://mp.weixin.qq.com/s/_fNioAkD--nI9WSH64O-_A?poc_token=HH6r-2ajxUVBKhJS6btRQEAl85tnczRGWRIAES19)

[17岁高中生写了个神级Prompt，直接把Claude强化成了满血o1](https://mp.weixin.qq.com/s/IAKD0FfcYehs5FsDkLbTJQ)

[https://github.com/richards199999/Thinking-Claude/tree/main](https://github.com/richards199999/Thinking-Claude/tree/main)

[OpenAI最大秘密，竟被中国研究者破解？复旦等惊人揭秘o1路线图](https://mp.weixin.qq.com/s/IOKFBgoWyietVe3NNNw9Hg)

[Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective](https://arxiv.org/abs/2412.14135)

## O1 Replication Journey

[上交大发布首个OpenAI o1复现项目进展报告，满满的经验洞察](https://mp.weixin.qq.com/s/ZO_Rv98OakPuBaZl9Tw5VA)

[上交大o1复现新突破：蒸馏超越原版，警示AI研发"捷径陷阱"](https://mp.weixin.qq.com/s/bJc_hSrXsUgrzAfSxAoYoA)

[O1 Replication Journey: A Strategic Progress Report](https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report.pdf)

[https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report-part2.pdf](https://github.com/GAIR-NLP/O1-Journey/blob/main/resource/report-part2.pdf)

[https://github.com/GAIR-NLP/O1-Journey](https://github.com/GAIR-NLP/O1-Journey)



## ScoRe

[强化学习让大模型自动纠错，数学、编程性能暴涨，DeepMind新作](https://mp.weixin.qq.com/s/CqxEoL50_FQTGtLYgh6omw)

[OpenAI o1技术初探3：如何让模型拥有自我纠错的能力](https://mp.weixin.qq.com/s/VHZ_BT27Dh2s5hVQSb33WA)


## LeCo

[COLM 24 | 从正确中学习？大模型的自我纠正新视角](https://mp.weixin.qq.com/s/F8KpJuiDE9DfSVb1ciLUSQ)

[Learning From Correctness Without Prompting Makes LLM Efficient Reasoner](https://arxiv.org/pdf/2403.19094)

[https://github.com/starrYYxuan/LeCo](https://github.com/starrYYxuan/LeCo)

## Marco-O1

[阿里推理模型来了！Marco-o1 发布即开源](https://mp.weixin.qq.com/s/taWAZsK_ITJYKM3q_Ssqwg)

[阿里国际版o1来了，Marco-o1：聚焦开放式问题推理](https://mp.weixin.qq.com/s/k1gwBWNYIn_tfviWxbj8fw)

[Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405)

[https://github.com/AIDC-AI/Marco-o1](https://github.com/AIDC-AI/Marco-o1)

## OmniSearch

[阿里多模态检索智能体，自带o1式思考过程！复杂问题逐步拆解，动态调整下一步检索策略](https://mp.weixin.qq.com/s/IU1SokQC5RwRNL2sr7qQLg)

[Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937)

[https://github.com/Alibaba-NLP/OmniSearch](https://github.com/Alibaba-NLP/OmniSearch)

## Coconut

[田渊栋团队论文火了！连续思维链优于CoT，打开LLM推理新范式](https://mp.weixin.qq.com/s/cqh3pCLMFJgSVpF0nNgt6w)

(toread)

[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/pdf/2412.06769)

一般而言，LLM 被限制在语言空间（language space）内进行推理，并通过思维链（CoT）来表达推理过程，从而解决复杂的推理问题。然而，语言空间可能并不总是最适合推理的。例如，很多单词token主要用于文本连贯性，而不是推理本身，而一些关键token则需要复杂的规划。

Coconut（连续思维链，Chain of Continuous Thought）不再通过语言模型头（language model head）和嵌入层将隐藏状态与语言 token 进行映射，而是直接将最后的隐藏状态（即连续思维）作为下一个token的输入嵌入。 

## O1的安全机制

[OpenAI发布49页长文，讲述o1的安全机制](https://mp.weixin.qq.com/s/-zPVmr6_dA35j-YZAIqGJg)

[OpenAI o1 System Card](https://cdn.openai.com/o1-system-card-20241205.pdf)

## O1架构

[「七万字长文」从认知架构到实践部署：o1与o1 Pro的系统性分析与内涵洞察 · 上篇](https://mp.weixin.qq.com/s/ioqtgG2nOr3fT5sNqLNZdg)

## 小模型的O1

(toread)

[让7B千问模型超越o1，微软rStar-Math惊艳登场，网友盛赞](https://mp.weixin.qq.com/s/d2aDtT9KRyZJ9Ac12v8AHA)

[rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/pdf/2501.04519)

[https://github.com/microsoft/rStar](https://github.com/microsoft/rStar)

## meta-cot

[迈向System 2推理，100页论文硬核讲述Meta-CoT](https://mp.weixin.qq.com/s/L_tErITBzUZ75GVGtbtdDQ)

[Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought](https://arxiv.org/pdf/2501.04682)

## sky-T1

(toread)

[450美元训练一个「o1-preview」？UC伯克利开源32B推理模型Sky-T1，AI社区沸腾了](https://mp.weixin.qq.com/s/aRUHeDheE4nwncbCLakgIQ)

[https://novasky-ai.github.io/posts/sky-t1/](https://novasky-ai.github.io/posts/sky-t1/)

[https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview](https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview)

## Monkey

[o3并非独门秘技，谷歌已发背后关键机制，方法更简单、成本更低](https://mp.weixin.qq.com/s/qdxC_QyJW17gyRfN66D59A)

[Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/abs/2407.21787)

## 其他的一些讨论

[OpenAI o1模型超全指南来了！](https://mp.weixin.qq.com/s/Wp8y-2q_05UFIMGyyY6DQQ) (使用技巧)

[耗资1.3万，ASU团队揭秘o1推理王者！碾压所有LLM成本超高，关键还会PUA](https://mp.weixin.qq.com/s/pSNC6tdhXcxqB9ofQKU3JQ)

[LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench](https://arxiv.org/abs/2409.13373)

[大模型是否有推理能力？DeepMind数月前的论文让AI社区吵起来了](https://mp.weixin.qq.com/s/NdRBGFT6systLwn7p2ER7Q)

[Amortized Planning with Large-Scale Transformers: A Case Study on Chess](https://arxiv.org/pdf/2402.04494)

[联手OpenAI，吴恩达推出一门o1推理新课程，还免费](https://mp.weixin.qq.com/s/J4uQWL_zccyW7PKsb8eSlw)

[https://www.deeplearning.ai/short-courses/reasoning-with-o1/](https://www.deeplearning.ai/short-courses/reasoning-with-o1/)

[4o-mini只有8B，o1也才300B！微软论文意外曝光GPT核心机密](https://mp.weixin.qq.com/s/bT_w-T9ElmPUXbYA1f7kCg)

+ o1-preview约300B；o1-mini约100B
+ GPT-4o约200B；GPT-4o-mini约8B
+ Claude 3.5 Sonnet 2024-10-22版本约175B


# DeepSeek R1

[DeepSeek-R1 发布，性能对标 OpenAI o1 正式版](https://mp.weixin.qq.com/s/atKyfC5l-BaStje8-F3FGQ)

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)，自己转存了一份：[pdf](https://github.com/daiwk/collections/blob/master/assets/DeepSeek_R1.pdf)

arxiv上：[https://arxiv.org/pdf/2501.12948](https://arxiv.org/pdf/2501.12948)

[https://huggingface.co/deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)，还有不少distill的，LM-studio已经有了。。[https://hf-mirror.com/lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF](https://hf-mirror.com/lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF)

[Jay Alammar：图解DeepSeek-R1](https://mp.weixin.qq.com/s/N9N7R-r_l6sBL2fsRXRGAw)

[https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1)

[https://levelup.gitconnected.com/drawing-deepseek-r1-architecture-and-training-process-from-scratch-72043da33955](https://levelup.gitconnected.com/drawing-deepseek-r1-architecture-and-training-process-from-scratch-72043da33955)

## 背景

o1提出了可以通过增加cot reasoning process的长度来进行inference time scaling，尝试复现的方法有：

+ process-based reward models：
  + [Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)
  + [Solving math word problems with process-and outcome-based feedback](https://arxiv.org/pdf/2211.14275)
  + [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/pdf/2312.08935)
+ reinforcement learning：
  + [Training language models to self-correct via reinforcement learning](https://arxiv.org/pdf/2409.12917)
+ MCTS/beam search的搜索方法：
  + [Alphazero-like tree-search can guide large language model decoding and training](https://arxiv.org/pdf/2309.17179)
  + [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5)
  + [Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search](https://arxiv.org/pdf/2408.08152)

但这些方法都没有o1效果好，因此R1不借助监督数据，使用纯RL(没有SFT)来完成self-evolution，从而探索LLM在reasoning上的潜能。使用DeepSeekV3作为base model，并使用[Deepseekmath: Pushing the limits of mathematical reasoning in open language models](https://arxiv.org/pdf/2402.03300)的GRPO作为RL框架。

几k个step后，DeepSeek-R1-Zero在一些reasoning的benchmark上取得了不错的效果，但仍然有可读性差、语言混合等问题，因此搞了DeepSeek-R1，包括少量的冷启数据和一个多阶段的训练pipeline。

+ 收集数千的cold-start数据来finetune DeepSeek-V3-Base模型
+ 执行类似DeepSeek-R1-Zero的面向reasoning的RL任务
+ 在RL快收敛的时候，在RL的checkpoint上通过拒绝采样构建新的SFT数据，并和DeepSeek-V3的数据集（只选一些领域，如writing、factual QA和self-cognition,自我认知）进行结合，**重训**DeepSeek-V3-Base
+ 用新数据finetune完后，再经过一个RL的过程，并考虑所有场景的prompts==>最终的模型

还搞了一些蒸馏模型，例如Qwen2.5-32B作为base模型，蒸馏DeepSeek-R1的效果比对它进行RL效果要好，说明大的base model发现的reasoning patterns对提升reasoning能力很关键。此外，发现14B的蒸馏模型比QWQ-32B-preview效果好很多，32B和70B的蒸馏模型效果更好

## DeepSeek-R1-Zero

### RL算法

&nbsp;

GRPO（Group Relative Policy Optimization）：放弃了Critic model，用group scores来替换。即对每个问题$$q$$，从老的策略$$\pi_{\theta_{\text {old }}}$$采样出一个group的输出$$\left\{o_1, o_2, \cdots, o_G\right\}$$，再通过最大化如下目标来优化policy model $$\pi_\theta$$：

XXX
\begin{aligned}
\mathcal{J}_{G R P O}(\theta) & =\mathbb{E}\left[q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\theta_{\text {old }}}(O \mid q)\right] \\
& \frac{1}{G} \sum_{i=1}^G\left(\min \left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)} A_i, \operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_i\right)-\beta \mathbb{D}_{K L}\left(\pi_\theta| | \pi_{r e f}\right)\right),
\end{aligned}
XXX

其中：

+ $$\mathbb{D}_{K L}\left(\pi_\theta \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-\log \frac{\pi_{r e f}\left(o_i \mid q\right)}{\pi_\theta\left(o_i \mid q\right)}-1$$用于限制新策略不要离ref model太远
+ $$\operatorname{clip}\left(\frac{\pi_\theta\left(o_i \mid q\right)}{\pi_{\theta_{o l d}}\left(o_i \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right)$$用于限制新旧策略的概率比例在1附近，即二者不要偏离太远
+ $$\varepsilon$$和$$\beta$$是超参
+ $$A_i$$是advantage，通过每个group的reward计算得来，不像ppo需要借助value model：

XXX
A_i=\frac{r_i-\operatorname{mean}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}{\operatorname{std}\left(\left\{r_1, r_2, \cdots, r_G\right\}\right)}
XXX

整句话的advantage如何分配到token上？参考deepseekmath论文：各token共享

![](../assets/grpo-advantage-token.png)

+ 大基座是出现reasoning涌现能力的关键（qwen 32bb用同样r1-zero的数据训练，没法出现aha moment）
+ GRPO可并行，可加速

对比PPO & GRPO

+ PPO：
  + Reference Model（Ref model）：原始LLM，不训练
  + Policy Model（Actor）：原始LLM，需要训练
    + policy：LLM产出的一条response；
    + action：LLM生成的每一个token
  + Reward Model（RM）：对一条response的打分。在RLHF的RM阶段更新，RL阶段不训练
  + Value Model（Critic）：类似RM，需要训练
  + GAE：从Reward Model对**整个句子**的打分，拆解到**每个token**应该怎么改进（图中的A，advantage），会考虑Value Model、Ref Model和Reward Model，还会有一个KL的约束，确保新的policy model不会离ref model太远
  + Importance Sampling：对比新advantage和原始policy的比值，只对advantage进行小比例更新，保证训练稳定
  + 缺点：
    + 需要Ref Model再算一遍，有计算量&耗时
    + 需要拆解到每个token，又有计算量&耗时
+ GRPO：（Group Relative Policy Optimization）：来自[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)，放弃了Critic model（图中的value model），用group scores来替换。
  + policy model/ref model/reward model都和ppo一样
  + 不需要value model
  + 并行执行更多的policy（即GRPO的group），产出的结果一起打分，减均值除以标准差，把每个policy和标准差的差距平均到每个token上，再用KL散度计算和ref model的差距

原始DeepSeekMath论文中的图：

![](../assets/ppo-grpo-deepseek-math.png)

形象的理解：

![](../assets/ppo-grpo.png)

从头开始实现：[DeepSeek关键RL算法GRPO，有人从头跑通了，贡献完整代码](https://mp.weixin.qq.com/s/e0-tVsaIgNajBTOl117ctg)


### Reward modeling

&nbsp;

用的是rule-based reward，包括如下两部分

+ accuracy rewards：response是否正确，例如对于一个数学问题，需要判断最终答案是否为给定的格式（specified format），并且能基于规则验证结果的正确性；leetcode问题则可以用编译器来生成预先定好的测试用例的结果。
+ format rewards：设计了一个format reward model，来强制模型将其思考过程输出在```<think>```和```</think>```之间。

没有设计neural reward model，因为：

+ 发现在大规模的RL过程中，neural reward model会受reward hacking的影响
+ 重新训练reward model需要额外的训练资源，并且会让训练pipeline过于复杂

### Training template

&nbsp;

```html
A conversation between User and Assistant. The user asks a question, and the Assistant solves it.
The assistant first thinks about the reasoning process in the mind and then provides the user
with the answer. The reasoning process and answer are enclosed within <think> </think> and
<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>
<answer> answer here </answer>. User: prompt. Assistant:
```

训练时使用如上的template，故意将约束限制在这种结构格式上，避免任何特定内容的偏见（例如强制反射推理或促进特定的解决问题的策略），以确保能够在RL过程中准确观察模型的自然进展。

### 效果

&nbsp;

指标含义：

+ pass@1：**首次**生成答案时的成功率
+ cons@64：majority vote (consensus) with 64 samples，模型在给出**64次生成尝试**中，是否能够**多次生成相同的正确答案**。(在[这里](https://openai.com/index/learning-to-reason-with-llms/)提到了，来自文章：[Self-consistency improves chain of thought reasoning in language model](https://arxiv.org/pdf/2203.11171))

![](../assets/r1-zero-performance.png)

+ 数学(美国数学邀请赛)：
  + AIME2024：pass@1上接近o1-0912，cons@64超了，均超越o1-mini
  + MATH-500：pass@1超越o1-mini和o1-0912
+ GPQA-Diamond（phd级别的科学问题，包括化学、物理、生物）：pass@1超越o1-mini，不如o1-0912
+ 代码：
  + LiveCode Bench：不如o1-0912和o1-mini
  + codeforces的rating：不如o1-0912和o1-mini
  
另外，在AIME2024上，随着训练，pass@1能从十几涨到70几。

### Self-evolution Process

&nbsp;

[](../assets/r1-zero-thinking-length.png)

+ 随着训练的进行，生成的response长度也在变长，说明思考时间也变长了，其中的reasoning tokens从数百涨到了数千。
+ 随着测试时间的增加，**涌现出了复杂的行为**，即不由自主地(spontaneously)出现了（这些行为并非预先在代码里设计的）：
  + 反思：重新看并且评估(revist and reevaluate)之前的steps
  + 探索：尝试其他方法来解决问题

### Aha Moment

&nbsp;

![](../assets/r1-zero-aha.png)

在某一个中间版本，模型突然输出上面的aha-moment，可以看到不需要显式地教模型如何解决，只要给予正确的激励（incentives），模型会自己开发出进阶的问题解决策略，这也是RL的power and beauty。

## DeepSeek-R1

2个目标：

+ 用一小部分高质量数据来做冷启能否提升推理效果，或者加速收敛？
+ 如何训练一个user-friendly的模型，既能提供准确和连贯的CoT，又有很强的通用能力？

### 阶段1：Cold Start

&nbsp;

从base model开始RL训练时，为了防止冷启阶段训练的不稳定，构建了少量（数千）的长CoT数据来finetune模型，作为初始的RL actor。试了几种方法

+ few shot prompting，用一个long CoT作为例子
+ 直接在prompt里要求模型生成详细的答案，并加上反思和验证
+ 收集deepseek-r1-zero的输出，并找人类标注员后处理进行修改

相比zero，冷启数据有如下好处：

+ 可读性：zero的可读性不太好，例如语言混合、缺少markdown高亮。因此设计了一个可读性比较好的pattern，即```|special_token|<reasoning_process>|special_token|<summary>```，并把没有summary的结果扔掉
+ 潜力：依据人类先验精心设计的pattern产出的冷启数据，取得了比zero更好的效果，故迭代地训练对于reasoning models应该是一种更好的方式

### 阶段2：Reasoning-oriented Reinforcement Learning

&nbsp;

在冷启数据上tune完之后，和r1-zero一样，进行大规模的RL训练（在reasoning-intensive任务的数据集上训的？）。此外，为了解决CoT里语言混合的问题，加了一个**language consistency reward**（计算目标语言在CoT中的比例），直接和原来的reward相加。虽然消融实验显示这种alignment会让效果略微下降，但这更符合人类偏好，可读性更强。

### 阶段3：Rejection Sampling and Supervised Fine-Tuning

&nbsp;

用阶段2的ckpt，然后在writing, role-playing, and other general-purpose这类任务上构建SFT数据集，在约80w的样本上对DeepSeek-V3-Base SFT了2个epoch。

+ reasoning data：在前一个RL阶段，只用那些能用rule-based rewards来衡量的数据。
  + 这个阶段加入了更多的数据，有一些是使用**生成式的reward model**，即把ground truth和模型预测结果输入给DeepSeek-V3做judgement。
  + 因为模型输出有时会很混乱或者无可读性，所以删掉一些满足这些条件的CoT：混合语言、长的段落、代码段
  + 对每个prompt，采样多个response，并只保留一个正确的。最终收集了大概60w的reasoning相关的训练样本
+ non-reasoning data：
  + 采用DeepSeek-V3的pipeline，并复用部分DeepSeek-V3的SFT数据集。
  + 使用DeepSeek-V3时，在prompt里提到在回答答案前要生成潜在的CoT；对于简单的如hello的问题，就不用输出CoT。
  + 最终收集了大概20w的non-reasoning训练样本


### 阶段4：Reinforcement Learning for all Scenarios

&nbsp;

将reward信号和多样化的数据分布相结合，提升helpfulness and harmlessness，同时refine reasoning能力，

+ reasoning数据：用R1-Zero的RL，使用rule-based reward
+ 通用数据：用V3的pipeline，还是用reward model，用以学习复杂和细分领域的人类偏好

另外，提升如下两方面的能力：

+ helpfulness：仅关注最终的summary，保证评估强调的是response的有用性和相关性，而尽量少地干涉潜在的reasoning过程
+ harmlessness：同时评估reasoning过程和summary，尽量缓解在生成的过程中出现的风险、偏见和有害内容

### 效果

+ 大部分都接近或者超过新版o1（r1-zero只能和早期o1相当），很多任务都比v3要好
+ 代码和o1差不太多，比v3提升很多，其中SWE  Verified这个工程类的不太行，因为目前训练语料里这类数据还比较少，下一版会优化
+ c-simpleQA（最后一行）上效果不如V3，因为有R1有safety的限制，导致模型会拒绝回答一些问题，如果去掉这个限制，准确率能达到70%
+ 在AlpacaEval2.0（写作）和ArenaHard(开放领域问答)上效果很好，产出的summary也很长，说明可能reasoning能力能产出长cot，对这类任务也有用
+ 数学上超越o1，大幅领先v3


## 蒸馏

基于Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B、Llama-3.3-70B-Instruct蒸馏，用的数据集是deepseek-R1的第二阶段RL使用的80w数据，只进行了SFT，没有RL。理论上RL应该会效果更好，留给其他研究者去搞了。

[https://medium.com/@prabhudev.guntur/how-to-distill-deepseek-r1-a-comprehensive-guide-c8ba04e2c28c](https://medium.com/@prabhudev.guntur/how-to-distill-deepseek-r1-a-comprehensive-guide-c8ba04e2c28c)可以参考这个自己蒸馏一个

### 效果

+ 仅sft蒸馏的效果：模型越大效果越好，32b和70b的已经在大部分任务上超越o1-mini了
+ 直接拿qwen-32b做RL（第二行）：和QwQ-32b-preview（第一行）差不太多，但明显不如sft蒸馏（第3行）的模型效果

最终结论：

+ 把大模型蒸馏到小模型可以有很好的效果，但对小模型做RL性价比不高
+ 有更强大的模型和更大规模的RL，才能更好地发挥蒸馏的作用，让小模型变得更强


## 失败的尝试

### PRM

&nbsp;

PRM（Process-supervised Reward Model）是 OpenAI 在[Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)中首次提出的概念。与之相对应的是ORM（Outcome-supervised Reward Model）。区别：

+ PRM：过程奖励模型，在生成过程中，对每一个**步骤**打分，是更细粒度的奖励模型。 
+ ORM：结果奖励模型，不管推理有多少步，对**完整的生成结果**进行**一次**打分，是一个反馈更稀疏的奖励模型。 

PRM可以在两个阶段生效：

+ Post-Training阶段：在RL过程中增加PRM，对采样的结果**按步骤**输出奖励值，为模型提供更精细的监督信号，来指导策略模型优化，提升模型按步推理的能力。 
+ Inference阶段：对generator模型做N次采样（如Beam Search等），并通过PRM对每个采样的每步推理进行打分，最终拟合一个整体过程打分，并选取打分最高的结果作为最终的答案。

问题：

+ 难以定义细粒度的步骤
+ 评估中间步骤是否正确很困难，用LLM自动评估结果不太行，手动标注又难以规模化应用
+ 基于模型的PRM就会有reward hacking问题， 重新训练奖励模型需要额外的训练资源

### MCTS

&nbsp;

MCTS（Monte Carlo Tree Search）是强化学习领域，详见AlphaGo论文[Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270)

具体操作步骤：使用已有的策略与环境做仿真交互，进行多次 rollout 采样，最终构成了一个从当前节点出发的一颗 Tree（每个 rollout 表示从当前节点到最终结束状态的多次与环境仿真交互的过程）。

这棵树的所有叶子节点都是结束状态，结束状态是可以量化收益的（比如方法1：答案错误收益-1，答案正确收益+3；再比如方法2：叶子节点的收益是到达叶子节点路径数/总路径数的概率，这是一种根据投票机制预估的价值，越多路径到达叶子节点，说明这个叶子节点越置信，那么这个叶子节点就有更高的奖励）。

一棵树的叶子节点有了奖励值，就可通过反向传播，计算每个中间节点的奖励值，最终计算出整个树所有节点的奖励值。MCTS一次rollout包括四个步骤：

+ select
+ expand
+ simulate
+ backprop 

MCTS也可以用在两阶段：

+ Post-Traing阶段：对于**每个problem构造一个搜索树**，然后进行树的游走遍历采样，再用采样的样本SFT或RL训练模型。
+ Inference阶段：在推理阶段，也是对一个problem探索多节点构造一棵搜索树，对于到达正确答案的路径，根据节点路径的置信度打分，**贪心选取最优路径**作为最终的推理结果。 

问题：
+ 搜索空间巨大，虽然设置最大扩展限制使得不会无限搜索，但是容易陷入局部最优
+ value model直接影响生成的质量，而训练一个细粒度的value model本质上是困难的

## 小结

![](../assets/deepseek-r1-flow.png)

对应的流程图[文档](https://s6alnvgc88.feishu.cn/wiki/JOutwfyGwiEAflkKImvcfpwgnQd)

## 使用注意

### deepseek-r1使用指南

[刚刚，DeepSeek官方发布R1模型推荐设置，这才是正确用法](https://mp.weixin.qq.com/s/RA1mhAyQOoXD5XOULAGgbQ)

+ temperature设置在0.5-0.7之间，推荐0.6，以防止无休止的重复或不连贯的输出。
+ 不要system prompt，直接放到user prompt里
+ 数学问题，建议prompt里直接```Please reason step by step, and put your final answer within \boxed{}.```
+ 有些时候模型会自己跳过think的过程，如果要强制输出，可以强制模型在output前输出```<think>\n```

上传文件的prompt：

```shell
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```

联网的prompt：

```shell
search_answer_zh_template = \
'''# 以下内容是基于用户发送的消息的搜索结果:
{search_results}
在我给你的搜索结果中，每个结果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的数字索引。请在适当的情况下在句子末尾引用上下文。请按照引用编号[citation:X]的格式在答案中对应部分引用上下文。如果一句话源自多个上下文，请列出所有相关的引用编号，例如[citation:3][citation:5]，切记不要将引用集中在最后返回引用编号，而是在答案对应部分列出。
在回答时，请注意以下几点：
- 今天是{cur_date}。
- 并非搜索结果的所有内容都与用户的问题密切相关，你需要结合问题，对搜索结果进行甄别、筛选。
- 对于列举类的问题（如列举所有航班信息），尽量将答案控制在10个要点以内，并告诉用户可以查看搜索来源、获得完整信息。优先提供信息完整、最相关的列举项；如非必要，不要主动告诉用户搜索结果未提供的内容。
- 对于创作类的问题（如写论文），请务必在正文的段落中引用对应的参考编号，例如[citation:3][citation:5]，不能只在文章末尾引用。你需要解读并概括用户的题目要求，选择合适的格式，充分利用搜索结果并抽取重要信息，生成符合用户要求、极具思想深度、富有创造力与专业性的答案。你的创作篇幅需要尽可能延长，对于每一个要点的论述要推测用户的意图，给出尽可能多角度的回答要点，且务必信息量大、论述详尽。
- 如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。
- 对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。
- 你需要根据用户要求和回答内容选择合适、美观的回答格式，确保可读性强。
- 你的回答应该综合多个相关网页来回答，不能重复引用一个网页。
- 除非用户要求，否则你回答的语言需要和用户提问的语言保持一致。

# 用户消息为：
{question}'''
```

### openai推理使用指南

[刚刚，DeepSeek揭秘R1官方同款部署设置，温度=0.6！OpenAI推理指南同时上线](https://mp.weixin.qq.com/s/M7DOriZntI5RjxCliUVyug)

[https://platform.openai.com/docs/guides/reasoning-best-practices](https://platform.openai.com/docs/guides/reasoning-best-practices)

简单理解：

+ 推理模型就像一位经验丰富的高级同事——你只需告诉他们最终目标，就能相信他们自主完成所有细节工作。
+ GPT模型则更像一位新手同事——你需要提供明确详细的指示，才能让他们准确完成特定的输出任务。

各自特点：

+ 速度和成本：选择GPT模型，因为它们处理速度更快，成本更低
+ 执行明确任务：选择GPT模型，它们在处理界定清晰的任务时表现出色
+ 准确性和可靠性：选择o系列模型，它们是可靠的决策专家
+ 复杂问题解决：选择o系列模型，它们善于处理模糊和复杂的问题

推理模型的优势：

+ 处理模糊任务：推理模型特别擅长处理信息有限或零散的情况，只需通过简单的提示词就能理解用户意图并妥善处理指令中的信息缺口。
值得注意的是，推理模型通常会在做出未经验证的猜测或填补信息空缺之前，主动提出澄清性问题。
+ 大海捞针：当需要处理大量非结构化信息时，推理模型特别擅长理解内容并精准提取出回答问题所需的关键信息。
+ 在大型数据集中发现关系和细微差别：
  + 推理模型特别擅长分析包含数百页密集、非结构化信息的复杂文档，如法律合同、财务报表和保险索赔等。这些模型在识别文档之间的关联性，并基于数据中隐含的事实做出决策方面，表现尤为突出。
  + 推理模型还特别擅长理解细微的政策和规则，并将其准确应用于具体任务中以得出合理结论。
+ 多步骤AI智能体规划：推理模型在AI智能体规划和策略制定中发挥着关键作用。将推理模型作为「计划者」时效果显著：它能为问题制定详细的多步骤解决方案，并根据具体需求（高智能或低延迟）选择和分配合适的GPT模型（执行者）来完成各个步骤。
+ 视觉推理能力：截至目前，o1是唯一一个具备视觉处理能力的推理模型。与GPT-4o相比，o1的独特优势在于它能够准确理解最具挑战性的视觉内容，包括结构不规则的图表和表格，以及质量欠佳的图片。
+ 代码审查、调试和质量改进：推理模型在审查和改进大规模代码方面表现突出。考虑到这类模型的较高延迟特性，通常将代码审查任务安排在后台运行。虽然GPT-4o和GPT-4o mini凭借较低的延迟可能更适合直接编写代码，但在那些对延迟要求相对不那么严格的代码生成场景中，o3-mini表现同样出色。
+ 评估和基准测试其他模型的响应：推理模型在对其他模型的输出进行基准测试和评估方面表现优异。数据验证对确保数据集的质量和可靠性至关重要，这一点在医疗保健等敏感领域尤其重要。

prompt注意点：

+ 用开发者消息取代系统消息：自o1-2024-12-17版本起，推理模型开始支持开发者消息（developer message）而非系统消息（system message）。
+ 保持提示词简洁明确：推理模型最擅长理解和响应简短、清晰的指令。
+ 避免使用CoT提示：由于模型内置推理能力，因此无需特别提示它们「一步一步思考」或「解释推理过程」。
+ 善用分隔符增强清晰度：使用Markdown、XML标签和章节标题等分隔符来明确区分输入的不同部分，这有助于模型准确理解各个章节的内容。
+ 优先尝试零样本学习：推理模型通常无需少样本示例即可产出优质结果，因此建议先尝试不含示例的提示词。如果对输出结果有更复杂的要求，再考虑在提示词中添加输入和期望输出的示例。请注意确保示例与提示词指令严格匹配，因为不一致可能导致性能下降。
+ 提供明确约束条件：如果需要对模型的响应施加具体限制（例如「提供预算控制在500美元以内的解决方案」），请在提示词中明确列出这些约束条件。
+ 明确定义目标：在指令中，请详细说明判定响应成功的具体参数，并引导模型持续优化推理过程，直到达成设定的成功标准。
+ Markdown格式说明：从o1-2024-12-17版本开始，API中的推理模型默认不会生成带有Markdown格式的响应。如果确实需要在响应中包含Markdown格式，请在开发者消息的首行添加「Formatting re-enabled」字符串。


# R1的部署

[完整的671B MoE DeepSeek R1怎么塞进本地化部署？详尽教程大放送！](https://mp.weixin.qq.com/s/GnHzsgvW90DGChENqTBsRw)

[https://github.com/kvcache-ai/ktransformers](https://github.com/kvcache-ai/ktransformers)

# R1的讨论

[华人研究团队揭秘：DeepSeek-R1-Zero或许并不存在「顿悟时刻」](https://mp.weixin.qq.com/s/_VK7fm8p3mpfhPh_zBdagA)

[为什么说DeepSeek的R1-Zero比R1更值得关注？](https://mp.weixin.qq.com/s/yhZR4PainDLR5-gWKiiDjg)

[LeCun痛批硅谷傲慢病！圈内爆火长文：DeepSeek R1-Zero比R1更重要，成AGI破局关键](https://mp.weixin.qq.com/s/mSAATPeW_FM9xadf1veLDA)

[DeepSeek用的GRPO占用大量内存？有人给出了些破解方法](https://mp.weixin.qq.com/s/28GRpZwqv4gMnrmItMQchQ)，对应：[https://github.com/huggingface/trl/issues/2709](https://github.com/huggingface/trl/issues/2709)

[刘知远硬核解读 DeepSeek：大模型强化学习技术原理与大模型技术发展研判](https://mp.weixin.qq.com/s/pAn87hbT3GkfMvbcdzEGCw)

[大神卡帕西拿DeepSeek R1讲强化学习！最新大模型内部机制视频爆火，“没有技术背景也能看懂”](https://mp.weixin.qq.com/s/lBc0-8ByRxJ3JBJpMcfzkQ)

对应代码：[https://github.com/EurekaLabsAI](https://github.com/EurekaLabsAI)

[陈巍：DeepSeek是否有国运级的创新？2万字解读与硬核分析DeepSeek V3/R1的架构](https://mp.weixin.qq.com/s/-yAjdqEn62Sz0bnzp9NEKA)

[Sebastian Raschka：关于DeepSeek R1和推理模型，我有几点看法](https://mp.weixin.qq.com/s/LT22OjbJWKDzTuQeO4yvlg)

[4500美元复刻DeepSeek神话，1.5B战胜o1-preview只用RL！训练细节全公开](https://mp.weixin.qq.com/s/82i7njl4j7nOoLm-igNHtw)

[从想太多到想不透？DeepSeek-R1等长推理模型也存在「思考不足」问题](https://mp.weixin.qq.com/s/LbyHxCbtyYsOelwu6bzs2w)

[r1技术分享](https://www.xiaohongshu.com/explore/67b061460000000029030e00?app_platform=ios&app_version=8.69.4&share_from_user_hidden=true&xsec_source=app_share&type=video&xsec_token=CBH6gEmgPsHp8CWDHlT5DUV9ifTEHPhC9nbQ8RonxCok0=&author_share=1&xhsshare=WeixinSession&shareRedId=ODY2NUg4NE82NzUyOTgwNjY0OTc1STdO&apptime=1739938712&share_id=4d27eb3ad43e4f7b9d9a7b12eb6526f2)

一些观点：

+ 打破LLM训练从Generalist到Reasoner的常规思路，使用RL先得到Reasoner，再经过SFT成为Generalist;
+ GRPO算法的高效性，让RL大规模训练取得了效果;
+ SFT with CoT数据的训练结果在原文中并未给出，但从最近一些复现工作和R1 distiled Qwen结果来看，SFT的作用可能比RL还大。
+ 好的Base模型本身就有Aha Moment，但是是Superfacial（表面）的reflection，不具有提升Accuracy的能力，RL可以增强该能力。

r1的解读（很长，142页）

[DeepSeek-R1 Thoughtology: Let’s <think> about LLM reasoning](https://arxiv.org/pdf/2504.07128)

模型思考过程分四步走：

+ 问题定义：先把问题说清楚
+ 开花期：拆解问题，给出初步方案
+ 重构期：反复验证和修正想法
+ 最终决策：确认并输出答案
	
有趣的发现：

+ 思考链长度有"最佳区间"，太长反而会答错
+ 模型会"死磕"用户给的信息，即使信息是错的也坚持跟着走
+ 中英文环境下性格迥异：中文更重视集体，英文更个人主义
+ 还会画ASCII艺术，虽然不太完美~
	
缺陷：

+ 上下文太长容易"走神"
+ 有时会生成有害内容
+ 在某些视觉任务上还不够连贯


# R1前后的一些工作

## S1

[s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393)

[https://github.com/simplescaling/s1](https://github.com/simplescaling/s1)

[训练1000样本就能超越o1，李飞飞等人画出AI扩展新曲线](https://mp.weixin.qq.com/s/ax_CCrqpgrp5j2mLOssY4w)

（从xhs上抄的）有几点思维误区：

- s1超过的是o1-preview，没有超过o1甚至o1-mini，有很大区别
- s1的效果不敌deepseek-r1 800k数据蒸馏的32B模型，差了不少，不是媲美
- s1即使使用全量59k数据的效果也没有提高很多，甚至在math上还有下降，所以核心是数据质量
- 1k数据是从59K数据中筛选出来的，不是直接有1K数据就可以
- s1使用1k数据是节省了训练时间，但蒸馏的难点在蒸馏数据的构造上

## LIMO

[817样本激发7倍推理性能：上交大「少即是多」定律挑战RL Scaling范式](https://mp.weixin.qq.com/s/c62TWyepruRYf_1xHFKw4g)

[LIMO: Less is More for Reasoning](https://arxiv.org/pdf/2502.03387)

[https://github.com/GAIR-NLP/LIMO](https://github.com/GAIR-NLP/LIMO)

## PRIME

[挑战DeepSeek-R1-Zero！PRIME：仅10%数据刷新数学推理SOTA，隐式奖励技术颠覆RL训练](https://mp.weixin.qq.com/s/JXGCZxcluvc4YkBVq2mXSA)

[Process Reinforcement through Implicit Rewards](https://arxiv.org/pdf/2502.01456v1)

## TPO

[推理时也能做偏好优化，无需额外重训练，来自上海AI Lab港中文等](https://mp.weixin.qq.com/s/OzAHrVUK57kY9kwVQql4eg)

[Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback](https://arxiv.org/abs/2501.12895)

[https://github.com/yafuly/TPO](https://github.com/yafuly/TPO)

## Huginn

[推理模型新路线开源！与DeepSeek截然不同，抛弃思维链不用人类语言思考](https://mp.weixin.qq.com/s/HK6fjolKDcHG6MD_cVgifg)

[Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)

[https://github.com/seal-rg/recurrent-pretraining](https://github.com/seal-rg/recurrent-pretraining)

抛弃长思维链和人类的语言，直接在连续的高维潜空间用隐藏状态推理，可自适应地花费更多计算来思考更长时间。

## Goedel-Prover

[哥德尔-Prover超过DeepSeek-Prover，金驰、陈丹琦团队造出当前最强形式化推理模型](https://mp.weixin.qq.com/s/IfYQdMyZ7FBEJCXNCTpZtQ)

[Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving](https://arxiv.org/abs/2502.07640v1)

[https://github.com/Goedel-LM/Goedel-Prover](https://github.com/Goedel-LM/Goedel-Prover)


## Kimi K1.5

[追平满血版o1的国产多模态模型终于来了！训练细节全部公开](https://mp.weixin.qq.com/s/FOAcS2jsTwNoZA2t1BJ66Q)

[https://github.com/MoonshotAI/kimi-k1.5](https://github.com/MoonshotAI/kimi-k1.5)

[Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf)，自己转存了一份：[pdf](https://github.com/daiwk/collections/blob/master/assets/Kimi_k1.5.pdf)

## O1 Embedder

[O1 Embedder: Let Retrievers Think Before Action](https://arxiv.org/abs/2502.07555)

生成关于输入查询的thought，然后和question一起拼接，然后分别独自生成嵌入，然后池化聚合。也就是说，这个Embedding模型比之前的模型多了个thought的输出。

微调，两个并行任务，一个是thought生成，一个是对比学习。

微调数据怎么来，那就是生成后进行打分评价。首先，使用LLM生成初始思想，然后使用检索评分器来根据初始思想和目标文档之间的相关性评分，最终通过多数投票选择最佳thought。

## bridge

[From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation](https://arxiv.org/pdf/2502.00330)

问题背景：传统的LLM在处理长文本时效率低下，尤其是在需要进行多步推理的任务中。近年来，随着长上下文LLM的发展，出现了多示例学习（many-shot ICL）的范式，即通过更多的示例来提升模型性能。然而，这种范式的效果和影响因素仍需进一步研究。

研究动机：为了提升多示例学习的效率，分析了影响多示例学习效果的关键因素，并发现**仅少数关键示例**就能显著提升性能。基于这一发现，团队提出了一种新的算法，通过**优化选择关键示例**和**生成新的示例**来进一步提升模型的推理能力。

bridge算法结合了优化和生成两个步骤：

+ 在优化步骤中，使用贝叶斯优化（Bayesian optimization）来发现关键示例；
+ 在生成步骤中，利用这些关键示例作为示范，重新生成更多的示例，从而在多示例学习中提高模型性能。

## BFS-Prover

[超越DeepSeek-ProverV1.5！豆包首个形式化数学推理模型BFS-Prover来了，直接开源](https://mp.weixin.qq.com/s/OneF2OtnYbDxeZhDzXPM4A)

[BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving](https://arxiv.org/abs/2502.03438)

[https://huggingface.co/bytedance-research/BFS-Prover/tree/main](https://huggingface.co/bytedance-research/BFS-Prover/tree/main)

## CoE

[为DeepSeek MoE模型带来「免费午餐」加速，专家链可大幅提升LLM的信息处理能力](https://mp.weixin.qq.com/s/qTD96rcSY1cKNmH8B30V-w)

## 显式CoT

[揭示显式CoT训练机制：思维链如何增强推理泛化能力](https://mp.weixin.qq.com/s/sE2ckVDJPdMPrC9dfcvSKA)

[Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization](https://arxiv.org/abs/2502.04667)

## 认知行为

[为什么Qwen能自我改进推理，Llama却不行？斯坦福找到了原理](https://mp.weixin.qq.com/s/OvS61OrDp6rB-R5ELg48Aw)

[Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/abs/2503.01307)


## DR, GRPO

[揭秘DeepSeek R1-Zero训练方式，GRPO还有极简改进方案](https://mp.weixin.qq.com/s/SBGO_1JXnI9CGcLL8eANBA)

[Understanding R1-Zero-Like Training: A Critical Perspective](https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf)

[https://github.com/sail-sg/understand-r1-zero](https://github.com/sail-sg/understand-r1-zero)

## AReaL

[200美金，人人可手搓QwQ，清华、蚂蚁开源极速RL框架AReaL-boba](https://mp.weixin.qq.com/s/Cx8QHv2TVl-0mIJzKT7BDA)

[https://github.com/inclusionAI/AReaL](https://github.com/inclusionAI/AReaL)

## TAO

[模型调优无需标注数据！将Llama 3.3 70B直接提升到GPT-4o水平](https://mp.weixin.qq.com/s/WfJtfEN1gHg5NvgCYwefsw)

[https://www.databricks.com/blog/tao-using-test-time-compute-train-efficient-llms-without-labeled-data](https://www.databricks.com/blog/tao-using-test-time-compute-train-efficient-llms-without-labeled-data)

TAO 的核心创新在于摒弃了人工标注数据，转而利用测试时计算引导模型探索任务的可能响应，再通过强化学习根据响应评估结果更新模型参数。

该流程通过可扩展的测试时计算（而非昂贵的人工标注）实现质量提升，并能灵活融入领域知识（如定制规则）。令人惊讶的是，在高质量开源模型上应用该方法时，其效果往往优于依赖人工标注的传统方案。

TAO 包含四个核心阶段：

+ 响应生成：该阶段首先收集任务相关的输入提示或查询样本。在Databricks平台上，这些提示可通过AI Gateway自动采集；
+ 响应评分：系统化评估生成响应的阶段。评分方法包含多种策略，例如基于奖励模型、偏好评分，或利用LLM评判器及定制规则进行任务特异性验证，确保每个响应都做到最优；
+ 强化学习（RL）训练：最终阶段采用基于强化学习的方法更新大语言模型，引导模型生成与高分响应高度契合的输出。通过这一自适应学习过程，模型持续优化预测能力以提升质量；
+ 持续改进：TAO仅需LLM输入样本作为数据源。用户与LLM的日常交互自然形成该数据（一旦模型部署使用，即可自动生成下一轮TAO训练数据）。在 Databricks平台上，借助TAO机制，模型会随着使用频次增加而持续进化。

虽然TAO在训练阶段使用了测试时计算，但最终产出的模型在**执行任务时仍保持低推理成本**。这意味着经过TAO调优的模型在推理阶段（与原版模型相比）具有完全相同的计算开销和响应速度，显著优于 o1、o3 和 R1 等依赖测试时计算的模型。实验表明：采用 TAO 训练的高效开源模型，在质量上足以比肩顶尖的商业闭源模型。

TAO为AI模型调优提供了一种突破性方法：

+ 不同于耗时且易出错的提示工程；
+ 也区别于需要昂贵人工标注数据的传统微调；
+ TAO 仅需工程师提供任务相关的典型输入样本，即可实现卓越性能。

## deepcoder

DeepCoder-14B-Preview

[UC伯克利华人开源14B「o3-mini」，代码版R1突袭OpenAI王座！](https://mp.weixin.qq.com/s/VxGofHl_KeuQa9MBD4AaTg)

## Think twice

[Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://arxiv.org/pdf/2503.19855)

多轮思考（Multi-round Thinking）：通过迭代地利用之前的答案作为提示，对模型的推理过程进行逐步优化。允许模型在多个推理轮次中重新考虑之前的答案，每次迭代只保留上一轮的最终答案，摒弃中间推理步骤。这一过程类似于人类的认知模式，有助于模型纠正常见的推理错误。

[https://github.com/a-m-team/a-m-models](https://github.com/a-m-team/a-m-models)

## glm-z1

[智谱深夜开源新一代GLM模型，推理速度快DeepSeek-R1八倍，还启用了全新域名Z.ai](https://mp.weixin.qq.com/s/kPAIeYwIAqyIPf1585jR-w)

[https://github.com/THUDM/GLM-4/blob/main/README_zh.md](https://github.com/THUDM/GLM-4/blob/main/README_zh.md)

[https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e](https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e)

## d1

[扩散LLM推理用上类GRPO强化学习！优于单独SFT，UCLA、Meta新框架d1开源](https://mp.weixin.qq.com/s/57onGdSBuiQfvEJpOdU_eg)

[d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/pdf/2504.12216)

[https://github.com/dllm-reasoning/d1](https://github.com/dllm-reasoning/d1)

## regtool

[ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)

![](../assets/regtool.png)

两阶段训练，先冷启注入pattern，再RL进一步泛化

+ 阶段1：收集纯文本数学推理数据->人工专家和Deepseek-R1双重验证过滤无效数据->直接prompt模型将手动计算步骤替换为代码片段，生成代码增强的推理数据->格式/答案验证->得到的数据集用来SFT冷启模型
+ 阶段2：RL rollout时候当检测到```\code```时停止生成，代码片段在sandbox中执行，并将解释器反馈拼接到原来生成序列中，使用是否做对（+1/-1）作为奖励信号进行PPO训练

## RLVR的局限

[RL真让大模型更会推理？清华新研究：其能力边界或仍被基座「锁死」](https://mp.weixin.qq.com/s/2-GDxs8j1QYh1VnW9iBnXw)

[Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)

RLVR（可验证奖励的强化学习）只是将采样做得更有效率，而其输出的正确答案，早已藏在基座模型的「基因」里。  

## RLFT

[谷歌DeepMind：大模型也很任性，知道最优路径偏要撞南墙](https://mp.weixin.qq.com/s/8wxEyYNYr5L9k0Kb64_O4g)

[LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/pdf/2504.16078)

## CoRT

[强迫模型自我争论，递归思考版CoT热度飙升！网友：这不就是大多数推理模型的套路吗？](https://mp.weixin.qq.com/s/lkQEy395JPnlntV1j6EfTQ)

[https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts](https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts)

## R1-Reward

[RL训练总崩溃？R1-Reward稳定解锁奖励模型Long-Cot推理能力](https://mp.weixin.qq.com/s/PHGC6lQt5mXuieK6-8DiYw)

[R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)

[https://github.com/yfzhang114/r1_reward](https://github.com/yfzhang114/r1_reward)

## CTM

[连续思维机器来了！Transformer八子之一创企推出，让AI不再「一步到位」拍脑袋做决定](https://mp.weixin.qq.com/s/L6Tlpf6xlL6VblTnV0MEfg)

[https://github.com/SakanaAI/continuous-thought-machines/](https://github.com/SakanaAI/continuous-thought-machines/)

[https://sakana.ai/ctm/](https://sakana.ai/ctm/)

[Continuous Thought Machines](https://arxiv.org/abs/2505.05522)

## INTELLECT-2

[全球闲置算力训个模型，性能媲美R1，老黄天塌了！Karpathy曾投资它](https://mp.weixin.qq.com/s/NjQi_KAE18YkIBEqttnUuQ)

## AM-Thinking-v1

[纯蒸馏模型 SOTA 出现！直接 SFT 成本直降 50 倍，数据已全部开源](https://mp.weixin.qq.com/s/OlVQzw_WwziQTi0QnyWuOw)

[Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)

2个数据集：[a-m-team/AM-Thinking-v1-Distilled](https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled)和[a-m-team/AM-Qwen3-Distilled](https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled)

## STILL系列

[https://github.com/RUCAIBox/Slow_Thinking_with_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)

## Embedded

[大模型推理的“左右脑”革命！华为盘古Embedded凭昇腾之力，让快慢思考合二为一](https://mp.weixin.qq.com/s/Eem1OYzEE1sM1-MWqsfcpA)

[Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)

## CoM

[微软等提出「模型链」新范式，与Transformer性能相当，扩展性灵活性更好](https://mp.weixin.qq.com/s/UFqC41KYaE3h6KnbLw5iXQ)

[Chain-of-Model Learning for Language Model](https://arxiv.org/pdf/2505.11820)

+ CoR（Chain-of-Representation，表征链）：任何表征总是可以看作是隐藏维度上多个子表征的组合。每个子表征对应一条链。通过使用不同数量的前导链（preceding chains），其对应的特征可以用来编码不同的知识（称之为 scale）
+ CoL（Chain-of-layer，链式层）：在不同尺度之间引入因果依赖关系，确保每个尺度只能使用其前面尺度的信息。
+ CoLM（语言模型链）：将CoL的思想应用于 Transformer 的每一层，重新构建了语言模型架构
+ CoLM-Air：在注意力模块中进一步引入了键值共享机制，该机制要求所有键和值都在第一个链中进行计算

![](../assets/cor.png)

## ProRL

[英伟达揭示RL Scaling魔力！训练步数翻倍=推理能力质变，小模型突破推理极限](https://mp.weixin.qq.com/s/RmeTW83hjTQYJLpl435o6A)

[ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/pdf/2505.24864)

[https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B](https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B)

将RL训练步数从传统的几百步大幅提升至2000步以上，释放了小模型潜藏的巨大潜力

+ **多样化可验证奖励任务**：引入了数学、编程、科学问答（STEM）、逻辑谜题、指令遵循等多领域数据，这些任务具有程序化可验证的正确答案，为RL训练提供了可靠、客观的监督信号，不再依赖「易被骗」的奖励模型。
+ **GRPO+DAPO**：在GRPO框架基础上，融合DAPO关键的**解耦裁剪**（Decoupled Clipping）来避免策略更新失衡，以及**动态采样**（Dynamic Sampling）来过滤掉「太容易」或「完全不会」的无效样本，提升训练效率。
+ **KL正则+周期性参考策略重置**：
  + 与一些去KL正则的做法相反，本论文发现**适度KL惩罚**是稳定训练的关键。
  + 引入**参考策略重置**机制：当KL骤增或性能下滑时，**重置参考策略为当前模型副本，并重置优化器**，让训练「重启」。这个简单机制有效打破训练停滞，使模型持续进化。

## TALE

[ACL 2025 | 基于Token预算感知的大模型高效推理技术](https://mp.weixin.qq.com/s/ErKi3J41U33C-TzMgHmXPw)

[Token-Budget-Aware LLM Reasoning](https://arxiv.org/pdf/2412.18547)

[https://github.com/GeniusHTX/TALE](https://github.com/GeniusHTX/TALE)


## 小结

[从ReFT, Kimi K1.5到DeepSeek R1，聊聊Reasoning Model的精巧实现](https://mp.weixin.qq.com/s/yaBMe-XOlINOBg6_CL0vEw)

[张俊林：MCST树搜索会是复刻OpenAI O1/O3的有效方法吗](https://mp.weixin.qq.com/s/_Uobawe6yf1eNzScNkySHA)


# R1的复现与部署

## open-r1

[https://github.com/huggingface/open-r1/](https://github.com/huggingface/open-r1/)

[开源22万条DeepSeek R1的高质量数据！你也能复现DeepSeek了](https://mp.weixin.qq.com/s/yIEisGrfguRkpjRnHmNYCg)

[https://huggingface.co/datasets/open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)

## unlock-deepseek

[DeepSeek R1 Zero中文复现教程来了！](https://mp.weixin.qq.com/s/Z7P61IV3n4XYeC0Et_fvwg)

## open-reasoner-zero

[1/30训练步骤复刻DeepSeek-R1-Zero，沈向洋姜大昕张祥雨等开源推理模型RL训练方法](https://mp.weixin.qq.com/s/vkEr6oa1JQ_ntzEYJ115yg)

[https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/)


## vllm跑gguf的r1

[https://github.com/vllm-project/vllm/pull/13167](https://github.com/vllm-project/vllm/pull/13167)

从源码安装参考[https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source)

```shell
export https_proxy=xxxxxx
pip3 install setuptools_scm ## 不知道是不是需要的
git clone https://github.com/vllm-project/vllm.git
cd vllm
VLLM_USE_PRECOMPILED=1 pip3 install --editable .

# 如果网络有问题，可以直接pip3 install .
# 下面这3步还是比较必要的
cp -r /usr/local/lib/python3.10/dist-packages/vllm /usr/local/lib/python3.10/dist-packages/vllm.bk
rm -rf /usr/local/lib/python3.10/dist-packages/vllm
cp -r ./vllm /usr/local/lib/python3.10/dist-packages
```

+ 去[https://huggingface.co/deepseek-ai/DeepSeek-R1/tree/main](https://huggingface.co/deepseek-ai/DeepSeek-R1/tree/main)把各个非model的小文件下载下来（可以直接```GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-R1```）也要下载，
+ 去[https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main)把config.json下载下来，把json里的torch_dtype里的bfloat16改成float16，覆盖掉上面那个目录里的config.json

上面的那些文件都放到```./unsloth_dir```目录下，注意，目前只能用Q2 Q4那种量化，1.58bit那种动态量化不支持

```shell
cd ./unsloth_dir/
# merge成一个gguf
llama.cpp/llama-gguf-split --merge ./DeepSeek-R1-Q2_K/DeepSeek-R1-Q2_K-00001-of-00005.gguf ./unsloth_dir/merge.gguf 
```

代码：

```python
from vllm import LLM, SamplingParams

import multiprocessing

if __name__ == "__main__":
    # 这坨要放main里，deepseek教我的
    multiprocessing.set_start_method('spawn', force=True)
    
    llm = LLM(model="./unsloth_dir/merge.gguf",
              tokenizer="./unsloth_dir/DeepSeek-R1",
              hf_config_path="./unsloth_dir/DeepSeek-R1",
              enforce_eager=True, 
              tensor_parallel_size=8, #
              trust_remote_code=True, 
              distributed_executor_backend="mp",
              max_model_len=2000)
    sampling_params = SamplingParams(temperature=0.5, max_tokens=2000)
    
    def print_outputs(outputs):
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt: {prompt!r}, Generated text\n: {generated_text}")
        print("-" * 80)
    conversation = [
        {
            "role": "user",
            "content": "中国的首都是哪里",
        },
    ]
    outputs = llm.chat(conversation,
                       sampling_params=sampling_params,
                       use_tqdm=False)
    print_outputs(outputs)
```

运行：

```shell
VLLM_MLA_DISABLE=1 VLLM_WORKER_MULTIPROC_METHOD=spawn python3 vllm_deepseek.py 
```



## unsloth

### 跑满血版1.58bit infer

[https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)

### 让小模型有reasoning能力

[https://unsloth.ai/blog/r1-reasoning](https://unsloth.ai/blog/r1-reasoning)


## Logic-RL与reinforce-lite

[10美元成功复现DeepSeek顿悟时刻，3B模型爆发超强推理！微软论文反驳涌现](https://mp.weixin.qq.com/s/xH86_m71lZnSVMM7q_jzXw)

### reinforce-lite

[https://medium.com/@rjusnba/overnight-end-to-end-rl-training-a-3b-model-on-a-grade-school-math-dataset-leads-to-reasoning-df61410c04c6](https://medium.com/@rjusnba/overnight-end-to-end-rl-training-a-3b-model-on-a-grade-school-math-dataset-leads-to-reasoning-df61410c04c6)

### Logic-RL

[Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning](https://arxiv.org/pdf/2502.14768)

## SGLang对R1的加速

[全球首个，最接近原版DeepSeek开源复现来了！R1四个月狂飙26倍](https://mp.weixin.qq.com/s/O8_qnIsxVoc98y3RVR9sYw)

[https://lmsys.org/blog/2025-05-05-large-scale-ep/](https://lmsys.org/blog/2025-05-05-large-scale-ep/)

[https://docs.sglang.ai/references/deepseek.html](https://docs.sglang.ai/references/deepseek.html)

# R1的微调

## ColossalAI

[DeepSeek V3+R1满血微调工具上线！一键启动，硬件要求降10倍](https://mp.weixin.qq.com/s/ywJAbcjXPef1RazHj1HIjg)

[https://github.com/hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI)

### lora sft满血deepseek V3/R1

+ 数据：[https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/lora_sft_data.jsonl)
+ fp8权重转bf16：[https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/fp8_cast_bf16.py](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/fp8_cast_bf16.py)
+ 脚本：[https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/lora_finetune.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/lora_finetune.py)

通过使用 LoRA 等优化，示例命令已将 SFT DeepSeek V3/R1 671B 最低硬件要求降低近 10 倍，可使用 32 个 Ascend 910B NPU 64GB（使用 ep=8,pp=4）或 24 个 H100/H800 GPU（使用 ep=8,pp=3）。如果你通过 --zero_cpu_offload 启用 CPU offload，硬件要求可以进一步降低，但会损失一定的训练速度。

```shell
colossalai run --hostfile path-to-host-file --nprocpernode 8 \
  lorafinetune.py --pretrained path-to-DeepSeek-R1-bf16 \
  --dataset path-to-dataset.jsonl --plugin moe \
  --lr 2e-5 --maxlength 256 -g --ep 8 --pp 3 \
  --batchsize 24 --lorarank 8 --loraalpha 16 \
  --numepochs 2 --warmupsteps 8 \
  --tensorboarddir logs --save_dir DeepSeek-R1-bf16-lora
```

也可以使用上述脚本，将并行度高效扩展至数百及数千卡，快速完成 DeepSeek V3/R1 671B 全参微调或并行加速。

### 用强化学习微调蒸馏版DeepSeek

奖励设计：

1. 奖励 = 0，如果格式是错误的；
2. 奖励 = 1， 如果格式是正确的但是结果是错误的；
3. 奖励 = 10，如果格式与结果都是正确的。

+ 模板和设定：[https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/conversation_template/Qwen_Qwen2.5-3B.json](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/conversation_template/Qwen_Qwen2.5-3B.json)
+ 启动脚本：[https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/train_grpo.sh](https://github.com/hpcaitech/ColossalAI/blob/main/applications/ColossalChat/examples/training_scripts/train_grpo.sh)


# OpenAI

## O3

[OpenAI：强化学习确实可显著提高LLM性能，DeepSeek R1、Kimi k1.5发现o1的秘密](https://mp.weixin.qq.com/s/jh5L5MV6jU8W6coFH7jsAg)

[Competitive Programming with Large Reasoning Models](https://arxiv.org/pdf/2502.06807)

[刚刚，OpenAI放出最后大惊喜o3，高计算模式每任务花费数千美元](https://mp.weixin.qq.com/s/KmxARRFsjXiLTF8HScsmOQ)

## O3与隐私

[一张照片、一句简单提示词，就被ChatGPT人肉开盒，深度解析o3隐私漏洞](https://mp.weixin.qq.com/s/_pDsKWz9f9rjXFHWN2UJ_A)

[Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model](https://arxiv.org/abs/2504.19373)

# Claude

## Claude 3.7 Sonnet

[全球首个混合推理模型：Claude 3.7 Sonnet来袭，真实编码力压一切对手](https://mp.weixin.qq.com/s/RzdrxKbHKKqtN-FvbOQsZw)

+ 既是普通的 LLM，又是推理模型。你可以选择何时希望模型正常回答，何时希望它在回答之前思考更长时间。在标准模式下，Claude 3.7 Sonnet 是前代 Claude 3.5 Sonnet 的升级版。在扩展思维模式下，它会在回答之前进行自我反思，从而提高其在数学、物理、指令遵循、编码和许多其他任务上的表现。Anthropic 发现，两种模式下，模型的提示词工作方式类似。
+ 当通过 API 使用 Claude 3.7 Sonnet 时，用户还可以控制思考预算。你可以告诉 Claude 思考不超过 N 个 token。对于任何 N 值，其输出限制为 128K 个 token。这允许用户在速度（和成本）和答案质量之间进行权衡。
+ 第三，在开发自家的推理模型时，Anthropic 对数学和计算机科学竞赛问题的优化较少，而是将重点转向更能反映企业实际使用 LLM 方式的现实任务。

[Claude 3.7 Sonnet System Card](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf)

# seed-thinking

[200B参数击败满血DeepSeek-R1，字节豆包推理模型Seed-Thinking-v1.5要来了](https://mp.weixin.qq.com/s/wUzb58pUnZ1s7fO9aYs1vQ)

[Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf)

## DAPO

[超越DeepSeek GRPO的关键RL算法，字节、清华AIR开源DAPO](https://mp.weixin.qq.com/s/_w_HtjNQiG-yP5LEN85o0Q)

[DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://dapo-sia.github.io/static/pdf/dapo_paper.pdf)

[https://github.com/volcengine/verl/tree/gm-tyx/puffin/main/recipe/dapo](https://github.com/volcengine/verl/tree/gm-tyx/puffin/main/recipe/dapo)

数据：[https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k)

## VAPO

[字节新作 VAPO：使用基于价值的强化学习框架进行长思维链推理](https://mp.weixin.qq.com/s/oFyW439e8B1rkogu5rjH_g)

[VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks](https://arxiv.org/abs/2504.05118)

# 多模态推理

## 综述

[Rule-based强化学习≠古早逻辑规则！万字拆解o1多模态推理最新进展](https://mp.weixin.qq.com/s/8pwCPuXzXoMJsDmdL9tPGA)

[Aligning Multimodal LLM with Human Preference: A Survey](https://arxiv.org/abs/2503.14504)



## 视觉推理

[视觉强化微调！DeepSeek R1技术成功迁移到多模态领域，全面开源](http://mp.weixin.qq.com/s/VCSUQXV7yv9MdIWQlxh7dQ)

[Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785)

[https://github.com/Liuziyu77/Visual-RFT](https://github.com/Liuziyu77/Visual-RFT)

## R1V

[全球首个工业界多模态推理模型开源！38B硬刚DeepSeek-R1，训练秘籍全公开](https://mp.weixin.qq.com/s/CYNce3oHoDrsOJSv5Pj6Nw)

[https://github.com/SkyworkAI/Skywork-R1V](https://github.com/SkyworkAI/Skywork-R1V)

[Skywork R1V: Pioneering Multimodal Reasoning withChain-of-Thought](https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf)


## Video-T1

[视频生成的测试时Scaling时刻！清华开源Video-T1，无需重新训练让性能飙升](https://mp.weixin.qq.com/s/HtJHXGgTAhi-uBWSsgqOKQ)

[Video-T1: Test-Time Scaling for Video Generation](https://arxiv.org/pdf/2503.18942)

[https://github.com/liuff19/Video-T1](https://github.com/liuff19/Video-T1)

## mureka o1(音乐)

[音乐界迎来自己的DeepSeek！全球首个音乐推理大模型Mureka O1上线，超越Suno](https://mp.weixin.qq.com/s/XF9HQr-qlXfWGaQlesCaHQ)

[MusiCoT: Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation](https://musicot.github.io/MusiCoT_paper.pdf)

[https://musicot.github.io/](https://musicot.github.io/)

## visual planning

[只用图像也能思考，强化学习造就推理模型新范式！复杂场景规划能力Max](https://mp.weixin.qq.com/s/KXx1t3jIlhLWu0rlVoQWNA)

[Visual Planning: Let’s Think Only with Images](https://arxiv.org/pdf/2505.11409)

[https://github.com/yix8/VisualPlanning](https://github.com/yix8/VisualPlanning)

## DeepEyes

[OpenAI未公开的o3「用图思考」技术，被小红书、西安交大尝试实现了](https://mp.weixin.qq.com/s/yyfeEmxmIaA7Qu5raGVHyA)

[DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning](https://arxiv.org/abs/2505.14362)

[https://github.com/Visual-Agent/DeepEyes](https://github.com/Visual-Agent/DeepEyes)

# deep research

## local-deep-research

[本地也能运行Deep Research！支持arXiv平台，兼容PDF、Markdown等](https://mp.weixin.qq.com/s/U5lXj0lhMR6x_Wy3GHpNfQ)

[https://github.com/LearningCircuit/local-deep-research](https://github.com/LearningCircuit/local-deep-research)

## CycleResearcher

[ICLR 2025 \| 真正「Deep」的「Research」，通过强化学习实现可自主进化的科研智能体来了！](https://mp.weixin.qq.com/s/-n3bo-mNklCIVFZV6splmw)

[CycleResearcher: Improving Automated Research via Automated Review](https://openreview.net/forum?id=bjcsVLoHYs)

[https://ai-researcher.net/](https://ai-researcher.net/)

[https://github.com/zhu-minjun/Researcher](https://github.com/zhu-minjun/Researcher)

# 高效reasoning

## reasoning economy

[港中文发布全新视角高效Reasoning综述！idea已充满大脑...](https://mp.weixin.qq.com/s/rgoJlXinM-NtAjnuXr4Kfw)

[Harnessing the Reasoning Economy A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377)

[https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers](https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers)

![](../assets/post-training-reason-economy.png)

+ 数据：
  + 干掉冗余、噪声、太长的数据
  + 保留高质量数据
+ 算法：
  + Long2Short RL：惩罚过长的输出
  + budget-aware tuning：简单问题输出短，复杂问题输出长
  + CoT Compression：显式/隐式压缩CoT
+ 模型：
  + adaptive Activated Parameters：类似稀疏激活
  + Model Cooperation：搞2个模型，简单问题走简单模型，复杂问题走复杂模型

![](../assets/test-time-reason-economy.png)

+ 输入侧：解码之前进行自适应预算分配
  + 对输入进行成本预估
+ 输出侧：
  + thinking pattern的自适应预算分配：简单问题直接剪枝，复杂问题需要回溯+验证+反思
  + 解码算法的自适应预算分配：简单问题贪心解码，复杂问题多一些采样

## stop overthinking

[大模型还有哪些值得研究的方向？ || 大模型高效推理(Efficient Reasoning)研究综述](https://mp.weixin.qq.com/s/vUcmlaJ4zMmTtDolJLqJrA)

[Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.16419)

[https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs](https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs)

![](../assets/efficient-cot.png)

+ model：
  + length reward：RL时加上新的reward，鼓励答案正确且cot短的
  + variable-length cot reasoning data：构造不同长度cot的数据集，然后sft
+ reasoning-output：
  + latent reasoning：将reasoning steps压缩成latent表示
  + dynamic reasoning：reasoning的过程改成投机采样、拒绝采样、tree-of-thoughts等方式
+ input prompt：
  + length prompts：let's use less than k tokens
  + routing by difficulty：训一个小模型，决定简单问题不思考，困难问题再思考


# 推理模型总结

[从自我进化视角出发，全面解析LLM的推理能力技术演进路径](https://mp.weixin.qq.com/s/hkYW0c26eLEHE9WgIQxwHw)

[A Survey on LLM Complex Reasoning through the Lens of Self-Evolution](https://github.com/daiwk/collections/blob/master/assets/Survey_of_LLM_Reaoning_by_Self_Evolution.pdf)

[https://github.com/cs-holder/Reasoning-Self-Evolution-Survey](https://github.com/cs-holder/Reasoning-Self-Evolution-Survey)

(toread)

更新的：

[DeepSeek-R1之后推理模型发展如何？Raschka长文梳理后R1时代14篇重要论文](https://mp.weixin.qq.com/s/OMblIwzOJg9POx2hDtjekg)


[从o1-mini到DeepSeek-R1，万字长文带你读懂推理模型的历史与技术](https://mp.weixin.qq.com/s/t19D5_2nsbIVI_eemLtx-Q)

[https://cameronrwolfe.substack.com/p/demystifying-reasoning-models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)


[https://openai.com/index/learning-to-reason-with-llms/](https://openai.com/index/learning-to-reason-with-llms/)这里的图：说明openai发现了在训练时用大规模的强化学习（train-time compute）以及在测试时花更多的时间思考（test-time computing）都能提升数学任务上的效果

![](../assets/compute-power-law.png)

# 世界模型

[怒斥Sora之后，LeCun放出「视觉世界模型」论文，揭示AI学习物理世界的关键​](https://mp.weixin.qq.com/s/KY-bTD-bxdB3Q-q97Gv7fg)

[100万token，一次能分析1小时YouTube视频，「大世界模型」火了](https://mp.weixin.qq.com/s/8ONe7_ejQQIT1UwqDGK-vg)

[WORLD MODEL ON MILLION-LENGTH VIDEO AND LANGUAGE WITH RINGATTENTION](https://arxiv.org/pdf/2402.08268.pdf)

[https://github.com/LargeWorldModel/LWM](https://github.com/LargeWorldModel/LWM)

[Sora是世界模拟器吗？全球首篇综述全面解析通用世界模型](https://mp.weixin.qq.com/s/rkq7NXGvB0O1Kstd_mTNJQ)

[Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond](https://arxiv.org/pdf/2405.03520)

[世界模型也扩散！训练出的智能体竟然不错](https://mp.weixin.qq.com/s/AWt1Jgvr2aj6sjkGRV9-hg)

[Diffusion for World Modeling: Visual Details Matter in Atari](https://arxiv.org/pdf/2405.12399)

[https://github.com/eloialonso/diamond](https://github.com/eloialonso/diamond)



## pandora

[通用世界模型问世：不学习就能生成新领域视频，可实时控制](https://mp.weixin.qq.com/s/Vj2W3BtKITV4mxwVhDJHzg)

[Pandora : Towards General World Model with Natural Language Actions and Video States](https://world-model.maitrix.org/assets/pandora.pdf)

[https://github.com/maitrix-org/Pandora](https://github.com/maitrix-org/Pandora)


[ACL 2024论文盖棺定论：大语言模型≠世界模拟器，Yann LeCun：太对了](https://mp.weixin.qq.com/s/FBqYb_gcBr5D204mDtmCOA)

[Can Language Models Serve as Text-Based World Simulators?](https://arxiv.org/pdf/2406.06485)

## WHALE

[WHALE来了，南大周志华团队做出更强泛化的世界模型](https://mp.weixin.qq.com/s/9vk__IpfMdYj57VjtV1HvQ)

[WHALE: TOWARDS GENERALIZABLE AND SCALABLE WORLD MODELS FOR EMBODIED DECISION-MAKING](https://arxiv.org/pdf/2411.05619)

## genie 2

[谷歌世界模型爆发：单张图生成可玩3D世界，还要和马斯克一起做AI游戏](https://mp.weixin.qq.com/s/PaUScHrclUwYABwT8LL5zg)

[https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)

## NWM

[LeCun团队新作：在世界模型中导航](https://mp.weixin.qq.com/s/V5rXxbLYmR8UuiVq-gsi9A)

[Navigation World Models](https://arxiv.org/pdf/2412.03572v1)

[https://www.amirbar.net/nwm/](https://www.amirbar.net/nwm/)

# 语言物理学

(toread)

[大模型边推理边纠错，有可能做到吗？这是ICML爆火的演讲](https://mp.weixin.qq.com/s/NOVFYmXiHUJ7x1SU7yH0CA)

[https://www.bilibili.com/video/BV1Yw4m1k7nH](https://www.bilibili.com/video/BV1Yw4m1k7nH)

# 蒸馏

## ABKD


[ICML Spotlight 2025丨追求概率质量的帕累托最优：基于广义α-β散度引导的知识蒸馏框架ABKD](https://mp.weixin.qq.com/s/UwRwDJJxWrS-9mVoHSUPDQ)

[ABKD: Pursuing a Proper Allocation of the Probability Massin Knowledge Distillation via α-β-Divergence](https://arxiv.org/pdf/2505.04560)

[https://github.com/ghwang-s/abkd](https://github.com/ghwang-s/abkd)

现有问题：

+ 前向KL：概率分配过于“佛系”，学生“雨露均沾”，难专注目标类
+ 反向KL：概率分配过于“内卷”，学生“死磕”高置信度类，忽略教师全局信息

ABKD引入α-β散度，统一前向/反向KL，并推广到此前未探索的海灵格距离和β-散度等。


# LLM+math

## mathscale

[【LLM-数学】MathScale 用于数学推理的指令调优扩展方法](https://mp.weixin.qq.com/s/tQUIGdViMZTb_9NNh3b3RQ)

[MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://arxiv.org/pdf/2403.02884.pdf)


## AlphaGeometry

[奥数能力金牌级：DeepMind几何推理模型登上Nature，代码开源，菲尔兹奖得主点赞](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650904746&idx=1&sn=d39a3d92078cecbd29bd0fc82560d1da&chksm=84e45cd4b393d5c24747f163fa0338761690447904a1a654aacf2344d7a16d36dfa2ac3ccdb0&scene=21#wechat_redirect)提出了AlphaGeometry

## AlphaProof & AlphaGeometry 2

[谷歌AI拿下IMO奥数银牌，数学推理模型AlphaProof面世，强化学习 is so back](https://mp.weixin.qq.com/s/LNzbyf0w412BIz71sROyzw)提出AlphaProof和AlphaGeometry 2

## WE-Math基准

[真相了！大模型解数学题和人类真不一样：死记硬背、知识欠缺明显，GPT-4o表现最佳](https://mp.weixin.qq.com/s/uU1lZV0Ymj31cmZryhffyQ)

[WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?](https://arxiv.org/pdf/2407.01284)

[https://github.com/We-Math/We-Math](https://github.com/We-Math/We-Math)

[https://huggingface.co/datasets/We-Math/We-Math](https://huggingface.co/datasets/We-Math/We-Math)

## case-based or rule-based

[ICML 2024｜Transformer究竟如何推理？基于样例还是基于规则](https://mp.weixin.qq.com/s/aVRiGW3xU_LpvxZzjDpwzQ)

[Case-Based or Rule-Based: How Do Transformers Do the Math?](https://arxiv.org/pdf/2402.17709)

[https://github.com/GraphPKU/Case_or_Rule](https://github.com/GraphPKU/Case_or_Rule)


# LLM常见难题

## LLM as a judge

[A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594)

[Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge](https://arxiv.org/pdf/2501.18099)，meta的2025年1月的

## 重复生成

[https://www.zhihu.com/question/616130636](https://www.zhihu.com/question/616130636)

[https://mp.weixin.qq.com/s/cSwWapqFhxu9zafzPUeVEw](https://mp.weixin.qq.com/s/cSwWapqFhxu9zafzPUeVEw)

[Interpreting the Repeated Token Phenomenon in Large Language Models](https://arxiv.org/pdf/2503.08908)

deepmind的文章，发现和attention sink（初始token会有很高的attn score）有关，初始注意力层负责标记序列中的第一个单词，而后期的一些特定神经元则会放大这些标记单词的隐藏状态值。当处理重复单词时，这一机制会失效，导致模型行为异常。

[https://github.com/yossigandelsman/attn_sinkhole](https://github.com/yossigandelsman/attn_sinkhole)

## 幻觉

### 综述

[OpenAI Lilian Weng万字长文解读LLM幻觉：从理解到克服](https://mp.weixin.qq.com/s/UGcui0rLW2Vz7y2Mt4atqA)

[https://lilianweng.github.io/posts/2024-07-07-hallucination/](https://lilianweng.github.io/posts/2024-07-07-hallucination/)


### 语义熵

[语义熵识破LLM幻觉！牛津大学新研究登Nature](https://mp.weixin.qq.com/s/fdLZ9DDqG9C_uxAAlKgQbw)

[Detecting hallucinations in large language models using semantic entropy](https://www.nature.com/articles/s41586-024-07421-0)

## Zilliz

[向量数据库的中场战事：长期主义者Zilliz如何全球突围](https://mp.weixin.qq.com/s/lRryjRiUGKdT11qfi62pUg)

## 记忆能力

[Localizing Paragraph Memorization in Language Models](https://arxiv.org/pdf/2403.19851v1.pdf)

对应代码：[https://github.com/googleinterns/localizing-paragraph-memorization](https://github.com/googleinterns/localizing-paragraph-memorization)

我们能否定位出语言模型中用于记忆其训练数据中整段文字的权重和机制？

+ 尽管记忆现象分布在模型的多个层级和组件中，但记忆段落的梯度在空间上有可辨别的模式，即**在较低模型层级的梯度比非记忆example的梯度大**。
+ 通过**仅微调高梯度的权重**，可以使模型**遗忘记忆的example**。
+ 定位了一个特别参与段落记忆的**低层注意力头**，它**主要关注**在语料库级单词频率分布中**最不频繁出现的独特、罕见的token**。
+ 总的来说，相较非记忆的续写，记忆续写不仅**更难以遗忘**，也**更难以损坏**。

### reasoning

[Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks](https://arxiv.org/pdf/2307.02477) MIT的

[Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/pdf/2402.16837) deepmind的

[How do Language Models Bind Entities in Context?](https://arxiv.org/pdf/2310.17191) UC berkeley的，ICLR2024

### memorizing

[Knowledge Neurons in Pretrained Transformers](https://arxiv.org/pdf/2104.08696) ACL 2022
 
[Language Modeling Is Compression](https://arxiv.org/pdf/2309.10668) ICLR 2024 deepmind

[Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://arxiv.org/pdf/2205.10770) meta NeurIPS 2022

## 越狱

[长文本之罪：Claude团队新越狱技术，Llama 2到GPT-4无一幸免](https://mp.weixin.qq.com/s/C0opoIzLCFojfmoa6poM8A)

## LLM compiler

[开发者狂喜！Meta最新发布的LLM Compiler，实现77%自动调优效率](https://mp.weixin.qq.com/s/Js0lUS_5ZPspVLazthkEOg)

[Meta Large Language Model Compiler: Foundation Models of Compiler Optimization](https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/)



## ProLong

[2024 年了，你的长文本训练数据真的够长吗？](https://mp.weixin.qq.com/s/5dVm-VWiZG09ixMMegKCbw)

[Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models](https://arxiv.org/pdf/2405.17915)

[https://github.com/October2001/ProLong](https://github.com/October2001/ProLong)


## 白化

在 transformer 领域里，“白化”（whitening）主要是指一种对句子嵌入进行后处理的方法，通过将句子向量的均值变为0，并将协方差矩阵变为单位矩阵，从而解决句子嵌入中的各向异性问题。这种技术能够提高句子嵌入在语义相似性任务中的表现，并且加快检索速度。

[Whitening Sentence Representations for Better Semantics and Faster Retrieval](https://ar5iv.labs.arxiv.org/html/2103.15316)

代码：[https://github.com/bojone/BERT-whitening](https://github.com/bojone/BERT-whitening)


[Transformer Scale Gate for Semantic Segmentation](https://arxiv.org/pdf/2205.07056v1)

## 蒸馏

[Revisiting Knowledge Distillation for Autoregressive Language Models](https://arxiv.org/pdf/2402.11890)

[Meta开发System 2蒸馏技术，Llama 2对话模型任务准确率接近100%](https://mp.weixin.qq.com/s/QycbrMXsR0nUsvHBx0_GBw)

[Distilling System 2 into System 1](https://arxiv.org/pdf/2407.06023v2)

## 证明者-验证者博弈

[OpenAI超级对齐团队遗作：两个大模型博弈一番，输出更好懂了](https://mp.weixin.qq.com/s/MiLYbYcYUPO9rdQjijF_tQ)

[Prover-Verifier Games improve legibility of LLM outputs](https://arxiv.org/pdf/2407.13692)

参考：[Learning to Give Checkable Answers with Prover-Verifier Games](https://arxiv.org/pdf/2108.12099)

## 道德风险

[GPT-4o模仿人类声音，诡异尖叫引OpenAI研究员恐慌！32页技术报告出炉](https://mp.weixin.qq.com/s/XSTNHTILAOkINg7mxssb6g)

openai的报告：[GPT-4o System Card](https://cdn.openai.com/gpt-4o-system-card.pdf)

之前deepmind也有一个报告[The Ethics of Advanced AI Assistants](https://arxiv.org/pdf/2404.16244)

## 选择性偏差

[ACL2024|大模型选择偏差在腾讯广告特征评测上的优化及应用](https://mp.weixin.qq.com/s/0P1D1H1HoXMwZg2nBiM07Q)

[Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors](https://arxiv.org/pdf/2406.01026)

给定一个问题(question)及其对应的选项内容(options)，大模型无法把选项内容(option content)和对应的选项标识符(symbol，特指选项标识A/B/C/D)关联到一起。例如，当把正确答案"the president"放到选项B时，模型能够正确选择出答案；当我们把正确答案放到C时，模型依然选择"B"，即模型偏向于选"B"或者第二个答案，而忽略了正确答案的内容。

## lost in the middle

[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172)

## reasoning boundary

[NeurIPS 2024 (Oral) | 如何量化与提升思维链的推理能力边界？](https://mp.weixin.qq.com/s/BwuGacSHKY4RTdvYNMa66Q)

[Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought](https://arxiv.org/abs/2410.05695)

[https://github.com/LightChen233/reasoning-boundary](https://github.com/LightChen233/reasoning-boundary)

## 语言≠思维

[语言≠思维，大模型学不了推理：一篇Nature让AI社区炸锅了](https://mp.weixin.qq.com/s/BgMNITn5e1RGUOHQLKv7yg)

[https://www.nature.com/articles/s41586-024-07522-w](https://www.nature.com/articles/s41586-024-07522-w)

# 多智能体

[《综述：全新大语言模型驱动的Agent》——4.5万字详细解读复旦NLP和米哈游最新Agent Survey](https://zhuanlan.zhihu.com/p/656676717)

[Agent > GPT5？吴恩达最新演讲：四种 Agent 设计范式（通俗易懂版）](https://mp.weixin.qq.com/s/6sh39yEO4YGZI-BGPjJnCg)

## JAT

[告别偏科，能玩转多模态、多任务、多领域的强化智能体终于来了](https://mp.weixin.qq.com/s/2GBB-w7hBf6equtqD8V0Lg)

[Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent](https://arxiv.org/pdf/2402.09844)

[https://github.com/huggingface/jat](https://github.com/huggingface/jat)

[https://huggingface.co/datasets/jat-project/jat-dataset](https://huggingface.co/datasets/jat-project/jat-dataset)

![jat](../assets/jat.png)

输入的序列元素是observations, actions, 和rewards的交替组合：

XXX
\left[\phi\left(s_0, 0.0\right), \phi\left(a_0\right), \phi\left(s_1, r_1\right), \phi\left(a_1\right), \ldots\right]
XXX

依据不同输入的数据类型，使用不同网络处理：

+ 图像：用CNN。
+ 连续向量：用线性层
+ 离散值：用线性投影层

预测任务：根据所有先前的观察和动作嵌入来预测下一个动作嵌入。

序列的构造方法：

+ 和文本相关的任务：用 GPT-2 的分词策略，将文本转换为一个整数序列，然后emb lookup映射到一个嵌入向量序列。
+ 和图像有关的任务：用ViT，将图像切割成小块后，通过线性层转换为嵌入向量序列。
+ 最终再将图像和文本的向量序列拼接在一起，形成一个统一的序列，输入到 Transformer 中。

## ReadAgent

[「有效上下文」提升20倍！DeepMind发布ReadAgent框架](https://mp.weixin.qq.com/s/xXJqJeqf8mzP9VW9kLIdgQ)

## 多模态agent

[一文详解多模态智能体（LMAs）最新进展（核心组件/分类/评估/应用）](https://mp.weixin.qq.com/s/lucGhu5-IPjIKbZ2o1q-PQ)

[Large Multimodal Agents: A Survey](https://arxiv.org/pdf/2402.15116)

[https://github.com/jun0wanan/awesome-large-multimodal-agents](https://github.com/jun0wanan/awesome-large-multimodal-agents)

## OpenDevin

[OpenDevin出技术报告了，大模型Agent开发者必读](https://mp.weixin.qq.com/s/tfREoiwjfCZauisCE3PpvQ)

[OpenDevin: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/pdf/2407.16741)

## autogpt

[GitHub星标超16万，爆火AutoGPT进阶版来了：定制节点、多智能体协同](https://mp.weixin.qq.com/s/dBL47yYoVNkyPoPG8pcLLA)

[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)

## DAAG

[三「模」联盟，谷歌DeepMind缔造终身学习智能体！](https://mp.weixin.qq.com/s/P-x8EDrfd1ydCnPP8MYu6g)

[Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](https://arxiv.org/pdf/2407.20798)

## VARP

[GPT-4o能玩《黑神话》！精英怪胜率超人类，无强化学习纯大模型方案](https://mp.weixin.qq.com/s/veHSbBxPIqRexG0OWtg4pw)

[Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case](https://arxiv.org/abs/2409.12889)

## MARL

[北大领衔，多智能体强化学习研究登上Nature子刊](https://mp.weixin.qq.com/s/_67dbIMDjktMEw4QYiIAUA)

[Efficient and scalable reinforcement learning for large-scale network control](https://www.nature.com/articles/s42256-024-00879-7)

## MMRole

[与「李白」赏图赋诗，同「猴哥」直面天命，人大高瓴提出MMRole多模态角色扮演](https://mp.weixin.qq.com/s/I8gyDv9K8uhB3EXF_2_zVw)

[MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://arxiv.org/abs/2408.04203)

[https://github.com/YanqiDai/MMRole](https://github.com/YanqiDai/MMRole)

## Swarm

[OpenAI今天Open了一下：开源多智能体框架Swarm](https://mp.weixin.qq.com/s/3-iKztrTuRURUGtles4-xA)

[https://github.com/openai/swarm](https://github.com/openai/swarm)

## agent-as-a-judge

[卷起来！让智能体评估智能体，Meta发布Agent-as-a-Judge](https://mp.weixin.qq.com/s/YX1cmIMDonUiosSg24boUQ)

[Agent-as-a-Judge: Evaluate Agents with Agents](https://arxiv.org/pdf/2410.10934)

[https://github.com/metauto-ai/agent-as-a-judge](https://github.com/metauto-ai/agent-as-a-judge)

## Hammer

[哪个模型擅长调用工具？这个7B模型跻身工具调用综合榜单第一](https://mp.weixin.qq.com/s/YsjjaTdDNWsoLXhr7mGOpQ)

[Hammer: Robust Function-Calling for On-Device Language Models via Function Masking](https://arxiv.org/abs/2410.04587)

[https://huggingface.co/MadeAgents](https://huggingface.co/MadeAgents)

[https://github.com/MadeAgents/Hammer](https://github.com/MadeAgents/Hammer)

## AgentOccam

[不靠更复杂的策略，仅凭和大模型训练对齐，零样本零经验单LLM调用，成为网络任务智能体新SOTA](https://mp.weixin.qq.com/s/UvNCUVBbH9TqfbEdoB7mTA)

[AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://arxiv.org/abs/2410.13825)

## 苏格拉底学习

[DeepMind用语言游戏让大模型学AlphaGo自我博弈，数据限制不存在了](https://mp.weixin.qq.com/s/EC5QdHcasev8JpTp-OKLKQ)

[Boundless Socratic Learning with Language Games](https://arxiv.org/abs/2411.16905)

## 从个人模拟到社会模拟

[智能体模拟《西部世界》一样的社会，复旦大学等出了篇系统综述](https://mp.weixin.qq.com/s/Uy_NYkDGp9CqmO2j9XOfCA)

[From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents](https://arxiv.org/abs/2412.03563)


## MetaGPT

ICLR2024

+ [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://arxiv.org/pdf/2308.00352)
+ [Data Interpreter: An LLM Agent For Data Science](https://arxiv.org/abs/2402.18679)
+ [AFlow: Automating Agentic Workflow Generation](https://arxiv.org/abs/2410.10762)

4w+的stars了

[https://github.com/geekan/MetaGPT](https://github.com/geekan/MetaGPT)

## insight-V

[多智能体架构Insight-V来了！突破长链视觉推理瓶颈](https://mp.weixin.qq.com/s/-8TvvTDa7zeEUlzcbtuPWg)

[Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432)

[https://github.com/dongyh20/Insight-V](https://github.com/dongyh20/Insight-V)

## 世界经济论坛的agent报告

[Navigating the AI Frontier: A Primer on the Evolution and Impact of AI Agents](https://reports.weforum.org/docs/WEF_Navigating_the_AI_Frontier_2024.pdf)

## Claude: building effective agents(MCP)

[Claude 官方发布《Agent 构建指南》，附 PDF 下载](https://mp.weixin.qq.com/s/hqNcLv3pKgZdqpGxAPlt2A)

[https://www.anthropic.com/research/building-effective-agents](https://www.anthropic.com/research/building-effective-agents)

Agent 系统分为两大类：

+ 工作流 (Workflows) ：
    + 特点：通过预定义的代码路径来编排 LLM 和工具的系统。更像是一个精心设计的流程，每一步都清晰可控。
    + 场景：当任务非常明确，而且可以分解成一系列固定的步骤时，就像流水线上的工作一样，用“工作流程”就足够了。
+ 智能体 (Agents)：
    + 特点：由 LLM 动态地指导自身流程和工具使用的系统。更像是一个自主的决策者，能够根据环境反馈灵活调整行动。
    + 场景：当任务需要很大的灵活性，而且需要模型自己做决策时，就像一个需要随机应变的指挥官，这时候“智能体”就更适合。

现有的框架：
+ LangGraph（LangChain 的工具）：就像一套功能强大的乐高套件，可以用来搭建各种复杂的 Agent 系统。[https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)
+ Amazon Bedrock 的 AI Agent 框架：就像一个专业的工具箱，提供了各种构建 Agent 系统的工具和组件。[https://aws.amazon.com/cn/bedrock/agents/](https://aws.amazon.com/cn/bedrock/agents/)
+ Rivet（拖放式 GUI LLM 工作流构建器）：就像一个可视化编辑器，可以通过拖拽的方式来构建 LLM 的工作流程，非常方便。[https://rivet.ironcladapp.com/](https://rivet.ironcladapp.com/)
+ Vellum（复杂工作流的构建和测试工具）：就像一个高级的实验室，可以用来构建和测试复杂的工作流程。[https://www.vellum.ai/](https://www.vellum.ai/)

### 基石：augmented LLM

![](../assets/augmented-llm.png)

通过**检索、工具和记忆**等机制扩展大语言模型的能力，这样大语言模型能够主动运用这些能力来生成自己的搜索查询、选择合适的工具，并决定保留哪些信息。

Anthropic有一个上下文协议（Model Context Protocol，MCP），允许开发者通过简单的客户端实现与不断增长的第三方工具生态系统集成，参考[https://www.anthropic.com/news/model-context-protocol](https://www.anthropic.com/news/model-context-protocol)

[openai也妥协了，全面拥抱MCP!](https://mp.weixin.qq.com/s/R8kp1WiegAgIXM2ufLf0ZQ)

### workflow

+ prompt chaining：将一个任务分解成一系列步骤，其中的每个LLM的调用都会处理前一个调用的输出，可以在任何中间步骤中添加程序化的检查（见图中的“门控 Gate”），以确保流程仍在正轨上，即有一个失败就exit

![](../assets/workflow-prompt-chaining.png)

+ Routing：将不同类型的客户服务查询（一般问题、退款请求、技术支持）导向不同的下游流程、提示和工具，例如将简单/常见的问题路由到较小的模型（如Claude 3.5 Haiku），将困难/不常见的问题路由到功能更强大的模型（如Claude 3.5 Sonnet），以优化成本和速度。

![](../assets/workflow-routing.png)

+ Parallelization：同时执行多个任务，并通过程序化方式整合结果。适用：
    + 分段处理：
        + 构建安全防护机制，一个模型实例负责处理用户查询，而另一个模型实例负责筛选不当内容或请求。
        + 自动化评估模型性能，每个模型调用负责评估模型在给定提示下的不同性能指标。
    + 多重投票：
        + 对代码进行漏洞审查，多个不同的提示分别审查代码，并在发现问题时进行标记。
        + 评估内容是否不当，多个提示从不同角度进行评估，或采用不同的投票阈值来平衡误报和漏报。

![](../assets/workflow-parallelization.png)

+ Orchestrator-workers（协调者-工作者模式）：一个中央LLM会**动态地分解任务**，并将这些子任务分配给不同的工作者模型，最后再整合所有工作者的结果。适用：
    + 需要对**多个文件**进行复杂修改的编码产品。
    + 需要从**多个来源**收集并分析信息以寻找相关内容的搜索任务。

![](../assets/workflow-orchestrator-workers.png)

+ Evaluator-optimizer：一个LLM负责调用**生成**响应，而另一个LLM调用则在一个**循环**中提供**评估和反馈**。适用：
    + 文学翻译，比如翻译模型**最初**可能无法捕捉到的细微差别，但评估器模型可以提供有用的评审意见。
    + 需要进行**多轮**搜索和分析以收集全面信息的复杂搜索任务，评估器可以用来决定是否需要进一步搜索。

![](../assets/workflow-evaluator-optimizer.png)

### Agents
**
当LLM在理解复杂输入、推理规划、可靠使用工具和从错误中恢复等关键能力上成熟时，智能体可以处理开放式问题，无需预先定义步骤，并能根据环境反馈**自主决策**。在特定节点或遇到困难时暂停的功能，以便引入人工干预或反馈。

![](../assets/auto-agents.png)

## google ai agents白皮书

[https://github.com/daiwk/collections/blob/master/assets/google-ai-agents-whitepaper.pdf](https://github.com/daiwk/collections/blob/master/assets/google-ai-agents-whitepaper.pdf)

## stanford的agent综述

[Agent AI: Surveying the Horizons of Multimodal Interaction](https://arxiv.org/pdf/2401.03568)

## 李宏毅的agent课

[台大李宏毅2025 AI Agent新课来了！](https://mp.weixin.qq.com/s/d5FnSATz3tPfCOu2a53uKQ)

[https://www.youtube.com/watch?v=M2Yg1kwPpts](https://www.youtube.com/watch?v=M2Yg1kwPpts)

[ppt](https://docs.google.com/presentation/d/1kTxukwlmx2Sc9H7aGPTiNiPdk4zN_NoH)

## google的A2A

[最新：Google 牵头搞了个 A2A，以后不同家的 AI 都能“加好友”了](https://mp.weixin.qq.com/s/fha3Yf-yK5D3JZ0bIX1tyw)

[5000字长文带你看懂，Agent世界里的A2A、MCP协议到底是个啥。](https://mp.weixin.qq.com/s/hr7wvpz-KRllwQkiKYf0Tg)

A2A协议是对 Anthropic 公司模型上下文协议 (MCP) 的补充，后者为智能体提供了有用的工具和上下文。A2A则更侧重于智能体之间的交互与协作

A2A促进了客户端 (client)智能体和远程 (remote)智能体之间的通信。客户端智能体负责制定和传达任务，远程智能体则负责执行这些任务以提供信息或采取行动。

这个交互过程包含几个关键能力：

+ 能力发现 (Capability discovery): 智能体可以通过JSON格式的Agent Card来宣告自身能力。这使得客户端智能体能找到最适合执行某项任务的远程智能体，并发起A2A通信。
+ 任务管理 (Task management): 通信围绕任务完成进行。协议定义了具有生命周期的任务 (task)对象。任务可以是即时完成的，也可以是长时运行的。任务的输出被称为工件 (artifact)
+ 协作 (Collaboration): 智能体之间可以发送消息，以沟通上下文、回复、工件或用户指令
+ 用户体验协商 (User experience negotiation): 每条消息包含parts，即完整的内容片段（如生成的图像）。每个部分都有指定的内容类型，允许客户端和远程智能体协商所需格式，并明确协商用户的UI能力（例如是否支持iframe、视频、Web表单等）。

## 字节的DeerFlow

[字节跳动开源了一款 Deep Research 项目](https://mp.weixin.qq.com/s/Le7Ic9FgcwAkQeHDuorXMw)

[https://github.com/bytedance/deer-flow](https://github.com/bytedance/deer-flow)

# 一些其他比较重要的工作

## 几篇出现频率比较高的论文

[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf) 引用数800+

[How can we know what language models know?](https://arxiv.org/pdf/1911.12543.pdf) 引用数800+

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)引用1800+

## Anthropic的一些工作

[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[Studying Large Language Model Generalization with Influence Functions](https://arxiv.org/pdf/2308.03296.pdf)

[Measuring Faithfulness in Chain-of-Thought Reasoning](https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf)

[从Claude 3中提取数百万特征，首次详细理解大模型的「思维」](https://mp.weixin.qq.com/s/cZhmvAva6NDLG84kD819Ww)

[Scaling Dictionary Learning to Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

[LLM惊现篡改代码获得奖励，欺骗人类无法根除逆转！Anthropic新作揭露惊人真相](https://mp.weixin.qq.com/s/Fgkkc3p7zIW8OrCvSU-2lA)


[SYCOPHANCY TO SUBTERFUGE: INVESTIGATING REWARD TAMPERING IN LANGUAGE MODELS](https://arxiv.org/pdf/2406.10162)


# 个性化搜索

随便找一篇[Denoising Attention for Query-aware User Modeling in Personalized Search](https://arxiv.org/pdf/2308.15968.pdf)，来看看它的参考文献：

学术界：

+ [A Transformer-based Embedding Model for Personalized Product Search](https://arxiv.org/pdf/2005.08936.pdf)，sigir20
+ [Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search](https://arxiv.org/pdf/2004.09424.pdf)，sigir21
+ [RLPer: A Reinforcement Learning Model for Personalized Search](http://playbigdata.ruc.edu.cn/dou/publication/2020_WWW_RLPer.pdf)，www20


工业界：

+ [A Zero Attention Model for Personalized Product Search](https://arxiv.org/pdf/1908.11322.pdf)，CIKM19，亚马逊
+ [Real-time Personalization using Embeddings for Search Ranking at Airbnb](https://github.com/daiwk/collections/blob/master/assets/airbnb-kdd18.pdf)，KDD18，airbnb
+ [End-to-End Deep Attentive Personalized Item Retrieval for Online Content-sharing Platforms](https://dl.acm.org/doi/pdf/10.1145/3366423.3380051)，www20，Google
+ [Towards Personalized and Semantic Retrieval: An End-to-End Solution for E-commerce Search via Embedding Learning](https://arxiv.org/pdf/2006.02282.pdf)，sigir20，京东
+ [Personalized Query Suggestions](https://guoweiwei.github.io/files/personalized-query-suggestion.pdf)，sigir20，LinkedIn
+ [A GNN-based Multi-task Learning Framework for Personalized Video Search](https://eprints.whiterose.ac.uk/181816/1/GNNVideoSearch_WSDM2022.pdf)，WSDM22，百度


## q-i双塔的改进

### 引入location+social

[Embedding-based Retrieval in Facebook Search](https://arxiv.org/pdf/2006.11632.pdf)，KDD20

q-d双塔结构，在两个塔的最底层均加入：

+ location：用户所处地理位置，如城市
+ social：facebook是社交网络，通过另一个基于graph的模型训练得到的user和item emb，直接加进来

### 引入用户行为序列

[Encoding History with Context-aware Representation Learning for Personalized Search](http://playbigdata.ruc.edu.cn/dou/publication/2020_sigir_context_ps.pdf)，sigir20，人大，提出HTPS

![htps-disambiguate-query](../assets/htps-disambiguate-query.png)

把用户历史的q-d pair对和当前query一起，过短期transformer和长期transformer得到输出$$q^l$$。

![htps-predict-intent](../assets/htps-predict-intent.png)

把$$q^l$$加上[mask]，过transoformer得到预估的intent $$q^p$$

然后将$$q^l$$和$$q^p$$通过gate nn融合得到最终的context-aware的query表示$$q^f$$

最终doc和query的打分包括两部分，通过$$\phi$$（一个MLP，激活是tanh）进行融合：

XXX
p(d \mid q, H)=\phi\left(p(d, q), p\left(d, q^H\right)\right)
XXX

+ $$p(d, q)$$：q和d的语义相似度，可以用正常的nlp模型得到
+ $$p\left(d, q^H\right)$$：q和d的个性化得分，公式如下，其中$$s^R$$是cos：

XXX
p\left(d, q^H\right)=\phi\left(s^R\left(q^s, d^w\right), s^R\left(q^l, d^w\right), s^R\left(q^p, d^w\right), s^R\left(q^f, d^w\right)\right)
XXX

有两个loss：

+ pred loss：预估intent，即下一个query，拿$$q^p$$与下一个query中各个词向量的avg算cos
+ rank loss：依据$$p(d \mid q, H)$$算lambda rank的pairwise loss

### 三塔+gnn邻居+mtl

[A GNN-based Multi-task Learning Framework for Personalized Video Search](https://eprints.whiterose.ac.uk/181816/1/GNNVideoSearch_WSDM2022.pdf)，WSDM22，百度，提出MGNN-PVS

现有的PSM(g personalized search methods)大多使用用户反馈（如点击）进行训练，缺点：

+ 反馈信号大部分表达的是吸引力而非相关性
+ 用户的历史信号比较稀疏，很难学好PSM

两张二部图：u-q和q-d

![gnn-personalized-video-search](../assets/gnn-personalized-video-search.png)

3个塔：

+ user：
    + user自己
    + 一跳邻居（u->q）的q
    + 二跳邻居（u->q->u）的u
+ query：
    + query自己
    + 一跳邻居（q->d）的doc
    + 二跳邻居（q->d->q）的query
+ doc：
    + doc自己的title向量（训练query-正title-负title的triplet loss）和video向量（训练video-正query-负query的triplet loss）
    + 二跳邻居（d->q->d）的doc

两个task：

+ ctr预估：u和q拼一起过nn得到个性化的q，再和d过nn得到的向量算内积，得到预估值，用交叉熵
+ 相关性预估：q过另一个nn，d过另一个nn，内积，用mse

# RAG

[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997.pdf)

[RAG全链路的关键模块解析](https://mp.weixin.qq.com/s/kNjOgfQs6yErNtRg6wFA3g)

[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)

[Meta提出全新文档级嵌入框架，利用LLM来增强信息检索能力](https://mp.weixin.qq.com/s/RCRHjrW6jF167HG169aFwg)

[LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding](https://arxiv.org/pdf/2404.05825.pdf)

## RankRAG

[RAG微调Llama 3竟超越GPT-4！英伟达GaTech华人学者提出RankRAG框架](https://mp.weixin.qq.com/s/87qeqDSwtitYsruH_2Jdww)

[RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/pdf/2407.02485)

## graphRAG

[微软开源的GraphRAG爆火，Github Star量破万，生成式AI进入知识图谱时代？](https://mp.weixin.qq.com/s/BX93FvDzW7WVLK66V2usBw)

[https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)

[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)

## RAGChecker

[给RAG系统做一次全面「体检」，亚马逊开源RAGChecker诊断工具](https://mp.weixin.qq.com/s/x4o7BinnwvTsOa2_hegcrQ)

[RAGCHECKER: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation](https://arxiv.org/pdf/2408.08067)

[https://github.com/amazon-science/RAGChecker](https://github.com/amazon-science/RAGChecker)

## TAG

[表格增强生成TAG登场：解锁AI自然语言与数据库的完美结合](https://mp.weixin.qq.com/s/6gkPA-xc7GsltM1Ywui_XQ)

## Storm

[斯坦福开源学术研究神器STORM再进化，AI智能体像人一样进行圆桌讨论](https://mp.weixin.qq.com/s/-NY8Xw8ihIFgUwy4LFLMgA)

[Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations](https://www.arxiv.org/pdf/2408.15232)

[https://github.com/stanford-oval/storm](https://github.com/stanford-oval/storm)

[https://storm.genie.stanford.edu/](https://storm.genie.stanford.edu/)

## Block-atttention RAG

[RAG新突破：块状注意力机制实现超低延迟检索增强](https://mp.weixin.qq.com/s/yv2iIpaJTi4g4nhZG1WLZw)

[Block-Attention for Efficient RAG](https://arxiv.org/pdf/2409.15355)

## 2024 rags

[RAG七十二式：2024年度RAG清单](https://mp.weixin.qq.com/s/icIduUFsJxOka4orKM2pCw)


## RAG的知识冲突

[深度解析RAG大模型知识冲突，清华西湖大学港中文联合发布](https://mp.weixin.qq.com/s/y9-DwgNb3Yftgf_Ulf6yDQ)

[Knowledge Conflicts for LLMs: A Survey](https://arxiv.org/pdf/2403.08319)

## myscaledb

[长文本杀不死RAG：SQL+向量驱动大模型和大数据新范式，MyScale AI数据库正式开源](https://mp.weixin.qq.com/s/JvyKnEbdOSb1fTwhiQTO5A)

[https://github.com/myscale/myscaledb](https://github.com/myscale/myscaledb)

## 多模态RAG

[多模态RAG技术：从语义抽取到VLM](https://mp.weixin.qq.com/s/VyW6xXHt39o5FTD6JkpcuA)


# LLM模型融合

[https://github.com/arcee-ai/mergekit](https://github.com/arcee-ai/mergekit)

[SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling](https://arxiv.org/pdf/2312.15166)

[LLM 合并新思路：进化算法+零训练->新任务](https://mp.weixin.qq.com/s/eSWdLT0p5uyd32OOod5lKQ)

[Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/pdf/2403.13187)

[https://github.com/SakanaAI/evolutionary-model-merge](https://github.com/SakanaAI/evolutionary-model-merge)

# LLM auto-ml

## LLaMA-NAS

[用神经架构搜索给LLM瘦身，模型变小，准确度有时反而更高](https://mp.weixin.qq.com/s/_cKq4a3uM4r6s5P5s9mWaA)

[LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](https://arxiv.org/pdf/2405.18377)

## SELA

[MetaGPT开源SELA，用AI设计AI，效果超越OpenAI使用的AIDE](https://mp.weixin.qq.com/s/9m933xV95uU-cX3qOQLC6Q)

[SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning](https://arxiv.org/abs/2410.17238)

[https://github.com/geekan/MetaGPT/tree/main/metagpt/ext/sela](https://github.com/geekan/MetaGPT/tree/main/metagpt/ext/sela)

# prompt engineering

[万字长文总结提示词技巧！新加坡首届GPT-4提示工程大赛冠军最新分享](https://mp.weixin.qq.com/s/AWnQL3forAP-gB7e2ZEXdQ) 提出了CO-STAR框架

[高能干货分享，有关提示词工程的一切都在这份教程里](https://mp.weixin.qq.com/s/RaIzHtRIShIcpXydRE6kQg)

[https://github.com/NirDiamant/Prompt_Engineering](https://github.com/NirDiamant/Prompt_Engineering)

[吴恩达：四个步骤，让大模型变得更好](https://mp.weixin.qq.com/s/ackyt5d2kqdzMy0-Ma_ElA)

[Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](https://arxiv.org/pdf/2311.16452)

让gpt4生成cot和答案的模板

![self-generated-cot-template](../assets/self-generated-cot-template.png)

![medprompt](../assets/medprompt.png)

看着是借助GPT4+COT+RAG+投票

+ 拿一坨question得到他们的向量，并按照上图的模板让gpt生成COT和答案，人工判断，对的存进知识库里
+ 预测阶段：
    + 拿测试question的向量从知识库里查出5个最像(cos距离)的(q, cot, answer)作为context
    + 循环5次：
        shuffle测试question的答案选项，让LLM回答
    + 对生成的答案投票，选票数最多的

## APE

[还在人工炼丹？自动提示工程指南来了，还带从头实现](https://mp.weixin.qq.com/s/TxzkRUPhsiqtLhCyrIsQrQ)

[https://github.com/marshmellow77/automated-prompt-engineering-from-scratch](https://github.com/marshmellow77/automated-prompt-engineering-from-scratch)

## PAS

[还在死磕AI咒语？北大-百川搞了个自动提示工程系统PAS](https://mp.weixin.qq.com/s/2etnB3hbRtOCth1notqyBQ)

[PAS: Data-Efficient Plug-and-Play Prompt Augmentation System](https://arxiv.org/abs/2407.06027)

## ell

[OpenAI前研究者发布提示词工程框架ell，升级版LangChain，支持版本控制和多模态](https://mp.weixin.qq.com/s/LaNbu4bVrWLG3ueopFTj5g)

[https://github.com/MadcowD/ell](https://github.com/MadcowD/ell)

## 一些实践

[gpt-4.1官方](https://www.xiaohongshu.com/explore/67fe98c1000000001d01477b?app_platform=ios&app_version=8.79&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBD6_QqRzI70CObFc2ZlGE4OyHkXfEAyuQpTgGllA2rLs=&author_share=1&xhsshare=WeixinSession&shareRedId=ODY2NUg4NE82NzUyOTgwNjY0OTc1STdO&apptime=1744847767&share_id=c452a695f467408c903e213ccf9f8d41)

# 可解释AI

[XAI有什么用？探索LLM时代利用可解释性的10种策略](https://mp.weixin.qq.com/s/V35k4UJZPtJkAHqYlZiO1A)

[Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/pdf/2403.08946.pdf)

[https://github.com/JacksonWuxs/UsableXAI_LLM](https://github.com/JacksonWuxs/UsableXAI_LLM)

## 综述

[可解释性终极追问，什么才是第一性解释？20篇CCF-A+ICLR论文给你答案](https://mp.weixin.qq.com/s/vCAw0d2uZ_MnLrl5MT9OKA)


## TransformerLens

Neel Nanda（deepmind）的项目

[https://transformerlensorg.github.io/TransformerLens/](https://transformerlensorg.github.io/TransformerLens/)

## ecco

[https://www.eccox.io/](https://www.eccox.io/)

[https://jalammar.github.io/explaining-transformers/](https://jalammar.github.io/explaining-transformers/)

[https://jalammar.github.io/hidden-states/](https://jalammar.github.io/hidden-states/)

## interpretability in the wild

[Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small](https://arxiv.org/pdf/2211.00593)

[https://github.com/redwoodresearch/Easy-Transformer](https://github.com/redwoodresearch/Easy-Transformer)

## activation engineering

[Activation Addition: Steering Language Models Without Optimization](https://arxiv.org/pdf/2308.10248)

## representation engineering

[Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/pdf/2310.01405)


## transformer-debugger

[https://github.com/openai/transformer-debugger/tree/main](https://github.com/openai/transformer-debugger/tree/main)

## painter

[八问八答搞懂Transformer内部运作原理](https://mp.weixin.qq.com/s/5qhpfHfzOIdKsG_wtgTR4A)

[Transformer Layers as Painters](https://arxiv.org/pdf/2407.09298v1)

## transformer explainer

[黑匣子被打开了！能玩的Transformer可视化解释工具，本地运行GPT-2、还可实时推理](https://mp.weixin.qq.com/s/vLyIrRyoWYjhMN4gTRgA6g)

[TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative Models](https://arxiv.org/pdf/2408.04619)

[http://poloclub.github.io/transformer-explainer/](http://poloclub.github.io/transformer-explainer/)

[https://bbycroft.net/llm](https://bbycroft.net/llm)

## superposition

[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

## 3Blue1Brown

[用最直观的动画，讲解LLM如何存储事实，3Blue1Brown的这个视频又火了](https://mp.weixin.qq.com/s/PSMfQLBBQZyG2GwgzatqvA)

[https://www.youtube.com/watch?v=9-Jl0dxWQs8](https://www.youtube.com/watch?v=9-Jl0dxWQs8)

## Monitor

[他们掰开神经元，终于让大模型9.8大于9.11了：神秘创业公司，开源AI「洗脑」工具](https://mp.weixin.qq.com/s/pOOBY6cBZUn86xRtO12FtQ)

[https://transluce.org/observability-interface](https://transluce.org/observability-interface)

[https://monitor.transluce.org/dashboard/chat](https://monitor.transluce.org/dashboard/chat)

# Lifelong learning of LLM

(toread)

[Towards Lifelong Learning of Large Language Models: A Survey](https://arxiv.org/abs/2406.06391)

[https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm](https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm)


# 自省

[LLM 比之前预想的更像人类，竟也能「三省吾身」](https://mp.weixin.qq.com/s/Ri-Wdl_Xk5OxWF5IIJmrxg)

[Looking Inward: Language Models Can Learn About Themselves by Introspection](https://arxiv.org/pdf/2410.13787)

# 具身智能

[大模型走向物理世界，TeleAI 发布大模型驱动的具身智能综述，覆盖300篇文献](https://mp.weixin.qq.com/s/vzDhVsPmBqNT1iuZDnPjRw)

[Embodied-AI with large models: research and challenges](https://www.sciengine.com/SSI/doi/10.1360/SSI-2024-0076)

## ReKep

[李飞飞团队提出ReKep，让机器人具备空间智能，还能整合GPT-4o](https://mp.weixin.qq.com/s/AdyOPA6RhFIu5sjra5cW2Q)

[ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://rekep-robot.github.io/rekep.pdf)

[https://github.com/huangwl18/ReKep](https://github.com/huangwl18/ReKep)

## GR-2

[GR-2登场！ByteDance Research提出机器人大模型，具备世界建模和强大泛化能力](https://mp.weixin.qq.com/s/h-69PKoCkPtj4_sq9589Tw)

[GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation](https://arxiv.org/pdf/2410.06158)

[https://gr2-manipulation.github.io/](https://gr2-manipulation.github.io/)

## RDT-1B

[清华开源全球最大双臂机器人扩散大模型RDT，懂调酒能遛狗，登顶HF具身热榜](https://mp.weixin.qq.com/s/LtuK9bN45Bkm2uDCi3cCvA)

[RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/pdf/2410.07864)

## HIL-SERL

[强化学习训练一两个小时，100%自主完成任务：机器人ChatGPT时刻真来了？](https://mp.weixin.qq.com/s/5pVajhtp8KSFz4AnV8PVTQ)

[Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning](https://hil-serl.github.io/static/hil-serl-paper.pdf)

## Genesis

[历时2年，华人团队力作，震撼开源生成式物理引擎Genesis，可模拟世界万物](https://mp.weixin.qq.com/s/ioYK3YV07f9m0Iu-l6tLsg)

[https://github.com/Genesis-Embodied-AI/Genesis](https://github.com/Genesis-Embodied-AI/Genesis)

## language of motion

[李飞飞团队统一动作与语言，新的多模态模型不仅超懂指令，还能读懂隐含情绪](https://mp.weixin.qq.com/s/W8wS87YlW_z9rsDfnmtDLQ)

[The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion](https://arxiv.org/abs/2412.10523v1)

## 视觉空间智能

[李飞飞、谢赛宁等探索MLLM「视觉空间智能」，网友：2025有盼头了](https://mp.weixin.qq.com/s/Z4Kv92fukfNTyE1tSpJslA)

[Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/pdf/2412.14171v1)


# LLM+芯片设计

[登上Nature的AI芯片设计屡遭质疑，谷歌发文反击，Jeff Dean：质疑者连预训练都没做](https://mp.weixin.qq.com/s/u1NNmulcykGkgZjJb_A-UA)

[That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design](https://arxiv.org/pdf/2411.10053)


# 其他

## 安全性

[Anthropic安全负责人：在超级AI「毁灭」人类之前，我们可以做这些准备](https://mp.weixin.qq.com/s/nxD8qeCfG1tjfpvlJ6uacg)

[OpenAI最新53页论文：ChatGPT看人下菜碟，对“小美”比“小帅”更友好](https://mp.weixin.qq.com/s/NnLAjHuBPHa-aBoT6IV4Pg)

[First-Person Fairness in Chatbots](https://cdn.openai.com/papers/first-person-fairness-in-chatbots.pdf)

[翁荔B站分享原文：AI安全与“培养”之道](https://mp.weixin.qq.com/s/92QyZcwteFXaKfJk3GdTcQ)

## time-LLM

[谁说大象不能起舞! 重编程大语言模型实现跨模态交互的时序预测 | ICLR 2024](https://mp.weixin.qq.com/s/K04haPMcbKiS6OkCihXAqQ)

[Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://arxiv.org/pdf/2310.01728.pdf)

[https://github.com/KimMeen/Time-LLM](https://github.com/KimMeen/Time-LLM)

将**时序预测任务**转换成一个可以由 LLMs 有效解决的**语言任务**，成功激活了llm做**高精度时序推理**的能力。

+ 时序输入重编程
+ 提示做前缀


## 长尾

[A Systematic Review on Long-Tailed Learning](https://arxiv.org/pdf/2408.00483)

## 文本匹配效果还行的模型

大多是基于sentence-bert的，m3e-base在电商语料上试过，效果不错

[https://huggingface.co/moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)

[https://huggingface.co/shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)


## 本地知识库

[https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)

## llm应用合辑

+ ChatGPT聚合站：[https://hokex.com](https://hokex.com)
+ 游戏生成站：[https://latitude.io/](https://latitude.io/)
+ 家庭作业辅助站：[https://ontimeai.com/](https://ontimeai.com/)
+ 文字转语音站：[https://www.resemble.ai/](https://www.resemble.ai/)
+ 艺术作画站：[https://starryai.com/](https://starryai.com/)
+ logo制作站：[https://www.logoai.com/](https://www.logoai.com/)
+ ai写作站：[https://www.getconch.ai/](https://www.getconch.ai/)
+ 音乐制作站：[https://soundraw.io/](https://soundraw.io/)
+ 声音模拟站：[https://fakeyou.com/](https://fakeyou.com/)
+ 一句话生成一段视频：[https://runwayml.com/](https://runwayml.com/)
+ 文字转语音：[https://murf.ai/](https://runwayml.com/)

## swiftsage

[大语言模型在开放世界中的推理能力探索实践](https://mp.weixin.qq.com/s/LZ6lkTTOom-mbqV9IJ-OZg)

[SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/pdf/2305.17390.pdf)


## 达摩院大模型技术交流

[https://developer.aliyun.com/live/248332](https://developer.aliyun.com/live/248332)

ppt：[链接](https://pan.baidu.com/s/1tbckFpa8W8qJ5yRw9yvJ9A#list/path=%2F) 密码：5yyf



## 回译

通过单语数据提升 NMT 模型最高效的方法之一是回译（back-translation）。如果我们的目标是训练一个英语到德语的翻译模型，那么可以首先训练一个从德语到英语的翻译模型，并利用该模型翻译所有的单语德语数据。然后基于原始的英语到德语数据，再加上新生成的数据，我们就能训练一个英语到德语的最终模型。

[Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381v2.pdf)

## nan问题

[解决pytorch半精度amp训练nan问题](https://zhuanlan.zhihu.com/p/443166496)

## 时间序列

[LLM用于时序预测真的不行，连推理能力都没用到](https://mp.weixin.qq.com/s/C-N0tyQrEOoNoADtH_thTA)

[Are Language Models Actually Useful for Time Series Forecasting?](https://arxiv.org/pdf/2406.16964)

## 人脑

[MIT大牛新作震惊学界！AI「长脑子」了？LLM惊现「人类脑叶」结构并有数学代码分区](https://mp.weixin.qq.com/s/6lRAS8m4XqEfFFEP1Qa43A)

[The Geometry of Concepts: Sparse Autoencoder Feature Structure](https://arxiv.org/abs/2410.19750)

## LLM for 算法设计

[调研180多篇论文，这篇综述终于把大模型做算法设计理清了](https://mp.weixin.qq.com/s/hfDzIBcw5HTxtSzpS_694g)

[A Systematic Survey on Large Language Models for Algorithm Design](https://arxiv.org/abs/2410.14716)

## 深度生成模型课程

[教授何恺明在MIT的第二门课——《深度生成模型》，讲座PPT陆续已出](https://mp.weixin.qq.com/s/t8S7cXVAXDWhS0ypzMXiCg)

[https://mit-6s978.github.io/schedule.html](https://mit-6s978.github.io/schedule.html)

[https://mit-6s978.github.io/assets/pdfs/lec1_intro.pdf](https://mit-6s978.github.io/assets/pdfs/lec1_intro.pdf)

[https://mit-6s978.github.io/assets/pdfs/lec2_vae.pdf](https://mit-6s978.github.io/assets/pdfs/lec2_vae.pdf)

[https://mit-6s978.github.io/assets/pdfs/lec3_ar.pdf](https://mit-6s978.github.io/assets/pdfs/lec3_ar.pdf)

[https://mit-6s978.github.io/assets/pdfs/lec4_gan.pdf](https://mit-6s978.github.io/assets/pdfs/lec4_gan.pdf)

[https://mit-6s978.github.io/assets/pdfs/lec5_diffusion.pdf](https://mit-6s978.github.io/assets/pdfs/lec5_diffusion.pdf)

## 时序db

influxdb

[https://github.com/influxdata/influxdb](https://github.com/influxdata/influxdb)

[https://jasper-zhang1.gitbooks.io/influxdb/content/Introduction/getting_start.html](https://jasper-zhang1.gitbooks.io/influxdb/content/Introduction/getting_start.html)


## 其他

[原来，这些顶级大模型都是蒸馏的](https://mp.weixin.qq.com/s/GdwH7jxK2T_Vhus2ZvwQbw)

[Distillation Quantification for Large Language Models](https://arxiv.org/abs/2501.12619)

[小模型性能饱和、表现不佳，根源是因为Softmax?](https://mp.weixin.qq.com/s/bvv-frM8bKhkZiqOa9nqDA)

[Why do small language models underperform? Studying LM Saturation via the Softmax Bottleneck](https://arxiv.org/pdf/2404.07647.pdf)


[Ilya Sutskever的推荐清单](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)

[传说中Ilya Sutskever精选论文清单：AI领域40大论文完整版「破解」完成](https://mp.weixin.qq.com/s/7Bj_K1Vjp2FtfklfJsAMbQ)

[2024年大模型LLM还有哪些可研究的方向？](https://www.zhihu.com/question/637595961)

[Hinton万字访谈：用更大模型「预测下一个词」值得全力以赴](https://mp.weixin.qq.com/s/OydltjpVwsQ7hNBH6hq_Og)

[ChatGPT如何「思考」？心理学和神经科学破解AI大模型，Nature发文](https://mp.weixin.qq.com/s/4nO4DQE6Llfo3fiFSPSMhQ)

[适应多形态多任务，最强开源机器人学习系统「八爪鱼」诞生](https://mp.weixin.qq.com/s/HPTfOlw25F5JcvlY-Vy9Tw)

[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)

[LeCun新作：神经网络在实践中的灵活性到底有多大？](https://mp.weixin.qq.com/s/PjlXwwG3t5Fqp5MfrBVvBQ)

[Just How Flexible are Neural Networks in Practice?](https://arxiv.org/pdf/2406.11463)

[清华包揽最佳论文+时间检验奖，山大获荣誉提名，SIGIR 2024奖项出炉](https://mp.weixin.qq.com/s/Z2Mj7etx6KvYrSn8LhrJwg)

[https://zhuanlan.zhihu.com/p/654910335](https://zhuanlan.zhihu.com/p/654910335)

# 一些记录

## 打印模型参数量

[https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model](https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model)

```python

pytorch_total_params = sum(p.numel() for p in model.parameters())

pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

# Load the model
from transformers import BartForConditionalGeneration
from transformers import T5ForConditionalGeneration
def cal(model):
  pytorch_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
  return pytorch_total_trainable_params

model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
print("bart-base")
print(cal(model)) # 6L 139420416 139M

model = BartForConditionalGeneration.from_pretrained("facebook/bart-base")
print("bart-base")
print(cal(model)) # 12L 406291456 406M

model = T5ForConditionalGeneration.from_pretrained("t5-small")
print("t5-small")
print(cal(model)) # 6L 60506624 65M

model = T5ForConditionalGeneration.from_pretrained("t5-base")
print("t5-base")
print(cal(model)) # 12L 222903552 223M


model = T5ForConditionalGeneration.from_pretrained("t5-large")
print("t5-large")
print(cal(model)) # 24L 737668096 738M

```

## 往现有tokenizer里加一些特殊token

[https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer](https://stackoverflow.com/questions/69191305/how-to-add-new-special-token-to-the-tokenizer)

```python
num_added_toks = tokenizer.add_tokens(['[EOT]'], special_tokens=True) ##This line is updated
model.resize_token_embeddings(len(tokenizer))

###The tokenizer has to be saved if it has to be reused
tokenizer.save_pretrained(<output_dir>)
```

示例

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

print("Before")
print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]


special_tokens_dict = {'additional_special_tokens': ['[EOT]']}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
# model.resize_token_embeddings(len(tokenizer))  # --> Embedding(30523, 768)

tok_id = tokenizer.convert_tokens_to_ids('[EOT]')  # --> 30522

print("After")
print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']
print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]
```

## python的读写锁

一个写，多个并行读：[https://pypi.org/project/readerwriterlock/](https://pypi.org/project/readerwriterlock/)

## pytorch的显存泄露

[https://github.com/pytorch/pytorch/issues/13246#issuecomment-445770039](https://github.com/pytorch/pytorch/issues/13246#issuecomment-445770039)

## torch profiling

[https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)

可以拿这个来可视化：[https://ui.perfetto.dev/](https://ui.perfetto.dev/)

- 点击open trace file上传json文件
- timeline中有两个python进程，点击cuda kernel会出现箭头，方便找到是**哪个op调用了该kernel**
  - 靠上的python进程是host侧进程（主要是用户代码中调用的一些**API/pytorch op**，能比较方便能和训练代码对应上）
  - 靠下的python进程是device（gpu）侧进程（记录实际cuda kernel 的执行和一些性能相关的数据）

device timeline比较稀疏的情况下训练性能较差，GPU利用率较低，可能需要排查下训练代码是否有问题

## 显存泄露排查

[https://pytorch.ac.cn/docs/stable/torch_cuda_memory.html](https://pytorch.ac.cn/docs/stable/torch_cuda_memory.html)

[https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/)

[https://pytorch.org/blog/understanding-gpu-memory-2/](https://pytorch.org/blog/understanding-gpu-memory-2/)

检查显存

```python
# (c) Meta Platforms, Inc. and affiliates. 
# https://pytorch.org/blog/understanding-gpu-memory-1/
import logging
import socket
from datetime import datetime, timedelta

import torch

from torchvision import models

logging.basicConfig(
   format="%(levelname)s:%(asctime)s %(message)s",
   level=logging.INFO,
   datefmt="%Y-%m-%d %H:%M:%S",
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = "%b_%d_%H_%M_%S"

# Keep a max of 100,000 alloc/free events in the recorded history
# leading up to the snapshot.
MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000

def start_record_memory_history() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not recording memory history")
       return

   logger.info("Starting snapshot record_memory_history")
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

def stop_record_memory_history() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not recording memory history")
       return

   logger.info("Stopping snapshot record_memory_history")
   torch.cuda.memory._record_memory_history(enabled=None)

def export_memory_snapshot() -> None:
   if not torch.cuda.is_available():
       logger.info("CUDA unavailable. Not exporting memory snapshot")
       return

   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f"{host_name}_{timestamp}"

   try:
       logger.info(f"Saving snapshot to local file: {file_prefix}.pickle")
       torch.cuda.memory._dump_snapshot(f"{file_prefix}.pickle")
   except Exception as e:
       logger.error(f"Failed to capture memory snapshot {e}")
       return

# Simple Resnet50 example to demonstrate how to capture memory visuals.
def run_resnet50(num_iters=5, device="cuda:0"):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   # Start recording memory snapshot history
   start_record_memory_history()

   for _ in range(num_iters):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # Create the memory snapshot file
   export_memory_snapshot()

   # Stop recording memory snapshot history
   stop_record_memory_history()

if __name__ == "__main__":
    # Run the resnet50 model
    run_resnet50()
```

同时profile cpu和显存

```python
# (c) Meta Platforms, Inc. and affiliates. 
# https://pytorch.org/blog/understanding-gpu-memory-1/
import logging
import socket
from datetime import datetime, timedelta

import torch

from torch.autograd.profiler import record_function
from torchvision import models

logging.basicConfig(
   format="%(levelname)s:%(asctime)s %(message)s",
   level=logging.INFO,
   datefmt="%Y-%m-%d %H:%M:%S",
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = "%b_%d_%H_%M_%S"

def trace_handler(prof: torch.profiler.profile):
   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f"{host_name}_{timestamp}"

   # Construct the trace file.
   prof.export_chrome_trace(f"{file_prefix}.json.gz")

   # Construct the memory timeline file.
   prof.export_memory_timeline(f"{file_prefix}.html", device="cuda:0")

def run_resnet50(num_iters=5, device="cuda:0"):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       for _ in range(num_iters):
           prof.step()
           with record_function("## forward ##"):
               pred = model(inputs)

           with record_function("## backward ##"):
               loss_fn(pred, labels).backward()

           with record_function("## optimizer ##"):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

if __name__ == "__main__":
    # Warm up
    run_resnet50()
    # Run the resnet50 model
    run_resnet50()
```


## 各型号gpu对比

[https://zhuanlan.zhihu.com/p/441153412](https://zhuanlan.zhihu.com/p/441153412)

## 查看python的栈

```shell
pip install py-spy
py-spy dump --pid 1199

```

打出来：

```
Process 1199: /usr/bin/python3.10 -u torch_main.py
Python v3.10.14 (/usr/bin/python3.10)

Thread 0x7F62A2C43740 (active): "MainThread"
    _wait_for_tstate_lock (threading.py:1116)
    join (threading.py:1096)
    main (torch_main.py:776)
    <module> (torch_main.py:785)
Thread 0xAABBBCC (idle): "Thread-1"
    wait (threading.py:324)
    wait (threading.py:607)
    run (threading.py:1376)
    _bootstrap_inner (threading.py:1016)
    _bootstrap (threading.py:973)
Thread 0xAAAAA (idle): "Thread-3 (process)"
    wait (threading.py:320)
    get (queue.py:171)
    process (abase_writer.py:73)
    run (threading.py:953)
    _bootstrap_inner (threading.py:1016)
    _bootstrap (threading.py:973)
Thread 0xA992ACDA (idle): "Thread-4 (process)"
    wait (threading.py:320)
    get (queue.py:171)
    process (abase_writer.py:73)
    run (threading.py:953)
    _bootstrap_inner (threading.py:1016)
    _bootstrap (threading.py:973)
Thread 0xAFF11AA (active): "Thread-5 (read_file)"
    get_seq (ecom_seq_reader.py:200)
    read_file (torch_main.py:494)
    run (threading.py:953)
    _bootstrap_inner (threading.py:1016)
    _bootstrap (threading.py:973)
Thread 0x9922BCDA (idle): "Thread-6"
    wait (threading.py:324)
    wait (threading.py:607)
    run (tqdm/_monitor.py:60)
    _bootstrap_inner (threading.py:1016)
    _bootstrap (threading.py:973)
```


## 国内的huggingface模型下载地址

[https://hf-mirror.com/](https://hf-mirror.com/)

## 一些报错的解法

### flash-attention2

[https://github.com/Dao-AILab/flash-attention/issues/451](https://github.com/Dao-AILab/flash-attention/issues/451)

```shell
FLASH_ATTENTION_FORCE_BUILD=TRUE pip install flash-attn
```


# GPU机型对比

+ L20：[https://www.techpowerup.com/gpu-specs/l20.c4206](https://www.techpowerup.com/gpu-specs/l20.c4206)
  + T flops：
    + tf32：59.8
    + fp32：59.8 
    + bf16：119.5
    + fp16：119.5
+ A800-40G：[https://www.techpowerup.com/gpu-specs/a800-pcie-40-gb.c3964](https://www.techpowerup.com/gpu-specs/a800-pcie-40-gb.c3964)
  + T flops：
    + tf32：156
    + fp32：19.5
    + bf16：312
    + fp16：77.97


# 一些问题和经验

坍缩（稳定召回那k个item），attention score太集中了，低秩特征（泛化特征）容易导致这个问题

看attention score的分布，如果第一层偏向target item，但第二层可能就很平均了，这种可能就释放不出收益，应该是没学好

auc离线有收益，在线没收益：

+ reload正确性，warmup有没有报错
+ nn的分发效率，20min降到x min，压缩、解压耗时
+ 学习充分性：发现ab开得更久的时候，就看到收益了。。
  + 累积梯度太小的 不充分
  + 历史样本变多
  + 多epoch(参考快手 阿里的一些做法，例如reset emb等)
  + 加一些辅助loss，例如生成式、蒸馏
+ 出nan：bf16转化等问题，加一些grad clip，norm等