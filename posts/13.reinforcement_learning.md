## 强化学习

### RL历史

[漫画带你图解强化学习](https://mp.weixin.qq.com/s/MdtjTRGV813t6Mn3JES-pw)

[强化学习70年演进：从精确动态规划到基于模型](https://mp.weixin.qq.com/s/sIS9VvZ3yTtn6puJScuHig)

### RL概述

[强化学习之路——清华博士后解读83篇文献，万字长文总结](https://mp.weixin.qq.com/s/eQslEpJIT1negsbzmcORcA)


### MAB相关

#### multitask+mab

[多任务学习时转角遇到Bandit老虎机](https://mp.weixin.qq.com/s/8Ks1uayLw6nfKs4-boiMpA)

### RL基础

#### srl+drl

[大白话之《Shallow Updates Deep Reinforcement Learning》](https://mp.weixin.qq.com/s/TmucqZyIp9KKMJ3uv3CWGw)

[Shallow Updates Deep Reinforcement Learning](https://arxiv.org/abs/1705.07461)

### 模仿学习

[今晚，NeurIPS 2019 Spotlight论文分享：不完备专家演示下的模仿学习](https://mp.weixin.qq.com/s/8gV8MzEOGBuu5jLPyg97OQ)

[Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement](https://arxiv.org/abs/1910.04417)

[视频 \| NeurIPS 2019分享：清华大学孙富春组提出全新模仿学习理论](https://mp.weixin.qq.com/s/XXO4hkjJkcZ5sTVVKWghEw)

ppt：[https://pan.baidu.com/s/1Zj59PAe4hYhDDh5zd4gWZg](https://pan.baidu.com/s/1Zj59PAe4hYhDDh5zd4gWZg)


### 推荐+强化学习

[Deep Reinforcement Learning in Large Discrete Action Spaces](https://arxiv.org/pdf/1512.07679.pdf)

### 2019强化学习论文

[2019年深度强化学习十大必读论文！DeepMind、OpenAI等上榜](https://mp.weixin.qq.com/s/vUIVDkxiQ5c9JhPvs6Pyng)

### ICLR2020强化学习相关

[ICLR 2020 高质量强化学习论文汇总](https://mp.weixin.qq.com/s/l8TP_cFMWFKebBowgJanSQ)

### HER

[“事后诸葛亮”经验池：轻松解决强化学习最棘手问题之一：稀疏奖励](https://mp.weixin.qq.com/s/BYgIk19vYPBVqXoLEsilRg)

本文介绍了一个“事后诸葛亮”的经验池机制，简称为HER，它可以很好地应用于稀疏奖励和二分奖励的问题中，不需要复杂的奖励函数工程设计。强化学习问题中最棘手的问题之一就是稀疏奖励。本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于所有的Off-Policy算法中。

Hindsight意为"事后"，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。

### 多智能体RL

#### LIIR

[LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning](https://papers.nips.cc/paper/8691-liir-learning-individual-intrinsic-reward-in-multi-agent-reinforcement-learning)


nips2019快手跟腾讯 AI Lab 和 Robotics X 合作，它希望智能体能快速学会利用自己所观测的信息来相互配合。比如说在星际争霸中，我们发现确实会产生多智能体合作的现象，模型会让一些防高血厚的单位去抗对方的输出，己方输出高的单元会躲到后方攻击。

虽然把所有的 agents 看成是一个 agent，理论上也可以学到最终的配合效果，但是效率会非常低，不具有可扩展性。我们的方法通过一种 intrinsic reward 的机制兼顾了可扩展性和效率，通过鼓励每个 agent 按照单体利益最大化的原则去学习自己的 policy，然后这种 intrinsic reward 的影响会越来越小最后快速达到学到整体最后的方案。

### AlphaStar

[Nature：闭关修炼9个月，AlphaStar达成完全体，三种族齐上宗师，碾压99.8%活跃玩家](https://mp.weixin.qq.com/s/6-iZv40wb0Zyfwmo_D6ibQ)

### TVT

[离人类更近一步！DeepMind最新Nature论文：AI会“回忆”，掌握调取记忆新姿势](https://mp.weixin.qq.com/s/onnV3Jc9xhyQSOB12lCDxw)

[Optimizing agent behavior over long time scales by transporting value](https://arxiv.org/abs/1810.06721)

[https://www.nature.com/articles/s41467-019-13073-w](https://www.nature.com/articles/s41467-019-13073-w)

[https://github.com/deepmind/deepmind-research/tree/master/tvt](https://github.com/deepmind/deepmind-research/tree/master/tvt)

### upside-down rl

[超有趣！LSTM之父团队最新力作：将强化学习“颠倒”过来](https://mp.weixin.qq.com/s/eZohnFXxl-hnrac6XVZr3g)

[Reinforcement Learning Upside Down: Don’t Predict Rewards - Just Map Them to Actions](https://arxiv.org/pdf/1912.02875.pdf)

RL算法要么使用价值函数预测奖励，要么使用策略搜索使其最大化。该研究提出一种替代方法：颠倒RL(Upside-Down RL)，主要使用监督学习来解决RL问题。

标准RL预测奖励，而UDRL使用奖励作为任务定义的输入，以及时间范围的表示和历史数据以及可期的未来数据的其他可计算函数。

UDRL学会将这些输入观察结果解释为命令，并根据过去(可能是偶然的)经历通过SL将它们映射为行为(或行为概率)。UDRL一般通过输入命令来实现高奖励或其他目标，例如：在一定时间内获得大量奖励！另一篇关于UDRL的首个实验的论文(Training agents with upside-down reinforcement learning)表明，UDRL在某些具有挑战性的RL问题上可以胜过传统的baseline算法。

我们还提出了一种相关的简单而且通用的方法来教机器人模仿人类。首先，对人模仿机器人当前的行为进行录像，然后让机器人通过监督学习将视频(作为输入命令)映射到这些行为上，然后让其概括和模仿先前未知的人类行为。这种Imitate-Imitator的概念实际上可以解释为什么生物进化导致父母会模仿婴儿的咿呀学语。

### 游戏+RL

#### 游戏AI历史(alphago系列)

[https://deepmind.com/research/case-studies/alphago-the-story-so-far](https://deepmind.com/research/case-studies/alphago-the-story-so-far)

[从α到μ：DeepMind棋盘游戏AI进化史](https://mp.weixin.qq.com/s/IcaxjdDLjihCK-nKBlJVWg)

alphago：[Mastering the game of Go with deep neural networks and tree search](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)

alphago zero：[Mastering the game of Go without Human Knowledge](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)

alpha zero: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815.pdf)

mu zero: [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://science.sciencemag.org/content/sci/362/6419/1140.full.pdf)

##### alphago

reward：1胜，-1负，中间状态0

3个网络：

+ SL：学习棋谱的P
+ RL：自己和自己下，学习一个P
+ V：学习s的长期收益
+ fast roolout：用一个简单的线性模型学习棋谱的P


[蒙特卡罗树搜索+深度学习 -- AlphaGo原版论文阅读笔记](https://blog.csdn.net/dinosoft/article/details/50893291)

mcts：

选Q+u最大的a，

首先模拟n次，

+ N(s,a)：对于第i次，如果经过当前的(s,a)，那么+1
+ Q(s,a)：对于第i次，如果走到叶子时经过了当前的(s,a)，那么把V(叶子)加上，最后除以N(s,a)
+ V(叶子)：(1-lambda) * value network的输出+lambda * fastrollout走到结束时的reward
+ u(s,a)：与P(s,a)/(1+N(s,a))成正比
+ P(s,a)：policy network的输出

最开始还没expand时，Q是0，那SL的P就是prior probabilities。P还能起到减少搜索宽度的作用，普通点得分很低。比较难被select到。有趣的结论是，比较得出这里用SL比RL的要好！！模仿人类走棋的SL结果更适合MCTS搜索，因为人类选择的是 a diverse beam of promising moves。而RL的学的是最优的下法（whereas RL optimizes for the single best move）。所以人类在这一点暂时获胜！不过另一方面，RL学出来的value networks在评估方面效果好。所以各有所长。**搜索次数N一多会扣分， 鼓励exploration其他分支。**

##### alphago zero

模型输出p和v，训练的时候通过mcts去选action。loss就是p的交叉熵+v的rmse

##### alpha zero

主要是特征改了一下，使得可以适用于各种棋,loss没变

##### muzero

模型加了个r，loss里加了个r，planning比较复杂，需要考虑r V P，还有次数N

#### 绝悟

[不服SOLO：腾讯绝悟AI击败王者荣耀顶尖职业玩家，论文入选AAAI，未来将开源](https://mp.weixin.qq.com/s/_qbzHG1IEOvcCpvlAKP0Dw)

[Mastering Complex Control in MOBA Games with Deep Reinforcement Learning](https://arxiv.org/abs/1912.09729)

以 MOBA 手游《王者荣耀》中的 1v1 游戏为例，其状态和所涉动作的数量级分别可达 10^600 和 10^18000，而围棋中相应的数字则为 10^170 和 10^360

为了实现有效且高效的训练，本文提出了一系列创新的算法策略：
 
+ 目标注意力机制；用于帮助 AI 在 MOBA 战斗中选择目标。
+ LSTM；为了学习英雄的技能释放组合，以便 AI 在序列决策中，快速输出大量伤害。
+ 动作依赖关系的解耦；用于构建多标签近端策略优化（PPO）目标。
+ 动作掩码；这是一种基于游戏知识的剪枝方法，为了引导强化学习过程中的探索而开发。
+ dual-clip PPO；这是 PPO 算法的一种改进版本，使用它是为了确保使用大和有偏差的数据批进行训练时的收敛性。

### RL+因果

[华为诺亚ICLR 2020满分论文：基于强化学习的因果发现算法](https://mp.weixin.qq.com/s/mCOSvEwTNoX-x3PphLUjhw)

### RL+Active learning

[Ready Policy One: World Building Through Active Learning](https://arxiv.org/pdf/2002.02693.pdf)

基于模型的强化学习（Model-Based Reinforcement Learning，MBRL）为样本高效学习提供了一个有前途的方向，通常可以实现连续控制任务（continuous control task）的 SOTA 结果。然而，许多现有的 MBRL 方法依赖于贪婪策略（greedy policy）与探索启发法的结合，甚至那些利用原则试探索奖金（exploration bonus）的方法也能够以特定方式构建双重目标。

### ES

[Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/pdf/1703.03864.pdf)

在本文中，研究者介绍了 Ready Policy One（RP1），这是一种将 MBRL 视为主动学习问题的框架。研究者的目标是在尽可能少样本中改进世界模型（world model）。RP1 通过利用混合目标函数来实现这一目标，该函数在优化过程中的适应性调整至关重要，从而使算法可以权衡不同学习阶段的奖励与探索。此外，一旦拥有足够丰富的轨迹批（trajectory batch）来改进模型，研究者会引入一种原则式机制（principled mechanism）来终止样本收集。

