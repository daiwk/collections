参考[https://github.com/changyeyu/LLM-RL-Visualized](https://github.com/changyeyu/LLM-RL-Visualized)，对应的书《大模型算法：强化学习、微调与对齐》）

# 概述

llm训练时的teacher forcing机制：生成第i个token的输入：
+ 推理时：是模型生成的第0到第i-1的序列
+ 训练时：训练数据中实际的token序列

例如输入的是12345，输入1的时候生成了8，那要预测3的时候，输入的就是18；而训练时输入的还是12

# SFT

## lora

+ lora的核心思想：微调前后模型的参数差异具有低秩性，所以可以用A和B两个矩阵来表示，这两个矩阵的秩是$$lora_{rank}$$
+ A一般是随机初始化，**B用0初始化**或者用很小的随机数初始化，为了保证在训练初期，lora不会对原始输出造成太大扰动
+ 参数$$lora_{alpha}$$用于缩放Lora输出，即$$W_{merge}=W+A*B*lora_{alpha}/lora_{rank}$$
+ 学习率一般开始的时候比较小，后面可以再调整，不过如果lora_alpha较大，可以适当减小学习率
+ 在推理的时候，可以先进行融合，其实就是算好$$W_{merge}$$，推理的时候直接用

## prefix-tuning

大概是在prompt的最前面加若干个虚拟token，然后这部分有自己的参数（最开始的emb，还有后面的k和v，这里的k和v都是2个mlp，先映射到小一点的d'，再映射到d），总的可训练参数量参考：

![](../assets/prefix-tuning-params.png)

## sft loss与logsoftmax

+ LM head：从hidden dim映射到vocab size
+ 计算时label是shift得到的(左移一位，因为要预测next item)
+ 正常是先过LM head，算出logit矩阵，然后每个位置对所有词表里的词算softmax得到p，再去算交叉熵（只有label那个字有log(p)），所以就是log(softmax(x))
+ prompt没有loss，只有response有loss，假设response长度为k，假设第i个位置对应的label是j，那交叉熵是$$-1/k\sum_{i=1}^{k} log(p_ij)$$

对于response里的第i个词来说，词表大小为$$n$$，$$x_{max}=max(x_0,x_1,...,x_n)$$，下式第三行就是第二行分子分母同时乘$$e^{-x_{max}}$$

XXX
\begin{aligned}
\operatorname{LogSoftmax}\left(x_i\right) & =\log \left(\operatorname{Softmax}\left(x_i\right)\right) \\
& =\log \left(\mathrm{e}^{x_i} / \sum_{j=0}^n \mathrm{e}^{x_j}\right) \\
& =\log \left(\mathrm{e}^{\left(x_i-x_{\text {max }}\right)} / \sum_{j=0}^n \mathrm{e}^{\left(x_j-x_{\text {max }}\right)}\right) \\
& =\left(x_i-x_{\text {max }}\right)-\log \left(\sum_{j=0}^n \mathrm{e}^{\left(x_j-x_{\text {max }}\right)}\right)
\end{aligned}
XXX

这个简化有2个好处：

+ 减少计算量：例如除法、少量对数
+ 数值稳定：上式是log-sum-exp技巧，减掉max可以避免溢出

另外还有一个常用名词logprobs（log probabilities），即logsoftmax的结果，因为softmax是0到1，所以logprobs取值范围是负无穷到0

## 如何减小SFT引入的幻觉

sft阶段引入的新知识越多，幻觉发生率也越高

模型主要通过预训练来获取知识，sft主要是用来教会模型如何更有效地利用这些知识。

所以，新知识应该在预训练阶段或者基于预训练模型继续预训练（CPT），再用sft进行优化

另外，sft选哪个版本的开源模型作为基础模型也有讲究：

+ 需要加一堆新知识进行深度微调时，选base版本，因为base版本未指令微调，更有通用性
+ 任务与现有指令任务类似（通用对话、开放域问答等），且指令数据不多，选指令微调版本（-instruct或者-chat）

# DPO

DPO（direct preference optimization）是监督学习，不是强化学习，loss如下

XXX
\begin{aligned}
& \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right) \\
& =-\mathbb{E}_{\left(x, y_{\mathrm{w}}, y_1\right) \sim \mathcal{D}}\left[\log \sigma\left(r\left(x, y_{\mathrm{w}}\right)-r\left(x, y_1\right)\right)\right] \\
& =-\mathbb{E}_{\left(x, y_{\mathrm{w}}, y_1\right) \sim \mathcal{D}}[\log \sigma(\underbrace{\beta \log \frac{\pi_\theta\left(y_{\mathrm{w}} \mid x\right)}{\pi_{\text {ref }}\left(y_{\mathrm{w}} \mid x\right)}}_{\text {优质回答的隐式奖励}\uparrow }-\underbrace{\beta \log \frac{\pi_\theta\left(y_1 \mid x\right)}{\pi_{\text {ref }}\left(y_1 \mid x\right)}}_{\text {劣质回答的隐式奖励 } \downarrow})]
\end{aligned}
XXX

其中，

+ $$r$$是策略模型和参考模型的比值，即一个隐式的奖励模型，$$r_{\theta}(x,y)=\beta log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$$
+ 策略模型是复制的sft模型，参考模型一般也是复制sft模型，有些时候可以选一个更复杂的模型，但要注意两者的kl散度和训练数据分布不要差太多

## DPO流程

![](../assets/dpo-training.png)

+ concat：将$$x$$和$$y_w$$与$$y_l$$分别concat，作为策略模型和参考模型的输入（实际会将三者concat一起）
+ gather：算完logit和softmax后，再把y部分的拿出来，算出隐式奖励，再算Loss

## beta作用

在RLHF中，$$\beta$$和最终奖励$$=r(x,y)$$、最终奖励$$=r_{\phi}(x,y)$$的关系如下，

XXX
r(x,y)=r_{\phi}(x,y)-\beta (log \pi _{\theta}(y|x)-log \pi _{ref}(y|x))
XXX

+ $$\beta$$很小时，策略模型和偏好数据分布更接近，
+ $$\beta$$很大时，策略模型和参考模型更接近

在DPO中，

+ $$\beta$$很小时，KL惩罚力度很小，策略模型有更多自由度去探索，策略模型和偏好数据分布更接近
+ $$\beta$$很大时，KL惩罚力度很大，策略模型始终要兼顾和参考模型的距离，策略模型和参考模型更接近

## DPO的效果

对无害性有很大提升，对基础能力基本无影响（MMLU等）

## DPO的梯度

XXX
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\beta \mathbb{E}_{\left(x, y_{\mathrm{w}}, y_l\right) \sim \mathcal{D}}[\underbrace{\sigma\left(r_\theta\left(x, y_l\right)-r_\theta\left(x, y_{\mathrm{w}}\right)\right)}_{\text {动态系数 }}[\underbrace{\nabla_\theta \log \pi_\theta\left(y_{\mathrm{w}} \mid x\right)}_{\text {优质回答的概率 } \uparrow}-\underbrace{\nabla_\theta \log \pi_\theta\left(y_l \mid x\right)}_{\text {劣质回答的概率 } \downarrow}]]
XXX

+ 增加优势回答出现的概率，降低劣势回答出现的概率
+ 动态系数：优势答案和劣势答案的隐式奖励差异（注意$$r_{\theta}(x,y)=\beta log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$$），如下图：
    + $$r_{\theta}(x,y_l)<r_{\theta}(x,y_w))$$时：隐式奖励模型能正确地给优质回答较高评分，图中左侧，说明已经学得比较好了，梯度比较小
    + $$r_{\theta}(x,y_l)>r_{\theta}(x,y_w))$$时：隐式奖励模型不太行，图中右侧，要加强梯度

![](../assets/dpo-grad.png)

解释：

![](../assets/dpo-dynamic-coeff.png)

# Training-free的方法

## 提示词工程

prompt设计原则：

+ 递进式尝试：先试zero-shot，效果不行再试few-shot，还是不行再尝试sft
+ 灵活运用系统消息：引导模型的行为、风格、角色等
+ 明确指令并用分隔符：在开头用冒号、三重引号、xml标签、换行等进行分隔
+ 明确期望的输出格式并提供输出示例：详细描述具体输出格式要求
+ 避免模糊描述：避免“较长”“较短”，要用具体数字，如5个要点、3段话
+ 同时提供禁止和替代指令：告诉模型不要做什么，还有对应的替代方案
+ 提供必要的上下文和角色信息：明确模型需要扮演的角色
+ 分解复杂任务：复杂任务可以分进行意图分类或者逐步拆解任务
+ 引导模型先思考后回答：明确指示模型在回答前先进行详细推理
+ 使用参考文本并基于参考作答：提供可信的参考文本，要求模型基于此作答，可以进一步要求模型标明引用来源或参考文献
+ 动态总结上下文：对话长度超出模型上下文窗口限制时，可以定期总结或筛选最相关信息

## CoT

![](../assets/cot-xot-etc.png)

+ 思维链：CoT，Chain of Thoughts，Google提出，[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)
+ 思维树：ToT，Tree of Thoughts，[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601)，普林斯顿提出，允许考虑多个不同推理路径并进行自我评估，以决定下一步行动，并在必要时前瞻或回溯
+ 思维图：GoT，Graph of Thoughts，[Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/pdf/2308.09687)，苏黎世联邦理工学院提出，能将模型生成的信息建模为任意图结构，其中信息单元（思维）作为顶点，边对应这些顶点间的依赖关系，能有效组合任意的信息单元
+ 自我一致性CoT：Self-consistency CoT，[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171)，Google提出，先生成多样化的推理路径集，然后从多个路径中选出最一致的答案
+ Auto-CoT：[Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/pdf/2210.03493)，自动构建CoT样本，先将问题聚类，再示例采样，每个类中选择一个具有代表性的问题，并用简单的Zero-shot CoT生成相应思维链
+ 思维记忆：Memory of Thought，MoT，[MoT: Memory-of-Thought Enables ChatGPT to Self-Improve](https://arxiv.org/pdf/2305.05181)，复旦提出，不用更新参数，预思考：在未标注数据集上思考并将思维保存为外部记忆；推理：回忆相关记忆以帮助推理并回答给定问题
+ XoT：Everything of Thoughts，[Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation](https://arxiv.org/pdf/2311.04254)，RL+MCTS，结合轻量级的策略和价值网络，对特定任务进行训练以进行思维搜索，并随后泛化到新问题。训练完后，用轻量的策略和价值网络通过MCTS高效地思维搜索，再基于MCTS和LLM写作的框架进行思维修正，进一步提升思维质量。能最大程度减少LLM调用次数

如何自动选择few-shot示例？langchain等框架中实现了example selector

+ 基于相似性
+ 基于n-gram重叠分数
+ 基于MMR(最大边际相关性)：在保证相关性的基础上引入多样性

多模态CoT：

+ [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/pdf/2302.00923)：交大，两阶段框架，推理理由生成阶段：根据文本prompt+图像生成推理理由；答案推断阶段：根据文本prompt+图片+推理理由推断出最终答案
+ [Improve Vision Language Model Chain-of-thought Reasoning](https://arxiv.org/pdf/2410.16198)，CMU，基于GPT-4o提取推理路径，对vlm进行微调以增强推理能力，再构造正负对加dpo提升效果

## 解码策略

以transformers.generate参数为例

+ beam search：每次保留**累积概率**的topk，最终有k个路径，再选出最高的那个
+ diverse beam search：[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424)，分成多个组（num_beam_groups），每个组内beam search，组间通过多样性惩罚降低生成结果的相似性
+ 受限beam search：[Guided Open Vocabulary Image Captioning with Constrained Beam Search](https://arxiv.org/pdf/1612.00576)，constraints参数指定限制条件，force_words_ids强制结果包含特定词/词组
+ temperature：**控制不同词的概率差距**。当调小时，x/T变大，然后经过exp会指数放大，这样高logit的会变得更巨大，词的差距会被拉大，所以模型的输出会更稳定，输出结果更确定；反之不同词的差距更小，输出更多样
+ 多项式采样：multinomial sampling，不是直接选概率最大的，而是按概率大小**随机采样**
    + top-k：只在概率前k的词中进行采样
    + top-p：累加概率大于p后截断，只对这些token采样，**控制长尾词的概率阈值**。top cummulative probability，是一个门槛，即从第一名往下数，累加后的概率大于p时，把后面的词扔掉，剩下的词再softmax，并去采样。调低会去掉长尾，输出稳定；调高会放低门槛，输出多样

==> temperature和top-p虽然原理不同，但均是越大越多样，适用创作等场景；越小越稳定，适用于代码、数学

其他解码策略：

+ contrastive search：对比搜索，[A Contrastive Framework for Neural Text Generation](https://arxiv.org/pdf/2202.06417)，在每个解码步骤中，从模型预测的最可能候选集中选择输出，以增强生成文本与人类编写前缀之间的语义连贯性，保持生成文本的词元相似度矩阵稀疏性，避免输出退化问题。
+ speculative decoding：投机解码，引入一个较小模型，能一次性预生成多个候选token，主模型也并行验证这些候选token。deepseek v3就整合了投机采样和多token预测(MTP)，实现了每次预测2个token
+ lookahead decoding：前瞻解码，[Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING](https://arxiv.org/pdf/2402.02057)，并行生成和验证多个n-gram，基于Jacobi迭代方法，打破自回归解码的顺序依赖，无需辅助模型或额外的数据存储，加速推理。
+ decoding by contrasting layers(DoLa)：对比解码的一种，[DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/pdf/2309.03883)，对比模型中较早层与最终层的logits的差异，DoLa能放大transformer层中特定部分局部化的事实知识，从而增强生成文本的真实性，旨在提高输出的事实准确性并减少幻觉现象。

控制生成细节的参数

+ max_length：包括prompt的总长度
+ max_new_tokens：不包括prompt的总长度
+ repetition_penalty：大于1增加重复惩罚，减少重复token生成
+ length_penalty：beam类方法的长度惩罚，大于0时鼓励生成较长序列，反之鼓励较短序列
+ force_word_ids/bad_words_ids：必须生成的token id列表/不能生成的token id列表
+ output_logits/output_scores：输出logits/scores

## RAG

主要的点在于如何对文档进行分片（chunk），然后就是正常的向量召回流程了。例如Anthoripic的[Contextual RAG](https://www.anthropic.com/news/contextual-retrieval)会给每个chunk加一个上下文，把这段context放到chunk前面，然后再去做embedding。类似如下prompt：

```python
<document>…全文…</document>

<chunk>…分片…</chunk>

指令：“给一个简短、精炼的 context，用于把这个 chunk 放到整篇文档语境里，提升检索；只输出这段 context，不要输出其它内容。”
```

## 工具调用

详见：[https://www.daiwk.net/5.llm_agentic#gong-ju-diao-yong](https://www.daiwk.net/5.llm_agentic#gong-ju-diao-yong)