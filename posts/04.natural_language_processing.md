## 自然语言处理

### nlp综述

[https://github.com/PengboLiu/NLP-Papers](https://github.com/PengboLiu/NLP-Papers)

### LSTM相关

[超生动图解LSTM和GRU，一文读懂循环神经网络！](https://mp.weixin.qq.com/s/vVDAB2U7478yOXUT9ByjFw)

### fasttext&word2vec

[fasttext源码解析](https://my.oschina.net/u/3800567/blog/2877570)

+ ```Dictionary::readWord```：空格分割，一次读出来一个word
+ ```Dictionary::add```：每个word求个hash，加进词典时，id就是从0开始的序号，同时记录一下词频
+ ```Dictionary::threshold```：按词频排序，扔掉低频词
+ ```Dictionary::initNgrams```：每个词，加上前缀BOW（<）和后缀（>），然后先扔进这个词的subwords里，然后再调用``` Dictionary::computeSubwords```把这个词的ngrams也扔进它的subwords里

整个词表，是word数+bucket这么大，其中bucket表示可容纳的subwords和wordNgrams的数量，默认200w


### 分词

[【Subword】深入理解NLP Subword算法：BPE、WordPiece、ULM](https://mp.weixin.qq.com/s/U9F8G-OUCb9kunTk_tZYFw)

### 语法解析

EMNLP 2019最佳论文

[Specializing Word Embeddings（for Parsing）by Information Bottleneck](http://cs.jhu.edu/~jason/papers/li+eisner.emnlp19.pdf)

预训练词向量，如ELMo和BERT包括了丰富的句法和语义信息，使这些模型能够在各种任务上达到 SOTA 表现。在本文中，研究者则提出了一个非常快速的变分信息瓶颈方法，能够用非线性的方式压缩这些嵌入，仅保留能够帮助句法解析器的信息。研究者将每个词嵌入压缩成一个离散标签，或者一个连续向量。在离散的模式下，压缩的离散标签可以组成一种替代标签集。通过实验可以说明，这种标签集能够捕捉大部分传统 POS 标签标注的信息，而且这种标签序列在语法解析的过程中更为精确（在标签质量相似的情况下）。而在连续模式中，研究者通过实验说明，适当地压缩词嵌入可以在 8 种语言中产生更精确的语法解析器。这比简单的降维方法要好。

### self-attention

[从三大顶会论文看百变Self-Attention](https://mp.weixin.qq.com/s/R9FoceRsPB3ceqKpnYPvbQ)

[包学包会，这些动图和代码让你一次读懂「自注意力」](https://mp.weixin.qq.com/s/Z0--eLLiFwfSuMvnddKGPQ)

[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)

### 文本匹配

[谈谈文本匹配和多轮检索](https://mp.weixin.qq.com/s/uoEX0TjJZmyquNch5Wikvg)

[搜索中的深度匹配模型](https://mp.weixin.qq.com/s/lcyw_kHNxPB-DUfNzTaj5g)

### 机器翻译

[102个模型、40个数据集，这是你需要了解的机器翻译SOTA论文](https://mp.weixin.qq.com/s/fvbL-mms0bKaF_FOAQaPjw)

1. Transformer Big + BT：回译

通过单语数据提升 NMT 模型最高效的方法之一是回译（back-translation）。如果我们的目标是训练一个英语到德语的翻译模型，那么可以首先训练一个从德语到英语的翻译模型，并利用该模型翻译所有的单语德语数据。然后基于原始的英语到德语数据，再加上新生成的数据，我们就能训练一个英语到德语的最终模型。

[Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381v2.pdf)

2. MASS：预训练

[MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450v5)

MASS 采用了编码器-解码器框架，并尝试在给定部分句子的情况下修复整个句子。如下所示为 MASS 的框架图，其输入句子包含了一些连续的 Token，并且中间会带有一些连续的 Mask，模型的任务是预测出被 Mask 掉的词是什么。相比 BERT 只有编码器，MASS 联合训练编码器与解码器，能获得更适合机器翻译的表征能力。

这里有：[https://daiwk.github.io/posts/nlp-paddle-lark.html#massmicrosoft](https://daiwk.github.io/posts/nlp-paddle-lark.html#massmicrosoft)

### nlp标准&数据集

#### 中文glue

[ChineseGLUE：为中文NLP模型定制的自然语言理解基准](https://mp.weixin.qq.com/s/14XQqFcLG1wMyB2tMABsCA)

[超30亿中文数据首发！首个专为中文NLP打造的GLUE基准发布](https://mp.weixin.qq.com/s/9yxYErAMy9o3BOEDzsgvPw)

[https://github.com/CLUEbenchmark/CLUE](https://github.com/CLUEbenchmark/CLUE)

[https://www.cluebenchmarks.com/](https://www.cluebenchmarks.com/)

[https://github.com/brightmart/nlp_chinese_corpus](https://github.com/brightmart/nlp_chinese_corpus)

[http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews](http://thuctc.thunlp.org/#%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86THUCNews)

#### 中文阅读理解数据集

[首个中文多项选择阅读理解数据集：BERT最好成绩只有68%，86%问题需要先验知识](https://mp.weixin.qq.com/s/Jr8ALNxD1Uw7O8sQdFpX_g)

[Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension](https://arxiv.org/abs/1904.09679)

[https://github.com/nlpdata/c3](https://github.com/nlpdata/c3)

#### 物理常识推理任务数据集

[PIQA: Reasoning about Physical Commonsense in Natural Language](https://arxiv.org/pdf/1911.11641.pdf)

「在不使用刷子涂眼影的情况下，我应该用棉签还是牙签？」类似这种需要物理世界常识的问题对现今的自然语言理解系统提出了挑战。虽然最近的预训练模型 (如 BERT) 在更抽象的如新闻文章和百科词条这种具有丰富文本信息的领域问答方面取得了进展，但在更现实的领域，由于报导的偏差，文本本质上是有限的，类似于「用牙签涂眼影是一个坏主意」这样的事实很少得到直接报道。人工智能系统能够在不经历物理世界的情况下可靠地回答物理常识问题吗？是否能够捕获有关日常物品的常识知识，包括它们的物理特性、承受能力以及如何操纵它们。

在本文中，研究者介绍了一个关于物理常识推理任务和相应的基准数据集 PIQA（Physical Interaction：Question Answering）进行评估。虽然人类应对这一数据集很容易 (95% 的准确率)，但是大型的预训模型很难 (77%)。作者分析了现有模型所缺乏的知识为未来的研究提供了重要的机遇。

#### 常识推理数据集WinoGrande

[WinoGrande: An Adversarial Winograd Schema Challenge at Scale](https://arxiv.org/abs/1907.10641)

研究者提出了 WINOGRANDE，一个有着 44k 个问题的大规模数据集。该数据集在规模和难度上较之前的数据集更大。该数据集的构建包括两个步骤：首先使用众包的方式设计问题，然后使用一个新的 AFLITE 算法缩减系统偏见（systematic bias），使得人类可以察觉到的词汇联想转换成机器可以检测到的嵌入联想（embedding association）。现在最好的 SOTA 模型可以达到的性能是 59.4 – 79.1%，比人脸性能水平（94%）低 15-35%（绝对值）。这种性能波动取决于训练数据量（2% 到 100%）。

本论文荣获了 AAAI 2020 最佳论文奖，文中提出的 WINOGRANDE 是一个很好的迁移学习资源；但同时也说明我们现在高估了模型的常识推理的能力。研究者希望通过这项研究能够让学界重视减少算法的偏见。

### 阅读理解

#### DCMN+

[AAAI 2020 \| 云从科技&上交大提出 DCMN+ 模型，在多项阅读理解数据集上成绩领先](https://mp.weixin.qq.com/s/5I8RKEnm6y8egf7X1mXNFA)

[DCMN+: Dual Co-Matching Network for Multi-choice Reading Comprehension](https://arxiv.org/pdf/1908.11511.pdf)

### 相关性模型

[Yahoo相关性模型总结](https://mp.weixin.qq.com/s/GPKc1ZJGFSixvOhzhYtYBA)

### ULMFiT

[ULMFiT面向文本分类的通用语言模型微调](https://zhuanlan.zhihu.com/p/55295243)

[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)

中文ulmfit：[https://github.com/bigboNed3/chinese_ulmfit](https://github.com/bigboNed3/chinese_ulmfit)

归纳迁移学习（Inductive Transfer Learning）对计算机视觉（Compute Vision，CV）产生了巨大影响，但对自然语言处理（Natural Language Processing，NLP）一直没有突破性进展，现有的NLP方法，仍然需要根据特定任务进行修改，并且从零开始训练。我们提出了通用语言模型微调（Universal Language Model Fine-tuning for Text Classification，ULMFiT），这是一种有效的迁移学习方法，可以适用于任何NLP任务，另外，我们引入了语言模型微调的关键技术。我们的方法在6个文本分类任务上显著优于现有的最先进方法，在大部分数据集上将错误率降低了18-24%。此外，ULMFiT仅用100个标记样本训练出来的性能，可以媲美从零开始训练（Training From Scratch）使用100倍以上数据训练出来的性能。

### bert/transformer相关总结

#### huggingface的nlp预训练模型库

用于NLP的预训练Transformer模型的开源库。它具有六种架构，分别是：
 
+ Google的BERT
+ OpenAI的GPT和GPT-2
+ Google / CMU的Transformer-XL和XLNet
+ Facebook的XLM

[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

#### bert

[BERT小学生级上手教程，从原理到上手全有图示，还能直接在线运行](https://mp.weixin.qq.com/s/ltVuXZ4nJh8Cb5X2UhB6tQ)

[BERT源码分析（PART I）](https://mp.weixin.qq.com/s/sSmTQ_cOLyAUV0aV0FkDvw)

[BERT源码分析（PART II）](https://mp.weixin.qq.com/s/1NDxWfBu_csu8qHV2tmmVQ)

[Dive into BERT：语言模型与知识](https://mp.weixin.qq.com/s/NjQtSKY85Np5IodRiKsrvg)

[关于BERT，面试官们都怎么问](https://mp.weixin.qq.com/s/c2PktKruzq_teXm3GAwe1Q)

主要讲了下面3篇：

[Language Models as Knowledge Bases?](https://arxiv.org/abs/1909.01066)

[Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855)

[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document)


#### gpt-2

[15亿参数最强通用NLP模型面世！Open AI GPT-2可信度高于所有小模型](https://mp.weixin.qq.com/s/nu2egJuG_yxIVfW9GfdlCw)

中文GPT2

[只需单击三次，让中文GPT-2为你生成定制故事](https://mp.weixin.qq.com/s/FpoSNNKZSQOE2diPvJDHog)

[https://github.com/imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)

[https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb](https://colab.research.google.com/github/imcaspar/gpt2-ml/blob/master/pretrained_model_demo.ipynb)

#### gpt-2 8b

[47分钟，BERT训练又破全新纪录！英伟达512个GPU训练83亿参数GPT-2 8B](https://mp.weixin.qq.com/s/ysQM7D761rtW4-423AUI5w)

#### distill gpt-2

[语言模型秒变API，一文了解如何部署DistilGPT-2](https://mp.weixin.qq.com/s/5B8bN2kplB4t1ctYJjN1zw)

huggingface的distill gpt-2：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

#### albert

[刚刚，Google发布24个小型BERT模型，直接通过MLM损失进行预训练](https://mp.weixin.qq.com/s/s0ysFH4CRvsHY1Gp3b4DPQ)

[ALBERT：用于语言表征自监督学习的轻量级 BERT](https://mp.weixin.qq.com/s/0V-051qkTk9EYiuEWYE3jQ)

[谷歌ALBERT模型V2+中文版来了：之前刷新NLP各大基准，现在GitHub热榜第二](https://mp.weixin.qq.com/s/nusSlw28h4bOlw5hDsc-Iw)

#### XLNet

[XLNet : 运行机制及和 Bert 的异同比较](https://mp.weixin.qq.com/s/VCCZOKJOhCEjxfnoLSuRKA)

[Transformer-XL与XLNet笔记](https://mp.weixin.qq.com/s/g7I_V5a3Puy9uK11A--Xqw)

[什么是XLNet中的双流自注意力](https://mp.weixin.qq.com/s/9QmhN4KfukCtAxzprKDbAQ)

#### ELECTRA

[2019最佳预训练模型：非暴力美学，1/4算力超越RoBERTa](https://mp.weixin.qq.com/s/_R-Bp5lLov-QIoPRl6fFMA)

[ELECTRA: 超越BERT, 19年最佳NLP预训练模型](https://mp.weixin.qq.com/s/fR5OrqxCv0udKdyh6CHOjA)

[ELECTRA: pre-training text encoders as discriminators rather than generators](https://openreview.net/pdf?id=r1xmh1btvb)

#### BART

[多项NLP任务新SOTA，Facebook提出预训练模型BART​](https://mp.weixin.qq.com/s/1-EJ36-lY9YZSLBG5c2aaQ)

[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)

自监督方法在大量NLP任务中取得了卓越的成绩。近期研究通过改进masked token的分布（即masked token被预测的顺序）和替换masked token的可用语境，性能获得提升。然而，这些方法通常聚焦于特定类型和任务（如span prediction、生成等），应用较为有限。

Facebook的这项研究提出了新架构BART，它结合双向和自回归Transformer对模型进行预训练。BART是一个适用于序列到序列模型的去噪自编码器，可应用于大量终端任务。预训练包括两个阶段：1）使用任意噪声函数破坏文本；2）学得序列到序列模型来重建原始文本。BART使用基于Tranformer的标准神经机器翻译架构，可泛化BERT、GPT等近期提出的预训练模型。

#### gated transformer-xl

[Stabilizing Transformers for Reinforcement Learning](https://arxiv.org/abs/1910.06764)

摘要：得益于预训练语言模型强大的能力，这些模型近来在NLP任务上取得了一系列的成功。这需要归功于使用了transformer架构。但是在强化学习领域，transformer并没有表现出同样的能力。本文说明了为什么标准的transformer架构很难在强化学习中优化。研究者同时提出了一种架构，可以很好地提升 transformer架构和变体的稳定性，并加速学习。研究者将提出的架构命名为Gated Transformer-XL(GTrXL)，该架构可以超过LSTM，在多任务学习 DMLab-30 基准上达到 SOTA 的水平。

推荐：本文是DeepMind的一篇论文，将强化学习和Transformer结合是一种新颖的方法，也许可以催生很多相关的交叉研究。

#### bert/transformer加速

##### bert蒸馏、量化、剪枝

[BERT 瘦身之路：Distillation，Quantization，Pruning](https://mp.weixin.qq.com/s/ir3pLRtIaywsD94wf9npcA)

##### reformer

[哈希革新Transformer：这篇ICLR高分论文让一块GPU处理64K长度序列](https://mp.weixin.qq.com/s/QklCVuukfElVDBFNxLXNKQ)

[Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB)

[https://github.com/google/trax/blob/master/trax/models/research/reformer.py](https://github.com/google/trax/blob/master/trax/models/research/reformer.py)

大型的 Transformer 往往可以在许多任务上实现 sota，但训练这些模型的成本很高，尤其是在序列较长的时候。在 ICLR 的入选论文中，我们发现了一篇由谷歌和伯克利研究者发表的优质论文。文章介绍了两种提高 Transformer 效率的技术，最终的 Reformer 模型和 Transformer 模型在性能上表现相似，并且在长序列中拥有更高的存储效率和更快的速度。论文最终获得了「8，8，6」的高分。在最开始，文章提出了将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度。此外，研究者使用可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数）。最终的 Reformer 模型和 Transformer 模型在性能上表现相同，同时在长序列中拥有更高的存储效率和更快的速度。

[大幅减少GPU显存占用：可逆残差网络(The Reversible Residual Network)](https://mp.weixin.qq.com/s/j6-x9ANF9b3Q1I_os_LJSw)


##### LTD-bert

[内存用量1/20，速度加快80倍，腾讯QQ提出全新BERT蒸馏框架，未来将开源](https://mp.weixin.qq.com/s/W668zeWuNsBKV23cVR0zZQ)

##### Q-bert

[AAAI 2020 \| 超低精度量化BERT，UC伯克利提出用二阶信息压缩神经网络](https://mp.weixin.qq.com/s/0qBlnsUqI2I-h-pFSgcQig)

[Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/pdf/1909.05840.pdf)

##### Adabert

[推理速度提升29倍，参数少1/10，阿里提出AdaBERT压缩方法](https://mp.weixin.qq.com/s/mObuD4ijUCjnebYIrjvVdw)

[AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search](https://arxiv.org/pdf/2001.04246v1.pdf)

#### t5

[谷歌T5模型刷新GLUE榜单，110亿参数量，17项NLP任务新SOTA](https://mp.weixin.qq.com/s/YOMWNV5BMI9hbB6Nr_Qj8w)

[谷歌最新T5模型17项NLP任务霸榜SuperGLUE，110亿参数量！](https://mp.weixin.qq.com/s/rFT37D7p0MiS8XGZM35bYA)

#### 哪吒+tinybert

[哪吒”出世！华为开源中文版BERT模型](https://mp.weixin.qq.com/s/He6Xujoe5Ieo95Tshx7PnA)

[NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204)

[https://github.com/huawei-noah/Pretrained-Language-Model](https://github.com/huawei-noah/Pretrained-Language-Model)

[华为诺亚方舟开源哪吒、TinyBERT模型，可直接下载使用](https://mp.weixin.qq.com/s/cqYWllVCgWwGfAL-yX7Dww)

#### XLM-R

[Facebook最新语言模型XLM-R：多项任务刷新SOTA，超越单语BERT](https://mp.weixin.qq.com/s/6oK-gevKLWDwdOy4aI7U7g)

[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)

来自facebook。针对多种跨语言的传输任务，大规模地对多语言语言模型进行预训练可以显著提高性能。在使用超过 2TB 的已过滤 CommonCrawl 数据的基础上，研究者在 100 种语言上训练了基于 Transformer 的掩模语言模型。该模型被称为 XLM-R，在各种跨语言基准测试中，其性能显著优于多语言 BERT（mBERT），其中 XNLI 的平均准确度为+ 13.8％，MLQA 的平均 F1 得分为+ 12.3％，而 FQ 的平均 F1 得分为+ 2.1％ NER。XLM-R 在低资源语言上表现特别出色，与以前的 XLM 模型相比，斯瓦希里语（Swahili）的 XNLI 准确性提升了 11.8％，乌尔都语（Urdu）的准确性提升了 9.2％。研究者还对获得这些提升所需的关键因素进行了详细的实证评估，包括（1）积极转移和能力稀释；（2）大规模资源资源的高低性能之间的权衡。最后，他们首次展示了在不牺牲每种语言性能的情况下进行多语言建模的可能性。XLM-Ris 在 GLUE 和 XNLI 基准测试中具有强大的单语言模型，因此非常具有竞争力。

#### bert+多模态

[BERT在多模态领域中的应用](https://mp.weixin.qq.com/s/THxlQX2MPXua0_N0Ug0EWA)

CV领域：VisualBert, Unicoder-VL, VL-Bert, ViLBERT, LXMERT。

### transformer+生成模型

#### 经典文本生成模型

[AI也能精彩表达：几种经典文本生成模型一览](https://mp.weixin.qq.com/s/GfP76I-BzzQcyLqQJoeXxw)

#### UniLM

[NeurIPS 2019 \| 既能理解又能生成自然语言，微软提出统一预训练新模型UniLM](https://mp.weixin.qq.com/s/J96WjZhnf_1vBRHbbGwtyg)

[Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)

[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)

#### LaserTagger

[谷歌开源文本生成新方法 LaserTagger，直击 seq2seq 效率低、推理慢、控制差三大缺陷！](https://mp.weixin.qq.com/s/xO9eBkFOxfzcmbMhqVcmGA)

[推断速度达seq2seq模型的100倍，谷歌开源文本生成新方法LaserTagger](https://mp.weixin.qq.com/s/_1lr612F3x8ld9gvXj9L2A)

序列到序列（seq2seq）模型给机器翻译领域带来了巨大变革，并成为多种文本生成任务的首选工具，如文本摘要、句子融合和语法纠错。模型架构改进（如 Transformer）以及通过无监督训练方法利用大型无标注文本数据库的能力，使得近年来神经网络方法获得了质量上的提升。

但是，使用 seq2seq 模型解决文本生成任务伴随着一些重大缺陷，如生成的输出不受输入文本支持（即「幻觉」，hallucination）、需要大量训练数据才能实现优秀性能。此外，由于 seq2seq 模型通常逐词生成输出，因此其推断速度较慢。

谷歌研究人员在近期论文《Encode, Tag, Realize: High-Precision Text Editing》中提出一种新型文本生成方法，旨在解决上述三种缺陷。该方法速度快、精确度高，因而得名 LaserTagger。

[Encode, Tag, Realize: High-Precision Text Editing](https://research.google/pubs/pub48542/)

[http://lasertagger.page.link/code](http://lasertagger.page.link/code)

#### pegasus

[华人博士一作：自动生成摘要超越BERT！帝国理工&谷歌提出新模型Pegasus](https://mp.weixin.qq.com/s/dyCEOvGOoIlo7ggra_TutQ)

[PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)

来自帝国理工学院和谷歌大脑团队的研究者提出了大规模文本语料库上具有新的自监督目的的大型 Transformer 预训练编码器-解码器模型 PEGASUS（Pre-training with Extracted Gap-sentences for Abstractive Summarization）。与抽取式文本摘要（extractive summary）相似，在 PEGASUS 模型中，输入文档中删除或 mask 重要句子，并与剩余句子一起作为输出序列来生成。研究者在新闻、科学、故事、说明书、邮件、专利以及立法议案等 12 项文本摘要下游任务上测试了 PEGASUS 模型，结果表明该模型在全部 12 项下游任务数据集上取得了 SOTA 结果（以 ROUGE score 衡量）。此外，该模型在低资源（low-resource）文本摘要中也有非常良好的表现，在仅包含 1000 个示例的 6 个数据集上超越了以往的 SOTA 结果。

#### T-NLG/DeepSpeed

[搞定千亿参数，训练时间只用1/3，微软全新工具催生超级NLP模型](https://mp.weixin.qq.com/s/4KIQQe_AfpLBOC9jL8puvQ)

[https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)

[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)


#### transformer应用于检索召回

ICLR2020 cmu+google：

[Pre-training Tasks for Embedding-based Large-scale Retrieval](https://arxiv.org/abs/2002.03932)

#### 一些bert/transformer的应用

[美团BERT的探索和实践](https://mp.weixin.qq.com/s/qfluRDWfL40E5Lrp5BdhFw)

[Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展](https://mp.weixin.qq.com/s/dF3PtiISVXadbgaG1rCjnA)


#### poly-encoder

[https://zhuanlan.zhihu.com/p/119444637](https://zhuanlan.zhihu.com/p/119444637)

### bert/transformer其他

[BERT系列文章汇总导读](https://mp.weixin.qq.com/s/oT2dtmfEQKyrpDTOrpzhWw)

[ALBERT、XLNet，NLP技术发展太快，如何才能跟得上节奏？](https://mp.weixin.qq.com/s/Toth-XKn2WKYkyDw6j5F3A)

[绝对干货！NLP预训练模型：从transformer到albert](https://mp.weixin.qq.com/s/Jgx9eHk9xiSOWEy0Ty3LoA)

[ALBERT一作蓝振忠：预训练模型应用已成熟，ChineseGLUE要对标GLUE基准](https://mp.weixin.qq.com/s/mvkFDy09BdKJC4Cja11PAA)

[有哪些令你印象深刻的魔改Transformer？](https://mp.weixin.qq.com/s/HS2tlT7t18cFytZVIsOXUg)

[BERT模型超酷炫，上手又太难？请查收这份BERT快速入门指南！](https://mp.weixin.qq.com/s/jVSW0KDhaXuaIeOzoPmCJA)

[https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb)

[Transformers Assemble（PART I）](https://mp.weixin.qq.com/s/NZM05zyUkldOwpNIbsOtDQ) 讲了3篇

[Transformers Assemble（PART II）](https://mp.weixin.qq.com/s/JdUVaQ3IyrflHvxIk5jYbQ) 又讲了三篇

[站在BERT肩膀上的NLP新秀们（PART III）](https://mp.weixin.qq.com/s/CxcyX5V9kBQDW8A4g0uGNA)

[BERT时代与后时代的NLP](https://mp.weixin.qq.com/s/U_pYc5roODcs_VENDoTbiQ)

[新预训练模型CodeBERT出世，编程语言和自然语言都不在话下，哈工大、中山大学、MSRA出品](https://mp.weixin.qq.com/s/wmAu4810wrK2n-pDezAo0w)

[AAAI 2020 \| BERT稳吗？亚马逊、MIT等提出针对NLP模型的对抗攻击框架TextFooler](https://mp.weixin.qq.com/s/3wPda43A-Jm6gl9ysxciEA)


[A Primer in BERTology: What we know about how BERT works](https://arxiv.org/pdf/2002.12327.pdf)

摘要：目前，基于 Transformer 的模型已经广泛应用于自然语言处理中，但我们依然对这些模型的内部工作机制知之甚少。在本文中，来自麻省大学洛威尔分校的研究者对流行的 BERT 模型进行综述，并综合分析了 40 多项分析研究。他们还概览了对模型和训练机制提出的改进，然后描画了未来的研究方向。

[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)


### 对话

#### 对话数据集

[谷歌发布世界最大任务型对话数据集SGD，让虚拟助手更智能](https://mp.weixin.qq.com/s/hNghBThK4FX0ON4Ypp2HzQ)

[Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset](https://arxiv.org/abs/1909.05855)

#### 对话领域的传统模型

[HMM模型在贝壳对话系统中的应用](https://mp.weixin.qq.com/s/AG_Khfb0D7Uo40puIVcx-Q)

#### convai

[GitHub超1.5万星NLP团队热播教程：使用迁移学习构建顶尖会话AI](https://mp.weixin.qq.com/s/lHzZjY98WxNeQDjTH7VXAw)

[https://convai.huggingface.co/](https://convai.huggingface.co/)

[https://github.com/huggingface/transfer-learning-conv-ai](https://github.com/huggingface/transfer-learning-conv-ai)

#### 微软小冰

[微软小冰是怎样学会对话、唱歌和比喻？我们听三位首席科学家讲了讲背后的原理](https://mp.weixin.qq.com/s/q7YpDssTcMLZrhV_JIikpg)

#### RASA

[【RASA系列】语义理解（上）](https://mp.weixin.qq.com/s/hBoD7wOX9a-auWJ0kMzQ7w)

#### 生成式对话

[生成式对话seq2seq：从rnn到transformer](https://mp.weixin.qq.com/s/qUxPgsgP-4XFmVMMzLH5Ow)

#### 开放领域聊天机器人

[Towards a Human-like Open-Domain Chatbot](https://arxiv.org/abs/2001.09977)

[不再鹦鹉学舌：26亿参数量，谷歌开放领域聊天机器人近似人类水平](https://mp.weixin.qq.com/s/TZJBSrp85p4gUY_aZY7gqw)

#### 问答系统

[AAAI 2020 提前看 \| 三篇论文解读问答系统最新研究进展](https://mp.weixin.qq.com/s/ose5Yak8hsqEg2TGO2Mj9w)

[Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring](https://arxiv.org/pdf/1912.00879.pdf)

[TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection](https://arxiv.org/pdf/1911.04118.pdf)

[On the Generation of Medical Question-Answer Pairs](https://arxiv.org/pdf/1811.00681.pdf)

### NER

[OpenNRE 2.0：可一键运行的开源关系抽取工具包](https://mp.weixin.qq.com/s/vYJk6tm2EeY9znYWlXYbaA)

[https://github.com/thunlp/OpenNRE](https://github.com/thunlp/OpenNRE)

### 知识图谱

[NAACL 2019开源论文：基于胶囊网络的知识图谱完善和个性化搜索](https://mp.weixin.qq.com/s/CF9foNqeWxGygAZelifLAA)

[知识图谱从哪里来：实体关系抽取的现状与未来](https://mp.weixin.qq.com/s/--Y-au6bwmmwUfOnkdO5-A)

### 关系提取

[关系提取简述](https://mp.weixin.qq.com/s/4lcnqp60045CIHa_mMXgVw)

### 常识知识与常识推理

[AAAI 2020学术会议提前看：常识知识与常识推理](https://mp.weixin.qq.com/s/0CWrelur99lwyuIxSyJyxA)