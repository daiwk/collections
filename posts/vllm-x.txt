### vllm跑gguf的r1

[https://github.com/vllm-project/vllm/pull/13167](https://github.com/vllm-project/vllm/pull/13167)

从源码安装参考[https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source)

```shell
export https_proxy=xxxxxx
pip3 install setuptools_scm ## 不知道是不是需要的
git clone https://github.com/vllm-project/vllm.git
cd vllm
VLLM_USE_PRECOMPILED=1 pip3 install --editable .

# 如果网络有问题，可以直接pip3 install .
# 下面这3步还是比较必要的
cp -r /usr/local/lib/python3.10/dist-packages/vllm /usr/local/lib/python3.10/dist-packages/vllm.bk
rm -rf /usr/local/lib/python3.10/dist-packages/vllm
cp -r ./vllm /usr/local/lib/python3.10/dist-packages
```

+ 去[https://huggingface.co/deepseek-ai/DeepSeek-R1/tree/main](https://huggingface.co/deepseek-ai/DeepSeek-R1/tree/main)把各个非model的小文件下载下来（可以直接```GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-R1```）也要下载，
+ 去[https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main)把config.json下载下来，把json里的torch_dtype里的bfloat16改成float16，覆盖掉上面那个目录里的config.json

上面的那些文件都放到```./unsloth_dir```目录下，注意，目前只能用Q2 Q4那种量化，1.58bit那种动态量化不支持

```shell
cd ./unsloth_dir/
# merge成一个gguf
llama.cpp/llama-gguf-split --merge ./DeepSeek-R1-Q2_K/DeepSeek-R1-Q2_K-00001-of-00005.gguf ./unsloth_dir/merge.gguf 
```

代码：

```python
from vllm import LLM, SamplingParams

import multiprocessing

if __name__ == "__main__":
    # 这坨要放main里，deepseek教我的
    multiprocessing.set_start_method('spawn', force=True)
    
    llm = LLM(model="./unsloth_dir/merge.gguf",
              tokenizer="./unsloth_dir/DeepSeek-R1",
              hf_config_path="./unsloth_dir/DeepSeek-R1",
              enforce_eager=True, 
              tensor_parallel_size=8, #
              trust_remote_code=True, 
              distributed_executor_backend="mp",
              max_model_len=2000)
    sampling_params = SamplingParams(temperature=0.5, max_tokens=2000)
    
    def print_outputs(outputs):
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt: {prompt!r}, Generated text\n: {generated_text}")
        print("-" * 80)
    conversation = [
        {
            "role": "user",
            "content": "中国的首都是哪里",
        },
    ]
    outputs = llm.chat(conversation,
                       sampling_params=sampling_params,
                       use_tqdm=False)
    print_outputs(outputs)
```

运行：

```shell
VLLM_MLA_DISABLE=1 VLLM_WORKER_MULTIPROC_METHOD=spawn python3 vllm_deepseek.py 
```
