# 一些思考

[The Second Half](https://ysymyth.github.io/The-Second-Half/)

[OpenAI姚顺雨：欢迎来到AI下半场！](https://zhuanlan.zhihu.com/p/1896310535865233616)

数十年来，人工智能主要致力于开发新的训练方法和模型。

我们不再仅仅问：“我们能否训练一个模型来解决X问题？”而是问：“我们应该训练人工智能去做什么，以及我们如何衡量真正的进步？”要在下半场取得成功，我们需要及时转变思维方式和技能组合，这些可能更接近产品经理的思维方式。

[Welcome to the Era of Experience](https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf)

[强化学习之父当头一棒：RL版「苦涩的教训」来了！通往ASI，绝非靠人类数据](https://mp.weixin.qq.com/s/MGrSWh-wcBrLKPboUmg05w)

[被《经验时代》刷屏之后，剑桥博士长文讲述RL破局之路](https://mp.weixin.qq.com/s/dLfbu56f02BtQR4bCjzNfw)


[生成式AI进入第二幕：交大携手创智学院提出「认知工程」，AI新纪元开始了](https://mp.weixin.qq.com/s/G3WTW2RiYNA6epyQoZZ9Ng)

[Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/pdf/2504.13828)

[https://github.com/GAIR-NLP/cognition-engineering](https://github.com/GAIR-NLP/cognition-engineering)

[中文版](https://github.com/GAIR-NLP/cognition-engineering/blob/main/assets/Cognition_Engineering_zh.pdf)，自己转存了[一份](https://github.com/daiwk/collections/blob/master/assets/Cognition_Engineering_zh.pdf)




# LLM概述

PLM（pretrained language models），即bert等

## LLM简史

+ 2017年的[Learning to generate reviews and discovering sentiment](https://arxiv.org/pdf/1704.01444.pdf)尝试用rnn来实现智能系统
+ 2018年的gpt1：[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。确定对自然语言文本建模的基本原则为**预测下一个单词**。
+ 2019年的gpt2：[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模，**无需使用标记数据进行显式微调**。
    + 参考[The natural language decathlon: Multitask learning as question answering](https://arxiv.org/pdf/1806.08730.pdf)中**多任务求解的概率形式**： $$p(output|input,task)$$ 。
    + 提出“由于特定任务的有监督目标与无监督目标（语言建模）相同，只是在序列的子集上进行评估，因此，无监督目标的全局最小值也是有监督目标的全局最小值”，即每个NLP任务可以看作**世界文本子集的单词预测问题**，如果模型有足够能力来复原世界文本，无监督语言建模可以解决各种问题。
    + 仅无监督与监督微调的SOTA相比效果还是不太行。虽然GPT2模型规模相对较小，但如对话等任务在其基础上做微调还是能拿到很好的效果的，例如[DIALOGPT : Large-scale generative pre-training for conversational response generation](https://arxiv.org/pdf/1911.00536.pdf)、[End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2](https://aclanthology.org/2020.acl-main.54.pdf)
+ 2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning（这点在**3.3亿的BERT和15亿的gpt2中看不到**）。**预训练和ICL有相同的语言建模范式**：预训练预测给定上下文条件下的后续文本序列，ICL预测正确的任务解决方案，其可被格式化为给定任务描述和示范下的文本序列。
+ GPT-3的两种改进方法：
    + 使用代码数据训练：GPT-3主要问题是缺乏对复杂任务的推理能力，2021年openai提出了Codex（[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)），在github代码上微调的GPT。[A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more](https://arxiv.org/pdf/2112.15594.pdf)发现Codex能解决非常困难的编程问题，还能在数学问题上有显著提升。[Text and code embeddings by contrastive pre-training](https://arxiv.org/pdf/2201.10005.pdf)提出了训练文本和代码emb的对比学习，在线性探测分类、文本搜索、代码搜索等任务上有所提升。GPT-3.5就是在基于代码的GPT（code-davinci-002）的基础上开发的。
    + 与人类对齐：2017年openai就在[learning from human preference](https://openai.com/research/learning-from-human-preferences)的博客中提出了应用强化学习来学习由人类标的偏好比较，此后2021年7月openai发表了PPO。2020年GPT-2用RL进行微调，[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)也做了相似工作。2022年提出了RLHF的InstructGPT([Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf))，其中的**SFT就对应于常说的指令微调**。在openai的博客[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research)中提出了训练AI系统的3个有前途的方向：**使用人类反馈、协助人类评估、做对齐研究**。
+ 2022年的ChatGPT：用类似InstructGPT的方式进行训练，专门**对对话能力进行优化**，将人类生成的对话（**扮演用户和AI两个角色**）与InstructGPT数据集结合起来**以对话形式生成**。
+ 2023年的GPT-4：将文本输入扩展到**多模态信号**。此外，
    + 提升安全性：在RLHF训练中加入**额外的安全奖励信号**，采用多种干预策略如Anthropic提出的[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)提到的红队评估（read teaming）机制以减轻幻觉、隐私和过度依赖问题。
    + 改进的优化方法：使用**可预测扩展**（predictable scaling）的机制，使用模型训练期间的一小部分计算量**以预测最终性能**。
    + 迭代部署的工程方案：[Lessons learned on language model safety and misuse](https://openai.com/research/language-model-safety-and-misuse)，遵循5阶段的开发和部署生命周期来开发模型和产品。

## LLM列表（持续更新中）

+ 百亿：除了LLaMA（最大650亿）和NLLB（最大545亿），大多数在100亿-200亿之间，通常需要**数百甚至上千**个GPU或TPU。
+ 千亿：OPT、OPT-IML、BLOOM和BLOOMZ与GPT-3(175B)大致相同，GLM有1300亿，Galactica有1200亿，通常需要**数千**个GPU或者TPU。

| ckpt? | 模型 |发布时间 | 大小 | 预训练数据规模 | 硬件 | 训练时间  |
|---|---|---|---|---|---|---|
| Y | [T5](https://arxiv.org/pdf/1910.10683.pdf) | 2019.10| 11B |  1万亿tokens | 1024 TPU v3  |  - |
| N | [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) | 2020.05 | 175B |  3000万亿tokens | -  |  - |
| N | [GShard](https://arxiv.org/pdf/2006.16668.pdf) | 2020.06 | 600B |  1万亿tokens | 2048 TPU v3 | 4天 |
| Y | [mT5](https://arxiv.org/pdf/2010.11934.pdf) | 2020.10 | 13B |  1万亿tokens | -  |  - |
| Y | [PanGu-$$\alpha$$](https://arxiv.org/pdf/2104.12369.pdf) | 2021.04 | 13B |  1.1TB | 2048 Ascend 910 | - |
| Y | [CPM-2](https://arxiv.org/pdf/2106.10715.pdf) | 2021.06 | 198B |  2.6TB | - | - |
| N | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | 2021.07 | 12B |  1000万亿tokens | - | - |
| N | [ERNIE 3.0](https://arxiv.org/pdf/2107.02137.pdf) | 2021.07 | 10B |  3750亿tokens | 384 v100 | - |
| N | [Jurassic-1](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | 2021.08 | 178B | 3000亿tokens | 800 GPU | - |
| N | [HyperCLOVA](https://arxiv.org/pdf/2109.04650.pdf) | 2021.09 | 82B | 3000亿tokens | 1024 A100 | 13.4天 |
| N | [FLAN](https://arxiv.org/pdf/2109.01652.pdf) | 2021.09 | 137B | - | 128 TPU v3 | 60小时 |
| N | [Yuan 1.0](https://arxiv.org/pdf/2110.04725.pdf) | 2021.10 | 245B | 1800亿tokens | 2128 GPU | - |
| Y | [T0](https://arxiv.org/pdf/2211.01786.pdf) | 2021.10 | 11B | - | 512 TPU v3 | 27小时 |
| N | [Anthropic](https://arxiv.org/pdf/2112.00861.pdf) | 2021.12 | 52B | 4000亿tokens | - | - |
| N | [WebGPT](https://arxiv.org/pdf/2112.09332.pdf) | 2021.12 | 175B |  - | - | - |
| N | [Gopher](https://arxiv.org/pdf/2112.11446.pdf) | 2021.12 | 280B |  3000亿tokens | 4096 TPU v3 | 920小时 |
| N | [ERNIE 3.0 Titan](https://arxiv.org/pdf/2112.12731.pdf) | 2021.12 | 260B |  - | - | - |
| N | [GLaM](https://arxiv.org/pdf/2112.06905.pdf) | 2021.12 | 1200B | 2800亿tokens | 1024 TPU v4 | 574小时 |
| N | [LaMDA](https://arxiv.org/pdf/2201.08239.pdf) | 2022.01 | 137B |  7680亿tokens | 1024 TPU v3 | 57.5天 |
| N | [MT-NLG](https://arxiv.org/pdf/2201.11990.pdf) | 2022.01 | 530B | 2700亿tokens | 4480 80G A100 | - |
| N | [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf) | 2022.02 | 41B | 9670亿tokens | - | - |
| N | [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) | 2022.03 | 175B |  - | - | - |
| N | [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) | 2022.03 | 70B | 1.4万亿tokens | - | - |
| Y | [CodeGen](https://arxiv.org/pdf/2203.13474.pdf) | 2022.03 | 16B | 5770亿tokens | - | - |
| Y | [GPT-NeoX-20B](https://arxiv.org/pdf/2204.06745.pdf) | 2022.04 | 20B | 825GB | 96 40G A100 | - |
| Y | [Tk-Instruct](https://arxiv.org/pdf/2204.07705.pdf) | 2022.04 | 11B |  - | 256 TPU v3 | 4小时 |
| N | [PaLM](https://arxiv.org/pdf/2204.02311.pdf) | 2022.04 | 540B | 7800亿tokens | 6144 TPU v4 | - |
| Y | [UL2](https://arxiv.org/pdf/2205.05131.pdf) | 2022.05 | 20B |  825GB | 96 40G A100 | - |
| Y | [OPT](https://arxiv.org/pdf/2205.01068.pdf) | 2022.05 | 175B | 1800亿tokens | 992 80G A100 | - |
| Y | [NLLB](https://arxiv.org/pdf/2207.04672.pdf) | 2022.07 | 54.5B |  - | - | - |
| N | [AlexaTM](https://arxiv.org/pdf/2208.01448.pdf) | 2022.08 | 20B | 1.3万亿tokens | 128 A100 | 120天 |
| N | [Sparrow](https://arxiv.org/pdf/2209.14375.pdf) | 2022.09 | 70B | 64 TPU v3 | - | - |
| N | [WeLM](https://arxiv.org/pdf/2209.10372.pdf) | 2022.09 | 10B | 3000亿tokens | 128 A100 40G | 24天 |
| N | [U-PaLM](https://arxiv.org/pdf/2210.11399.pdf) | 2022.10 | 540B | - | 512 TPU v4 | 5天 |
| N | [Flan-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | 512 TPU v4 | 37小时 |
| N | [Flan-U-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | - | - |
| Y | [GLM](https://arxiv.org/pdf/2210.02414.pdf) | 2022.10 | 130B | 4000亿tokens | 768 40G A100 | 60天 |
| Y | [Flan-T5](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 11B |  - | - | - |
| Y | [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) | 2022.11 | 176B | 3660亿tokens | 384 80G A100 | 105天 |
| Y | [mT0](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 13B |  - | - | - |
| Y | [Galactica](https://arxiv.org/pdf/2211.09085.pdf) | 2022.11 | 120B | 1060亿tokens | - | - |
| Y | [BLOOMZ](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 176B |  - | - | - |
| Y | [OPT-IML](https://arxiv.org/pdf/2212.12017.pdf) | 2022.12 | 175B |  - | 128 40G A100 | - |
| Y | [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) | 2023.02 | 65B | 1.4万亿tokens | 2048 80G A100 | 21天 |
| N | [GPT-4](https://arxiv.org/pdf/2303.08774.pdf) | 2023.03 | - |  - | - | - |
| Y | [CodeGeeX](https://arxiv.org/pdf/2303.17568.pdf) | 2022.09 | 13B | 8500亿tokens | 1536 Ascend 910 | 60天 |
| N | [PanGU-$$\Sigma$$](https://arxiv.org/pdf/2303.10845.pdf) | 2023.03 | 1085B | 3290亿tokens | 512 Ascend 910 | 100天 |
| Y | [Pythia](https://arxiv.org/pdf/2304.01373.pdf) | 2023.04 | 12B | 3000亿tokens | 256 40G A100 | - |

可以直接把对应的md丢给gpt，叫它导出一个excel，然后就可以自定义排序或者画散点图看了


## LLM开源库

+ transformers：huggingface的库
+ [deepspeed](https://github.com/microsoft/DeepSpeed)：微软的库，与pytorch兼容，训练了MT-NLG、BLOOM等模型，包括各种分布式训练优化技术，如**内存优化**（**ZeRO**、**梯度检查点**等）和**管道并行**。
+ megatron-lm：英伟达的库，同样包括各种分布式训练技术，包括**模型和数据并行**、**混合精度**训练和**FlashAttention**。（[Megatron-lm: Training multi-billion parameter language models using model parallelism](https://arxiv.org/pdf/1909.08053.pdf)、[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)和[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)）
+ [jax](https://github.com/google/jax)：google的库，允许用户在**带有硬件加速（GPU或TPU）**的情况下进行**数组的高效运算**，可以在**各种设备**高效计算，支持**自动微分**和**即时编译**等功能。
+ [colossal-AI](https://arxiv.org/pdf/2110.14883.pdf)：HPC-AI Tech的库，基于pytorch，可以使用[PatrickStar](Patrickstar: Parallel training of pre-trained models via a chunk-based memory management)提出的方法优化异构内存管理，分布了基于LLaMA的[ColossalChat](https://medium.com/pytorch/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+ [BMTrain](https://github.com/OpenBMB/BMTrain)：openBMB的库，强调代码简洁、低资源占用和高可用性
+ [FastMoE](Fastmoe: A fast mixture-of-expert training system)：专门用于MoE模型的训练库，基于pytorch，简化了将transformer转换为MoE模型的过程
+ [semantic-kernel](https://github.com/microsoft/semantic-kernel)：微软的开源库

![AI的4场景战役](../assets/4wars-in-aistck.png)

一些开源的小模型：[从零训练的 1B 以下小模型汇总](https://mp.weixin.qq.com/s/IWuMj6ywge2NAUhYmYQBLA)

## 一些综述

+ [Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media](https://github.com/daiwk/collections/blob/master/assets/LLM/foundation%20models%20NLP.pdf)
+ [大规模语言模型：从理论到实践](../assets/LLM/LLM-TAP.pdf)，[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf)邱锡鹏等
+ 人大的大模型综述：[https://github.com/RUCAIBox/LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)，[自己存了一份pdf](https://github.com/daiwk/collections/blob/master/assets/LLM/LLM_Survey_Chinese.pdf)，（**！！！本文大部分内容按这个来组织！！！**）
+ [Talking about large language models](https://arxiv.org/pdf/2212.03551.pdf)
+ [Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)，引用数2k+
+ [A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt](https://arxiv.org/pdf/2302.09419.pdf)，唐杰等
+ [Pre-Trained Models: Past, Present and Future](https://arxiv.org/pdf/2106.07139.pdf)
+ [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)
+ [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/pdf/2105.10311.pdf)
+ [A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)
+ [Towards reasoning in large language models: A survey](https://arxiv.org/pdf/2212.10403.pdf)
+ [Reasoning with language model prompting: A survey](https://arxiv.org/pdf/2212.09597.pdf)
+ [Dense Text Retrieval based on Pretrained Language Models: A Survey](https://arxiv.org/pdf/2211.14876.pdf)
+ [Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://zhuanlan.zhihu.com/p/395795968)
+ [如何高效部署大模型？CMU最新万字综述纵览LLM推理MLSys优化技术](https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ)：[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)
+ [一篇对大语言模型（LLMs）进行全面、深入分析的43页综述（Word2Vec作者出品）](https://mp.weixin.qq.com/s/5fbx0lM9V-Q7xYbeDauuHw)==>[Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)，[自己存了一份](https://github.com/daiwk/collections/blob/master/assets/LLM/Large%20Language%20Models-%20A%20Survey.pdf)
+ [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/pdf/2308.07107v3.pdf)
+ [awesome-LLM-resourses](https://github.com/WangRongsheng/awesome-LLM-resourses)
+ [14天速成LLM高手！大佬开源学习笔记，GitHub狂揽700星](https://mp.weixin.qq.com/s/aDkH9E5b0yNd1J_Kthkh5Q)：[https://github.com/hesamsheikh/ml-retreat](https://github.com/hesamsheikh/ml-retreat)
+ [GitHub超火开发者路线图库有AI学习路线了！star数近30万](https://mp.weixin.qq.com/s/P9hWHGsiWcfEfMq54E02xg)：[https://github.com/kamranahmedse/developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)
+ [写的真好，万字长文串烧LLM大模型技术原理](https://mp.weixin.qq.com/s/ArTr-Df10LiTkJ4k9zlmZA)：[https://zhuanlan.zhihu.com/p/713794852](https://zhuanlan.zhihu.com/p/713794852)


[大模型面试八股](https://zhuanlan.zhihu.com/p/643560888)

[大模型八股答案（一）——基础知识](https://zhuanlan.zhihu.com/p/643829565)

[大模型八股答案（二）——训练框架](https://zhuanlan.zhihu.com/p/643836163)

[Jeff Dean演讲回顾LLM发展史，Transformer、蒸馏、MoE、思维链等技术都来自谷歌](https://mp.weixin.qq.com/s/5EUQlD5isnEEzXBw1AJEig)

## 课程

[https://github.com/mlabonne/llm-course](https://github.com/mlabonne/llm-course)

[https://github.com/PacktPublishing/LLM-Engineers-Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)

[https://github.com/decodingml/llm-twin-course](https://github.com/decodingml/llm-twin-course)


## 扩展法则(scaling law)

### openai的扩展法则

2020年,openai的[Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)通过拟合模型在不同数据大小（2000w到230亿个token）、不同的模型大小（7.68亿到15亿个**非嵌入参数**）的性能，提出了在**计算预算**$$c$$的条件下，$$L$$是用nats表示的交叉熵损失，模型性能与**模型规模**$$N$$、**数据集规模**$$D$$以及**训练计算量**$$C$$间存在如下幂律关系：

XXXL(N)=(\frac{N_c}{N})^{\alpha _N}, {\alpha}_N\sim 0.076,N_c\sim 8.8\times 10^{13}XXX

XXXL(D)=(\frac{D_c}{D})^{\alpha _D}, {\alpha}_D\sim 0.05,N_c\sim 5.4\times 10^{13}XXX

XXXL(C)=(\frac{C_c}{C})^{\alpha _C}, {\alpha}_C\sim 0.05,C_c\sim 3.1\times 10^{8}XXX

其中，$$N_c$$表示非嵌入参数数量，$$D_c$$表示训练token数量,$$C_c$$表示FP-days。

[Go Wider Instead of Deeper](https://arxiv.org/pdf/2107.11817)说了，transformer效果的提升**不在于计算量的变大**，而应该在于通过**提升模型的hidden dim**来增加模型参数量

### Chinchilla扩展法则

DeepMind在[Training compute-optimal large language models](https://arxiv.org/pdf/2203.15556.pdf)中提出了Chichilla扩展法则来指导LLM**最优计算量**的训练。通过变化更大范围的模型大小（7000w到160亿参数）和数据大小（50亿到5000亿个token）进行实验，拟合了如下的扩展法则：

XXX
L(N, D)=E+\frac{A}{N^\alpha}+\frac{B}{D^\beta}
XXX

其中$$E=1.69,A=406.4,B=410.7,\alpha = 0.34, \beta =0.28$$，通过在约束条件$$C\approx 6ND$$下优化损失$$L(N,D)$$，将计算预算最优地分配给模型大小和数据大小的方法：

XXX
N_{o p t}(C)=G\left(\frac{C}{6}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{6}\right)^b
XXX

其中$$a=\frac{\alpha}{\alpha+\beta}$$，$$b=\frac{\beta}{\alpha+\beta}$$，$$G$$是由$$A,B,\alpha,\beta$$计算出的扩展系数。

随着计算预算的增加，

+ openai的扩展法则更偏向于将更大预算分给**模型大小**，因为其对比各模型时使用了固定的训练数据量和学习率等超参，低估了数据量的作用。每增加10倍的计算量，应该让数据集大小增加为约1.8倍，模型参数量增加为约5.5倍。即**模型参数量更加的重要**。
+ Chinchilla扩展法则认为**模型大小和数据大小要同比例增加**，即$$a$$和$$b$$取值差不多。因为其在无视模型大小的前提下，发现设置与数据量差不多match的学习率能获得更好的loss。每增加10倍的计算量，应该让数据集大小增加为约3.16倍，模型参数量也增加为约3.16倍。即**数据集大小和模型参数量一样重要**。

然而，有一些能力（如涌现）无法根据扩展法则进行预测，只有当模型达到一定规模时才会出现。

![chinchilla-law](../assets/chinchilla-law.png)

飘红的就是常见的10B模型，大概要205B的token来训练，能达到**计算最优点**，当然**并不一定是loss最小的点**，这个可以参考llama3的现象

### scaling law的一些讨论

[Scaling Laws 又失灵了？谷歌新研究：扩散模型不是越大越好](https://mp.weixin.qq.com/s/ia9L6jr_lwowYHgLI1k_4g)

[Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://arxiv.org/pdf/2404.01367.pdf)

[腾讯混元、北大发现Scaling law「浪涌现象」，解决学习率调参难题](https://mp.weixin.qq.com/s/ff5_O0H5VQNkArKroJkEZQ)

[Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling](https://arxiv.org/pdf/2405.14578)

SIGIR24最佳论文：[Scaling Laws For Dense Retrieval](https://arxiv.org/pdf/2403.18684)

[中科大联合华为诺亚提出Entropy Law，揭秘大模型性能、数据压缩率以及训练损失关系](https://mp.weixin.qq.com/s/F4OFP1lzAGH4RSXcXBw7mw)

#### 词表的scaling law

&nbsp;

[NeurIPS 2024 | 大模型的词表大小，同样适用于Scaling Law](https://mp.weixin.qq.com/s/_DTvTMCtrW9WV3vELjU9jw)

[Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)

[https://github.com/sail-sg/scaling-with-vocab/](https://github.com/sail-sg/scaling-with-vocab/)

#### scaling law for precision

[Scaling Laws终结，量化无用，AI大佬都在审视这篇论文](https://mp.weixin.qq.com/s/JhtOlj5Y4UYM3W3koeMmqw)

[Scaling Laws for Precision](https://arxiv.org/abs/2411.04330)

你训练的 token 越多，你需要的精度就越高。

### Densing law

[LLM最大能力密度100天翻一倍！清华刘知远团队提出Densing Law](https://mp.weixin.qq.com/s/O_jtO2ZuL11XB9GlaURsWg)

[Densing Law of LLMs](https://arxiv.org/pdf/2412.04315v2)

## 涌现能力

![llm-capabilities](../assets/llm-capabilities.png)

涌现能力：在小型模型中不存在而在大型模型中产生的能力，当规模达到一定程度时，性能显著提升，超出随机水平（参考
[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)）。与物理学中的**相变**现象类似（物质从一种相（状态）转变为另一种相的过程，通常伴随着能量的吸收或释放，并且涉及不同的物理性质，例如固体、液体和气体之间的转变）。

[普林斯顿DeepMind用数学证明：LLM不是随机鹦鹉！「规模越大能力越强」有理论根据](https://mp.weixin.qq.com/s/oYYuqbelBfCCSLW4Qo4POA)

[A Theory for Emergence of Complex Skills in Language Models](https://arxiv.org/abs/2307.15936)：


![涌现](../assets/emergent%20ability.png)

LLM的3种典型涌现能力及其对应代表模型：

### 上下文学习(in-context learning)

GPT-3（[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)）提出，只要提供一个自然语言指令和/或几个任务演示，语言模型就能通过完成输入文本的词序列的方式来为测试实例生成预期输出，不用额外的梯度更新。

+ ICL能力小模型不具备：1750亿的GPT-3有ICL能力，但GPT-1和GPT-2无此能力。
+ ICL能力取决于具体下游任务：130亿的GPT-3能在算术任务上有ICL，但1750亿的GPT-3在波斯语QA上无能为力。


### 指令遵循(instruction following)

使用**自然语言描述的混合多任务数据集进行微调（指令微调）**，LLM在**未见过的以指令形式描述的任务**上表现出色，具有更好的泛化能力。例如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)、[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)、[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)。

在[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的实验中，当模型大小达到680亿时，经过指定微调的LaMDA-PT开始在未见过的任务上显著优于未微调的模型，而80亿或更小的模型则没有这个现象。

在[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)的实验中，PaLM至少在620亿参数上才能在4个评估基准的各种任务上表现良好。

[精准0误差，输入价格打骨折！OpenAI官宣API支持结构化输出，JSON准确率100％](https://mp.weixin.qq.com/s/257SBcB2hr-xKPNYkUEErQ)


### 逐步推理(multi-step reasoning)

对于涉及多个推理步骤的复杂任务（如数学），可以使用**思维链（Chain-of-Thought, CoT）**提示策略（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)），让LLM通过**利用中间推理步骤的提示机制**来解决这类任务。

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)发现，CoT在模型大于600亿的PaLM和LaMBDA变体中能够提升在算术推理基准任务的效果，而当模型大于1000亿时，相比标准提示的优势更明显。

[How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

## LLM关键点

如何让LLM能够**通用**且**有能力**？

### 扩展

更大的模型、数据规模和更多的训练计算，但计算预算是有限的，可以用扩展法更高效地分配计算资源，如Chinchilla在**相同计算预算下增加训练token数**，优于更大模型规模的Gopher，同时需要数据清理。

### 训练

+ 分布式的训练框架：包括DeepSpeed（[Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters](https://dl.acm.org/doi/abs/10.1145/3394486.3406703)）和Megatron-LM（[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)和[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)）
+ 优化技巧：有助于提升训练稳定性和模型性能，如**重新开始以克服训练损失激增**（[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)）和**混合精度训练**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

### 能力引导

当LLM执行某些特定任务时，可能不会显式地展示出其通用求解器的能力，**设计合适的任务指令或具体的ICL策略**可以**激发**这种能力，例如

+ 通过**包含中间推理步骤的CoT提示**
+ 使用**自然语言表达的任务描述**，对LLM进行**指令微调**

### 对齐微调

由于预训练语料库包括高质量和低质量的数据，LLM可能生成有毒、偏见甚至有害的内容，要让LLM和人类价值观保持一致，如**有用性、诚实性和无害性**。RLHF相关工作如[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)能够产生高质量、无害的回答（例如拒绝回答侮辱性问题）。


### 工具操作

LLM本质是基于海量文本语料库进行文本生成训练的，对于不适合以文本形式表达的任务表现不佳（如数字计算），且其能力受限于预训练数据，无法获取最新信息。可以利用外部工具：

+ [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/pdf/2302.04761.pdf)能利用计算器进行准确计算
+ [Webgpt: Browser-assisted question-answering with human feed- back](https://arxiv.org/pdf/2112.09332.pdf)能利用搜索引擎检索未知信息

# LLM数据集

## 常用数据集

llm中文数据集：[https://juejin.cn/post/7238921093553438779](https://juejin.cn/post/7238921093553438779)

+ Books：
    + [BookCorpus](https://arxiv.org/pdf/1506.06724.pdf)：超过11000本电子书，用于GPT和GPT-2。
    + [Gutenberg](https://www.gutenberg.org/)：超过70000本文学作品，包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共领域，用于MT-NLG和LLaMA。
    + Books1和Books2：比BookCorpus大得多，但未公开，用于GPT-3。
+ CommonCrawl：最大的开源网络爬虫数据库之一，**百万亿字节**，有大量噪音和低质信息，需要过滤，有如下4个子集：
    + [C4](https://www.tensorflow.org/datasets/catalog/c4)：包括en（806G，训练T5、LaMDA、Gopher、UL2）、en.noclean（6T）、realnewslike（36G）、webtextlike（17G）、multilingual（38T，训练mT5）。
    + [CC-Stories](https://arxiv.org/pdf/1806.02847.pdf)：31G，内容以故事的形式展示
    + [CC-News](https://arxiv.org/pdf/1907.11692.pdf)：76G
    + [RealNews](https://arxiv.org/pdf/1905.12616.pdf)：120G
+ Reddit Links：Reddit上的帖子，高赞通常比较有用，可以拿来创建高质量数据集。
    + [WebText](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)：由Reddit上的高赞链接组成，未公开，对应的开源版是[OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)。
    + [Pushshift.io](https://arxiv.org/pdf/2001.08435.pdf)：实时更新的数据集，包括Reddit自创建以来的历史数据，有数据存储，也有实用工具，供用户搜索、总结和统计分析。
+ Wikipedia：大部分文章使用写作风格，并支持引用，英语版本用于大多数LLM，如GPT-3、LaMDA、LLaMA，还有多语言版。
+ Code：包括开源许可证的公共代码库（如github）和与代码相关的问答平台（如StackOverflow）,Google公开了[BigQuery](https://cloud.google.com/bigquery?hl=zh-cn)数据集，CodeGen用的BIGQUERY是其的一个子集。
+ 其他：
    + [The Pile](https://arxiv.org/pdf/2101.00027.pdf)有800G，包括书籍、网站、代码、科学论文和社交媒体平台，有22个子集，用于GPT-J(6B)、CodeGen(16B)、Megatron-Turing NLG（530B）。
    + [ROOTS](https://arxiv.org/pdf/2303.03915.pdf)由各种小数据集组成，共1.6T，包括59种语言（自然语言和编程语言），用于BLOOM。


## 数据收集

### 数据获取

+ 通用文本数据：
    + 网页：例如CommonCrawl，同时需要过滤和处理以提高质量
    + 对话文本：公共对话数据如PushShift.io，对于在线社交媒体的对话数据，可以**转换成树形结构**，每句话与回应其的话相连。多方的对话树可以划分为预训练语料库中的多个子对话。过度引入对话数据可能会有潜在风险（[OPT: open pre-trained transformer language models](https://arxiv.org/pdf/2205.01068.pdf)）：陈述性指令和直接疑问句被错误地认为是对话的开始，导致指令的有效性下降。
    + 书籍：更正式的长文本，利于**学习语言知识**、**建模长期依赖关系**、**生成叙述性和连贯的文本**。
+ 专用文本数据：
    + 多语言文本：BLOOM的预训练语料中包括了46种语言，PaLM包含了122种
    + 科学文本：如arxiv论文、科学教材、数学 网页等，通常需要特定的标记化和预处理。
    + 代码：一是编程问答社区，二是开源代码仅为。对应长距离依赖和准确的执行逻辑，可能是复杂推理能力的来源。将推理任务格式化为代码形式还能帮LLM生成更准确的结果（如[Language models of code are few-shot commonsense learners](https://arxiv.org/pdf/2210.07128.pdf)和[Autoformalization with large language models](https://arxiv.org/pdf/2205.12615.pdf)）

### 数据预处理

+ 质量过滤：有一些基于分类器的方法，例如维基百科的数据为正样本，负采样其他数据训练二分类器，但这种方法会删除方言、口语和社会语言的高质量文本，可能导致有偏、减少多样性。还有启发式的方法，主要包括：
    + 基于语言的过滤：如果该llm主要用于某种语言，可以把其他语言删了
    + 基于度量的过滤：利用生成文本的评估度量（如**perplexity**）来检测和删除不自然的句子
    + 基于统计的过滤：如**标点符号分布**、**符号和单词比例**、**句子长度**等
    + 基于关键词的过滤：删除噪声或无用元素，如**HTML标签**、**超链接**、**模板**、**攻击性词语**等。
+ 去重：[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)中发现重复数据会降低多样性，可能导致训练不稳定。下面3个级的去重都很有用
    + 句子级：删掉包含重复单词和短语的句子，因为可能在语言建模中引入**重复模式**（[The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751.pdf)）(后面的章节会讲)
    + 文档级：通过文档间的表层特征（如n-gram或单词重合率）来删掉重复文档
    + 数据集级：训练集中删掉测试集可能出现的重复文本，防止训练集和评估集间的重叠
+ 隐私去除：删掉可识别个人信息（PII），如基于关键词（姓名、地址、电话号码）识别。另外，[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/pdf/2202.06539.pdf)发现LLM在隐私攻击下的脆弱性可能归因于预训练语料中存在**重复PII数据**。
+ 分词：可以直接利用已有分词器，也可以使用专门为预训练语料库设计的分词器，如SentencePiece，而且**BPE**(byte pair encoding)能**确保分词后的信息不会丢失**，但其中的如NFKC([Unicode normalization forms](https://unicode.org/reports/tr15/))的**归一化技术**可能会**降低分词的性能**。

### 预训练语料的重要性

+ 混合来源：不同领域和场景的数据能让LLM有更强大的泛化能力。需要**仔细设置数据分布**，Gopher对数据分布消融，发现增加书籍数据可以提升捕捉长期依赖的能力，增加c4数据集比例可以提升其在c4验证集上的效果，但单独训练过多的某个领域数据会影响LLM在其他领域的泛化能力。
+ 数据量：模型性能方面，**数据大小**也能看到与模型大小类似的**扩展法则**。LLaMA发现，用更多数据训练更长时间，较小的模型也能实现良好性能。
+ 数据质量：Gopher、GLaM和T5都发现，在清理后的数据上训练能提升llm效果。数据的重复可能导致『双下降现象』（[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)和[Deep double descent: Where bigger models and more data hurt](https://arxiv.org/pdf/1912.02292.pdf)），甚至会导致训练不稳定。此外，[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)还发现，**重复数据会降低LLM从上下文复制的能力**，从而影响**ICL中的泛化能力**。

注：**双下降**指的是随着模型复杂性的增加，可能**loss先下降，然后再升高，最后又下降**：
+ 当模型的复杂性低于数据的复杂性时，增加模型的复杂性可以帮助减少训练误差。
+ 当模型的复杂性超过数据的复杂性时，增加模型的复杂性反而可能导致训练误差增加。这是因为模型开始过拟合数据，捕获数据中的噪声而非实际的模式。
+ 当模型的复杂性远大于数据的复杂性时，训练误差可能再次开始减少。这是因为模型有足够的能力来对数据的噪声进行平滑，同时仍然能够捕获数据的实际模式。

## benchmark

MMLU

[Measuring massive multitask language understanding](https://arxiv.org/pdf/2009.03300)

# LLM模型架构

## MoE原理

[MoE模型的前世今生](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

[MoE 系列论文解读：Gshard、FastMoE、Tutel、MegaBlocks 等](https://mp.weixin.qq.com/s/T5eJZWGH3yRpK9bxmvhhTA)

[CMU开源GRIFFIN：一种新颖的无需训练的MoE方法，提高大模型的生成效率！](https://mp.weixin.qq.com/s/33ISRxfXp4Z7OCN1dBKGyA)

[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/pdf/2404.01365.pdf)

[From Sparse to Soft Mixtures of Experts](https://arxiv.org/pdf/2308.00951): softmoe

MoE的核心问题是一个**如何把token分配给哪个专家**的**离散优化**问题，有如下离散+稀疏的分配方法，都需要**辅助loss**来**平衡每个专家的负载**，以减少drop tokens：

+ 线性规划
+ 强化学习
+ 人为固定规则
+ 最优运输方法
+ 贪心topk token-choose-expert
+ 贪心topk expert-choose-token

![softmoe](../assets/softmoe.png)

在softmoe中，假设N个token，S个slot，E个expert

代码：

[https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97](https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97)和[https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128](https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128)

[算法、系统和应用，三个视角全面读懂混合专家（MoE）](https://mp.weixin.qq.com/s/3UEFwy87f8O4H1Tt4VVnig)


[A Survey on Mixture of Experts](https://arxiv.org/pdf/2407.06204)

![moe-gating](../assets/moe-gating.png)

[从ACL 2024录用论文看混合专家模型（MoE）最新研究进展](https://mp.weixin.qq.com/s/BCsRHvHnn3B8oOODjim-Kg)

### DeepSeekMOE

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066)

[https://github.com/deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)

动机：

+ 专家不够分化：以往的MoE模型专家数量很少，假如模型的知识涉及的领域很多，平均一个专家要包含很多领域的知识，即不够**专**
+ 专家有冗余：假设每个token只能选一个专家，又假设每个token都需要常识知识，结果就是不论选哪个专家，这个专家的参数里都有常识知识，因此有冗余了——最好的情况是有个专家专门负责提供常识知识，所有 token 都会用一下这个专家。

解法：

+ **增加专家数量**：专家是个FFN，假设两个矩阵是$$dh$$和$$hd$$，如果将它拆成2个专家，就是拆成2个$$dh/2$$和$$h/2*d$$，假设原来一个token选top-1专家，现在就是选top-2专家，拆前拆后的计算量和参数量没变，好处就是排列组合多样性更多了，选择也更灵活了。
+ **增设共享专家**：有的专家是必选的，除此以外，每个token按照自己的喜好，再来选top-k。比如有64个专家，那么第一个专家是所有token都要选的，除此以外，每个token还从剩下的63个里选择自己的top-1，其实就是top-2。

### Dynamic MoE

[Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/pdf/2403.07652)

[https://github.com/ZhenweiAn/Dynamic_MoE](https://github.com/ZhenweiAn/Dynamic_MoE)



### XMoE

是上面两种方法的并集

[XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection](https://arxiv.org/pdf/2403.18926)

[https://github.com/ysngki/XMoE](https://github.com/ysngki/XMoE)

### HyperMoE

[HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/pdf/2402.12656)

[https://github.com/Bumble666/Hyper_MoE](https://github.com/Bumble666/Hyper_MoE)

### Expert Pruning

[Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/pdf/2402.14800)

[https://github.com/Lucky-Lance/Expert_Sparsity](https://github.com/Lucky-Lance/Expert_Sparsity)

### MixLoRA

[Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/pdf/2402.15896)

### ESFT

[Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv.org/pdf/2407.01906)



### 多模态MoE

[混合专家更有主见了，能感知多模态分情况行事，Meta提出模态感知型专家混合](https://mp.weixin.qq.com/s/1FNqu0CwPmMFuDTMhli7WA)

[Chameleon: Mixed-modal early-fusion foundation models](https://arxiv.org/pdf/2405.09818)单一Transformer架构，可以根据下一个token的预测目标，对由离散图像和文本token组成的混合模态序列进行建模，从而在不同模态之间进行无缝推理和生成。然而对于Chameleon这样各种模态会在模型训练的早期混合起来的模型，想要拓展它的能力，需要投入大量算力。

[MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)使用路由式稀疏架构（routed sparse architecture）

### HMOE

[优化传统MoE结构，腾讯混元团队提出专家差异化新思路](https://mp.weixin.qq.com/s/gOXGL_MReneAZCmO7lBIng)

[HMoE: Heterogeneous Mixture of Experts for Language Modeling](https://arxiv.org/pdf/2408.10681)

在 HMoE 中，**每个专家的大小不再相同**，从而赋予了每个专家不同的表达能力。这种差异化设计使得路由可以根据专家的实际能力动态分配不同难度的 token，有效解决了专家专业化程度不足的问题。 

### OLMoE

[OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/pdf/2409.02060)

[https://huggingface.co/allenai/OLMoE-1B-7B-0924](https://huggingface.co/allenai/OLMoE-1B-7B-0924)

[https://github.com/allenai/OLMoE](https://github.com/allenai/OLMoE)

### 元象MOE

[中国最大开源MoE模型，255B参数无条件免费商用，元象发布](https://mp.weixin.qq.com/s/vX0gt2YfsbUbrCpvNBFHZw)

XVERSE-MoE-A36B，该模型总参数255B，激活参数36B，达到100B模型性能的「跨级」跃升。

[https://github.com/xverse-ai/XVERSE-MoE-A36B](https://github.com/xverse-ai/XVERSE-MoE-A36B)

### MoEUT

[Jurgen、曼宁等大佬新作：MoE重塑6年前的Universal Transformer，高效升级](https://mp.weixin.qq.com/s/CckHYrUpwCpft53-lZ4DwA)

[MoEUT: Mixture-of-Experts Universal Transformers](https://arxiv.org/pdf/2405.16039)

[https://github.com/robertcsordas/moeut](https://github.com/robertcsordas/moeut)

### MoA

[无问芯穹提出混合稀疏注意力方案MoA，加速长文本生成，实现最高8倍吞吐率提升](https://mp.weixin.qq.com/s/rjGAJfusY_CHSx3Q0SHVmg)

[https://github.com/thu-nics/MoA](https://github.com/thu-nics/MoA)

[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](https://arxiv.org/abs/2406.14909)

### GRIN

[专家模型不要专家并行！微软开源MoE新路径](https://mp.weixin.qq.com/s/pt-AlH_z4e3PNiKC9Iyz7A)

[GRIN: GRadient-INformed MoE](https://arxiv.org/abs/2409.12136)


## 主流框架

+ **编码器-解码器架构(encoder-decoder)**：标准Transformer，如T5、BART，**只有少数LLLM还用这种结构**，如Flan-T5
+ **因果解码器架构(causual decoder)**：也叫**decoder-only**，**单向注意力掩码**，输入和输出token通过解码器以相同方式进行处理，以GPT系列为代表，现有大部分LLM都是这种架构，如OPT、BLOOM、Gopher等。
+ **前缀解码器架构(prefix decoder)**：修正因果解码器的掩码机制，使其能**对前缀token执行双向注意力**，并且**仅对生成的token执行单向注意力**（和encoder-decoder类似），即[Unified language model pre-training for natural language understanding and generation](https://arxiv.org/pdf/1905.03197.pdf)提出的uni-lm。[What language model architecture and pretraining objective works best for zero-shot generalization?](https://arxiv.org/pdf/2204.05832.pdf)建议不从头开始预训练，而是**继续训练因果编码器，然后将其转换成前缀编码器以加速收敛**。例如U-PaLM从PaLM演化而来，还有GLM-130B也是这种架构。

![uni-lm](../assets/uni-lm.png)

[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)

对于这3种架构，都可以用**MoE**进行扩展，每个输入的**一小部分神经网络权重**被**稀疏激活**，如[Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)和GLaM。[Unified scaling laws for routed language models](https://arxiv.org/pdf/2202.01169.pdf)发现，通过**增加专家数量或总参数大小**，性能会有显著改进。

### 讨论：为什么现在的LLM都是Decoder only的架构？

&nbsp;

[https://www.zhihu.com/question/588325646/answer/2940298964](https://www.zhihu.com/question/588325646/answer/2940298964)

+ **泛化性能强**：ICML 22的[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf).在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在**各种下游任务上zero-shot泛化性能最好**；另外，ACL23的[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/pdf/2212.10559.pdf)等工作表明，decoder-only模型相当于基于给出的几个示例**隐式地进行梯度下降**，对应的in-context learning泛化能力更强，
+ **秩**的讨论：[Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)的讨论，$$n\times d$$和$$d\times n$$相乘后（$$n\gg d$$）再加上softmax后，秩不超过$$d$$，而decoder-only中有一个下三角矩阵的mask，所以输入的是一个下三角矩阵，而下三角矩阵的行列式是对角线之积，且有softmax，对角线肯定大于0，所以是满秩的(行列式不为0-->矩阵经过变换后不会有一行或者一列全为0-->当前矩阵满秩)
+ 预训练**任务难度**更大：相比encoder-decoder，decoder-only架构里**每个位置能接触到的信息更少**，故难度更高，当模型大小和数据量够的时候，上限更高
+ 隐式学习了**位置信息**：[Transformer Language Models without Positional Encodings Still Learn Positional Information](https://aclanthology.org/2022.findings-emnlp.99.pdf)，encoder里对语序的区分能力较弱，需要结合position encoding，而causual attention隐式地具备了这种建模位置的能力。
+ **工程效率**：支持**复用kv-cache**，对多轮对话更友好，**『DIN的FLOPS』**一节里有讲

[盛名一时的BERT哪去了？这个问题的答案昭示了LLM范式的转变](https://mp.weixin.qq.com/s/fKeorQYwRlmmepuG1_aJlQ)

[What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising)

#### 去噪目标

&nbsp;

**去噪目标**指的是span corruption任务的任意变体，即填充(infilling)或填空(fill in the blank)。表达方式有很多，比如span长度、随机性、sentinel token等。

+ BERT类的模型中，大部分是**in-place**的去噪目标，例如对mask tokens的分类head，
+ T5的做法则是通过encoder-decoder或decoder-only模型来处理数据变换，即把masked token move to the back给模型预测。

去噪目标的效果很好，可以作为常规语言建模的**补充目标**，但不足以单独作为目标，因为去噪有两个缺点：

+ **更少的loss exposure**：在去噪目标中，只有**少量token会被mask和学习**，而常规语言建模则接近100%，使得**每个FLOP的样本效率非常低**
+ 比常规语言建模更不自然：以一种奇怪的方式重新设定输入输出格式，不太适合少样本学习

#### PrefixLM vs decoder-only

&nbsp;

注：归纳偏置(inductive bias)指模型在预测未遇到的输入时，做的一些假设的集合，例如最小描述长度（奥卡姆剃刀）指的就是当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。

对语言模型来说，双向注意力是一种有趣的**归纳偏置**，相比于较小规模的场景，双向注意力**在规模较大时可能就没那么重要了**，或者可能对不同的任务或模态有不同的影响，例如PaliGemma就用的prefixLM（[PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/pdf/2407.07726)）

![paligemma](../assets/paligemma.png)

PrefixLM也存在缓存问题，是这类架构的一个固有缺陷

#### encoder-decoder的优缺点

&nbsp;

+ 相比decoder-only的优势：endocer不受causal的限制，可以激进地试各种pooling/linear attention，因此可以offload一些**不那么重要的context到encoder中**，也可以**让encoder更小**，例如[Charformer](https://arxiv.org/pdf/2106.12672)
+ 相比prefixLM的缺点：输入和目标必须分配固定的预算，例如输入预算是1024token，那么encoder就**必须pad**到1024，而这可能会浪费大量计算。相反，在 PrefixLM 中，输入和目标可以直接concat起来，从而可以缓解这个问题


## 组件配置

### 标准化（norm）

&nbsp;

LN(layer norm)能缓解LLM训练不稳定的问题，其位置很重要。

![pre-ln](../assets/pre-ln.jpeg)

+ 前置LN：最初Transformer使用后置LN，但大多数LLM采用前置LN以实现更稳定的训练，尽管会有一些性能损失([On layer normalization in the transformer architecture](https://arxiv.org/pdf/2002.04745.pdf))。[Sandwich-LN](https://arxiv.org/pdf/2105.13290.pdf)在残差连接前添加额外的LN，虽然能避免数值爆炸，但有时会无法稳定LLM的训练，可能导致训练崩溃（[GLM-130B: an open bilingual pre-trained model](https://arxiv.org/pdf/2210.02414.pdf)）
+ [RMS Norm](https://arxiv.org/pdf/1910.07467.pdf)：训练和性能都不错，在Gopher和Chinchilla里使用
+ [Deep Norm](https://arxiv.org/pdf/2203.00555.pdf)：比LN有更好的训练稳定性，和后标准化一起用在GLM-130B里

![deep-norm](../assets/deep%20norm.png)

+ [DyT](https://arxiv.org/pdf/2503.10622)：用$$tanh(\alpha x)$$替换掉layer norm里的normalize

```python
class DyT(nn.Module):
    def __init__(self, num_features, alpha_init_value=0.5):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        return x * self.weight + self.bias
```

此外，**在emb后直接加额外的LN**能提升训练稳定性，但会导致**显著的性能下降**([What language model to train if you have one million GPU hours?](https://arxiv.org/pdf/2210.15424.pdf))，在后来的LLM中**被移除**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

[神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！](https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ)

[On the Nonlinearity of Layer Normalization](https://arxiv.org/pdf/2406.01255)

### 激活函数

&nbsp;

FFN中的激活函数：

+ [GeLU](https://arxiv.org/pdf/1606.08415.pdf)：大部分都是这个
+ [GLU(gated linear units)的变体](https://arxiv.org/pdf/2002.05202.pdf)：应用在PaLM和LaMDA等模型中，如SwiGLU和GeGLU有更好的效果，但在FFN中的参数量比GeLU要大50%


原始Transformer中

XXX\operatorname{FFN}\left(x, W_1, W_2, b_1, b_2\right)=\max \left(0, x W_1+b_1\right) W_2+b_2XXX

T5中把bias干掉了

XXX\operatorname{FFN}_{\operatorname{ReLU}}\left(x, W_1, W_2\right)=\max \left(x W_1, 0\right) W_2XXX

然后，$$\operatorname{GELU}(x)=x \Phi(x)$$，同时$$\operatorname{Swish}_\beta(x)=x \sigma(\beta x)$$，接下来

XXX\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes(x V+c)XXX

XXX\operatorname{Bilinear}(x, W, V, b, c)=(x W+b) \otimes(x V+c)XXX

XXX\operatorname{ReGLU}(x, W, V, b, c)=\max (0, x W+b) \otimes(x V+c)XXX

XXX\operatorname{GEGLU}(x, W, V, b, c)=\operatorname{GELU}(x W+b) \otimes(x V+c)XXX

XXX\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)XXX

对应起来就是

XXX\operatorname{FFN}_{\mathrm{GLU}}\left(x, W, V, W_2\right)=(\sigma(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {Bilinear }}\left(x, W, V, W_2\right)=(x W \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\operatorname{ReGLU}}\left(x, W, V, W_2\right)=(\max (0, x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {GEGLU }}\left(x, W, V, W_2\right)=(\operatorname{GELU}(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {SwiGLU }}\left(x, W, V, W_2\right)=\left(\operatorname{Swish}_1(x W) \otimes x V\right) W_2XXX


### 位置编码

&nbsp;

Transformer的self-attention有转换不变性，故要位置编码以引入绝对或相对位置信息来建模序列。

+ 绝对位置编码：
    + 正弦函数：原始Transformer中使用
    + 可学习的位置编码：LLM中常用
+ 相对位置编码：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)提出，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的，根据**k和q之间的偏移量**生成emb
+ Alibi：[Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf)提出，使用**k和q之间距离的惩罚**来给注意力分数加bias，[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf)发现其有更好的**零样本泛化能力**和更强的**外推能力**，能够在**比训练序列更长的序列**上表现良好。
+ RoPE：[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)提出，**k和q之间的分数用相对位置信息计算**，利于建模长序列，在PaLM、LLaMA、GLM-130B中都有应用。
  

[Transformer升级之路：RoPE的底数设计原则](https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ)

[Base of RoPE Bounds Context Length](https://arxiv.org/pdf/2405.14591)

[Decoder-only的LLM为什么需要位置编码？](https://mp.weixin.qq.com/s/3sBYrKyEPP93nwigaOAAAA)

[HuggingFace工程师亲授：如何在Transformer中实现最好的位置编码](https://mp.weixin.qq.com/s/_vhyBrTM041FfHEohwLqMA)

(toread)

### 注意力机制和Bias

+ 稀疏注意力：[Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf))，**计算复杂度更低**，GPT-3用了
+ FlashAttention：[Flashattention: Fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/pdf/2205.14135.pdf)，考虑显存访问
+ 其他attention：如[Random feature attention](https://arxiv.org/pdf/2103.02143.pdf)、[Big bird: Transformers for longer sequences](https://arxiv.org/pdf/2007.14062.pdf)
+ 移除bias：PaLM和Galactica中将bias删了，能够增加训练稳定性。


### 优化器

&nbsp;

[月之暗面开源改进版Muon优化器，算力需求比AdamW锐减48%，DeepSeek也适用](https://mp.weixin.qq.com/s/E65ULmjlK7Lv81dqvAubcQ)

### 小结

#### 归一化位置

&nbsp;

sublayer表示FFN或self-attention模块

| 方法 | 公式 | 
|------|---------------|
| post Norm | $$\operatorname{Norm}(\mathbf{x}+\operatorname{Sulayerb}(\mathbf{x}))$$ |
| pre Norm | $$\mathbf{x}+\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x}))$$ |
| Sandwich Norm | $$\mathbf{x}+\operatorname{Norm}(\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x})))$$ |

#### 归一化方法

| 方法 | 公式 | 
|------|---------------|
|Layer Norm| $$\frac{\mathrm{x}-\mu}{\sqrt{\sigma}} \cdot \gamma+\beta, \quad \mu=\frac{1}{d} \sum_{i=1}^d x_i, \quad \sigma=\sqrt{\frac{1}{d} \sum_{i=1}^d(x_i-\mu)^2}$$ |
|RMSNorm| $$\frac{\mathrm{x}}{\operatorname{RMS}(\mathrm{x})} \cdot \gamma, \quad \operatorname{RMS}(\mathbf{x})=\sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}$$ |
|Deep Norm| $$LayerNorm (\alpha \cdot \mathbf{x}+\operatorname{Sublayer}(\mathbf{x}))$$ |

#### 激活函数

| 方法 | 公式 | 
|------|-----------------------|
|ReLU| $$\operatorname{ReLU}(\mathbf{x})=\max (\mathbf{x}, \mathbf{0})$$ |
| GeLU | $$\operatorname{GeLU}(\mathbf{x})=0.5 \mathrm{x} \otimes[1+\operatorname{erf}(\mathbf{x} / \sqrt{2})], \quad \operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$$ |
|Swish | $$\operatorname{Swish}(\mathbf{x})=\mathbf{x} \otimes \operatorname{sigmoid}(\mathbf{x})$$ |
|SwiGLU|$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$ |
|GeGLU|$$\operatorname{GeGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{GeLU}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$|

#### 位置嵌入

+ $$A_{ij}$$：**q和k之间**的**注意力分数**
+ $$r_{i-j}$$：基于**q和k之间偏移**的可学习标量
+ $$\mathbf{R}_{\theta, i-j}$$：旋转角度为$$t\cdot \theta$$的旋转矩阵

| 方法 | 公式 | 
|------|--------------|
|绝对位置编码| $$\mathbf{x}_i=\mathbf{x}_i+\mathbf{p}_i$$ |
|相对位置编码|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T+r_{i-j}$$|
|RoPE|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T$$|
|Alibi|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T-m(i-j)$$|


## 预训练任务

### 语言建模

&nbsp;

语言建模是**仅解码器LLM**的常见目标，给定token序列$$\mathbf{x}=\left\{x_1, \ldots, x_n\right\}$$，旨在基于序列中前面的token，自回归地预估目标token：

XXX
\mathcal{L}_{L M}(\mathbf{x})=\sum_{i=1}^n \log P\left(x_i \mid x_{<i}\right)
XXX


对应到代码里：

```python
hidden_states = outputs[0]
logits = self.lm_head(hidden_states)

# gemma系列模型会做soft-cap
cap = self.config.logits_soft_cap
logits = nn.functional.tanh(logits / cap) * cap

logits = logits.float()
loss = None
if labels is not None:
    # Shift so that tokens < n predict n
    ## 假设句子长度为4，词表有3个词，
    ## logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3], [0.6, 0.1, 0.3] ]
    ## labels: [0, 2, 1, 0]
    ## shift_logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3]
    shift_logits = logits[..., :-1, :].contiguous()
    ## shift_labels: [2, 1, 0]
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    # [bs, seq_len - 1, vocab_size] 变为 [(bs * (seq_len - 1)), vocab_size]
    shift_logits = shift_logits.view(-1, self.config.vocab_size)
    # [bs, seq_len - 1] 变为 [(bs * (seq_len - 1))]
    shift_labels = shift_labels.view(-1)
    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels)
```

**前缀解码器**架构使用的是前缀语言建模任务，其loss**不涉及对前缀内token的预测**，故预训练时**涉及的序列中token较少**，故当预训练token数相同时，前缀语言模型的**性能往往略低**于传统语言模型任务。

另外，自回归的loss：

+ 训练时：是可以并行的，因为每个位置的label是已知的，可以并行算，
+ 预测时：是串行的，因为得预测完了第t个词，才能去预测第t+1个词。

### 去噪自编码

&nbsp;

DAE是BERT待模型的常见任务，即MLM（masked language model），输入$$\mathbf{x}_{\backslash \tilde{\mathbf{x}}}$$是一些**有随机替换区间的损坏文本**，目标是恢复被替换的token $$\tilde{\mathbf{x}}$$：

XXX
\mathcal{L}_{D A E}(\mathbf{x})=\log P\left(\tilde{\mathbf{x}} \mid \mathbf{x}_{\backslash \tilde{\mathbf{x}}}\right)
XXX

在T5和GLM-130B中使用，**自回归地恢复替换区间**。

### 其他任务

multi-token prediction，一次预估未来的k个词

[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)

![multi-token-prediction](../assets/multi-token-prediction.png)


# 典型LLM简介

llm榜单：

[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

![llm-families](../assets/llm-families.png)


## GPT系列

### GPT3

&nbsp;

2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning。

### CODEX

&nbsp;

[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)

能够解析自然语言，并生成代码。CODEX是gpt3在github上收集的代码语料上进行finetune得到的，并且在微软的copilot中使用。

### WebGPT

&nbsp;

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[https://openai.com/blog/webgpt/](https://openai.com/blog/webgpt/)

为了回答开放性问题，使用基于文本的浏览器对gpt3进行finetune，包括如下3个步骤：

+ 学习使用人类示范（demonstration）数据来模仿人类的浏览行为
+ 学习一个reward函数来预测人类偏好
+ 用强化学习和拒绝采样来优化reward函数


注：[重要性采样和拒绝采样](https://zhuanlan.zhihu.com/p/664233442)

+ **重要性采样**的关键是**降低方差**：因为相同的样本量，用$$\pi(x)$$分布采样得到的结果方差较大(或者是$$\pi(x)$$不好采样)，而用$$p(x)$$采样的样本得到的结果方差较小，用来估计原分布$$\pi(x)$$
+ **拒绝采样**：引入易于采样的分布$$Q(x)$$，然后从中随机地筛掉某些样本(根据**接受概率**接受或者拒绝样本)，使得剩下的样本服从分布$$P(x)$$

拒绝采样的步骤：

+ 从辅助分布$$Q(x)$$中采样得到样本$$x_i$$
+ 计算接受概率$$A = P(x_i) / (M \times Q(x_i))$$，其中$$M$$是一个常数，满足$$P(x) \leq M \times Q(x)$$对于所有$$x$$成立
+ 以概率$$A$$接受样本$$x_i$$，即生成一个随机数$$u$$，如果$$u \leq A$$，则接受样本$$x_i$$；否则拒绝样本$$x_i$$。

重复上述步骤，直到获得足够数量的样本。


### InstructGPT

&nbsp;

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

sft+rm+rl，在最小性能降低的情况下，提升了生成结果的真实性，并降低了毒害性

### ChatGPT&GPT-4

&nbsp;

2022.11.30推出了ChatGPT，基于GPT3.5，即InstructGPT的兄弟

2023.3推出了GPT-4，多模态LLM，能输入图像和文本

o3/o4-mini：[Agent 要被吃进大模型了](https://mp.weixin.qq.com/s/89QkOqY3vWzQvPPbGkBodQ)

## LLaMA系列

### LLaMA

&nbsp;

2023年2月发布，[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)，开源的LLaMA-13B比 GPT3 175B在很多任务上都更好

参考代码：
[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

[https://github.com/meta-llama/llama](https://github.com/meta-llama/llama)

之前的工作考虑的是在训练预算有限的前提下，如何提升模型性能（2022年deepmind的[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)的Chinchilla）,llama考虑在预测时的预算。例如chinchilla是一个10b的模型在200b的token上训练，但其实一个7b的模型当用了1T的token后，性能仍在提升。LLama-13b比gpt3在大多数benchmark上好，但size只有1/10，在一个GPU上就能跑。

llama只用公开数据训练，而Chinchilla、PaLM、GPT-3都有自己的未公开数据集。其他的OPT、GPT-NeoX、BLOOM、GLM虽然也只用公开数据集，但打不过PaLM-62B或者Chinchilla

#### 预训练数据

&nbsp;

+ English CommonCrawl(67%)：使用CCNet pipeline，去重、用fasttext把非英文的页面删了，用n-gram把低质内容删了。此外，还训了一个线性模型，对页面进行分类：作为维基百科的引用 vs 随机采样的页面，最后把不属于引用这个类别的页面删了
+ C4(15%)：与CCNet类似，主要区别在质量过滤是基于启发式的规则，如标点符号的存在，或者词数和句子数
+ github(4.5%)：使用Google BigQuery里的公开github数据集，只用Apache、BSD和MIT证书的。低质判断是启发式规则，如字母数字占比、行的长度等，用正则删掉head等样式，最终以文件粒度进行去重。
+ wikipedia(4.5%)：2022年6-8月的数据，包括20种语言
+ Gutenberg and Books3(4.5%)：两个书籍数据集，对有90%以上内容重复的书籍做去重。
+ Arxiv(2.5%)：拿原始的tex文件，删掉first section之前的东西，还有一些注释、宏
+ Stack Exchange(2%)：高质量的问答网站，按答案的分数排序

![llama_data](../assets/llama_data.png)

tokenizer：BPE，使用sentencepiece的实现。将所有numbers切成单个数字，回退到字节去处理未知的utf8字符（fallback to bytes to decompose unknown UTF-8 characters）

总共有1.4T的token，对大部分训练数据，每个token在训练时只用了一次，除了维基和book大概用了两次。

附：gpt4说：当我们说"一个token只训练一次"，我们其实是在说在一个epoch（一个完整遍历训练集的过程）中，我们只遍历一次完整的数据集。如果一个特定的token在数据集中出现多次，那么在一个epoch中，这个token就会被用来训练模型多次。

![llama](../assets/llama_params.png)


![一些大模型](../assets/LLM/WechatIMG322.jpg)


#### 网络结构

&nbsp;

+ SwiGLU激活函数(PaLM)：取代ReLU，[Glu variants improve trans- former](https://arxiv.org/abs/2002.05202)，把PaLM里的$$4d$$改了$$2/34d$$

说白了就是输入$$x$$，SwiGLU激活完是$$swish(w_1(x)) * w_3(x)$$，其中swish又叫silu，是$$f(x)=x \cdot sigmoid(x)$$

然后再过一个$$w_2$$，得到$$w_2(swish(w_1(x)) * w_3(x))$$就是最终的ffn输出

以下是transformers里的实现：[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act] ## 默认是silu，即swish

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) 
                for i in range(self.config.pretraining_tp)], dim=-1)

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

这个是llama官方代码的实现：[https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345](https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345)

```python
class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
        ffn_dim_multiplier: Optional[float],
    ):
        """
        Initialize the FeedForward module.

        Args:
            dim (int): Input dimension.
            hidden_dim (int): Hidden dimension of the feedforward layer.
            multiple_of (int): Value to ensure hidden dimension is 
                a multiple of this value.
            ffn_dim_multiplier (float, optional): 
                Custom multiplier for hidden dimension. Defaults to None.

        Attributes:
            w1 (ColumnParallelLinear): Linear transformation for the first layer.
            w2 (RowParallelLinear): Linear transformation for the second layer.
            w3 (ColumnParallelLinear): Linear transformation for the third layer.

        """
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if ffn_dim_multiplier is not None:
            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```


+ Rotary embeddings(GPTNeo)：删掉原来的绝对位置编码，加上rotary positional embedding(RoPE)，网络的每一层都加，参考[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)
+ pre-normalization(gpt3)：提升训练**稳定性**，对每个子层的输入做norm，而非输出。此外，使用的是RMSNorm函数([Root mean square layer normalization](https://arxiv.org/abs/1910.07467))取代标准的layer-norm
    + layernorm计算单个样本在单层中所有激活的均值和标准差，并使用这些统计数据来归一化该层的激活。
    + RMSnorm只计算激活的平方根均值（RMS），而不是标准差。这样做的一个好处是计算上更简单，因为它省去了计算均值的步骤，只关注激活的规模（scale）而非其准确的分布。$$\operatorname{RMSNorm}\left(x_i\right)=\frac{x_i}{\sqrt{\frac{1}{H} \sum_{j=1}^H x_j^2}+\epsilon}$$，其中$$H$$是该层的神经元个数，而且也不用求均值

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        Initialize the RMSNorm normalization layer.

        Args:
            dim (int): The dimension of the input tensor.
            eps (float, optional): A small value added to the denominator 
                for numerical stability. Default is 1e-6.

        Attributes:
            eps (float): A small value added to the denominator 
                for numerical stability.
            weight (nn.Parameter): Learnable scaling parameter.

        """
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        Apply the RMSNorm normalization to the input tensor.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The normalized tensor.

        """
        # rsqrt(x)= 1/ sqrt(x)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        Forward pass through the RMSNorm layer.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor after applying RMSNorm.

        """
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

优化器：AdamW，cosine学习率schedule，最终学习率是最大学习率的10%。0.1的weight decay和1.0的gradient cliping，使用2000steps的warmup

#### 训练加速

&nbsp;

+ 对causal multi-head attention加速：实现在[http://github.com/facebookresearch/xformers](http://github.com/facebookresearch/xformers)中，降低内存使用和运行时间，参考[self-attention does not need $$o(n^2)$$ memory](https://arxiv.org/pdf/2112.05682.pdf)，以及[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)。思想是
    + 不存储attention weights
    + 不计算被mask的key/query得分
+ 减少xxx：


### LLaMA2

&nbsp;

2023年7月，[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

+ 基于公开数据集预自监督地训练一个llama-2
+ llama-2-chat模型:
    + sft后得到初始版本
    + 使用RLHF迭代地更新（拒绝采样+ppo） 

![llama-2-chat](../assets/llama-2-chat.png)

[https://zhuanlan.zhihu.com/p/636784644](https://zhuanlan.zhihu.com/p/636784644)

使用了GQA（grouped query attention）(参考[Gqa: Training generalized multi-query transformer models from multi-head checkpoints](https://arxiv.org/pdf/2305.13245.pdf))，在注意力机制中对**K/V进行参数共享**的方案，可以在**推理**过程中**减小KV缓存**。

![gqa](../assets/gqa.png)


### LLaMA3

#### 原始LLaMa3

&nbsp;

2024年4月

[开源大模型Llama 3王者归来！最大底牌4000亿参数，性能直逼GPT-4](https://mp.weixin.qq.com/s/KCyL8WTzXutPQ_k0Vl9Vwg)

[Llama 3超大杯有何惊喜？Meta会一直开源吗？当初为何笃信元宇宙？扎克伯格新访谈回应一切](https://mp.weixin.qq.com/s/e2n4ttcT8raDU877t53GPQ)

[Llama 3细节公布！AI产品总监站台讲解：Llama系列超庞大生态系统](https://mp.weixin.qq.com/s/iDAlop_LNv9evZtfPMPyUg)

[OpenAI 前创始成员、特斯拉自动驾驶前负责人 Andrej Karpathy 发表 Meta Llama 3 笔记](https://mp.weixin.qq.com/s/701PSyi954QHz_mt6Ddn-Q)

[Karpathy称赞，从零实现LLaMa3项目爆火，半天1.5k star](https://mp.weixin.qq.com/s/1poG0tEjmym1456mmR66nQ)

[Karpathy点赞，这份报告教你如何用 LLaMa 3创建高质量网络数据集](https://mp.weixin.qq.com/s/luZGMG1RRUT4X_ckt8hsCQ)

[https://github.com/naklecha/llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)

三个版本：8B 和 70B 参数的模型，还有一个 405B 参数的密集模型（还在训练之中，但已经在逼近GPT-4的领域，例如84.8 MMLU vs. 86.5 4Turbo），8B版本基本上与Llama-2的最大版本一样强大。

Llama 3的主要亮点：

+ 训练语料：基于超过15T token训练，比Llama 2数据集(2T)的7倍还多：
    + **scaling law的新发现**：对于一个8B模型，Chinchilla的**计算最优点**将是训练约200B词汇(前面提到了10B模型，大概要205B的token来训练，以此类推)，所以这里**超出了75倍**，而且**还未收敛**。可见，我们经常使用的LLMs在训练上显著不足，可能是100-1000倍或更多，远未达到它们的收敛点。
+ tokenizer：词汇数量从Llama 2的32K增加到Llama 3的128K，增加了4倍，拥有更多的词汇可以在长度上**更有效地压缩序列**。
+ 上下文窗口：从Llama 2的4096和Llama 1的2048增加到了**8192**，相比GPT4的128k还差得很远
+ 训练效率：比 Llama 2 高 3 倍，做了很多工程优化；
+ 模型结构：在Llama 2中，只在更大的模型使用了**分组查询注意力（GQA）**，但llama3的所有模型都使用了，包括最小的8B模型。
+ 新能力范畴：Llama-2 只能使用非常特定的工具，而 Llama-3 能使用好得多的工具，无需人工编程就能让其使用谷歌执行搜索，类似的功能还有编程和运行代码等。

[https://github.com/meta-llama/](https://github.com/meta-llama/)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

#### 多模态llama3

&nbsp;

[多模态 Llama-3 它来了 ！！[全网首发微调教程]](https://mp.weixin.qq.com/s/IFVIkVvMqeAL0lZF9OvRSA)

[https://github.com/InternLM/XTuner](https://github.com/InternLM/XTuner)

#### 中文llama3

&nbsp;

[首批中文版Llama3模型来了，解释成语、答弱智吧问题](https://mp.weixin.qq.com/s/ny0gBOxf4-tJiwjgp3o9HQ)

[https://github.com/CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat)

[https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)


### LLama3.1

[最强模型Llama 3.1 405B正式发布，扎克伯格：开源引领新时代](https://mp.weixin.qq.com/s/QUWumWsTF_Qq77tdlyCHdg)

[微调大模型，AMD MI300X就够了！跟着这篇博客微调Llama 3.1 405B，效果媲美H100](https://mp.weixin.qq.com/s/eJwg4GwH--9IVFedum_BnA)

[The Llama 3 Herd of Models](https://github.com/daiwk/collections/blob/master/assets/LLM/llama3.1.pdf)

[Llama3 92页技术报告中文全文详解](https://mp.weixin.qq.com/s/9Mnd1ass0XYUGW_UqDF8bw)

还开放了一个生态系统：

+ model：[https://github.com/meta-llama/llama-models/tree/main/models/llama3_1](https://github.com/meta-llama/llama-models/tree/main/models/llama3_1)
+ tool-chain：[https://github.com/meta-llama/llama-toolchain](https://github.com/meta-llama/llama-toolchain)
+ agent-system：[https://github.com/meta-llama/llama-agentic-system](https://github.com/meta-llama/llama-agentic-system)


#### 整体架构

&nbsp;

![multimodal-llama3.1](../assets/multimodal-llama3.1.png)

有2个主要的stage：

+ language model pre-training：用多语言的语料进行next token prediction
    + 使用超过15万亿(15T)个token训练Llama 3.1 405B，context window是8k tokens
    + 又加上一个continued pretraining stage，将context windows增加到128k tokens
+ language model post-training：sft+dpo+新的能力（如工具使用，能提升代码和推理能力）+safety

(toread)

这里有一些总结：[关于post-training和一些思考](https://mp.weixin.qq.com/s/Bpd6_zq9kmTTeHxZ9WJJpw)

为了支持多模态，还加入了如下3个stage：

+ 多模态encoder pre-training：
    + image encoder：在大量的img-text pair上训练，让模型能理解图片+描述
    + speech encoder：自监督，mask掉部分的speech输入，通过离散token表示来重建被mask的部分
+ vision adapter training：
    + 训练一个adapter将pretrained的image encoder融入到pretrained language model里，adapter包括一系列的cross-attention层将image-encoder表示输入language model。在image-text pair上训练，对齐图文表示。训练时**更新image-encoder的参数**，但**不更新language model的参数。**
    + 基于这个image-encoder来训练一个video-adapter，用的是video-text数据，让模型能聚合多帧间的信息。视频侧的temporal aggregator是有一个perceiver的resampler
+ speech adapter training：将speech encodings转成token表示，直接输入finetuned language model。在sft阶段，**adaper和encoder的参数联合更新**，但**不更新language model的参数**，还集成进了一个text-to-speech的系统。


#### 预训练

&nbsp;

在超过16,000个H100（80G HBM3）上训练，训练平台是[Meta open compute project, grand teton ai platform](https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/)，基于[MAST: Global scheduling of ml training across geo-distributed datacenters at hyperscale](https://www.usenix.org/system/files/osdi24-choudhury.pdf)进行schedule

选择了decoder only transformer with minor adaptations，而**不是MOE**，以最大限度地**提高训练稳定性**。

+ 使用GQA with 8 key-value heads，提升推理速度，在decoding时降低k-v cache的大小
+ 使用attention mask，**让同一序列里的不同documents不计算self-attention**。在标准的pretraining中影响不大，但**对于超长序列的continued pre-training非常重要**
+ 128K tokens的vocab，100k的tiktoken tokenizer+28k的额外token，更好地支持非英语的语言。相比llama2，每个token能压缩的字符3.17变成3.94，**压缩率更高了**，即同样计算量能读更多文本，同时也能提升下游任务效果。
+ 把RoPE的base frequency超参**加大到500,000**，能更好地**支持更长contexts**，[Effective Long-Context Scaling of Foundation Models](https://aclanthology.org/2024.naacl-long.260.pdf)说这个值对32768长度的context很有效。


参数量：

|                          | 8B          | 70B         | 405B        |
|--------------------------|-------------|-------------|-------------|
| **Layers**               | 32          | 80          | 126         |
| **Model Dimension**      | 4,096       | 8192        | 16,384      |
| **FFN Dimension**        | 6,144       | 12,288      | 20,480      |
| **Attention Heads**      | 32          | 64          | 128         |
| **Key/Value Heads**      | 8           | 8           | 8           |
| **Peak Learning Rate**   | 3e-4    | 1.5e-4  | 8e-5    |
| **Activation Function**  | SwiGLU      | SwiGLU      | SwiGLU      |
| **Vocabulary Size**      | 128,000     | 128,000     | 128,000     |
| **Positional Embeddings**| RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) |


#### 后训练

&nbsp;

采用iterative post-training procedure，即预训练后进行多轮对齐，每轮都使用sft、RS（拒绝采样）、直接偏好优化(DPO, Direct Preference Optimization)，能够为**每轮**创建**最高质量的合成数据**，并提高每项能力（capability）的性能。

+ 使用**合成数据生成(synthetic data generation)**来产生绝大多数SFT示例，并**多次迭代**以在所有能力生成越来越高质量的合成数据。
+ 采用了多种数据处理技术来过滤这些合成数据，达到最高质量，并可以**跨能力**来**扩展微调数据量**。

#### 推理

&nbsp;

从bf16量化为fp8

+ 推理：对精度要求较高，对数值范围要求偏低，故一般用FP8-E4M3
+ 训练：Gradients通常对精度损失不太敏感，但需要更高的范围(例如clip变小的时候，模型效果会变差，说明grad的range是比较大的)，故一般用FP8-E5M2

![](../assets/fp8.jpeg)

- 离线量化： 在推理前，利用离线dump数据/训练依赖/权重本身，产生量化scale，保存在模型中，推理中直接使用。history 
- 在线量化： 推理中，利用输入数据在线计算scale，需要在线统计a/b的amax，推理速度慢。就是current scale

### LLama-3.1-Minitron

[英伟达玩转剪枝、蒸馏：把Llama 3.1 8B参数减半，性能同尺寸更强](https://mp.weixin.qq.com/s/zxW9EagxGJX-rS5loNLKXw)

[Compact Language Models via Pruning and Knowledge Distillation](https://www.arxiv.org/pdf/2407.14679)

NVIDIA用TensorRT-LLM优化了Llama 3.1 8B和Llama-3.1-Minitron 4B模型。

### LLama-Nemotron

[英伟达253B开源新王登场，Llama 4三天变陪衬！直逼DeepSeek-R1成推理天花板](https://mp.weixin.qq.com/s/QbUTBKG9vrIVTA-6qII2gg)

[英伟达开源最新大模型Nemotron 70B后，只有OpenAI o1一个对手了](https://mp.weixin.qq.com/s/ebJkBkGAn8QS-_xVK__MMw)

[公开模型一切，优于DeepSeek-R1，英伟达开源Llama-Nemotron家族](https://mp.weixin.qq.com/s/Ofw7l6XPNNinXvFReGI3vw)

[Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/pdf/2505.00949)

[https://github.com/NVIDIA/NeMo-Aligner/tree/llama-nemotron-dev](https://github.com/NVIDIA/NeMo-Aligner/tree/llama-nemotron-dev)

[https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo)

[https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b](https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b)

post training的数据集：

[https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)

开源了LN-Nano（8B）、LN-Super（49B）与LN-Ultra（253B），还有独立变体LN-UltraLong（8B，支持超长上下文）。

#### 构建面向推理优化的模型

&nbsp;

LN-Super和LN-Ultra通过Puzzle框架实现高效推理优化。Puzzle是一个神经网络架构搜索（Neural Architecture Search, NAS）框架，能够在真实部署约束条件下，将大型语言模型转化为面向硬件高效的变体([Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146))

![](../assets/llama-nemotron-puzzle.png)

以Llama 3 Instruct模型为起点（LN-Super(49B)基于Llama3.3-70B-Instruct，LN-Ultra(253B)基于Llama 3.1-405B-Instruct），Puzzle通过逐模块局部蒸馏（block-wise local distillation）方法构建可替代的Transformer模块库。每个模块独立训练且可**并行处理**，允许每个替代模块在**精度与效率之间进行权衡**（accuracy-efficiency tradeoff），模块变体主要包括以下几种类型：

+ **移除注意力机制（Attention removal）**：部分模块**完全省略注意力机制**，从而显著减少计算开销和KV cache内存占用。 
+ **可变FFN维度（Variable FFN dimensions）**：通过调整**FFN的中间维度**，在不同粒度下实现模型压缩（如将隐藏层维度压缩至原始的87%、75%、50%，甚至低至 10%）。

尽管Puzzle同样支持其他结构替换方式（如多GQA中不同的kv head数、线性注意力替代方案、以及不执行操作的替换模块），但实际评估结果表明，在优化LN-Super和LN-Ultra两个模型的总体吞吐量与内存节省方面，最有效的技术仍是移除注意力机制与FFN压缩。

在模块库构建完成后，Puzzle通过逐层选取模块的方式组装完整模型。模块选择过程由**整数混合规划**（Mixed-Integer Programming, MIP）**求解器**控制，在给定的约束条件下（如硬件兼容性、最大推理延迟、总内存预算或指定推理吞吐量）确定效率最优的模块配置。

由于每一层支持多个具有不同精确度–效率权衡方案的模块变体，Puzzle允许用户精确定位至任何位于精度-效率帕累托前沿（Pareto frontier）上的模型配置点。例如，Puzzle 可生成满足特定智能体系统（agentic systems）或部署流程所需约束（如内存不可超出上限或端到端响应时间严格受限）的模型。

**用FFN融合实现纵向压缩（Vertical Compression with FFN Fusion）**：针对LN-Ultra模型，引入了一种额外的压缩技术——FFN融合（FFN Fusion），该方法旨在降低模型的序列深度，并进一步缩短推理延迟。利用Puzzle移除部分注意力层后的结构特性：在这种结构下，模型中经常会出现**连续的FFN模块序列**。FFN Fusion会识别出这类序列，并将其替换为**更少但更宽的FFN层**，这些宽层可并行执行，从而减少序列处理步骤的数量，同时保留模型的表达能力，还能显著提升计算资源的利用率，特别是在多GPU环境中，可以有效降低跨层通信带来的开销。

在NAS之后，进行了continue pretraining(CPT)：

+ LN-Super：使用([Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/abs/2411.19146)里的Distillation Mix数据集，以知识蒸馏目标函数训练了400亿个Token
+ LN-Ultra：首先使用相同的蒸馏数据集进行了650亿Token的蒸馏训练，随后在Nemotron-H第四阶段预训练数据集上进行了额外880亿Token的持续预训练。

LN-Ultra不仅实现了与基准模型Llama 3.1-405B-Instruct相当的性能，还在多个关键基准测试上取得超越。说明**即使进行激进的架构优化，也可通过短周期的蒸馏与预训练恢复并提升模型性能。**

#### 合成数据

&nbsp;

设计了「detailed thinking on/off」指令机制，在合成数据中**显式标记是否需要展开详细推理过程**，引导模型在训练中学习何时进行逐步思考、展示推理链条，何时直接给出简明答案。指令为「on」时，模型输出完整的中间推理过程并展示解题思路；指令为「off」时，模型仅呈现最终结果。

拿deepseek-r1、llama-4等作为teacher来生产数据，产出的数据总共33,011,757条，里面66%是数学的，30%是代码的（这两部分里都是reasoning off占大头），剩下的是科学和instruction following

#### SFT

&nbsp;

基于合成数据来进行SFT，不同大小的student用的seq_len、epoch数不太一样，也有一些小trick，详见原文

#### 推理能力强化学习

&nbsp;

只蒸馏的话，student很难超越teacher，所以需要强化学习，考虑到资源问题，只对LN-Ultra搞了GRPO

两个reward：

+ 准确率奖励（Accuracy rewards）：每个训练样本提供标准答案（数字、句子或段落），使用Llama-3.3-70B-Instruct模型判定策略模型响应是否与标准答案一致。
+ 格式奖励（Format rewards）：和DeepSeek一致，在detailed thinking on时，需将推理过程置于```<think></think>```标签之中；而在off时，确保不包含思考标签。

一些trick：

+ 样本过滤：由LN-Super为每道题生成8个独立回答，计算通过率，并过滤通过率大于等于0.75的样本，提升总体训练数据难度。
+ 课程学习（curriculum learning）：渐进式批处理策略（progressive batching），使用预计算通过率作为样本难度指标，在固定batch size下，动态计算每个batch的目标难度分布。该分布以高斯函数建模，从早期批次集中在高通过率（简单样本），逐步过渡至后期批次的低通过率（高难度样本）。每个batch中，样本按目标分布随机分配，并根据不同通过率池中剩余样本量进行容量填充。**确保样本难度在batch层面逐步递进，同时batch内部保持随机性。**
+ 基于fp8的生成阶段：修改vLLM，支持bf16转fp8，然后用fp8做gemm

#### 用于偏好优化的强化学习

&nbsp;

+ 指令跟随：在前面的科学推理任务的强化学习之后，对LN-Super和LN-Ultra再进行短期的强化学习，提升指令跟随能力。参考[Instruction-following evaluation for large language models](https://arxiv.org/abs/2311.07911)，生成包含1至10条详细指令的合成提示词，用RLOO（[Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms](https://arxiv.org/abs/2402.14740)）在batch128跑了120个steps，用自定义指令跟随验证器作为奖励函数。发现能同时提升指令跟随任务和推理类任务的效果。
+ RLHF：增强模型的通用协助能力（helpfulness）与多轮聊天能力
    + LN-Super：用iterative online RPO（[ Reward-aware preference optimization: A unified mathematical framework for model alignment](https://arxiv.org/abs/2502.00203)）来最大化HelpSteer2数据集上的Llama-3.1-Nemotron-70B-Reward
    + LN-Ultra：一样的方法，只是换成了GRPO
    + LN-Nano：2轮的offline RPO
        + 第一轮：用混合了reasoning和非reasoning的数据，来提升reasoning控制能力
        + 第二轮：用使用当前正在训练（优化）的策略生成的数据（on-policy的数据），来提升指令遵循能力

### Llama-3.2

[刚刚，Llama 3.2 来了！支持图像推理，还有可在手机上运行的版本](https://mp.weixin.qq.com/s/3JP9UgfXNMlI5jaYHyekYA)

Llama 3.2 

+ 最大的两个模型11B和90B：都支持图像推理，包括文档级的图表理解、图像描述和视觉定位任务，比如直接根据自然语言描述定位图像中的事物。
+ 轻量级的1B和3B：都是纯文本模型，但也具备多语言文本生成和工具调用能力。

[Sebastian Raschka最新博客：从头开始，用Llama 2构建Llama 3.2](https://mp.weixin.qq.com/s/RuTRkJPeEP1hqWevxn9h6Q)

### Llama-3.3

[新版Llama 3 70B反超405B！Meta开卷后训练，谷歌马斯克都来抢镜](https://mp.weixin.qq.com/s/6Iv4VzMlYrkmSsAo_IRGTg)

Llama 3.3能用70B实现405B的效果，主要是“运用了后训练技术的最新进展”，其中包括在线偏好优化（online preference optimization）。

[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)

[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)


### Llama-4

[Meta深夜开源Llama 4！首次采用MoE，惊人千万token上下文，竞技场超越DeepSeek](https://mp.weixin.qq.com/s/Kqk36zfBrZvu6IpCp2lBbw)

[https://ai.meta.com/blog/llama-4-multimodal-intelligence/](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)


| 模型 | 结构 | 参数量 | 特点 |
|------|-----|------|---------|
| Llama 4 Behemoth | 多模态MoE，288B激活，16专家 | 2T | 还没训完，暂未开源|
| Llama 4 Maverick | 多模态MoE，17B激活，128专家 | 400B |推理+编程与deepseek v3相当 |
| Llama 4 Scout | 多模态MoE，17B激活，16专家 | 109B|10M上下文窗口 |

+ 架构：
    + iRoPE架构：使用无位置嵌入的交错注意力层（interleaved attention layers），并通过推理时的温度缩放([Scalable-Softmax Is Superior for Attention](https://arxiv.org/pdf/2501.19399))来增强长上下文泛化能力。
    + 原生多模态设计：通过早期融合将文本和视觉token无缝整合到统一的模型骨干中，能够使用大量未标记的文本、图像和视频数据对模型进行联合预训练。此外，还改进了Llama 4中的视觉编码器，基于MetaCLIP，以更好地使编码器适应LLM。
+ 训练：
    + MetaP：能够可靠地设置模型超参数，例如每层的学习率和初始化规模。选定的超参数在不同批量大小、模型宽度、深度和训练 token 值之间具有良好的迁移性。
    + 数据：200种语言的数据，比llama3多了10倍的多语言token；训练所用的数据混合总量超过30万亿（30T）token，是Llama 3预训练数据混合量的两倍多，涵盖了多样化的文本、图像和视频数据集。
    + fp8训练：兼具质量并确保高FLOPs利用率，使用FP8和32K GPU预训练Llama 4 Behemoth模型时，实现了每GPU有390 TFLOPs。
+ mid-training：
    + 利用专门的数据集扩展长上下文，使得Llama 4 Scout具备业界领先的1000万(10M)输入上下文长度。
+ post-training：
    + 轻量级监督微调（SFT）> 在线强化学习（RL）> 轻量级直接偏好优化（DPO）。发现SFT和DPO可能会过度约束模型，限制在线RL阶段的探索能力:
        + 轻量级监督微调（SFT）：Meta 使用Llama模型作为评判，移除了超过50%的标记为简单的数据，并在剩余较难的数据集上进行SFT。剪枝比例：2T的模型95%，较小模型50%
        + RL：
            + 通过policy model进行pass@k分析，采样难度较高的prompt，并构建难度逐渐增加的训练课程
            + 训练过程中动态过滤掉零advantage的prompt，并构建包含多种能力的混合prompt训练batch
            + 从多种系统指令中采样，确保模型在推理和编码任务中保持指令遵循能力(这个可能讲的是dpo？)
        + 轻量级直接偏好优化（DPO）：处理与模型响应质量相关的边缘情况，有效实现了模型智能与对话能力的良好平衡。
+ 新的框架：
    + 现有的分布式训练框架为了将所有模型加载到内存中而牺牲了计算内存，Meta的新基础设施能够灵活地将不同模型分配到不同的GPU上，并根据计算速度在多个模型之间平衡资源。
    + 对MoE并行化的设计进行了优化
    + 完全异步的在线强化学习训练框架

### Alpaca

[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html?trk=cndc-detail)

Stanford的羊驼（Alpaca）模型，有70亿（7b）参数，**没有使用RLHF**，而是使用**监督学习**的方法，参考[Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)（代码[https://github.com/yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)）

数据集是通过查询基于GPT-3的text-davinci-003模型的结果，得到的52k的指令-输出对（instruction-output pairs）。

因此，Alpaca本质上使用的是一种弱监督（weakly supervised）或以知识蒸馏（knowledge-distillation-flavored）为主的微调，即“**用 LLM 来训练 LLM**”。

![Alpaca](../assets/alpaca.jpeg)

[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

### Vicuna

通过ShareGPT收集的用户对话数据，对llama进行finetune得到的13B模型。效果接近chatgpt的92%，而且训练消耗比较低，大概只要300美元。

### Guanaco

&nbsp;

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)也是对llama进行微调，使用了QLoRA，能够在一台48G的GPU上微调65B的模型。只需要在单台GPU上finetune 24小时就能达到99.3%的chatgpt的效果。

[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)

[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

QLoRA将一个固定的4 bit量化的预训练权重转成low Rank Adapters来更新梯度

### Koala

&nbsp;

[Koala: A dialogue model for academic research](https://bair.berkeley.edu/blog/2023/04/03/koala/)

使用用户与闭源大模型交互的用户输入和模型返回数据进行训练

### Mistral

&nbsp;

[Mistral 7b](https://arxiv.org/pdf/2310.06825.pdf)

7B参数比最好的13B模型（llama-2-13B）要更好，而且比llama-34B在reasoning、数学、代码生成都更好。

+ 使用**grouped-query attention**来做更快的infer
+ 使用**滑动窗口attention**来用更低的infer消耗来高效地处理**任意长度的序列**。

[https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/) 提出了moe的8x7b

[Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)

[Mixtral 8x7B(Mistral MoE) 模型解析](https://mp.weixin.qq.com/s/-5yp0KU6_vkpWmY9wROWbg)

[Mistral开源8X22B大模型，OpenAI更新GPT-4 Turbo视觉，都在欺负谷歌](https://mp.weixin.qq.com/s/hf4uq3yrHxTGhQuzl61Imw)

[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)

[Mistral AI两连发：7B数学推理专用、Mamba2架构代码大模型](https://mp.weixin.qq.com/s/fFB0A0vv_2Deb0rWd4tagw)

mathtral：[https://huggingface.co/mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)

codestral-mamba: [https://huggingface.co/mistralai/mamba-codestral-7B-v0.1](https://huggingface.co/mistralai/mamba-codestral-7B-v0.1)，基于mamba2，可以直接用trt-llm启动

[准狙击Llama 3.1？Mistral AI开源Large 2，123B媲美Llama 405B](https://mp.weixin.qq.com/s/6_d_e6DlQpyONdWxd8EZvA)

Mistral AI 基于此前Codestral 22B和Codestral Mamba的经验，在很大一部分代码上训练了 Mistral Large 2。其表现远远优于上一代的Mistral Large，并且与GPT-4o、Claude 3 Opus和Llama 3 405B等顶尖模型相当。

[https://huggingface.co/mistralai/Mistral-Large-Instruct-2407](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)

[​Mistral首个多模态模型Pixtral 12B来了！还是直接放出24GB磁力链接](https://mp.weixin.qq.com/s/c_fmpkuimmEtY6pD9f_bZQ)

[https://huggingface.co/mistral-community/pixtral-12b-240910](https://huggingface.co/mistral-community/pixtral-12b-240910)

[发力了，Mistral对标ChatGPT全面升级le Chat，还祭出超大杯多模态模型](https://mp.weixin.qq.com/s/sABAaxKNGM-odULaM0-_Zg)

[最强代码模型刷新：Mistral新品上线即登顶，上下文窗口增至256k](https://mp.weixin.qq.com/s/BaZhEs7CV8-kQzdrkcMU2A)

[单个4090就能跑，Mistral开源多模态小模型，开发者：用来构建推理模型足够香](https://mp.weixin.qq.com/s/t173XAigACpw9NWPePhUUQ)

### phi-3

[微软发布Phi-3，性能超Llama-3，可手机端运行](https://mp.weixin.qq.com/s/kb_gfaYkXiW_cR22K2bX9g)

[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)

### phi-4

[微软首个多模态Phi-4问世，56亿参数秒杀GPT-4o！LoRA华人大佬带队](https://mp.weixin.qq.com/s/UQK5b2-ZobnQJwr73p5U8Q)

[https://huggingface.co/microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)

## PaLM系列

### PaLM

&nbsp;

2022年4月提出了第一个PaLM：[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)，直到2023年3月还是private的。是一个540B(5400亿)参数的模型，在包含了780B的tokens的高质量数据集上预训练。使用Pathways的6144块TPU v4进行训练。

### U-PaLM

&nbsp;

[Transcending scaling laws with 0.1% extra compute](https://arxiv.org/pdf/2210.11399.pdf)提出了8B，62B和540B的U-PaLM模型，用**UL2R**来对PaLM进行继续训练，用的是UL2的**mixture-of-denoiser objective**([UL2: Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131.pdf))

### Flan-PaLM

&nbsp;

Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）是instrunction-finetuned版本的U-PaLM。使用了更多的任务、更大的模型，以及CoT数据。使用了473个数据集，146类的task，总共1836个task。

![flan-palm](../assets/flan-palm.png)

[https://huggingface.co/google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)


### PaLM-2

&nbsp;

[PaLM 2 Technical Report](https://arxiv.org/pdf/2305.10403.pdf)是一个更计算高效型的LLM，有更好的多语种和reasoning能力，在很多任务上都比PaLM好，并且在infer上比PaLM要更快更高效。

### Med-PaLM

&nbsp;

Nature的[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)提出了Med-PaLM，在PaLM上用parameter-efficient方法进行instruction prompt tuning，使用少量的典型范例（exemplars）让LLM对齐到新领域。

Med-PaLM2([Towards expert- level medical question answering with large language models](https://arxiv.org/pdf/2305.09617.pdf))通过med-domain的finetuning和ensemble refinement prompting，效果比Med-PaLM要好。


## 其他LLM

### FLAN

&nbsp;

[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)

通过instruction tuning，大模型能很好地提升在没见过任务上的zero-shot能力。对一个137B的预训练模型在60个NLP数据集上使用instruction template进行instruction tuning。

![flan](../assets/flan.png)

一些instruction template：

![flan-instruction-template](../assets/flan-instruction-template.png)

[https://github.com/google-research/flan](https://github.com/google-research/flan)

### Gopher

&nbsp;

[Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf)基于152个多样的任务，对比了从44M到208B（Gopher）的不同transformer，发现Gopher在大多数任务上均达到sota：

| Model         | Layers | Number Heads | Key/Value Size | d_model | Max LR     | Batch Size |
|---------------|--------|--------------|----------------|---------------|------------|------------|
| 44M           | 8      | 16           | 32             | 512               | $$6 \times 10^{-4}$$  | 0.25M      |
| 117M          | 12     | 12           | 64             | 768               | $$6 \times 10^{-4}$$  | 0.25M      |
| 417M          | 12     | 12           | 128            | 1,536             | $$2 \times 10^{-4}$$  | 0.25M      |
| 1.4B          | 24     | 16           | 128            | 2,048             | $$2 \times 10^{-4}$$  | 0.25M      |
| 7.1B          | 32     | 32           | 128            | 4,096             | $$1.2 \times 10^{-4}$$  | 2M        |
| Gopher 280B | 80     | 128          | 128            | 16,384            | $$4 \times 10^{-5}$$  | 3M -> 6M    |

### T0

&nbsp;

[Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207)设计了一个系统，将任意nlp任务映射成一个人类可读的prompt格式。训练了一个encoder-decoder的T0模型，输入文本，输出文本，在混合数据集上进行多任务学习。

### ERNIE 3.0

&nbsp;

[ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/pdf/2107.02137)结合自回归网络和autoencoding网络，让模型能够同时做语言理解和语言生成任务，可以支持zero-shot、few-shot和finetuning。有10B参数，在4TB的文本和图谱数据上训练。

![ernie3.0](../assets/ernie3.0.png)

### 文心X1和4.5

&nbsp;

[提前免费！百度连发两款模型，我们实测：能听歌看电影，还会蛐蛐人](https://mp.weixin.qq.com/s/XKim3L7__c9VXJ2bj4FYhA)

推理模型文心X1：

+ 应用了递进式强化学习训练方法
+ 具备 “长思维链”，采用了思维链和行动链耦合的端到端训练方式，不是只学习思考或者只学习行动，而是把思考和行动结合起来
+ 采用了多元统一的奖励系统

文心4.5：

+ FlashMask 动态注意力掩码：[FlashMask: Efficient and Rich Mask Extension of FlashAttention](https://arxiv.org/abs/2410.01359)，通过列式稀疏掩码表示方法，将传统的二维稠密掩码矩阵转换为一维的行索引区间表示，从而显著降低了存储复杂度，从$$O(N^2)$$降低到$$O(N)$$。极致的加速了大模型训练效率，尤其是长序列场景下的训练效率。
+ 多模态异构专家扩展技术：结合多模态数据处理与混合专家模型（MoE）的创新架构，旨在通过针对不同模态特点构建的**异构专家模块**，解决多模态融合中的梯度不均衡问题，提升模型的多模态融合能力。多模态异构专家扩展技术在多个应用场景中展现了显著优势，例如多模态问答任务，这种技术能够更高效地处理复杂的多模态数据。
+ 时空维度表征压缩技术：把图片和视频里的关键信息进行浓缩的方法。在不丢失重要细节的情况下，在时空维度对图片和视频的语义表征进行高效压缩，让这些数据变得更小、更高效。这样一来，训练多模态模型（也就是同时处理文字、图片、视频等多种数据的模型）时就能更快、更省资源。
+ 基于知识点的大规模数据构建技术：知识分级采样优先把重要的、核心的知识挑出来，接着把挑出来的知识点进行压缩，去掉多余的部分，把相关的知识融合在一起。对于那些比较少见、不太好找的知识点，专门合成一些相关内容，确保这些重要的小众知识也不会被遗漏。这样一来，模型幻觉大大降低。
+ 基于自反馈的Post-training技术：通过多种评估方式不断优化模型的后训练方法。简单来说，它让模型在训练过程中不断反思自己的表现，根据不同的评价标准调整自己，从而让模型变得更稳定、更可靠。这种技术还能显著提升预训练模型理解人类意图的能力，让模型的回答更符合人类的期望。

### RETRO

&nbsp;

[参数量仅为4%，性能媲美GPT-3：开发者图解DeepMind的RETRO](https://baijiahao.baidu.com/s?id=1721015293574115195&wfr=spider&for=pc)

[http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/)

[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)

Retrieval Enhanced Transformer(Retro)基于与preceding(前述) tokens的相似度，从大的语料库里检索出文档段（document chunks）作为条件，增强了自回归模型的能力。通过少25%的参数，在Pile数据集上达到了与gpt3和Jurassic-1相当的效果

![retro](../assets/retro.png)

+ 左图：
  + 输入一个长度为12的序列，每4个token一个chunk，切成3个chunk
  + 每个chunk通过freezed的bert去语料库中拿2个相似neighbors出来，每个neighbor过bert得到向量
  + 邻居作为k和v，原来的3个chunk作为k，做attention(CCA, chunked cross attention)
+ 右图：CCA的结构
  + 保证了因果性，即chunk1的邻居只对chunk1的last token以及chunk2的所有token有影响


### GLaM

&nbsp;

[Glam: Efficient scaling of language models with mixture-of-experts](https://arxiv.org/pdf/2112.06905)提出了Generalist Language Model(GLaM)，用稀疏激活的MOE架构，同时能scale模型容量，也能相比dense更可观地降低训练成本。最大的模型有1.2T参数，是gpt3的7倍。但只需要GPT3的1/3的能量来训练，而且在infer时也只有一半的flops，且在29个NLP任务上都有更好的0/1/few shot效果。

![Glam](../assets/glam.png)

如图，2 in 64的结构，每个token只会取64个experts里的top 2相关的expert，对这两个的输出加权平均输入给后面的层


### LaMDA

&nbsp;

[Lamda: Language models for dialog applications](https://arxiv.org/pdf/2201.08239)，有137B的参数，在1.5T的公开对话数据和互联网文本上pretrain，用标注数据微调，以及让模型能够咨询（consult）外部知识源能够让模型在安全性和factual grounding上有很好的改进。

### OPT

&nbsp;

[OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068)提出了从125M到175B的预训练decoder-only模型

### Chinchilla

&nbsp;

### Galactica

&nbsp;

### CodeGen

&nbsp;

### AlexaTM

&nbsp;

### Sparrow

&nbsp;

### MoD

&nbsp;

### BLOOM

&nbsp;

### GLM

&nbsp;

ACL22 [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)

iclr23 [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)


### GLM-4

[GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级](https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg)

[https://github.com/THUDM/GLM-4](https://github.com/THUDM/GLM-4)

### Pythia

&nbsp;

### Orca

&nbsp;

### StarCoder

&nbsp;

### KOSMOS

&nbsp;

### Claude

&nbsp;

[全球最强大模型一夜易主，GPT-4时代终结！Claude 3提前狙击GPT-5，3秒读懂万字论文理解力接近人类](https://mp.weixin.qq.com/s/WqQWS-hiQ1i1Ve6IPH3djw)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)

+ 对齐：使用Constitutional AI([Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf))， explicitly specifying rules and principles based on sources like the UN Declaration of Human Rights.
+ 基于Collective Constitutional AI([Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input))，added an additional principle to Claude’s constitution to encourage respect for disability rights
+ 公开了部分RHLF数据：[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)，对应的论文是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[力压GPT-4o！新王Claude 3.5 Sonnet来了，直接免费可用](https://mp.weixin.qq.com/s/HnQ7D4iDVgWteZZdTJoadg)

[大模型代肝，自动刷《崩铁》升级材料，Claude操纵计算机还能这么用！](https://mp.weixin.qq.com/s/F8CX6_VWebWLy26NeRNQsw)

[The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/pdf/2411.10323)

[https://github.com/showlab/computer_use_ootb](https://github.com/showlab/computer_use_ootb)

### grok

&nbsp;

[马斯克开源Grok-1：3140亿参数迄今最大，权重架构全开放，磁力下载](https://mp.weixin.qq.com/s/hvt5zwoazDx26KOaKuTs_w)

[https://github.com/xai-org/grok-1](https://github.com/xai-org/grok-1)

[马斯克发布Grok 1.5！编码和数学能力大幅提升](https://mp.weixin.qq.com/s/QwM1uk61o1DMpe5EPq9giQ)

+ 上下文窗口提升16倍，达到128k
+ 不使用通用的Python语言+Pytorch框架，采用分布式训练架构，使用Rust、JAX+Kubernetes构建。
+ 提出了自定义训练协调器，可自动检测到有问题的节点，然后剔除。
+ 优化了checkpointing、数据加载和训练重启等流程，最大限度地减少故障停机时间。

[马斯克的首款多模态大模型来了，GPT-4V又被超越了一次](https://mp.weixin.qq.com/s/2GDjZS6ctayAF8e8eFb3CQ)

Grok-1.5V: [https://x.ai/blog/grok-1.5v](https://x.ai/blog/grok-1.5v)

grok-2：[Grok-2来了，能生图识图、性能比肩GPT-4o，马斯克：发展猛如火箭](https://mp.weixin.qq.com/s/nBaY2srcMSzvEoecOyh1Cg)

[目前对Grok 3分析最为透彻的一篇文章](https://mp.weixin.qq.com/s/h5cFZnU4T7K7YscMYc61pQ)


### Octopus

[超越GPT-4，斯坦福团队手机可跑的大模型火了，一夜下载量超2k](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)

[Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744.pdf)

[https://huggingface.co/NexaAIDev/Octopus-v2](https://huggingface.co/NexaAIDev/Octopus-v2)

[参数量不到10亿的OctopusV3，如何媲美GPT-4V和GPT-4？](https://mp.weixin.qq.com/s/mUpX-nvo221WVii-gnjUmQ)

[Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/pdf/2404.11459.pdf)

[Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)

### Cohere Command R+

[开源模型打败GPT-4！LLM竞技场最新战报，Cohere Command R+上线](https://mp.weixin.qq.com/s/uUeGQFWel5NLfFRFFF8w9g)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit](https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit)

1040亿的参数量，相比于Grok-1（3140亿）还差了一些，但Command R+并非Grok那种MoE架构，所以这1040亿参数是实打实的完全用于推理，而Grok-1的活跃参数为860亿。相比commend R：

+ 高级检索增强生成（RAG）与引用以减少幻觉
+ 10种主要语言的多语言覆盖，支持全球业务运营
+ 工具的运用以自动化复杂的业务流程

[明确了：文本数据中加点代码，训练出的大模型更强、更通用](https://mp.weixin.qq.com/s/3Ks72bIGZrNNUukqaHLSQg)

[To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)


### CT-LLM：以中文为中心的LLM

[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/pdf/2404.04167.pdf)

当前，绝大多数LLM基本上都是以英文语料库训练得到的，然后经过SFT来匹配不同的语种。本文作者考虑以中文为基础的预训练模型是否可以激活对其它语言的能力。

作者从头开始训练中文大模型，在训练过程中「主要纳入中文文本数据」，最终作者得到了一个2B规模的中文Tiny LLM（CT-LLM）。结果表明，该模型在中文任务上表现出色，且通过SFT也能很好的支持英文。

### OpenELM

[苹果卷开源大模型，公开代码、权重、数据集、训练全过程，OpenELM亮相](https://mp.weixin.qq.com/s/uwDoKG2Q9-w37ogewBJTrQ)

[OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619.pdf)

[https://github.com/apple/corenet](https://github.com/apple/corenet)

[超强Siri即将到来！苹果10篇重磅AI研究全总结，iOS 18关键一瞥](https://mp.weixin.qq.com/s/zQu0BtDYG-XAAX87i4MGIA)

[苹果智能背后模型公布：3B模型优于Gemma-7B，服务器模型媲美GPT-3.5-Turbo](https://mp.weixin.qq.com/s/xkZrD_8zsmVfYYohH7ucUw)

### Arctic

[仅需Llama3 1/17的训练成本，Snowflake开源128x3B MoE模型](https://mp.weixin.qq.com/s/0mqx1xkyhOXDGpbu42d_5g)

[全球最大开源模型再刷爆纪录！4800亿参数MoE击败Llama 3、Mixtral](https://mp.weixin.qq.com/s/Wbs30QvvtWtYB6mp47Z8NA)

[https://huggingface.co/Snowflake/snowflake-arctic-instruct](https://huggingface.co/Snowflake/snowflake-arctic-instruct)

### FALCON 2

[时隔一年Falcon回归！110亿参数5.5万亿token，性能超越Llama 3](https://mp.weixin.qq.com/s/CZNVl_GmYm_aPidMJrRHUg)

### MiniCPM

#### MOEfication

[大模型隐藏玩家上桌：DeepSeek 向左，面壁向右](https://mp.weixin.qq.com/s/DrtUv-7fMh8NqdaSHHJ8uQ)

[MoEfication：Transformer Feed-forward layers are Mixtures of Experts](https://arxiv.org/abs/2110.01786)

#### MiniCPM-Llama3-V

[登顶Top2！MiniCPM-V 8B新版本：GPT-4V水准小钢炮，8G显存，4070轻松推理！](https://mp.weixin.qq.com/s/TQVHJlZDExD3nMPRsqa_5w)

[可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术](https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ)

[RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)

[https://github.com/RLHF-V/RLAIF-V](https://github.com/RLHF-V/RLAIF-V)

#### MiniCPM 3.0

[小模型杀疯了！仅4B参数性能超GPT-3.5！无限长文本性能超Kimi](https://mp.weixin.qq.com/s/qJM9OTDHS3pJB9ozFuRP1g)

[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/pdf/2404.06395)


### DCFormer

[彩云科技DCFormer大模型发布，效率是Transformer的两倍！](https://zhuanlan.zhihu.com/p/699582521)

[Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/pdf/2405.08553)

[https://github.com/Caiyun-AI/DCFormer](https://github.com/Caiyun-AI/DCFormer)

### hunyuan

[腾讯混元又来开源，一出手就是最大MoE大模型](https://mp.weixin.qq.com/s/GVFhlelNgrIDnzIIyW3yXg)

[Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265)

[https://github.com/Tencent/Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large)

[https://llm.hunyuan.tencent.com/](https://llm.hunyuan.tencent.com/)

### hunyuan turbo-s

[https://github.com/Tencent/llm.hunyuan.turbo-s](https://github.com/Tencent/llm.hunyuan.turbo-s)


### 阶跃星辰

[在「最难LLM评测榜单」上，阶跃万亿参数模型拿下中国第一](https://mp.weixin.qq.com/s/rGv2r_-owZ3jIx3IPk6XfQ)

[六小虎「阶跃星辰」疯狂了！连发6款大模型，多模态霸榜第一](https://mp.weixin.qq.com/s/qcFGKQlX6OnMoiMPJCze_w)

### Tülu 3(基于llama3.1)

(toread)

[这才是真开源模型！公开「后训练」一切，性能超越Llama 3.1 Instruct](https://mp.weixin.qq.com/s/sTtBkVkqy0CQtpzcR6SN-A)

[TÜLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/pdf/2411.15124)

[https://github.com/allenai/open-instruct](https://github.com/allenai/open-instruct)

提出了具有可验证奖励的强化学习(Reinforcement Learning with Verifiable Rewards, RLVR)，可能和openai的rft有关系

[https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)

### baichuan

[百川新模型超GPT-4o近20%，首创自约束训练方案突破瓶颈，主打「领域增强」](https://mp.weixin.qq.com/s/7oupuln3BQHPa7zpfGdMXQ)

Baichuan4-Finance

评测benchmark：[https://github.com/FLAME-ruc/FLAME/tree/main](https://github.com/FLAME-ruc/FLAME/tree/main)

### minimax-01

[MiniMax开源4M超长上下文新模型！性能比肩DeepSeek-v3、GPT-4o](https://mp.weixin.qq.com/s/pj6XZbyZu_mx2KRwIF1Q9g)

[MiniMax押注线性注意力，让百万级长文本只用1/2700算力｜对话MiniMax-01架构负责人钟怡然](https://mp.weixin.qq.com/s/NigAnui9fXbfresW8KIX-Q)

[MiniMax-01: Scaling Foundation Models with Lightning Attention](https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf)

m层lightning attention后再接1层softmax attention，参数456B，每次推理激活45.9B。



## post training的一些总结

[工业界主流大语言模型后训练(Post-Training)技术总结](https://mp.weixin.qq.com/s/sm2HEKJaeIUy9yXZCj0L-g)

[长文 | 大模型Post-Training总结](https://mp.weixin.qq.com/s/mL3Zc85zoHCjNOaTX2icTA)


## 小模型

[权重、代码、数据集全开源，性能超越Mistral-7B，苹果小模型来了](https://mp.weixin.qq.com/s/M58y8F5WeOH6i5nNKhtHbw)

[DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/pdf/2406.11794)

[https://huggingface.co/apple/DCLM-7B](https://huggingface.co/apple/DCLM-7B)

[小模型卷起来了：Mistral联合英伟达开源12B小模型，128k上下文](https://mp.weixin.qq.com/s/7oSxdFyqJ7MUpbfuNB_n5Q)

Mistral NeMo 使用基于 Tiktoken 的新分词器 Tekken，该分词器经过 100 多种语言的训练，能比以前 Mistral 模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。在压缩源代码、中文、意大利文、法文、德文、西班牙文和俄文时，它的效率要高出约 30%。在压缩韩文和阿拉伯文时，它的效率是原来的 2 倍和 3 倍。事实证明，与 Llama 3 分词器相比，Tekken 在压缩所有语言中约 85% 的文本方面更胜一筹。

Mistral NeMO 经历了高级微调和对齐阶段。与 Mistral 7B 相比，它在遵循精确指令、推理、处理多轮对话和生成代码方面的能力大大提升。

+ 基础模型：[https://huggingface.co/mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)
+ 指令微调模型：[https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)

[大模型已过时，小模型SLM才是未来？苹果正在研究这个](https://mp.weixin.qq.com/s/vAa1Tmse-Sn_nhaceWC1lg)

[Computational Bottlenecks of Training Small-scale Large Language Models](https://arxiv.org/pdf/2410.19456)

[研究大模型门槛太高？不妨看看小模型SLM，知识点都在这](https://mp.weixin.qq.com/s/sg7HveGDjMEj-ZcHS3YizA)

[A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/abs/2411.03350)

[https://github.com/FairyFali/SLMs-Survey](https://github.com/FairyFali/SLMs-Survey)



### BitNet

[微软开源爆火1.58bit大模型推理框架！千亿参数模型量化后单CPU可跑，速度每秒5-7个token](https://mp.weixin.qq.com/s/gerCRxj4eULOut9PtMlNog)

[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)

[https://github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)

### Zamba 2

[7B新王登基！Zamba 2完胜同级模型，推理效率比Llama 3提升20%，内存用量更少](https://mp.weixin.qq.com/s/6_dQod3hS1IJ_xU09x_jrg)

用了mamba 2

[Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/pdf/2405.16712)

+ hf版本：[https://huggingface.co/Zyphra/Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)
+ 纯pytorch版本：[https://github.com/Zyphra/Zamba2](https://github.com/Zyphra/Zamba2)



## 小结

[开源模型进展盘点：最新Mixtral、Llama 3、Phi-3、OpenELM到底有多好？](https://mp.weixin.qq.com/s/bgdDYkGHbPZMMSJPIutFSQ)



# 新的模型结构

## 长上下文的问题

[长序列（Long Context）大模型笔记](https://mp.weixin.qq.com/s/nUX3MbKbyxw6b6mhgoL-Mw?poc_token=HK37E2ejmhfX-g6qnVG3pIxFJXZEVk-vPpwriYAj)

[LLM长上下文的问题](https://mp.weixin.qq.com/s/5e5HtxJrxNuhsVxrKsL2gA)

以中文为例，大部分模型每个token对应的中文字数都>1.5个字，所以200k的token就对应30w字的上下文

对长文本的几个要求：

+ 在文本比较长的时候，还能保证通顺，**ppl要足够低**
+ 能attention到前面提过的细节，**不能自我矛盾**

注意：如果训练时是2k长度的语料，而推理设定8k窗口，那么PPL会急剧上升，因为

+ RoPE不能很好地处理没有训练过的**位置编码**
+ 推理时**注意力机制**所处理的token数量远超训练时的数量，导致注意力机制的崩坏

### 两阶段训练方式

+ 直接输入连续长文本（如书籍）
+ 多个中等文本拼接，再通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。

如果简单地增加长度，例如从4k变到32k，长度增加8倍，为了加速计算需要缓存中间结果（如QK的结果是$$s^2$$的空间复杂度），所以显存会扩大$$8^2=64$$倍。一般的做法是2阶段：

+ 第一阶段：用2k或者4k训练一个基础模型，让模型学好**文本内容**和**短位置关系**
+ 第二阶段：用**比第一阶段小的数据量优化**模型在**长上下文的效果**，具体做法见下节

### 针对位置编码的插值类方法

#### 线性插值

&nbsp;

[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf)

#### NTK-Aware Interpolation

&nbsp;

#### NTK-by-parts

&nbsp;

#### Dynamically NTK Scaled RoPE

&nbsp;

### 针对attention score的缩放方法

#### YaRN

&nbsp;

#### logn

&nbsp;

### lossless long context

[专访月之暗面杨植麟：lossless long context is everything](https://mp.weixin.qq.com/s/UMY0qZsCGh87KnW4wjfvoA)

## 外推问题

[语言模型窗口外推技术综述](https://mp.weixin.qq.com/s/5CqrlUZHrciMAFmeV2oYdw)

### longRoPE

[LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens](https://mp.weixin.qq.com/s/4ryyv59ofNOD--RCSdqktQ)

### CoPE

[解决Transformer根本缺陷，CoPE论文爆火：所有大模型都能获得巨大改进](https://mp.weixin.qq.com/s/JxB6JU6MxO3709mkg7penw)

[Contextual Position Encoding: Learning to Count What’s Important](https://arxiv.org/pdf/2405.18719)

### DAPE

[NeurIPS 2024 | Transformer长度外推，全新位置编码DAPE大幅提升模型性能](https://mp.weixin.qq.com/s/-7YsAMYYO92nItRJbqSrpw)

[DAPE: Data-Adaptive Positional Encoding for Length Extrapolation](https://arxiv.org/abs/2405.14722)

[https://github.com/chuanyang-Zheng/DAPE](https://github.com/chuanyang-Zheng/DAPE)


## retrieval head

[Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/pdf/2404.15574)


## Hawk & Griffin

[RNN效率媲美Transformer，谷歌新架构两连发：同等规模强于Mamba](https://mp.weixin.qq.com/s/RtAZiEzjRWgqQw3yu3lvcg)

[Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/pdf/2402.19427)

### 简介

global attention在infer阶段的效率和序列长度成二次关系，而且序列长度与KV cache呈线性增长关系。[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)提出的multi-query attention(MQA)可以通过一个**constant factor**来**减小cache size**，部分缓解这个问题，但cache还是与序列长度线性相关。

循环模型能够将整个序列压缩到一个**fixed-size的hidden state**，并通过迭代进行更新。但要想取代transformer，rnn不仅需要在效果上可比，还要相似的硬件效率，相关工作如下：

+ [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396)
+ [Long range language modeling via gated state spaces](https://arxiv.org/pdf/2206.13947)提出了GSS block
+ [Simplified state space layers for sequence modeling](https://arxiv.org/pdf/2208.04933)
+ [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ [Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)
+ [Hyena hierarchy: Towards larger convolutional language models](https://arxiv.org/pdf/2302.10866)
+ [Mamba: Linear-time sequence modeling with selective state spaces](https://arxiv.org/pdf/2312.00752)

本文提出了新的**RG-LRU层**，一种新的gated linear recurrent layer，并基于此提出了将MLP和RG-LRU结合的Hawk，还有将MLP和RG-LRU与local attention混合的Griffin。

![griffin-hawk](../assets/griffin-hawk.png)

+ 对于最多超过7B的模型，Hawk和Griffin发现了**held-out loss**和**训练FLOPS**间的**power law scaling**（上图左边）
+ Hawk-3B比Mamba-3B在下游任务上要好，而训练的token数只有mamba的一半；Griffin-7B和14B与Llama-2效果相当，而训练数据只有其1/7
+ 训练在TPU-v3上完成，用JAX中的Pallas实现了RG-LRU(Real-Gated Linear Recurrent Unit)层的内核([https://jax.readthedocs.io/en/latest/pallas/index.html](https://jax.readthedocs.io/en/latest/pallas/index.html))，减小内存transfer
+ infer阶段比MQA的**吞吐高很多**（上图右边），而且对长序列有**更低的latency**
+ 训练时在长序列上比transformer更好，而且能够高效地学习**复制和检索**的任务。但如果没有finetune，直接对比pretrain的效果，transformer会更好

### 网络结构

包括三大类组件：

+ residual block: 
+ MLP block：
+ temporal-mixing block


![griffin-arch](../assets/griffin-arch.png)

#### Residual Block

&nbsp;

受pre-norm transformer的启发，用的RMSNorm

#### MLP block

&nbsp;

参考[Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083)（类似star两个W直接element-wise product），采用了gated MLP，即如下两个linear的结果（输入维度$$D$$，输出维度都是$$MD,M=3$$）进行element-wise product(类似GeGLU（[Glu variants improve transformer]((https://arxiv.org/pdf/2002.05202.pdf))）)，再过一个linear

+ 直接过一个linear，但没有激活
+ linear后加一个GeLU激活（实际用的```nn.functional.gelu(input, approximate="tanh")```）

看recurrentGemma-2b的[config](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json)，发现总共有26个block，一个block依次包括：

+ rg-lru或者MQA，每个block里的选择方式是(rg,rg,att,rg,rg,att,rg,rg,att,...)
+ 再过一个gated MLP

```python
  "_block_types": [
    "recurrent",
    "recurrent",
    "attention"
  ],
  "num_hidden_layers": 26,
  "attention_window_size": 2048,

```

#### Temporal-mixing block

&nbsp;

##### global Multi-Query Attention(MQA)

&nbsp;

[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)中为了加速推理，采用了MQA的方法，本文固定了head的维度为$$D_{head}=128$$，head的个数$$H$$也固定，且$$HD_{head}=D$$，所以model的维度$$D$$需要是128的倍数。没用绝对位置编码，用了RoPE。

##### local(sliding-window) MQA

&nbsp;

[Longformer: The long-document transformer](https://arxiv.org/pdf/2004.05150)提出了可以用local attention，即滑动窗口attention。让每个位置只和前面的固定个tokens去算attentioin，可以

+ 降低计算的FLOPS
+ 让KV cache的size的上界变成了**window的size**，从而不是序列长度的二次关系

##### RG-LRU

&nbsp;

类似GSS block，也类似Mamba，输入$$D$$，分别过一个linear得到两个分支，均是$$D_{RNN}$$：

+ 分支1：过GeLU激活，同上，实际用的```nn.functional.gelu(input, approximate="tanh")```
+ 分支2：
  + 参考[Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)的H3模型里的Shift-SSM，过一个Conv1D，其temporal filter dim是4
  + Conv1D的参数只有$$4D_{RNN}$$，因此再过一个RG—LRU模块

两个分支的输出element-wise product一下，再过一个linear得到$$d$$。

conv1d参考torch官方[doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)

+ 输入：$$\left(N, C_{i n}, L_{i n}\right)$$
+ 输出：$$\left(N, C_{\text {out }}, L_{\text {out }}\right)$$

其中，

XXX
L_{\text {out }}=\left\lfloor\frac{L_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel\_size }-1)-1}{\text { stride }}+1\right\rfloor
XXX

当```groups==in_channels and out_channels=K*in_channels```，也叫depthwise convolution，K是depthwise乘子。即**每个输入通道都有自己的卷积核，并且只作用于该通道**。

```python
        ## conv1d_width = 4, 
        ## 对图像卷积来讲，padding就是让输出的shape不小于输入shape
        ## 输入是hidden_size=2560，而lru_width也是2560
        self.conv_1d = nn.Conv1d(
                    config.lru_width, # in_channels
                    config.lru_width, # out_channels
                    kernel_size=config.conv1d_width,
                    groups=config.lru_width,
                    padding=config.conv1d_width - 1,
                )
        
        ## 输入[bs, seq_len, hidden_size]
        x_branch = self.linear_x(input_states)
        ## 变成[bs, hidden_size, seq_len]
        x_branch = x_branch.transpose(1, 2)
        if use_cache:
            if cache_position.shape[0] != 1:  # prefill
                self.conv1d_state = nn.functional.pad(x_branch, 
                    (self.conv1d_width - x_branch.shape[-1] - 1, 0))
                x_branch = self.conv_1d(x_branch)[..., :seq_len]
            else:  # decoding
                conv_state = torch.cat((self.conv1d_state, x_branch), -1)
                x_branch = torch.sum(conv_state * self.conv_1d.weight[:, 0, :], dim=-1) 
                    + self.conv_1d.bias
                x_branch = x_branch.unsqueeze(-1)
                self.conv1d_state = conv_state[:, :, 1:]
        else:
            # 前面维度不变，最后一维截断到seq_len
            x_branch = self.conv_1d(x_branch)[..., :seq_len]
```


参考[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)提出的LRU(Linear Recurrent Unit)，并参考传统LSTM和GRU引入了gate：

XXX
\begin{aligned}
r_t & =\sigma\left(W_a x_t+b_a\right), \quad \text { recurrence gate } \\
i_t & =\sigma\left(W_x x_t+b_x\right), \quad \text { input gate } \\
a_t & =a^{c r_t}, \\
h_t & =a_t \odot h_{t-1}+\sqrt{1-a_t^2} \odot\left(i_t \odot x_t\right) .
\end{aligned}
XXX

其中，recurrent weight $$a=\sigma(\Lambda)$$是一个对角矩阵，$$\Lambda$$是一个可学习的参数。$$c$$是一个常数8，为了**计算稳定**，在log-space计算$$a^{c r_t}$$，即先算出$$\log a_t$$，再取exp。

XXX
\log a_t=\log a^{c r_t}=\log \sigma(\Lambda)^{c r_t}=-\operatorname{csoftplus}(\Lambda) \odot r_t 
XXX

(这个公式可能有点问题，感觉应该是$$-\operatorname{csoftplus}(-\Lambda) \odot r_t$$，不过$$\Lambda$$是一个可学习的nn.Parameter，其实这个错误无所谓吧)

```python
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.block_width = config.lru_width // self.num_attention_heads

        self.recurrent_param = nn.Parameter(torch.empty([config.lru_width]))
        self.input_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.input_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))

        self.recurrent_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.recurrent_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))
        self.recurrent_states = None

    def forward(xxx):
        ## reshape成适合多头的情况
        reshape_act = activations.reshape(batch_size * seq_len, self.num_attention_heads, self.block_width)
        ## (num_attention_heads, batch_size * seq_len, block_width)
        reshape_act = reshape_act.permute(1, 0, 2)

        ## 批量矩阵乘法（baddbmm），在reshape_act和self.input_gate_weight之间进行，
        ## 并加上偏置self.input_gate_bias。这一步计算输入门的原始值。
        res = torch.baddbmm(self.input_gate_bias[:, None, :], reshape_act, self.input_gate_weight)
        input_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        ## 类似input_gate
        res = torch.baddbmm(self.recurrent_gate_bias[:, None, :], reshape_act, self.recurrent_gate_weight)
        recurrent_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        # Compute the parameter `A` of the recurrence.
        # 上面的公式
        log_recurrent_gate = -8.0 * recurrent_gate * nn.functional.softplus(self.recurrent_param)
        recurrent_gate = torch.exp(log_recurrent_gate)
```

还有如下几个特点：

+ $$W_a$$和$$W_x$$用LeCun init初始化
+ 初始化$$\Lambda$$，使得在训练开始时，$$a^c$$均匀分布在0.9和0.999之间，类似[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ 不像大部分SSM（例如[Hippo: Recurrent memory with optimal polynomial projections](https://arxiv.org/pdf/2008.07669)）基于orthogonal polynomials（正交多项式）理论进行初始化。
+ 也不像[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)在底层的连续系统上进行离散化定义
+ 不像原始的LRU用了复数，虽然[ On the universality of linear recurrences followed by nonlinear projections](https://arxiv.org/pdf/2307.11888v1)说复数的表达能力更强，但在实践中对语言模型并没有什么用（[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)）

##### gate特点

&nbsp;

先复习一下LSTM：

![lstm-peephole-lstm](../assets/lstm-peephole-lstm.png)

GRU：

+ 重置门（reset gate）：如果重置门关闭，会**忽略掉历史信息**，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的**输入门和遗忘门合并**，用于控制**历史信息对当前时刻隐层输出的影响**。如果更新门接近1，会把历史信息传递下去。

![gru](../assets/gru.png)

+ 两个gate只和$$x_t$$有关，**和$$h_{t-1}$$无关**。
+ input gate $$i_t$$和LSTM类似，直接对输入$$x_t$$进行filter或者scale down。
+ recurrent gate $$r_t$$和之前的gate机制不同：
    + mamba里的selection机制和GRU的**update gate**类似，在**之前的状态**和**当前输入$$x_t$$**之间进行插值（interpolate），功能类似LSTM的**forget gate**，能够**reset状态，并遗忘之前的信息**
    + 本文的recurrent gate则类似于在LRU的更新和之前的隐藏状态之间进行插值，能够**有效地丢弃输入**，并且**保持之前历史里的所有信息**。使得模型在处理不相关或重复输入（**uniformative inputs**）时，更能达到**超指数的记忆能力**，以更有效地保留有用信息(因为这个gate**和$$h_{t-1}$$无关**)。


## feedback attention memory

[TransformerFAM: Feedback attention is working memory](https://arxiv.org/pdf/2404.09173.pdf)

## infini-attention

[Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143.pdf)

[【重磅】谷歌重塑Transformer：无限记忆力，无限长输入，LLM基础研究重大突破](https://mp.weixin.qq.com/s/bV2b9uJ4GFQPhhggHT3VIA)

将**压缩记忆**整合进标准的点积注意力机制，并在单个Transformer块内同时实现了**掩码局部注意力**和**长期线性注意力机制**

![infini-attention](../assets/infini-attention.png)

与transformer-xl对比：

![infini-attention-vs-transformer-xl](../assets/infini-attention-vs-transformer-xl.png)

[https://github.com/mustafaaljadery/gemma-2B-10M](https://github.com/mustafaaljadery/gemma-2B-10M)

[https://github.com/dingo-actual/infini-transformer](https://github.com/dingo-actual/infini-transformer)

## MEGA

[Mega: Moving average equipped gated attention](https://arxiv.org/pdf/2209.10655.pdf)

[https://github.com/facebookresearch/mega](https://github.com/facebookresearch/mega)

### 标准的self-attention公式

&nbsp;

对于序列长度=$$n$$的输入$$\boldsymbol{X}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\right\} \in \mathbb{R}^{n \times d}$$，输出$$\boldsymbol{Y}=\left\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_n\right\} \in \mathbb{R}^{n \times d}$$

XXX
\boldsymbol{Y}=\operatorname{Attn}(\boldsymbol{X})=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \boldsymbol{V}
XXX

XXX
\boldsymbol{Q}=\boldsymbol{X} W_q+b_q,\boldsymbol{K}=\boldsymbol{X} W_k+b_k,\boldsymbol{V}=\boldsymbol{X} W_v+b_v
XXX

其中，$$\text { Attn : } \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$$，$$W_q, W_k, W_v \in \mathbb{R}^{d \times d}$$，$$b_q, b_k, b_v \in \mathbb{R}^d$$，而且有两种定义方式：

+ $$f(\cdot)=f_{\text {softmax }}(\cdot)$$，同时$$\tau(\boldsymbol{X})=\sqrt{d}$$，这是经典操作[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
+ $$f(\cdot)=f_{\mathrm{relu}^2}(\cdot)$$即$$relu(x)^2$$，同时$$\tau(\boldsymbol{X})=n$$，这是[Primer: Searching for efficient transformers for language modeling](https://arxiv.org/pdf/2109.08668.pdf)和[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)提出的

![gau](../assets/gau.png)

对于$$h$$个attention heads，计算$$\boldsymbol{A}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \in \mathbb{R}^{n \times n}$$需要的时间和空间复杂度都是$$O(hn^2)$$

### EMA

&nbsp;

对于输出的序列$$\boldsymbol{Y}$$采用Exponential Moving Average(指数滑动平均)如下

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha}) \odot \mathbf{y}_{t-1}
XXX

其中，$$\boldsymbol{\alpha} \in(0,1)^d$$表示权重衰减，$$\odot$$是element-wise product

$$\boldsymbol{\alpha}$$越大，$$1-\boldsymbol{\alpha}$$越小，对历史的衰减也越快：

![ema](../assets/ema.png)

EMA的计算看成$$n$$个**独立的卷积**，可以通过**FFT(快速傅立叶变换)**来加速计算。具体。。。再看看

**Damped EMA**：由于输入的x是d维向量，可以引入一个因子$$\boldsymbol{\delta} \in(0,1)^d$$让EMA更鲁棒：

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha} \odot \boldsymbol{\delta}) \odot \mathbf{y}_{t-1}
XXX

### MEGA

&nbsp;

EMA可以看成是一种**与位置相关的归纳偏置（inductive bias）**，即假设当前位置与之前位置满足滑动平均的关系，而attention矩阵的计算其实并没有考虑位置信息，所以可以把二者结合一下。

![mega](../assets/mega.png)

其中的multi-dimensional damped EMA大致流程如下：

+ **先变成h维**：先把$$\boldsymbol{X} \in \mathbb{R}^{n \times d}$$通过矩阵$$\beta$$映射成$$\boldsymbol{U}\in \mathbb{R}^{n \times h}$$
+ **计算EMA**：然后通过EMA得到$$\boldsymbol{h}\in \mathbb{R}^{n \times h}$$（具体$$\mathbf{h}_t^{(j)}=\boldsymbol{\alpha}_j \odot \mathbf{u}_t^{(j)}+\left(1-\boldsymbol{\alpha}_j \odot \boldsymbol{\delta}_j\right) \odot \mathbf{h}_{t-1}^{(j)}$$）
+ **再变回d维**：再通过一个矩阵$$\eta$$变回$$\boldsymbol{Y}\in \mathbb{R}^{n \times h}$$

然后看整个流程：

+ 先计算EMA

XXX
\begin{aligned}
\boldsymbol{X}^{\prime} & =\operatorname{EMA}(\boldsymbol{X}) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Z} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_z+b_z\right) & & \in \mathbb{R}^{n \times z}
\end{aligned}
XXX

+ 再基于EMA的结果计算QK，基于原始的X计算V：

XXX
\begin{array}{ll}
\boldsymbol{Q}=\boldsymbol{\kappa}_q \odot \boldsymbol{Z}+\boldsymbol{\mu}_q & \in \mathbb{R}^{n \times z} \\
\boldsymbol{K}=\boldsymbol{\kappa}_k \odot \boldsymbol{Z}+\boldsymbol{\mu}_k & \in \mathbb{R}^{n \times z} \\
\boldsymbol{V}=\phi_{\text {silu }}\left(\boldsymbol{X} W_v+b_v\right) & \in \mathbb{R}^{n \times v}
\end{array}
XXX

+ 计算带位置bias的attention：

XXX
\boldsymbol{O}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}+\boldsymbol{b}_{\mathrm{rel}}\right) \boldsymbol{V} \quad \in \mathbb{R}^{n \times v}
XXX

+ 通过reset gate $$\boldsymbol{\gamma}$$和update gate $$\boldsymbol{\varphi}$$计算输出

XXX
\begin{aligned}
\boldsymbol{\gamma} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_\gamma+b_\gamma\right) & & \in \mathbb{R}^{n \times v} \\
\boldsymbol{\varphi} & =\phi_{\text {sigmoid }}\left(\boldsymbol{X}^{\prime} W_{\varphi}+b_{\varphi}\right) & & \in \mathbb{R}^{n \times d} \\
\hat{\boldsymbol{H}} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_h+(\boldsymbol{\gamma} \boldsymbol{O}) U_h+b_h\right) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Y}&=\boldsymbol{\varphi} \odot \hat{\boldsymbol{H}}+(1-\boldsymbol{\varphi}) \odot \boldsymbol{X} \quad & & \in \mathbb{R}^{n \times d}
\end{aligned}
XXX


这里把attention里的softmax改成了如下的laplace函数：

XXX
f_{\text {laplace }}(x ; \mu, \sigma)=0.5 \times\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma \sqrt{2}}\right)\right]
XXX

其中，$$\operatorname{erf}(x)=\frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} \mathrm{~d} t=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \mathrm{~d} t$$是[误差函数](https://zh.wikipedia.org/wiki/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0)，

为了让laplace逼近$$f_{\text {relu }^2}$$，对于$$x=\sqrt{2}$$这个点，求解如下方程

XXX
\begin{aligned}
& f_{\text {relu2 }}(\sqrt{2})=f_{\text {laplace }}(\sqrt{2}) \\
& f_{\text {relu2 }}^{\prime}(\sqrt{2})=f_{\text {laplace }}^{\prime}(\sqrt{2})
\end{aligned}
XXX

可以得到$$\mu=\sqrt{1 / 2}$$，$$\sigma=\sqrt{1 / 4 \pi}$$，对应的曲线和准确率如下：

![laplace-vs-relu2](../assets/laplace-vs-relu2.png)

### mega-chunk

![mega-chunk](../assets/mega-chunk.png)

将序列切分成长度固定为$$c$$的$$k=n/c$$个chunk，对**每个chunk独立计算**上面的attention，这样复杂度就变成了$$O(kc^2)=O(nc)$$，由于有EMA，所以这样做还是能够保持一定程度的长距离依赖。

## MEGALODON

[Meta无限长文本大模型来了：参数仅7B，已开源](https://mp.weixin.qq.com/s/VML5hExo5iPsyEavxzIZSA)

[革命新架构掀翻Transformer！无限上下文处理，2万亿token碾压Llama 2](https://mp.weixin.qq.com/s/xgP9P51gjqJ93FYSWfPeaA)

[MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

[https://github.com/XuezheMax/megalodon](https://github.com/XuezheMax/megalodon)

[Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

基于MEGA进行改进，能够同时实现

+ 高效训练（减少通信和计算量）
+ 高效推理（保持恒定的KV缓存）


## MOD

[Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/pdf/2404.02258.pdf)


## multi-head moe

[微软让MoE长出多个头，大幅提升专家激活率](https://mp.weixin.qq.com/s/ZCRyb63M2DL4hOQh7uxxaw)

[Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)

[https://github.com/yushuiwx/MH-MoE](https://github.com/yushuiwx/MH-MoE)

## Lory

[150B token从头训练，普林斯顿Meta发布完全可微MoE架构Lory](https://mp.weixin.qq.com/s/UKIXGJTFzSeSZvoTe_c9CQ)

[Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](https://arxiv.org/pdf/2405.03133)

## Perciever

[Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf)

## Aaren

[Bengio等人新作：注意力可被视为RNN，新模型媲美Transformer，但超级省内存](https://mp.weixin.qq.com/s/mRt2A1n1CmO7uqzuLQHxkw)

[Attention as an RNN](https://arxiv.org/pdf/2405.13956)

### many-to-one RNN

对query向量$$q$$的attention可以看成是一个函数，对输入的$$N$$个token $$x_{1: N}$$通过他们的key和value $$\left\{\left(k_i, v_i\right)\right\}_{i=1}^N$$ 变换成输出 $$\text { Attention }\left(q, k_{1: N}, v_{1: N}\right)$$，假设$$\bar{s}_i=\operatorname{dot}\left(q, k_i\right)$$，那么输出就是

XXX
o_N=\sum_{i=1}^N \operatorname{softmax}(s)_i v_i=\frac{\sum_{i=1}^N \exp \left(s_i\right) v_i}{\sum_{i=1}^N \exp \left(s_i\right)}=\frac{\hat{a}_N}{\hat{c}_N}
XXX

可以发现，分子和分母其实都可以写成递推形式$$\hat{a}_k=\hat{a}_{k-1}+\exp \left(s_k\right) v_k$$和$$\hat{c}_k=\hat{c}_{k-1}+\exp \left(s_k\right)$$，由于直接这么做可能会产生很大或者很小的值（例如算exp），所以可以通过如下方式进行缓解：计录到第k步的最大值$$m_k=\max _{i \in\{1, \ldots, k\}} s_i$$，然后减掉它，即$$a_k=\sum_{i=1}^k \exp \left(s_i-m_k\right) v_i$$和$$c_k=\sum_{i=1}^k \exp \left(s_i-m_k\right)$$，这样就可以改写为如下形式：

XXX
\begin{aligned}
a_k & =a_{k-1} \exp \left(m_{k-1}-m_k\right)+v_k \exp \left(s_k-m_k\right) \\
c_k & =c_{k-1} \exp \left(m_{k-1}-m_k\right)+\exp \left(s_k-m_k\right) \\
m_k & =\max \left(m_{k-1}, s_k\right)
\end{aligned}
XXX

其中第一行其实就是$$s_i-m_k=s_i-m_k+m_{k-1}-m_k$$，然后exp一下，$$exp(s_i-m_k)=exp(s_i-m_k)exp(m_{k-1}-m_k)$$，总结成如下图：

![attention-rnn-cell](../assets/attention-rnn-cell.png)

所以Attention的RNN cell就是输入$$\left(a_{k-1}, c_{k-1}, m_{k-1}, q\right)$$，输出$$\left(a_k, c_k, m_k, q\right)$$，初始的状态是$$\left(a_0, c_0, m_0, q\right)=(0,0,0,q)$$

然后就可以将之前的attention看成如下几类many-to-one的RNN了：

+ 传统attention只计算最后的一个输出
+ self-attention使用**输入token**作为初始状态
+ Perceiver的cross-attention使用**依赖input的隐变量**作为初始状态

![many-to-one-rnn](../assets/many-to-one-rnn.png)

对于新来的token而言：

+ 传统的RNN一般是流式地输入数据，因此只需要O(1)的内存和计算就行了
+ Transformer需要把这个新token当成一个初始状态加进来，所以需要把之前时间步的再重新算一次，需要O(N)的计算
+ Perceiver中的隐变量是依赖输入的，而且这个新token会改变value，因此初始状态也会变，所以需要从头算一遍，需要O(NL)的计算，L是隐变量个数（可以参考代码[perceiver_torch](https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py)和如下gpt4o的回答）

![perceiver](../assets/perceiver.png)

### many-to-many RNN

要计算$$\left\{o_i=\operatorname{Attention}\left(q, x_{1: i}\right)\right\}_{i=1}^N$$，通过如下的并行前缀扫描算法，对于$$N$$个序列数据的$$N$$个前缀，通过关联运算符$$\oplus$$并行计算，能够高效地通过$$\left\{x_k\right\}_{k=1}^N$$计算$$\left\{\bigoplus_{i=1}^k x_i\right\}_{k=1}^N$$

![parallel-prefix-scan](../assets/parallel-prefix-scan.png)

由于$$\operatorname{Attention}\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)=o_k=\frac{a_k}{c_k}$$，为了计算$$\left\{\text { Attention }\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)\right\}_{k=1}^N$$，只需要先按这个并行扫描算法计算$$\left\{a_k\right\}_{k=1}^N$$、$$\left\{c_k\right\}_{k=1}^N$$和$$\left\{m_k\right\}_{k=1}^N$$，再把$$a_k$$和$$c_k$$结合起来就行。

接来来定义三元组$$\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right)$$，其中

+ $$A$$：一些下标的集合
+ $$\mathrm{m}_A=\max _{i \in A} s_i$$
+ $$\mathrm{u}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right)$$
+ $$\mathrm{w}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right) v_i$$

所以并行扫描算法的输入是$$\left\{\left(\mathrm{m}_{\{i\}}, \mathrm{u}_{\{i\}}, \mathrm{W}_{\{i\}}\right)\right\}_{i=1}^N=\left\{\left(s_i, 1, v_i\right)\right\}_{i=1}^N$$，再来定义操作$$\oplus$$：

XXX
\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right) \oplus\left(\mathrm{m}_B, \mathrm{u}_B, \mathrm{w}_B\right)=\left(\mathrm{m}_{A \cup B}, \mathrm{u}_{A \cup B}, \mathrm{w}_{A \cup B}\right)
XXX

其中，

+ $$\mathrm{m}_{A \cup B}=\max \left(\mathrm{m}_A, \mathrm{~m}_B\right)$$
+ $$\mathrm{u}_{A \cup B}=\mathrm{u}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{u}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$
+ $$\mathrm{w} _{A \cup B}=\mathrm{w}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{w}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$

这个并行扫描算法最终输出下式，即$$\left\{\left(m_k, c_k, a_k\right)\right\}_{k=1}^N$$：

XXX
\left\{\left(\mathrm{m}_{\{1, \ldots, k\}}, \mathrm{u}_{\{1, \ldots, k\}}, \mathrm{w}_{\{1, \ldots, k\}}\right)\right\}_{k=1}^N=\left\{\left(m_k, \sum_{i=1}^k \exp \left(s_i-m_k\right), \sum_{i=1}^k \exp \left(s_i-m_k\right) v_i\right)\right\}_{k=1}^N
XXX


![many-to-many-rnn](../assets/many-to-many-rnn.png)

### Aaren

Aaren(attention as a recurrent newral network)的结构如下：

XXX
\begin{aligned}
h_1^{(0)}, \ldots, h_N^{(0)} & \leftarrow x_1, \ldots, x_N \\
{\left[h_1^{(j+1)}, \ldots, h_N^{(j+1)}\right] } & \leftarrow \operatorname{Aaren}\left(q^{(j)},\left[h_1^{(j)}, \ldots, h_N^{(j)}\right]\right)
\end{aligned}
XXX

transformer的query是输入的token，而Aaren的query token $$q$$是在训练的过程中通过bp学习的。迭代地计算$$y_k$$只需要常数级的计算，因为它依赖$$h_{k-1}$$和$$x_k$$。

transformer：

+ 使用kv cache时需要线性的内存
+ 需要保存所有之前的tokens，包括在中间层的那些

aaren：

+ 只需要常数级的内存
+ 不需要保存之前的所有tokens

![stacking-aaren](../assets/stacking-aaren.png)

## Matmul-free

[从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗](https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag)

## H2O attention

[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/pdf/2306.14048)

+ sparsity for small cache size：kv cache很稀疏，只要5%效果就可以了
+ Heavy-Hitters for low miss rate：一小部分的token贡献了大部分attention score
+ 贪心法选择需要干掉的token：假设总共需要保留k个token，对于第i个token，加进来后，遍历集合里的所有token，看干掉哪一个对attention score影响最小，就把它干掉

## TransNAR

[拯救Transformer推理能力！DeepMind新研究TransNAR：给模型嵌入「算法推理大脑」](https://mp.weixin.qq.com/s/YPICpkYHAC7zTLC_0M_XkQ)

[Transformers meet Neural Algorithmic Reasoners](https://arxiv.org/pdf/2406.09308)

基于：[Neural Algorithmic Reasoning](https://arxiv.org/pdf/2105.02761)

## softmax数学原理

[通向概率分布之路：盘点Softmax及其替代品](https://mp.weixin.qq.com/s/tA9kJqD279dHnivzPkvbjg)

把softmax的分母记为$$Z(x)$$，其对数是max的一个光滑近似：

XXX
\begin{array}{r}
\log Z(\boldsymbol{x})=\log \sum_{j=1}^n e^{x_j}=\operatorname{logsumexp}(\boldsymbol{x}) \\
\lim _{\tau \rightarrow 0^{+}} \tau \operatorname{logsumexp}(\boldsymbol{x} / \tau)=\max (\boldsymbol{x})
\end{array}
XXX

当$$\tau$$取1时，可以得到$$\operatorname{logsumexp}(\boldsymbol{x}) \approx \max (\boldsymbol{x})$$

## StreamingLLM

[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453)

[深度解析streamingLLM之无限长文本生成能力](https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&mid=2247484158&idx=1&sn=666bd5fed13815b5765124646206bac5&chksm=cea54ae8f9d2c3fea1f3d5dfc94fc3d48abbd02a1278584bfd76ff8fe52ce59ce8966ced9cd5&token=562866785&lang=zh_CN#rd)

attention sink：输入给LLM推理开头的几个intial tokens是非常特殊的，就像水龙头一样，出水那一瞬间吸引了人们大量的attention。而且intial tokens与生成token的绝对距离距离和语义信息都不重要，重要的是这第一个或者前面几个token。

softmax需要所有位置的值的总和为1，因此必须给某些位置权重，即使这个位置对输出没有任何贡献，可能导致在backward的过程中产生错误的权重更新，而这个错误在后续的过程中很难被纠正。因此，模型倾向于将不必要的注意力值转嫁给特定的token。其实在[https://www.evanmiller.org/attention-is-off-by-one.html](https://www.evanmiller.org/attention-is-off-by-one.html)就提出了，要给softmax的分母+1：$$(\operatorname{softmax_1}(\mathrm{x}))_i=\frac{\exp \left(x_i\right)}{\sum_j \exp \left(x_j\right)+1}$$

当每个token都趋向负无穷时，softmax的极限是1/k（k是token数），也就是会让每个token的概率都是1/k

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}(\operatorname{softmax}(x))_i=\frac{1}{k}>0
XXX

而$$$$softmax_1$$$$因为分母有个1（即$$exp(0)=1$$），相当于在最前面引入了一个或者多个的无意义的global_token。同样当每个token都趋向负无穷时，因为分母已经有一个1了，所以可以让其他正常token都趋于0

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}\left(\operatorname{softmax}_1(x)\right)_i=0
XXX


[https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

集成到trt-llm里了：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)

+ trtllm-build时，加上```--streamingllm enable```
+ infer时，加上```--sink_token_length```设置sink的token数，加上```--max_attention_window_size```设置sliding_window

## 是否还要RAG?

[谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强](https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg)

## memory^3

[鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆](https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw)

[Memory3 : Language Modeling with Explicit Memory](https://arxiv.org/pdf/2407.01178)

## TSLLM

[没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练](https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg)

[AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training](https://arxiv.org/pdf/2309.17179)

[https://github.com/waterhorse1/LLM_Tree_Search](https://github.com/waterhorse1/LLM_Tree_Search)

## PEER

[单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE](https://mp.weixin.qq.com/s/-oocmPNRT5ddvNwIvYxiQA)

[MoE也有Scaling Law，「百万专家」利用率近100%！DeepMind华人挑战MoE极限](https://mp.weixin.qq.com/s/tBe9DZvzB6NB8HhLYP31CQ)

[Mixture of A Million Experts](https://arxiv.org/pdf/2407.04153)

## TTT

[彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态](https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ)

[Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620)

[https://github.com/test-time-training/ttt-lm-jax](https://github.com/test-time-training/ttt-lm-jax)

[https://github.com/test-time-training/ttt-lm-pytorch](https://github.com/test-time-training/ttt-lm-pytorch)

![ttt](../assets/ttt.png)

[连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路](https://mp.weixin.qq.com/s/tfrG21mfteVAkjqYx5mDsQ)

[The Surprising Effectiveness of Test-Time Training for Abstract Reasoning](https://ekinakyurek.github.io/papers/ttt.pdf)

## Axiomatic Training

[6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码](https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ)

[公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4](https://mp.weixin.qq.com/s/Yera281WG8RNAOrRUYjH6g)

[Teaching Transformers Causal Reasoning through Axiomatic Training](https://arxiv.org/pdf/2407.07612v1)

训练大模型新范式——**公理框架（Axiomatic Framework）**，作者从头开始训练了6700万参数的模型，仅使用了简单的因果链作为训练数据。在推断复杂图表中的因果关系时，**67M模型的表现超越了十亿级参数LLM**，甚至可以与GPT-4相媲美。

## PNN

[AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键](https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw)

[Training of Physical Neural Networks](https://arxiv.org/pdf/2406.03372)

## JRT

[小技巧大功效，「仅阅读两次提示」让循环语言模型超越Transformer++](https://mp.weixin.qq.com/s/zdPlK4IHeEiW0ikmQMPJUA)，注：这里的transformer++指的就是llama2的架构

[Just read twice: closing the recall gap for recurrent language models](https://arxiv.org/pdf/2407.05483)

[https://github.com/HazyResearch/prefix-linear-attention](https://github.com/HazyResearch/prefix-linear-attention)

![jrt](../assets/jrt.png)

### order的重要性

&nbsp;

线性attention的recurrent state很小，存储空间有限，往往很难选择要存储哪些state。

如果集合A和集合B有交集，且A元素比B多，先出现集合A的时候，需要存储整个A，而先出现集合B的时候，则只需要存储集合B

### JRT-Prompt

&nbsp;

上下文学习任务以$$(\mathcal{C}, \mathcal{Q}, \mathcal{Y})$$作为输入，$$\mathcal{C}$$为一些上下文来源（如文档或代码存储库），$$\mathcal{Q}$$为给定上下文时对模型的一些问题或请求，$$\mathcal{Y}$$为答案。

+ 使用自回归的标准上下文学习模型$$\mathcal{A}$$，输入$$\mathcal{C}$$和$$\mathcal{Q}$$，并根据正确的完成情况$$Y$$来评估生成的输出$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q})$$。
+ JRT-PROMPT：在提示模型输出答案之前会在上下文中重复提示中的信息（如问题和文档），例如$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q}, \mathcal{C}, \mathcal{Q})$$。在上下文第二次出现时，模型根据完整的上下文来决定存储哪些信息。

示例：

```shell
# 原来的prompt
## input: 
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年

# JRT的prompt
## input：
百度成立于哪一年？百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年
```

### base的linear transformer

&nbsp;

通过$$\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{\tilde{d}}$$，使得$$\phi\left(\boldsymbol{q}_i\right)^{\top} \phi\left(\boldsymbol{k}_j\right) \approx \exp \left(\boldsymbol{q}_i^{\top} \boldsymbol{k}_j / \sqrt{d}\right) .$$

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i\left(\phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j\right)}{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i \phi\left(\boldsymbol{k}_j\right)}
XXX

先计算k和v的乘法，时间和空间复杂度是$$\mathcal{O}(N d \tilde{d})$$，而softmax attention是$$O\left(N^2 d\right)$$

infer阶段包括两个phases：

+ prefill：并行处理prompt，得到两个state：
    + KV-state：$$\boldsymbol{s}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j$$
    + K-state：$$\boldsymbol{z}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top}$$
+ decoding：计算如下3步，其中$$\boldsymbol{s}_i \in \mathbb{R}^{d \times \tilde{d}}$$，$$\boldsymbol{z}_i \in \mathbb{R}^{\tilde{d}}$$，一个decode step有$$O(1)$$的时间和空间复杂度，而softmax attention加上kv-caching有$$O(N)$$
    + $$\boldsymbol{s}_i=\boldsymbol{s}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top} \boldsymbol{v}_i$$
    + $$\boldsymbol{z}_i=\boldsymbol{z}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top}$$
    + $$\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{s}_i}{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{z}_i}$$

### JRT-RNN架构

&nbsp;

PLA（Prefix Linear Attention）受Prefix-LM启发，主要有2个特点：

+ prefix-LM在encoder和decoder的projection是共享的，而JRT-RNN的encoder用$$\boldsymbol{k}_e, \boldsymbol{v}_e$$，decoder用$$\boldsymbol{k}_d, \boldsymbol{v}_d$$
+ 编码器使用了non-causal的线性注意力，而解码器使用标准causal线性注意力。

prefill阶段，并行地对长度为$$l$$的prompt进行如下计算，如果长度$$l< M$$，进行left-pad到$$M$$长度

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}\right)}{\phi\left(\boldsymbol{v} q_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top}\right)}
XXX

初始化如下：

XXX
\boldsymbol{s}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}\right),\ \boldsymbol{z}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top}\right)
XXX

对于decoding阶段，输出$$y_i, i>M$$，和base的linear transformer一样，不需要修改

训练loss是ntp和mlm的混合，假设序列长度是$$N$$，前$$M$$个token算MLM，后面的$$N-M$$个token算NTP：

XXX
\mathcal{L}=\frac{w_1 \mathcal{L}_{\mathrm{NTP}}+w_2 \mathcal{L}_{\mathrm{MLM}}}{w_1+w_2}
XXX

### 效率提升

虽然Linear attention比softmax attention要快，但其实不如精心优化的softmax attention（如flash attention），[Simple linear attention language models balance the recall-throughput tradeof](https://arxiv.org/pdf/2402.18668)实现了一个io-aware的kernel（[https://github.com/HazyResearch/based/](https://github.com/HazyResearch/based/)），在prefill阶段精细地partitioning & storing暛的matrix-valued recurrent state across warp-registers，PLA参考这个实现了自己的加速版。


## LM-steer

ACL 2024

[LM-Steer: Word Embeddings Are Steers for Language Models](https://arxiv.org/pdf/2305.12798)

[https://github.com/Glaciohound/LM-Steer](https://github.com/Glaciohound/LM-Steer)

发现词向量空间上的线性变换空间等价于对语言模型生成样式的调节，并以此设计了名为 LM-Steers 的语言模型调控方法。我们发现词向量的这种调节作用普遍存在于各种尺寸的语言模型中。它只需要学习原始模型 0.2% 的参数就可以引导各种风格。在语言模型去毒化和生成情感控制等任务上，LM-Steers 可以实现与最先进的受控生成方法相当或更好的性能，同时保持更好的生成质量平衡。学习到的 LM-Steer 还可以充当文本风格的解读器：它可以解释各种文本样式与词向量哪些维度相关，并且可以用于寻找最具代表性的文本片段。 LM-Steer 可通过显式计算来在不同语言模型之间转移，而不需要额外训练。我们还可以简单地通过缩放 LM-Steer 来实现风格的连续控制，或者实现多种生成控制的组合。

## razerattention

减少kv-cache的大小

[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://www.arxiv.org/pdf/2407.15891)

受anthropic的induction head([In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html))启发:

+ short head：对长文没有任何响应
+ long head(induction head)：对长文能直接找到对应位置，并且对那附近的词有很强的信号

## MLP-Mixer

[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)

## sigmoid attention

[Sigmoid注意力一样强，苹果开始重新审视注意力机制](https://mp.weixin.qq.com/s/4DvgsqkyNcj6HBrAHTygCA)

[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/pdf/2409.04431)

[https://github.com/apple/ml-sigmoid-attention](https://github.com/apple/ml-sigmoid-attention)

证明了从理论上讲，与 softmax 注意力相比，具有sigmoid注意力的Transformer是**通用函数逼近器**，并且受益于**改进的正则化**。

## lstm+transformer

[LSTM+Transformer王炸创新，荣登Nature](https://mp.weixin.qq.com/s/OTwbZEy-v93-fzvh1Z4m2Q)

## AnyGraph

[港大黄超团队推出AnyGraph, 首次揭秘图大模型的Scaling Law](https://mp.weixin.qq.com/s/iqQi4ZP7FdpHnMxZByOyoQ)

[AnyGraph: Graph Foundation Model in the Wild](https://arxiv.org/pdf/2408.10700)

[https://github.com/HKUDS/AnyGraph](https://github.com/HKUDS/AnyGraph)


## 液态神经网络

[给机器人装上「虫脑」？非Transformer液态神经网络终于来了！MIT CSAIL负责人创业成果](https://mp.weixin.qq.com/s/oowid3yCpFNTALCvgdSwXg)

Liquid Foundation Models（LFM），1B、3B和40B LFM在各个规模上均能实现SOTA性能，同时保持更小的内存占用和更高效的推理。2020年就有了[Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439)

## 差分transformer

[这篇论文非常火！差分Transformer竟能消除注意力噪声，犹如降噪耳机](https://mp.weixin.qq.com/s/hG_S85HkyAkTFAI2iQjl6g)

[Differential Transformer](https://arxiv.org/pdf/2410.05258)

Transformer往往会过度关注不相关的上下文，即注意力噪声（attention noise），而差分Transformer则能放大对答案范围的注意力并消除噪音，从而增强上下文建模的能力。

## SparseLLM

[NeurIPS 2024｜SparseLLM：突破性全局剪枝技术，大语言模型稀疏化革命](https://mp.weixin.qq.com/s/mfdkUFXCsB50iqKgxv7hQQ)

[SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)

[https://github.com/BaiTheBest/SparseLLM](https://github.com/BaiTheBest/SparseLLM)

## minLSTM+minGRU

[图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?](https://mp.weixin.qq.com/s/ueid-TAw-9OjtKFKA5lqSw)

[Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201v1)

## MixCon

[北大林宙辰团队全新混合序列建模架构MixCon：性能远超Mamba](https://mp.weixin.qq.com/s/FfCYq1Bx6d7NARKvDPwz2Q)

[MixCon: A Hybrid Architecture for Efficient and Adaptive Sequence Modeling](https://zhouchenlin.github.io/Publications/2024-ECAI-MixCon.pdf)

## self-lengthen

[阿里千问提出Self-Lengthen，大模型实现自迭代扩展输出长度](https://mp.weixin.qq.com/s/1m1hUkhs3altxjYP6IxUVw)

[Language Models Can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933)

[https://github.com/QwenLM/Self-Lengthen](https://github.com/QwenLM/Self-Lengthen)

## BLT

[Tokenization不存在了？Meta最新研究，无需Tokenizer的架构来了](https://mp.weixin.qq.com/s/7ju-PjPZVPrBLQ1qFnFoKw)

[Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)

## R2L tokenizer

[从2019年到现在，是时候重新审视Tokenization了](https://mp.weixin.qq.com/s/zmeFYfxWD1nZq_MocgGeeQ)

[https://huggingface.co/spaces/huggingface/number-tokenization-blog](https://huggingface.co/spaces/huggingface/number-tokenization-blog)

## memory layers at scale

[Meta探索大模型记忆层，扩展至1280亿个参数，优于MoE](https://mp.weixin.qq.com/s/R5JOMkLVbbI7yPJmdcT2pQ)

[Memory Layers at Scale](https://arxiv.org/pdf/2412.09764)

[https://github.com/facebookresearch/memory](https://github.com/facebookresearch/memory)

## TITANS

(toread)

[谷歌新架构一战成名，打破Transformer记忆瓶颈，姚班校友钟沛林新作](https://mp.weixin.qq.com/s/APE_CJ4rEQYV8ngyaTAXWw)

[近8年后，谷歌Transformer继任者「Titans」来了，上下文记忆瓶颈被打破](https://mp.weixin.qq.com/s/EUCZ1oSuyzR9M9X9r5SYBw)

[Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663v1)

## Transformer^2

[Transformer^2要做「活」的AI模型，动态调整权重，像章鱼一样适应环境](https://mp.weixin.qq.com/s/_vdA_KygkFWqFE5Xm6CDRg)

[TRANSFORMER2 : SELF-ADAPTIVE LLMS](https://arxiv.org/pdf/2501.06252)

## MTA

[Multi-Token突破注意力机制瓶颈，Meta发明了一种很新的Transformer](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650963426&idx=2&sn=4cb5432a3b547f12136a50fdebbb19f8)

[Multi-Token Attention](https://arxiv.org/pdf/2504.00927)

# Gemmini&Gemma系列

## Gemini 1.0

[Gemini: a family of highly capable multimodal models](https://arxiv.org/pdf/2312.11805.pdf)

## Gemini 1.5

[谷歌Gemini 1.5深夜爆炸上线，史诗级多模态硬刚GPT-5！最强MoE首破100万极限上下文纪录](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652444347&idx=1&sn=51ae7e3e100e24fd49b0f75924e74695&chksm=f093b48285369da37b6148803e41fb272c51c013bc31ac1ba6b09672ff1efd38269af1192b53&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)

[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)

+ 更新的 Gemini 1.5 Pro，其大部分功能和基准都超过了 2 月份的版本
+ Gemini 1.5 Flash，一种更轻量级的变体，专为提高效率而设计，并且在性能方面的减益很小。

此前的SOTA模型能处理**20万(200K)**的token，Gemini 1.5能稳定处理**100万(1M)**的token（极限为**1000万(10M)**的token），能够处理11小时的音频、1小时的视频、超过3w行的代码库、超过70w个单词


## gemma

[https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/)

[Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)

代码：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py)


## codegemma

+ 专门处理代码补全和代码生成任务的 7B 预训练变体
+ 用于代码聊天和指令跟随的 7B 指令调优变体
+ 在本地计算机上运行快速代码补全的 2B 预训练变体

[CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)

![codegemma](../assets/codegemma.png)

## RecurrentGemma

[RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf)

在infer阶段，需要检索**KV cache**，并加载到内存中，而KV cache会随着序列长度线性增长。[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)通过**local attention**来降低cache大小，但模型效果会变差。recurrent gemma将输入序列**压缩到一个固定大小的state中**，从而不会降低效果，对长序列能降低内存占用并高效infer。

基于Griffin架构，将**门控线性递归**与**本地滑动窗口注意力**混合在一起，在生成长序列时实现快速推理，相比gemma：

+ 减少内存用量：内存要求越低，就越能在内存有限的设备（例如单个 GPU 或 CPU）上生成较长的样本。
+ 吞吐量较高：能够以明显较高的batch_size执行推理，这意味着每秒可以生成更多tokens，尤其是在生成长序列时。

相比原始griffin：

+ 对输入emb**乘以一个常数**（等于model width(下表中的2560)的平方根，如下代码所示）。输入和输出的emb是tied的，这个常数**没有乘到output上去**。gemma里也有一个类似的因子
+ 对**recurrent layers（RG-LRU）**在训练时并**没有进行weight decay(本质是L2正则化，参考[https://zhuanlan.zhihu.com/p/607909453](https://zhuanlan.zhihu.com/p/607909453))**，当bp到**开方操作**时，为了训练稳定会加最大值为1000的**梯度clip**

```python
def __init__(self, ...):
    #...
    self.register_buffer(
        "normalizer", torch.tensor(self.config.hidden_size**0.5, 
            dtype=torch.bfloat16), persistent=False
    )
def forward(self, ...):
    #...
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)

    hidden_states = inputs_embeds

    if use_cache and inputs_embeds.shape[1] != 1:  
        # TODO let's maybe only call in the `generate`?
        self._setup_cache(self.config, hidden_states.shape[0], 
        hidden_states.device, hidden_states.dtype)

    if cache_position is None:
        cache_position = torch.arange(hidden_states.shape[1], 
        device=hidden_states.device)
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    causal_mask = self._update_causal_mask(attention_mask, 
        inputs_embeds, cache_position)

    ## 这里就是那个因子
    hidden_states = hidden_states * self.normalizer.type(hidden_states.dtype)
```

具体参数：

| 变量 | 大小 |
|----------|-------------|
| 总参数量 | 2.7b |
| 非emb参数量 | 2.0b |
| emb参数量 | 0.7b |
| vocab size | 256k |
| model width | 2560 |
| rnn width | 2560 |
| MLP expansion factor | 3，即intermediate_size=2560*3=7680 |
| Depth | 26 |
| Attention heads | 10 |
| local attention window size | 2048 |

训练：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py)

在线推理的C++实现(cpu版本)

[https://github.com/google/gemma.cpp](https://github.com/google/gemma.cpp)

## gemma-2

[单张A100全精度推理！谷歌明星开源模型Gemma 2上新9B/27B，挑战3140亿Grok-1](https://mp.weixin.qq.com/s/z3h1eExDgItDf38Xar6yPg)

[谷歌「诚意之作」，开源9B、27B版Gemma2，主打高效、经济！](https://mp.weixin.qq.com/s/0Iy3gOlWRLKRCMnrXQavaA)

[https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)

[https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/pdf/2408.00118)

用了RoPE和GeGLU，网络结构上：

+ **局部滑动窗口**和**全局注意力**：在每隔一层中交替使用局部滑动窗口注意力([Longformer: The long-document transformer]((https://arxiv.org/pdf/2004.05150)))和全局注意力（[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)），局部注意力层的滑动窗口大小设置为4096个token，而全局注意力层的跨度设置为8192个token。
+ **Logit软封顶**：根据**Gemini 1.5的方法**，研究团队在每个注意力层和最终层限制logit，使得logit的值保持在$$−soft\_cap,+soft\_cap$$之间。对于9B和27B模型，注意力对数封顶设置为50.0，最终对数封顶设置为30.0。截至本文发表时，注意力logit软封顶与常见的FlashAttention实现不兼容，因此他们已从使用FlashAttention的库中移除了此功能。对模型生成进行了有无注意力logit软封顶的消融实验，发现大多数预训练和后期评估中，生成质量几乎不受影响。本文中的所有评估均使用包含注意力logit软封顶的完整模型架构。然而，某些下游性能可能仍会受到此移除的轻微影响。

XXX
\text { logits } \leftarrow \text { soft\_cap } * \text { tanh(logits } / \text { soft\_cap) }
XXX

+ **RMSNorm**进行post-norm和pre-norm。为了稳定训练，用RMSNorm对每个变换子层、注意力层和前馈层的输入和输出进行归一化。 
+ **gqa**：27B和9B模型均使用GQA，num_groups = 2，基于消融实验表明在保持下游性能的同时提高了推理速度。

[https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py](https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py)


[Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[https://huggingface.co/google/gemma-scope](https://huggingface.co/google/gemma-scope)

## datagemma

[整合海量公共数据，谷歌开源AI统计学专家DataGemma](https://mp.weixin.qq.com/s/Fr8I9VwiyHcMnhrWMcgDeQ)

[Knowing When to Ask - Bridging Large Language Models and Data](https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf)

提出了一种名为**检索交错生成（Retrieval Interleaved Generation，RIG）**的新方法，可以可靠地将Data Commons中的公共统计数据整合到LLM的响应中。RIG是一种受工具启发的方法，可以将统计数据标记与适合从Data Commons检索的自然语言问题交错。

为了获得这种能力，他们利用Gemini 1.5的帮助生成了一个指令-响应数据集，并在此基础上对LLM进行了微调。RIG方法将事实准确性从5-7%提高到了约58%。

## gemini 2.0

[OpenAI深夜被狙，谷歌Gemini 2.0掀翻牌桌！最强智能体组团击毙o1](https://mp.weixin.qq.com/s/RfTGAlYVlH1RlTzuzugrKQ)

[谷歌“狙击”OpenAI，发布新一代大模型！主打Agent+多模态](https://mp.weixin.qq.com/s/c39MbjULBW5M_8vdO-VUXQ)

[我扒出了Gemini 2.0超实时多模态幕后的黑科技，第六代TPU芯片Trillium！](https://mp.weixin.qq.com/s/BZpL9PwvCWmR4R6h2502Kw)

[谷歌逆风翻盘暴击OpenAI，90天王者归来！44页报告押注25年三大技术前沿](https://mp.weixin.qq.com/s/b0yqPre6wMlP59cH6Qrbdw)

[data and ai trends report 2024](https://services.google.com/fh/files/misc/data_ai_trends_report.pdf)

## gemini 2.5

[DeepSeek逼出谷歌新推理模型：40分优势超GPT4.5登顶竞技场，支持原生多模态，但依然败给了“竹竿问题”](https://mp.weixin.qq.com/s/U75bF7gQ6wlQlaflQvrIQQ)

[谷歌终于登顶一次了！最强推理模型Gemini 2.5 Pro实测体验，真的有点东西](https://mp.weixin.qq.com/s/unlZoM5oCH3HCw2kxy6X_w)

[谷歌首款混合推理Gemini 2.5登场，成本暴降600%！思考模式一开，直追o4-mini](https://mp.weixin.qq.com/s/Tx6dbqKwPTx9dqAW1xzT3w)

Gemini 2.5 Pro的上下文窗口是1M tokens，并且支持原生多模态：可以理解庞大数据集并处理来自不同信息源的复杂问题，包括文本、音频、图像、视频，甚至是整个代码库。2.5 pro擅长创造视觉上引人注目的Web应用程序和智能体代码。

[刚刚，Gemini 2.5 Pro升级，成编程模型新王](https://mp.weixin.qq.com/s/6wv6tUVYsJPfaN-7HxUjoQ)


[52页PPT，谷歌Gemini预训练负责人首次揭秘！扩展定律最优解](https://mp.weixin.qq.com/s/IB_Bum3Md5mafAaEVxk9QA)

[https://vladfeinberg.com/assets/2025-04-24-princeton-talk.pdf](https://vladfeinberg.com/assets/2025-04-24-princeton-talk.pdf)

## gemini 2.0 flash thinking

[推理最强也最快，谷歌发布Gemini 2.0 Flash Thinking，全面超越o1-preview](https://mp.weixin.qq.com/s/NkTP17j6HYIz95sHxCameA)

[https://ai.google.dev/gemini-api/docs/thinking-mode?hl=zh-cn](https://ai.google.dev/gemini-api/docs/thinking-mode?hl=zh-cn)

[https://colab.research.google.com/github/google-gemini/cookbook/blob/main/gemini-2/thinking.ipynb](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/gemini-2/thinking.ipynb)

我自己复制的一个：[https://colab.research.google.com/github/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb](https://colab.research.google.com/github/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb)

对应的github：[https://github.com/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb](https://github.com/daiwk/llms_new/blob/main/gemini-2/thinking.ipynb)

## gemma-3

[Google 发布多模态开源模型 Gemma 3：性能与功能全面升级，附技术报告英中对照版](https://mp.weixin.qq.com/s/WyBNvIsPKMGjSMk_F2Xrpg)

+ 架构设计：Gemma 3基于decoder-only架构，与前两代Gemma模型类似。但在细节上有所创新，例如采用GQA和RMSNorm，还引入了**QK-norm**替代Gemma 2中的软封顶机制，以提升性能和稳定性。
+ 训练策略：Gemma 3的训练过程包括预训练和指令微调两个阶段。在预训练阶段，模型使用知识蒸馏技术，从大规模数据中学习语言和视觉表示。训练数据涵盖了文本、图像以及多语言等多种类型，且经过严格的筛选和清洗，以减少有害内容和低质量数据的影响。在指令微调阶段，Gemma 3 采用了新颖的训练方法，重点关注数学、聊天、指令遵循和多语言等能力的提升。

[一台3090就能跑Gemma 3 27B！谷歌发布Gemma 3全系QAT版模型](https://mp.weixin.qq.com/s/OlfazHQpsrkQ_CtyQvXbJg)


# DeepSeek系列

## DeepSeek-V2

[一块钱100万token，超强MoE模型开源，性能直逼GPT-4-Turbo](https://mp.weixin.qq.com/s/tAA8XUbU__9FgvEvXxsykw)

[https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)

[从MHA到MLA看Attention优化：谈谈DeepSeek拼多多级的推理价格](https://mp.weixin.qq.com/s/l2uUXGQ-8Rj_nI3JG5lZ_g)

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://spaces.ac.cn/archives/10091)

[DeepSeek开源数学大模型，高中、大学定理证明新SOTA](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

![](../assets/mla-deepseekmoe.png)

主要包括2部分：

+ MLA：图中一列是一个head，输入一个q/k/v，每个的dim都是$$d$$，split成$$n_h$$份，每份的dim是$$d_h=d/n_h$$
    + MHA(Multi-head Attention)：等价于多个独立的单头注意力的拼接
    + GQA(Grouped-Query Attention)：出自[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)，将所有Head分为**g个组**（g可以整除h），**每组共享**同一对K、V
    + MQA(Multi-Query Attention)：出自[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)，让**所有Attention Head共享同一个K、V**
    + MHA(Multi-head Latent Attention)：

![](../assets/mha-gqa-mqa-mla.png)

| attn | KV Cache per Token | Capability |
|-------|---------------------------------|-------|
| MHA | $$2n_hd_hl$$ | Strong   |
| GQA | $$2n_gd_hl$$ | Moderate  |
| MQA | $$2d_hl$$  | Weak  |
| MLA | $$(d_c + d_h^R)l \approx \tfrac{9}{2}d_hl$$ | Stronger |


+ DeepSeekMoE



## DeepSeek-Prover-V1.5

[DeepSeek开源数学大模型，高中、大学定理证明新SOTA](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

[DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152)

[https://github.com/deepseek-ai/DeepSeek-Prover-V1.5](https://github.com/deepseek-ai/DeepSeek-Prover-V1.5)

## Deepseek-v3

[4篇DeepSeek神作论文速读，屠龙少年的名言：You play the game I play the rule.](https://mp.weixin.qq.com/s/AxwK_gtWQfxzT9aqPaVRGw)

[DeepSeek-V3 正式发布](https://mp.weixin.qq.com/s/iFZOQsUNkpkXPDvOkE99wQ)

[论文](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)

下载了一个：[https://github.com/daiwk/collections/blob/master/assets/DeepSeek_V3.pdf](https://github.com/daiwk/collections/blob/master/assets/DeepSeek_V3.pdf)

[DeepSeek-V3技术报告解读](https://mp.weixin.qq.com/s/-Ggy94bM8k-AaYarTMSI5Q)

[Deepseek V3 预训练策略解读](https://mp.weixin.qq.com/s/TzaaOOwe32V1IXxiLwcKGw)

[MiniMax-01 与 DeepSeek-V3 对比](https://mp.weixin.qq.com/s/x7sN0RwaJjzxHiAReslSRQ)

[DeepSeek绕开CUDA垄断，V3论文细节再挖出！英伟达护城河不存在了？](https://mp.weixin.qq.com/s/t4VTGX7fMTFpur8F91fvjQ)

直接写PTX（并行线程执行），PTX是专门为其GPU设计的中间指令集架构，位于高级GPU编程语言（如CUDA C/C++或其他语言前端）和低级机器代码（流处理汇编或SASS）之间。PTX是一种接近底层的指令集架构，将GPU呈现为数据并行计算设备，因此能够实现寄存器分配、线程/线程束级别调整等细粒度优化，这些是CUDA C/C++等语言无法实现的

[首次披露！DeepSeek V3 发布软硬一体协同训练论文，公开「降成本」秘诀](https://mp.weixin.qq.com/s/LFOsOzINZNmyVDMRtGk83g)

[Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/pdf/2505.09343)


## NSA

[刚刚！DeepSeek梁文锋亲自挂名，公开新注意力架构NSA](https://mp.weixin.qq.com/s/SlhNdGNFpGlicIkMUE21og)

[【手撕NSA】DeepSeek新作-原生稀疏注意力-超长文(附代码)](https://mp.weixin.qq.com/s/-sbxKky1qWBVYzuyNMg5jw)

[Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)

## FlashMLA

[“源神”DeepSeek！突破H800性能上限，FlashMLA重磅开源，算力成本还能降](https://mp.weixin.qq.com/s/OZmMTW4JyiP4GWkzShlqAg)

[https://github.com/deepseek-ai/FlashMLA](https://github.com/deepseek-ai/FlashMLA)

## DeepEP

[DeepSeek开源第二弹，为MoE和EP量身定制的通信库！暂和英伟达显卡绑定](https://mp.weixin.qq.com/s/2ecxmq9zbwFlzOalprok0Q)

[https://github.com/deepseek-ai/DeepEP](https://github.com/deepseek-ai/DeepEP)

## DeepGEMM

[DeepSeek-R2曝5月前上线！第三弹DeepGEMM 300行代码暴击专家优化内核](https://mp.weixin.qq.com/s/4aKxWIpb2-9DZKrSSAYoWg)

[https://github.com/deepseek-ai/DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)


## DualPipe

[DeepSeek一口气开源3个项目，还有梁文锋亲自参与，昨晚API大降价](https://mp.weixin.qq.com/s/aeK5eJMwqNuqJaZczKrhgQ)

[https://github.com/deepseek-ai/DualPipe](https://github.com/deepseek-ai/DualPipe)

## EPLB

[https://github.com/deepseek-ai/eplb](https://github.com/deepseek-ai/eplb)

## profile-data

[https://github.com/deepseek-ai/profile-data](https://github.com/deepseek-ai/profile-data)

## 3fs

[DeepSeek开源周最后一天：让数据处理「从自行车升级到高铁](https://mp.weixin.qq.com/s/p8KGnYIqmCkXPF4jjRiKjQ)

[https://github.com/deepseek-ai/3FS](https://github.com/deepseek-ai/3FS)

在3fs上的数据处理框架：[https://github.com/deepseek-ai/smallpond](https://github.com/deepseek-ai/smallpond)

## 推理系统

[https://zhuanlan.zhihu.com/p/27181462601](https://zhuanlan.zhihu.com/p/27181462601)

中文版：[https://github.com/deepseek-ai/open-infra-index](https://github.com/deepseek-ai/open-infra-index)

对应英文版：[https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md](https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)


优化目标：更大的**吞吐**，更低的**延迟**。
方案：大规模跨节点专家并行(Expert Parallelism, EP)

EP好处：

+ 增加batchsize，提高gpu矩阵乘法效率，**提升吞吐**
+ 专家分散在不同GPU，每个GPU只计算少量专家，访存更少，**降低延迟**

EP的复杂性：

+ 引入了跨节点传输，需要设计合理计算流程使**计算**和**传输**可以**同步进行**
+ 因为多节点，需要**DP**（数据并行），不同DP间要**负载均衡**

### 大规模跨节点专家并行

V3/R1的结构：从第4层开始是MOE，每层有**1个共享**专家和**256个路由**专家，每次激活8个路由专家，即**8 in 256**，稀疏度很高，因此需要用**更大的overall batchsize**，才能给每个专家提供足够的专家batchsize

+ Prefill：
    + 路由专家EP32
    + MLA和共享专家DP32
    + 一个部署单元是4节点（32卡），每张卡包括：
        + 9个路由专家（共9*32=288个专家），其中：
            + 1个冗余专家，共1*32=32个
            + 8个路由专家，共8*32=256个
        + 1个共享专家，共1*32=32个
+ Decode：
    + 路由专家EP144
    + MLA和共享专家DP144，
    + 一个部署单元是18节点(18*8=144卡)，每张卡包括：
        + 2个路由专家，共2*144=288
            + 总共32个冗余专家
            + 总共288-32=256个正常的路由专家 
        + 1个共享专家，共1*144=144个

### 计算通信重叠

多机多卡的EP会引入较大的通信开销，可以用双batch重叠来掩盖通信开销，提升整体吞吐：

+ **prefill阶段**：2个batch的计算和通信交错进行，一个batch计算时可以掩盖另一个batch的通信开销。即attn->dispatch->mlp->shared/combine这4步（这里是shared和combine并行）

![](../assets/overlap-prefill.png)

+ **decode阶段**：不同阶段的执行时间有差别， attention部分拆成了两个stage（图中的attn-0（attn前的一些proj操作等）和attn-1（核心的attn计算等）），共5个stage的流水线（attn0->attn1->dispatch/shared->mlp->combine）来实现计算和通信的重叠（这里是shared和dispatch并行）。

![](../assets/overlap-decode.png)

细节可以参考[https://github.com/deepseek-ai/profile-data](https://github.com/deepseek-ai/profile-data)

### 尽可能地负载均衡

尽可能为每个GPU分配均衡的计算负载+通信负载

+ prefill load balancer：
    + 问题：不同DP实例上的请求数、长度不同，导致core-attention的计算量、dispatch发送量不同
    + 目标：
        + 各GPU的**计算量**尽量相同==>core-attention计算负载均衡
        + **输入的token数量**尽量相同==>dispatch发送量负载均衡，避免部分GPU处理时间过长
+ decode load balancer：
    + 问题：不同DP实例上的请求数量、长度不同，导致core-attention计算量（与 KVCache 占用量相关）、dispatch发送量不同
    + 目标：
        + 各GPU的**KVCache占用量**尽量相同==>core-attention计算负载均衡
        + **请求数量**尽量相同==>dispatch发送量负载均衡
+ expert-parallel load balancer：
    + 问题：对于给定MoE模型，存在一些**天然的高负载专家**，导致不同GPU的专家计算负载不均衡
    + 目标：每个GPU上的**专家计算量均衡**==>最小化所有GPU的dispatch接收量(即MOE的输入)的最大值

### 架构图

![](../assets/deepseek-infer.jpg)

prefill和decode拆了2个service，并且每个前面有负载均衡，内部有EP的负载均衡

### 线上效果

H800，精度和训练一致：

+ 矩阵计算和dispatch传输用fp8
+ core-attn计算和combine用bf16

白天用所有节点部署服务，晚上减少节点，峰值278节点（每个节点8卡H800，共278 * 8=2224卡，前面提到了prefill要32卡，decode要144卡，其实就是32x+14y=2224），平均占用226.75个节点，每张卡2美金1小时，那就是2 * 226.75 * 8 * 24 = 87072美金/天

+ 输入总token数608B，有342B命中kvcache缓存，命中率56.3%
+ 输出总token数168B，平均输出速度20~22tps(tokens per second)，这里指的应该是**单请求**的生成速度，平均每输出一个token的KVCache长度是4989。
+ 平均每台H800(8卡)的吞吐量：
    + prefill任务：输入吞吐约 73.7k tokens/s（含缓存命中）。
        + 推测：每秒每8卡输入的token数是608000000000 / 226.75/86400=31k，好像对不太上这个73.7k，可能猜测：缓存那部分非常快，非缓存很慢，而这31k是假设全天流量不变的平均值
    + decode任务：输出吞吐约 14.8k tokens/s。
        + 推测：每秒每8卡输出的token数是168000000000 / 226.75 /86400=8575，前面提到了每个请求每秒的输出token数是20~22，那就相当于每8卡机器每秒能处理有8575/21=408个请求，所以总的返回qps大概是408 * 226.75=9.25k

## GRM+SPCT

[刚刚，DeepSeek公布推理时Scaling新论文，R2要来了？](https://mp.weixin.qq.com/s/TbPWc4ooFyf0it5pEaLRUA)

[Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495)

critique：生成的文本评价，即评论

### RM分类

![](../assets/rm-patterns.png)

RM可以进行如下分类：

+ reward生成范式：输入query和一些responses
  + scalar：只输出reward值
  + semi-scalar：生成critique和scalar
  + generative：只输出critique
+ 打分范式：输入query和2个responses
  + pointwise：输出每个response的得分
  + pairwise：输出二者的相对得分

几种常见RM：

+ Bradley-Terry，scalar + pointwise
+ PairRM：scalr + pairwise
+ CLoud：semi-scalr + pointwise
+ LLM-as-a-Judge：semi-scalr或者generative + pairwise
+ Pointwise GRM：generative + pointwise

### SPCT

核心：principle（准则）不再只是辅助性的输入，而是使模型能在生成过程中**主动生成**并**基于此生成对应的critique**

Self-Principled Critique Tuning (SPCT)

![](../assets/spct.png)

+ 训练（2阶段）：输入query+2个response
  + RFT（rejective fine-tuning）：使GRM能够生成格式正确且适配多种输入类型的principle与critique
    + 使用预训练的pointwise-GRM，采样得到N个priciple + response，和goundtruth比较(groundtruth对应的reward是不是比其他N-1个结果对应的reward高)，满足下面条件的扔了，剩下的构造成一个数据集
      + 全对的就是too-easy的，扔掉；
      + 错误的也扔掉
    + 同时还有一定比例的hint sampling，即把groundtruth提前泄露到GRM的prompt里去，让模型倾向于去对齐groudtruth
  + RL（rule-based online RL）：通过不断优化生成的准则和评论，进一步增强泛化型奖励生成能力。
    + 也是用的GRPO，只有accuracy reward，没format reward，加大KL-penalty
+ 推理：输入query+2个response
  + 并行采样出一堆的principles+critiques，即m条结果
  + voting：
    + 基于生成式reward：即每个里的response1的reward求和，同理response2的reward求和，得到1和2的总得分
    + 基于meta RM：训练一个pointwise scalar RM，用二分类来判断principle+critique对不对(label和RFT的一样)。使用：对这m个结果打分，干掉质量差的（图中就是只保留了第1和第3个），然后对剩下的$$k_{meta}$$个去投票

## DeepSeek-Prover-V2

[DeepSeek开源Prover-V2强推理模型，网友：奥数从没这么简单过](https://mp.weixin.qq.com/s/UYn-9vX5rpWpB3Y1xFW8Jg)

专为Lean 4 打造的开源大语言模型，专注于形式化定理证明。

[https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/DeepSeek_Prover_V2.pdf](https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/DeepSeek_Prover_V2.pdf)

下载了一份：[pdf](https://github.com/daiwk/collections/blob/master/assets/DeepSeek_Prover_V2.pdf)

两阶段训练，建立了两种互补的证明生成模式：

1. 高效非思维链（non-CoT）模式：针对快速生成正式的Lean证明代码进行优化，专注于生成简洁的证明，没有显式的中间推理步骤。 
2. 高精度思维链（CoT）模式：系统地阐述中间推理步骤，强调透明度和逻辑进展，然后构建最终的正式证明。

与 DeepSeek-Prover-V1.5一致，这两种生成模式由两个不同的引导提示控制。

+ 第一阶段采用专家迭代，在课程学习框架内训练一个非CoT证明模型，同时通过基于子目标的递归证明合成难题的证明。选择非CoT生成模式是为了加速迭代训练和数据收集过程。
+ 在此基础上，第二阶段利用了冷启动链式思维（CoT）数据，通过将 DeepSeek-V3 复杂的数学推理模式与合成形式证明相结合而生成。CoT模式通过进一步的强化学习阶段得到增强，遵循了通常用于推理模型的标准训练流程。

DeepSeek-Prover-V2的非CoT模式训练过程遵循专家迭代的范式，这是开发形式化定理证明器广泛采用的框架。在每次训练迭代中，当前最佳证明策略用于生成那些在先前迭代中未解决的难题的证明尝试。这些成功的尝试经由Lean证明助手验证后，被纳入SFT数据集以训练改进的模型。

这一迭代循环不仅确保模型能够从初始演示数据集中学习，还能提炼出自己的成功推理轨迹，逐步提高其解决更难问题的能力。总体训练过程与 DeepSeek-Prover-V1 的训练过程大致一致，仅对训练问题的分布进行了两项修改：

+ Prover-V2引入了来自自动形式化和各种开源数据集的额外问题，扩大了训练问题领域的覆盖范围。
+ 新模型通过子目标分解生成的问题来扩充数据集，旨在解决MiniF2F基准测试有效划分中的更多挑战性实例。

在DeepSeek-V3-Base-671B上使用恒定的学习率5e-6，在16384个token的上下文中进行监督微调。训练语料库由两个互补来源组成：

+ 通过专家迭代收集的非CoT数据，生成无需中间推理步骤的Lean代码；
+ 冷启动CoT数据，将DeepSeek-V3的高级数学推理过程提炼为结构化的证明路径。非CoT组件强调精益定理证明器生态系统中的形式验证技能，而CoT示例明确地建模了将数学直觉转化为形式证明结构的认知过程。

Prover-V2采用GRPO，通过为每个定理提示采样一组候选证明并根据它们的相对奖励优化策略，消除了对单独critic模型的需求。训练使用二元奖励，每个生成的Lean证明如果被验证为正确则获得1个奖励，否则为0。为了确保有效学习，研究人员精心挑选训练提示，仅包括那些对监督微调模型具有足够挑战性但可解决的问题。模型在每次迭代中采样256个不同的问题，为每个定理生成32个候选证明，最大序列长度为32768个token。

模型蒸馏：

+ 把DeepSeek-Prover-V1.5-Base-7B的最大上下文长度从4096个token扩展到了32768个，并使用DeepSeek-Prover-V2-671B强化学习阶段收集的rollout数据对这个扩展上下文模型进行微调。
+ 除了CoT推理模式外，还整合了专家迭代过程中收集的非CoT证明数据，以实现一种成本效益高的证明选项，该选项能够生成简洁的形式化输出，并且模型规模较小。
+ 7B模型也采用了与671B模型训练相同的强化学习阶段以提升性能。


# Qwen系列

## 早期的Qwen

早期的：

[Qwen1.5-110B：首个国产千亿参数开源大模型](https://mp.weixin.qq.com/s/Lb08elR2r23UN5kWnhF-BA)

4月26日，Qwen开源了其第一个千亿参数大模型Qwen1.5-110B，这应该也是国内第一个千亿规模的开源大模型。其包含1100亿参数，更重要的是这是一个Dense模型，而非MoE模型。从各项评测来看，Qwen1.5-110B足以与Llama3-70B相抗衡，部分指标也取得了更高的水平。

[阿里云Qwen2.5发布！再登开源大模型王座，Qwen-Max性能逼近GPT-4o](https://mp.weixin.qq.com/s/D2D9hga06bg2AMuGnizN5w)

所有 Qwen2.5 系列模型都在 18 万亿（18T）tokens 的数据上进行了预训练。在语言模型方面，Qwen2.5开源了7个尺寸：0.5B、1.5B、3B、7B、14B、32B、72B，每个都在同等参数赛道创造了业界最佳成绩。这些型号的设定充分考虑了下游场景的不同需求：

+ 3B：适配手机等端侧设备的黄金尺寸；
+ 32B：最受开发者期待的「性价比之王」，可在性能和功耗之间获得最佳平衡
+ Qwen2.5-32B的整体表现甚至超越了Qwen2-72B。

[阿里除夕发布Qwen2.5-Max反超DeepSeek V3，一句话开发小游戏](https://mp.weixin.qq.com/s/4gf6qQXDlq0fc3rGkzJj2g)

Qwen2.5-Max同DeepSeek V3一样是超大规模的MoE模型，


## QwQ-max-preview

[阿里旗舰推理模型硬刚DeepSeek！官宣独立APP，发布公告AI亲自写](https://mp.weixin.qq.com/s/UjwuAKaRhKpQ-9eA2pSh8g)

[https://chat.qwen.ai](https://chat.qwen.ai)

## QWQ-32B

[https://qwenlm.github.io/blog/qwq-32b/](https://qwenlm.github.io/blog/qwq-32b/)

[https://huggingface.co/Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)

目前只有博客，写得挺简单的，看介绍是用了两阶段RL：

+ 1阶段RL：在冷启ckpt上，只拿代码和数学语料，基于outcome-base reward，随着训练的进行，发现这两方面的能力都有持续提升
  + 数学：有一个accuracy verifier，检查最终解决方案是否正确
  + 代码：有一个代码执行server，检查代码是否通过预定义好的测试用例
+ 2阶段RL：没说用什么语料，但用的是general reward model加上rule-based verifiers产生的reward，发现只要少量的step就能提升模型的通用能力，如指令遵循、人类偏好对齐、agent性能，而且代码和数学能力下降不明显

## Qwen2.5-Omni

[阿里深夜开源Qwen2.5-Omni，7B参数完成看、听、说、写](https://mp.weixin.qq.com/s/BZv7tuMX6yZr5LvqKZBJCw)

[Qwen2.5-Omni Technical Report](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)

[https://github.com/QwenLM/Qwen2.5-Omni/tree/main](https://github.com/QwenLM/Qwen2.5-Omni/tree/main)

[https://github.com/QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)

## Qwen3

[猛击OpenAI o1、DeepSeek-R1！刚刚，阿里Qwen3登顶全球开源模型王座，深夜爆火](https://mp.weixin.qq.com/s/gvWJMfy2ah4IHqyO-tP8MA)

[Qwen3：思深，行速](https://mp.weixin.qq.com/s/OvobsCPW0IwxeSm8pljv-A)

开源了两个MoE模型的权重：

+ Qwen3-235B-A22B：拥有2350多亿总参数和220多亿激活参数
+ Qwen3-30B-A3B：拥有约300亿总参数和30亿激活参数

此外，还有6个Dense模型：Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B和Qwen3-0.6B

### 预训练

Qwen2.5是在18万亿个token上进行预训练的，而Qwen3使用的数据量几乎是其两倍，达到了约36万亿个token，涵盖了119种语言和方言。

预训练过程分为三个阶段。

+ 第一阶段：模型在超过30万亿个token上进行了预训练，上下文长度为4Ktoken。这一阶段为模型提供了基本的语言技能和通用知识。
+ 第二阶段：通过增加知识密集型数据（如STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的5万亿个token上进行了预训练。
+ 第三阶段：使用高质量的长上下文数据将上下文长度扩展到32Ktoken，确保模型能够有效地处理更长的输入。

效果：

+ Qwen3 Dense基础模型的整体性能与参数更多的Qwen2.5基础模型相：Qwen3-1.7B/4B/8B/14B/32B-Base分别与Qwen2.5-3B/7B/14B/32B/72B-Base表现相当。特别是在STEM、编码和推理等领域，Qwen3 Dense基础模型的表现甚至超过了更大规模的Qwen2.5模型。
+ Qwen3 MoE基础模型在仅使用10%激活参数的情况下，达到了与Qwen2.5 Dense基础模型相似的性能，带来了训练和推理成本的显著节省。

### 后训练

![](../assets/qwen3-post-training.png)

四阶段：

+ 长思维链冷启动：使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。
+ 长思维链强化学习：重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。
+ 思维模式融合：在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，**将非思考模式整合到思考模型中**。确保了推理和快速响应能力的无缝结合。
+ 通用强化学习：在包括指令遵循、格式遵循和Agent能力等在内的20多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。

### 使用

vllm：

```shell
vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1
# 要禁用思考模式，可以移除--reasoning-parser以及--enable-reasoning
```

软切换机制，可以在```enable_thinking=True```时动态控制模型的行为，即在用户prompt或系统prompt中添加```/think```和```/no_think```来逐轮切换模型的思考模式。

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

classQwenChatbot:
    def __init__(self, model_name="Qwen3-30B-A3B/Qwen3-30B-A3B"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.history = []

    def generate_response(self, user_input):
        messages = self.history + [{"role": "user", "content": user_input}]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        response_ids = self.model.generate\
          (**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # Update history
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})

        return response

# Example Usage
if __name__ == "__main__":
    chatbot = QwenChatbot()

    # First input (without /think or /no_think tags, 
    # thinking mode is enabled by default)
    user_input_1 = "How many r's in strawberries?"
    print(f"User: {user_input_1}")
    response_1 = chatbot.generate_response(user_input_1)
    print(f"Bot: {response_1}")
    print("----------------------")

    # Second input with /no_think
    user_input_2 = "Then, how many r's in blueberries? /no_think"
    print(f"User: {user_input_2}")
    response_2 = chatbot.generate_response(user_input_2)
    print(f"Bot: {response_2}") 
    print("----------------------")

    # Third input with /think
    user_input_3 = "Really? /think"
    print(f"User: {user_input_3}")
    response_3 = chatbot.generate_response(user_input_3)
    print(f"Bot: {response_3}")
```

MCP：可以使用MCP配置文件，使用Qwen-Agent内置的工具，或者自行集成其他工具。

```python
from qwen_agent.agents import Assistant

# Define LLM
llm_cfg = {
    'model': 'Qwen3-30B-A3B',
    # Use the endpoint provided by Alibaba Model Studio:
    # 'model_type': 'qwen_dashscope',
    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),
    # Use a custom endpoint compatible with OpenAI API:
    'model_server': 'http://localhost:8000/v1',  # api_base
    'api_key': 'EMPTY',
    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define Tools
tools = [
    {'mcpServers': {  # You can specify the MCP configuration file
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            "fetch": {
                "command": "uvx",
                "args": ["mcp-server-fetch"]
            }
        }
    },
  'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 
  'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)
```

# Kimi系列

## MOBA

[撞车DeepSeek NSA，Kimi杨植麟署名的新注意力架构MoBA发布，代码也公开](https://mp.weixin.qq.com/s/okrYBqSRxUrXQiHjo-nlYA)

[MoBA: Mixture of Block Attention for Long-Context LLMs](https://github.com/MoonshotAI/MoBA/blob/master/MoBA_Tech_Report.pdf)

[https://github.com/MoonshotAI/MoBA](https://github.com/MoonshotAI/MoBA)


## Kimi-VL

[月之暗面开源轻量级MoE多模态模型，支持推理，效果超过GPT-4o！](https://mp.weixin.qq.com/s/1mvFrL022d-VWPY2k8hcfA)

[Kimi-VL technical report](https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf)

[https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85)

[Kimi 16B胜GPT-4o！开源视觉推理模型：MoE架构，推理时仅激活2.8B](https://mp.weixin.qq.com/s/EsjsqnInCOJpU9SpJgyoOA)



# mamba-transformer混合


## SSM

[挑战Transformer的Mamba是什么来头？作者博士论文理清SSM进化路径](https://mp.weixin.qq.com/s/oXSwnL0sD96nnnqJyko7UA)

[Pretraining Without Attention](https://arxiv.org/pdf/2212.10544.pdf) 

[预训练无需注意力，扩展到4096个token不成问题，与BERT相当](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864837&idx=2&sn=c57f0b8a7daf7d45093448c8ff5df3fc&chksm=84e538bbb392b1ad857f3ad2c2d6d6dbc9562ff492a7cda336dc07abe4d3912c19fa4c30d365&scene=21#wechat_redirect)

[Diffusion Models Without Attention](https://arxiv.org/pdf/2311.18257.pdf)

[丢掉注意力的扩散模型：Mamba带火的SSM被苹果、康奈尔盯上了](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650900014&idx=4&sn=315ca75ba700a93598b5c581ad2fb735&chksm=84e44250b393cb4619e7c24b9f6affe88e4c71ebb97d0c43b5301edd0a11154af6322f68cfd4&scene=21#wechat_redirect)

[一文看懂Mamba，Transformer最强竞争者](https://mp.weixin.qq.com/s/gwE_OVWigV71Qiup7F_9Cg)

[Long Range Language Modeling via Gated State Spaces](https://arxiv.org/abs/2206.13947) 认为 Transformer 和 SSM 完全可以互补。

选择性状态空间模型(selective state space model)是Mamba论文作者Albert Gu此前主导研发的S4架构(Structured State Spaces for Sequence Modeling)的一个简单泛化。

[https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf](https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf)

序列模型在训练和推理时的侧重点：

+ 训练时：在整个list上计算loss，需要优化forward的耗时
+ 推理时：一次输入一个时间步，需要高效地顺序处理

### SSM原理

输入$$u(t) \in \mathbb{R}$$，输出是$$y(t) \in \mathbb{R}$$，引入**状态**$$x(t) \in \mathbb{R}^N$$，目标是学习映射$$u(t) \mapsto y(t)$$，

XXX
\begin{aligned}
x^{\prime}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t)
\end{aligned}
XXX

结合[维基百科](https://en.wikipedia.org/wiki/State-space_representation)的图，方便理解

![ssm](../assets/ssm.png)

各变量含义如下（看起来和RNN很像）：

+ $$\boldsymbol{A}\in \mathbb{R}^{N\times N}$$：状态矩阵
+ $$\boldsymbol{B}\in \mathbb{R}^{N\times N}$$：输入矩阵
+ $$\boldsymbol{C}\in \mathbb{R}^{N\times N}$$：输出矩阵
+ $$\boldsymbol{D}\in \mathbb{R}^{N\times N}$$：feedforward矩阵或者feedthrough矩阵
+ $$x^{\prime}(t):=\frac{d}{d t} \mathbf{x}(t)$$是$$x(t)$$关于$$t$$的导数，所以图里有一个**积分**$$\int$$操作把它变回$$x(t)$$

假设序列长度为$$L$$，那么

+ 时间复杂度：
    + SSM：$$O(N^2L)$$，因为$$\boldsymbol{A} x(t)$$是$$N\times N$$和$$N\times 1$$的矩阵乘法，复杂度是$$N^2$$
    + RNN：$$O(N^2L)$$，和SSM类似
    + CNN：$$O(kLM^2)$$，假设$$k$$个卷积核，每个卷积核$$M\times M$$，一般$$M \ll N$$
+ 空间复杂度：
    + SSM：$$O(N^2+NL)$$，因为中间状态$$x(t) \in \mathbb{R}^N$$是需要**额外存储**的
    + RNN：$$O(N^2)$$，所有时间步共享权重，不需要像SSM专门为每个时间步存一个状态$$x(t)$$
    + CNN：$$kM^2$$

所以传统SSM的时空复杂度都很高，作者提出了S4(structured state space model)

====>hippo的假设会让模型更关注宽波，对于很窄的毛刺信号效果不好，所以对大海捞针任务效果并不好

## Mamba

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)

[https://huggingface.co/state-spaces](https://huggingface.co/state-spaces)

[https://github.com/havenhq/mamba-chat](https://github.com/havenhq/mamba-chat)

[MODELING SEQUENCES WITH STRUCTURED STATE SPACES](https://stacks.stanford.edu/file/druid:mb976vf9362)

## Mamba2

[再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升](https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g)

[Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/pdf/2405.21060)

[https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)

## SAMBA

[长文本模型近期研究工作梳理](https://mp.weixin.qq.com/s/5u2w08twsJ6CgHZ2FpN8JA)

[SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/pdf/2406.07522)

## Mamba in llama

[Mamba作者新作：将Llama3蒸馏成混合线性 RNN](https://mp.weixin.qq.com/s/jZGcBxhVUfb-Qv1UkqYK4A)

[The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/pdf/2408.15237)


## Mamba+Transformer

[Mamba真比Transformer更优吗？Mamba原作者：两个都要！混合架构才是最优解](https://mp.weixin.qq.com/s/omImpaiddmSJ968bCZ8qmw)

[An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887)

## Block-State Transformer

[Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)

## Jamba

[Mamba做大做强！混合Transformer，打败Transformer](https://mp.weixin.qq.com/s/Lde0G_zysfrGcRUX5nOf8g)

[https://huggingface.co/ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1)

让mamba做短程头，让transformer做长程头

## Falcon Mamba

[非Transformer架构站起来了！首个纯无注意力大模型，超越开源巨头Llama 3.1](https://mp.weixin.qq.com/s/ET9gghK4asEr5ObuW2padw)

[Welcome FalconMamba: The first strong attention-free 7B model](https://huggingface.co/blog/falconmamba)

[https://huggingface.co/tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b)

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py)

核心是```FalconMambaMixer```这个类

无需增加内存存储，就可以处理任意长度的序列，并且能够在单个 24GB A10 GPU 上运行。

训练数据有5500GT ，主要由RefinedWeb数据集组成，并添加了来自公共源的高质量技术数据、代码数据和数学数据。

采用**多阶段训练策略**进行训练，上下文长度从2048增加到了8192。此外，受到**课程学习**概念的启发，整个训练阶段精心选择了混合数据，充分考虑了数据的**多样性和复杂性**。在最后的训练阶段，使用了一小部分高质量精选数据（即来自 Fineweb-edu 的样本），以进一步提升性能。

大部分训练是在**256个H100 80GB GPU**上完成的，采用了 3D 并行（TP=1、PP=1、DP=256）与 ZeRO 相结合的策略。

用adamW+WSD（预热 - 稳定 - 衰减）学习率schedule，在前50 GT的训练过程中，batch大小从b_min=128增加到了b_max=2048


## Nemotron-H

[https://research.nvidia.com/labs/adlr/nemotronh/](https://research.nvidia.com/labs/adlr/nemotronh/)

## STORM

[Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/pdf/2503.04130)

## VAMBA

[长视频理解新突破！Mamba混合架构让显存消耗腰斩，处理10万视频token不费力](https://mp.weixin.qq.com/s/eGBF_jJqmpwNT6D-t0BK5Q)

[VAMBA: Understanding Hour-Long Videos with Hybrid Mamba-Transformers](https://arxiv.org/pdf/2503.11579)

