# LLM概述

PLM（pretrained language models），即bert等

## LLM简史

+ 2017年的[Learning to generate reviews and discovering sentiment](https://arxiv.org/pdf/1704.01444.pdf)尝试用rnn来实现智能系统
+ 2018年的gpt1：[Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)，生成式预训练（Generative pre-training, gpt），用transformer的decoder，参数量117m（0.1b），无监督预训练和有监督微调。确定对自然语言文本建模的基本原则为**预测下一个单词**。
+ 2019年的gpt2：[Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)模型结构小改，增加数据，参数量变大为15亿（1.5b），无监督语言建模，**无需使用标记数据进行显式微调**。
    + 参考[The natural language decathlon: Multitask learning as question answering](https://arxiv.org/pdf/1806.08730.pdf)中**多任务求解的概率形式**： $$p(output|input,task)$$ 。
    + 提出“由于特定任务的有监督目标与无监督目标（语言建模）相同，只是在序列的子集上进行评估，因此，无监督目标的全局最小值也是有监督目标的全局最小值”，即每个NLP任务可以看作**世界文本子集的单词预测问题**，如果模型有足够能力来复原世界文本，无监督语言建模可以解决各种问题。
    + 仅无监督与监督微调的SOTA相比效果还是不太行。虽然GPT2模型规模相对较小，但如对话等任务在其基础上做微调还是能拿到很好的效果的，例如[DIALOGPT : Large-scale generative pre-training for conversational response generation](https://arxiv.org/pdf/1911.00536.pdf)、[End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2](https://aclanthology.org/2020.acl-main.54.pdf)
+ 2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning（这点在**3.3亿的BERT和15亿的gpt2中看不到**）。**预训练和ICL有相同的语言建模范式**：预训练预测给定上下文条件下的后续文本序列，ICL预测正确的任务解决方案，其可被格式化为给定任务描述和示范下的文本序列。
+ GPT-3的两种改进方法：
    + 使用代码数据训练：GPT-3主要问题是缺乏对复杂任务的推理能力，2021年openai提出了Codex（[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)），在github代码上微调的GPT。[A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more](https://arxiv.org/pdf/2112.15594.pdf)发现Codex能解决非常困难的编程问题，还能在数学问题上有显著提升。[Text and code embeddings by contrastive pre-training](https://arxiv.org/pdf/2201.10005.pdf)提出了训练文本和代码emb的对比学习，在线性探测分类、文本搜索、代码搜索等任务上有所提升。GPT-3.5就是在基于代码的GPT（code-davinci-002）的基础上开发的。
    + 与人类对齐：2017年openai就在[learning from human preference](https://openai.com/research/learning-from-human-preferences)的博客中提出了应用强化学习来学习由人类标的偏好比较，此后2021年7月openai发表了PPO。2020年GPT-2用RL进行微调，[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)，[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)也做了相似工作。2022年提出了RLHF的InstructGPT([Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf))，其中的**SFT就对应于常说的指令微调**。在openai的博客[Our approach to alignment research](https://openai.com/blog/our-approach-to-alignment-research)中提出了训练AI系统的3个有前途的方向：**使用人类反馈、协助人类评估、做对齐研究**。
+ 2022年的ChatGPT：用类似InstructGPT的方式进行训练，专门**对对话能力进行优化**，将人类生成的对话（**扮演用户和AI两个角色**）与InstructGPT数据集结合起来**以对话形式生成**。
+ 2023年的GPT-4：将文本输入扩展到**多模态信号**。此外，
    + 提升安全性：在RLHF训练中加入**额外的安全奖励信号**，采用多种干预策略如Anthropic提出的[Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned](https://arxiv.org/pdf/2209.07858.pdf)提到的红队评估（read teaming）机制以减轻幻觉、隐私和过度依赖问题。
    + 改进的优化方法：使用**可预测扩展**（predictable scaling）的机制，使用模型训练期间的一小部分计算量**以预测最终性能**。
    + 迭代部署的工程方案：[Lessons learned on language model safety and misuse](https://openai.com/research/language-model-safety-and-misuse)，遵循5阶段的开发和部署生命周期来开发模型和产品。

## LLM列表（持续更新中）

+ 百亿：除了LLaMA（最大650亿）和NLLB（最大545亿），大多数在100亿-200亿之间，通常需要**数百甚至上千**个GPU或TPU。
+ 千亿：OPT、OPT-IML、BLOOM和BLOOMZ与GPT-3(175B)大致相同，GLM有1300亿，Galactica有1200亿，通常需要**数千**个GPU或者TPU。

| ckpt? | 模型 |发布时间 | 大小 | 预训练数据规模 | 硬件 | 训练时间  |
|---|---|---|---|---|---|---|
| Y | [T5](https://arxiv.org/pdf/1910.10683.pdf) | 2019.10| 11B |  1万亿tokens | 1024 TPU v3  |  - |
| N | [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) | 2020.05 | 175B |  3000万亿tokens | -  |  - |
| N | [GShard](https://arxiv.org/pdf/2006.16668.pdf) | 2020.06 | 600B |  1万亿tokens | 2048 TPU v3 | 4天 |
| Y | [mT5](https://arxiv.org/pdf/2010.11934.pdf) | 2020.10 | 13B |  1万亿tokens | -  |  - |
| Y | [PanGu-$$\alpha$$](https://arxiv.org/pdf/2104.12369.pdf) | 2021.04 | 13B |  1.1TB | 2048 Ascend 910 | - |
| Y | [CPM-2](https://arxiv.org/pdf/2106.10715.pdf) | 2021.06 | 198B |  2.6TB | - | - |
| N | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | 2021.07 | 12B |  1000万亿tokens | - | - |
| N | [ERNIE 3.0](https://arxiv.org/pdf/2107.02137.pdf) | 2021.07 | 10B |  3750亿tokens | 384 v100 | - |
| N | [Jurassic-1](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | 2021.08 | 178B | 3000亿tokens | 800 GPU | - |
| N | [HyperCLOVA](https://arxiv.org/pdf/2109.04650.pdf) | 2021.09 | 82B | 3000亿tokens | 1024 A100 | 13.4天 |
| N | [FLAN](https://arxiv.org/pdf/2109.01652.pdf) | 2021.09 | 137B | - | 128 TPU v3 | 60小时 |
| N | [Yuan 1.0](https://arxiv.org/pdf/2110.04725.pdf) | 2021.10 | 245B | 1800亿tokens | 2128 GPU | - |
| Y | [T0](https://arxiv.org/pdf/2211.01786.pdf) | 2021.10 | 11B | - | 512 TPU v3 | 27小时 |
| N | [Anthropic](https://arxiv.org/pdf/2112.00861.pdf) | 2021.12 | 52B | 4000亿tokens | - | - |
| N | [WebGPT](https://arxiv.org/pdf/2112.09332.pdf) | 2021.12 | 175B |  - | - | - |
| N | [Gopher](https://arxiv.org/pdf/2112.11446.pdf) | 2021.12 | 280B |  3000亿tokens | 4096 TPU v3 | 920小时 |
| N | [ERNIE 3.0 Titan](https://arxiv.org/pdf/2112.12731.pdf) | 2021.12 | 260B |  - | - | - |
| N | [GLaM](https://arxiv.org/pdf/2112.06905.pdf) | 2021.12 | 1200B | 2800亿tokens | 1024 TPU v4 | 574小时 |
| N | [LaMDA](https://arxiv.org/pdf/2201.08239.pdf) | 2022.01 | 137B |  7680亿tokens | 1024 TPU v3 | 57.5天 |
| N | [MT-NLG](https://arxiv.org/pdf/2201.11990.pdf) | 2022.01 | 530B | 2700亿tokens | 4480 80G A100 | - |
| N | [AlphaCode](https://arxiv.org/pdf/2203.07814.pdf) | 2022.02 | 41B | 9670亿tokens | - | - |
| N | [InstructGPT](https://arxiv.org/pdf/2203.02155.pdf) | 2022.03 | 175B |  - | - | - |
| N | [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) | 2022.03 | 70B | 1.4万亿tokens | - | - |
| Y | [CodeGen](https://arxiv.org/pdf/2203.13474.pdf) | 2022.03 | 16B | 5770亿tokens | - | - |
| Y | [GPT-NeoX-20B](https://arxiv.org/pdf/2204.06745.pdf) | 2022.04 | 20B | 825GB | 96 40G A100 | - |
| Y | [Tk-Instruct](https://arxiv.org/pdf/2204.07705.pdf) | 2022.04 | 11B |  - | 256 TPU v3 | 4小时 |
| N | [PaLM](https://arxiv.org/pdf/2204.02311.pdf) | 2022.04 | 540B | 7800亿tokens | 6144 TPU v4 | - |
| Y | [UL2](https://arxiv.org/pdf/2205.05131.pdf) | 2022.05 | 20B |  825GB | 96 40G A100 | - |
| Y | [OPT](https://arxiv.org/pdf/2205.01068.pdf) | 2022.05 | 175B | 1800亿tokens | 992 80G A100 | - |
| Y | [NLLB](https://arxiv.org/pdf/2207.04672.pdf) | 2022.07 | 54.5B |  - | - | - |
| N | [AlexaTM](https://arxiv.org/pdf/2208.01448.pdf) | 2022.08 | 20B | 1.3万亿tokens | 128 A100 | 120天 |
| N | [Sparrow](https://arxiv.org/pdf/2209.14375.pdf) | 2022.09 | 70B | 64 TPU v3 | - | - |
| N | [WeLM](https://arxiv.org/pdf/2209.10372.pdf) | 2022.09 | 10B | 3000亿tokens | 128 A100 40G | 24天 |
| N | [U-PaLM](https://arxiv.org/pdf/2210.11399.pdf) | 2022.10 | 540B | - | 512 TPU v4 | 5天 |
| N | [Flan-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | 512 TPU v4 | 37小时 |
| N | [Flan-U-PaLM](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 540B |  - | - | - |
| Y | [GLM](https://arxiv.org/pdf/2210.02414.pdf) | 2022.10 | 130B | 4000亿tokens | 768 40G A100 | 60天 |
| Y | [Flan-T5](https://arxiv.org/pdf/2210.11416.pdf) | 2022.10 | 11B |  - | - | - |
| Y | [BLOOM](https://arxiv.org/pdf/2211.05100.pdf) | 2022.11 | 176B | 3660亿tokens | 384 80G A100 | 105天 |
| Y | [mT0](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 13B |  - | - | - |
| Y | [Galactica](https://arxiv.org/pdf/2211.09085.pdf) | 2022.11 | 120B | 1060亿tokens | - | - |
| Y | [BLOOMZ](https://arxiv.org/pdf/2211.01786.pdf) | 2022.11 | 176B |  - | - | - |
| Y | [OPT-IML](https://arxiv.org/pdf/2212.12017.pdf) | 2022.12 | 175B |  - | 128 40G A100 | - |
| Y | [LLaMA](https://arxiv.org/pdf/2302.13971.pdf) | 2023.02 | 65B | 1.4万亿tokens | 2048 80G A100 | 21天 |
| N | [GPT-4](https://arxiv.org/pdf/2303.08774.pdf) | 2023.03 | - |  - | - | - |
| Y | [CodeGeeX](https://arxiv.org/pdf/2303.17568.pdf) | 2022.09 | 13B | 8500亿tokens | 1536 Ascend 910 | 60天 |
| N | [PanGU-$$\Sigma$$](https://arxiv.org/pdf/2303.10845.pdf) | 2023.03 | 1085B | 3290亿tokens | 512 Ascend 910 | 100天 |
| Y | [Pythia](https://arxiv.org/pdf/2304.01373.pdf) | 2023.04 | 12B | 3000亿tokens | 256 40G A100 | - |

可以直接把对应的md丢给gpt，叫它导出一个excel，然后就可以自定义排序或者画散点图看了



## LLM开源库

+ transformers：huggingface的库
+ [deepspeed](https://github.com/microsoft/DeepSpeed)：微软的库，与pytorch兼容，训练了MT-NLG、BLOOM等模型，包括各种分布式训练优化技术，如**内存优化**（**ZeRO**、**梯度检查点**等）和**管道并行**。
+ megatron-lm：英伟达的库，同样包括各种分布式训练技术，包括**模型和数据并行**、**混合精度**训练和**FlashAttention**。（[Megatron-lm: Training multi-billion parameter language models using model parallelism](https://arxiv.org/pdf/1909.08053.pdf)、[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)和[Reducing activation recomputation in large transformer models](https://arxiv.org/pdf/2205.05198.pdf)）
+ [jax](https://github.com/google/jax)：google的库，允许用户在**带有硬件加速（GPU或TPU）**的情况下进行**数组的高效运算**，可以在**各种设备**高效计算，支持**自动微分**和**即时编译**等功能。
+ [colossal-AI](https://arxiv.org/pdf/2110.14883.pdf)：HPC-AI Tech的库，基于pytorch，可以使用[PatrickStar](Patrickstar: Parallel training of pre-trained models via a chunk-based memory management)提出的方法优化异构内存管理，分布了基于LLaMA的[ColossalChat](https://medium.com/pytorch/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
+ [BMTrain](https://github.com/OpenBMB/BMTrain)：openBMB的库，强调代码简洁、低资源占用和高可用性
+ [FastMoE](Fastmoe: A fast mixture-of-expert training system)：专门用于MoE模型的训练库，基于pytorch，简化了将transformer转换为MoE模型的过程
+ [semantic-kernel](https://github.com/microsoft/semantic-kernel)：微软的开源库

![AI的4场景战役](../assets/4wars-in-aistck.png)

一些开源的小模型：[从零训练的 1B 以下小模型汇总](https://mp.weixin.qq.com/s/IWuMj6ywge2NAUhYmYQBLA)

## 一些综述

+ [Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media](https://github.com/daiwk/collections/blob/master/assets/LLM/foundation%20models%20NLP.pdf)
+ [大规模语言模型：从理论到实践](../assets/LLM/LLM-TAP.pdf)，[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf)邱锡鹏等
+ 人大的大模型综述：[https://github.com/RUCAIBox/LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)，[自己存了一份pdf](https://github.com/daiwk/collections/blob/master/assets/LLM/LLM_Survey_Chinese.pdf)，（**！！！本文大部分内容按这个来组织！！！**）
+ [Talking about large language models](https://arxiv.org/pdf/2212.03551.pdf)
+ [Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://arxiv.org/pdf/2107.13586.pdf)，引用数2k+
+ [A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt](https://arxiv.org/pdf/2302.09419.pdf)，唐杰等
+ [Pre-Trained Models: Past, Present and Future](https://arxiv.org/pdf/2106.07139.pdf)
+ [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)
+ [Pretrained Language Models for Text Generation: A Survey](https://arxiv.org/pdf/2105.10311.pdf)
+ [A survey for in-context learning](https://arxiv.org/pdf/2301.00234.pdf)
+ [Towards reasoning in large language models: A survey](https://arxiv.org/pdf/2212.10403.pdf)
+ [Reasoning with language model prompting: A survey](https://arxiv.org/pdf/2212.09597.pdf)
+ [Dense Text Retrieval based on Pretrained Language Models: A Survey](https://arxiv.org/pdf/2211.14876.pdf)
+ [Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章](https://zhuanlan.zhihu.com/p/395795968)
+ [如何高效部署大模型？CMU最新万字综述纵览LLM推理MLSys优化技术](https://mp.weixin.qq.com/s/Uue0SxH6W_tI8K4Zb0igLQ)：[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems](https://arxiv.org/abs/2312.15234)
+ [一篇对大语言模型（LLMs）进行全面、深入分析的43页综述（Word2Vec作者出品）](https://mp.weixin.qq.com/s/5fbx0lM9V-Q7xYbeDauuHw)==>[Large Language Models: A Survey](https://arxiv.org/pdf/2402.06196.pdf)，[自己存了一份](https://github.com/daiwk/collections/blob/master/assets/LLM/Large%20Language%20Models-%20A%20Survey.pdf)
+ [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/pdf/2308.07107v3.pdf)
+ [awesome-LLM-resourses](https://github.com/WangRongsheng/awesome-LLM-resourses)
+ [14天速成LLM高手！大佬开源学习笔记，GitHub狂揽700星](https://mp.weixin.qq.com/s/aDkH9E5b0yNd1J_Kthkh5Q)：[https://github.com/hesamsheikh/ml-retreat](https://github.com/hesamsheikh/ml-retreat)
+ [GitHub超火开发者路线图库有AI学习路线了！star数近30万](https://mp.weixin.qq.com/s/P9hWHGsiWcfEfMq54E02xg)：[https://github.com/kamranahmedse/developer-roadmap](https://github.com/kamranahmedse/developer-roadmap)
+ [写的真好，万字长文串烧LLM大模型技术原理](https://mp.weixin.qq.com/s/ArTr-Df10LiTkJ4k9zlmZA)：[https://zhuanlan.zhihu.com/p/713794852](https://zhuanlan.zhihu.com/p/713794852)


[大模型面试八股](https://zhuanlan.zhihu.com/p/643560888)

[大模型八股答案（一）——基础知识](https://zhuanlan.zhihu.com/p/643829565)

[大模型八股答案（二）——训练框架](https://zhuanlan.zhihu.com/p/643836163)


## 扩展法则(scaling law)

### openai的扩展法则

2020年,openai的[Scaling laws for neural language models](https://arxiv.org/pdf/2001.08361.pdf)通过拟合模型在不同数据大小（2000w到230亿个token）、不同的模型大小（7.68亿到15亿个**非嵌入参数**）的性能，提出了在**计算预算**$$c$$的条件下，$$L$$是用nats表示的交叉熵损失，模型性能与**模型规模**$$N$$、**数据集规模**$$D$$以及**训练计算量**$$C$$间存在如下幂律关系：

XXXL(N)=(\frac{N_c}{N})^{\alpha _N}, {\alpha}_N\sim 0.076,N_c\sim 8.8\times 10^{13}XXX

XXXL(D)=(\frac{D_c}{D})^{\alpha _D}, {\alpha}_D\sim 0.05,N_c\sim 5.4\times 10^{13}XXX

XXXL(C)=(\frac{C_c}{C})^{\alpha _C}, {\alpha}_C\sim 0.05,C_c\sim 3.1\times 10^{8}XXX

其中，$$N_c$$表示非嵌入参数数量，$$D_c$$表示训练token数量,$$C_c$$表示FP-days。

[Go Wider Instead of Deeper](https://arxiv.org/pdf/2107.11817)说了，transformer效果的提升**不在于计算量的变大**，而应该在于通过**提升模型的hidden dim**来增加模型参数量

### Chinchilla扩展法则

DeepMind在[Training compute-optimal large language models](https://arxiv.org/pdf/2203.15556.pdf)中提出了Chichilla扩展法则来指导LLM**最优计算量**的训练。通过变化更大范围的模型大小（7000w到160亿参数）和数据大小（50亿到5000亿个token）进行实验，拟合了如下的扩展法则：

XXX
L(N, D)=E+\frac{A}{N^\alpha}+\frac{B}{D^\beta}
XXX

其中$$E=1.69,A=406.4,B=410.7,\alpha = 0.34, \beta =0.28$$，通过在约束条件$$C\approx 6ND$$下优化损失$$L(N,D)$$，将计算预算最优地分配给模型大小和数据大小的方法：

XXX
N_{o p t}(C)=G\left(\frac{C}{6}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{6}\right)^b
XXX

其中$$a=\frac{\alpha}{\alpha+\beta}$$，$$b=\frac{\beta}{\alpha+\beta}$$，$$G$$是由$$A,B,\alpha,\beta$$计算出的扩展系数。

随着计算预算的增加，

+ openai的扩展法则更偏向于将更大预算分给**模型大小**，因为其对比各模型时使用了固定的训练数据量和学习率等超参，低估了数据量的作用。每增加10倍的计算量，应该让数据集大小增加为约1.8倍，模型参数量增加为约5.5倍。即**模型参数量更加的重要**。
+ Chinchilla扩展法则认为**模型大小和数据大小要同比例增加**，即$$a$$和$$b$$取值差不多。因为其在无视模型大小的前提下，发现设置与数据量差不多match的学习率能获得更好的loss。每增加10倍的计算量，应该让数据集大小增加为约3.16倍，模型参数量也增加为约3.16倍。即**数据集大小和模型参数量一样重要**。

然而，有一些能力（如涌现）无法根据扩展法则进行预测，只有当模型达到一定规模时才会出现。

![chinchilla-law](../assets/chinchilla-law.png)

飘红的就是常见的10B模型，大概要205B的token来训练，能达到**计算最优点**，当然**并不一定是loss最小的点**，这个可以参考llama3的现象

### scaling law的一些讨论

[Scaling Laws 又失灵了？谷歌新研究：扩散模型不是越大越好](https://mp.weixin.qq.com/s/ia9L6jr_lwowYHgLI1k_4g)

[Bigger is not Always Better: Scaling Properties of Latent Diffusion Models](https://arxiv.org/pdf/2404.01367.pdf)

[腾讯混元、北大发现Scaling law「浪涌现象」，解决学习率调参难题](https://mp.weixin.qq.com/s/ff5_O0H5VQNkArKroJkEZQ)

[Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling](https://arxiv.org/pdf/2405.14578)

SIGIR24最佳论文：[Scaling Laws For Dense Retrieval](https://arxiv.org/pdf/2403.18684)

[中科大联合华为诺亚提出Entropy Law，揭秘大模型性能、数据压缩率以及训练损失关系](https://mp.weixin.qq.com/s/F4OFP1lzAGH4RSXcXBw7mw)

#### 词表的scaling law

&nbsp;

[NeurIPS 2024 | 大模型的词表大小，同样适用于Scaling Law](https://mp.weixin.qq.com/s/_DTvTMCtrW9WV3vELjU9jw)

[Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies](https://arxiv.org/abs/2407.13623)

[https://github.com/sail-sg/scaling-with-vocab/](https://github.com/sail-sg/scaling-with-vocab/)

#### scaling law for precision

[Scaling Laws终结，量化无用，AI大佬都在审视这篇论文](https://mp.weixin.qq.com/s/JhtOlj5Y4UYM3W3koeMmqw)

[Scaling Laws for Precision](https://arxiv.org/abs/2411.04330)

你训练的 token 越多，你需要的精度就越高。

### Densing law

[LLM最大能力密度100天翻一倍！清华刘知远团队提出Densing Law](https://mp.weixin.qq.com/s/O_jtO2ZuL11XB9GlaURsWg)

[Densing Law of LLMs](https://arxiv.org/pdf/2412.04315v2)

## 涌现能力

![llm-capabilities](../assets/llm-capabilities.png)

涌现能力：在小型模型中不存在而在大型模型中产生的能力，当规模达到一定程度时，性能显著提升，超出随机水平（参考
[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)）。与物理学中的**相变**现象类似（物质从一种相（状态）转变为另一种相的过程，通常伴随着能量的吸收或释放，并且涉及不同的物理性质，例如固体、液体和气体之间的转变）。

[普林斯顿DeepMind用数学证明：LLM不是随机鹦鹉！「规模越大能力越强」有理论根据](https://mp.weixin.qq.com/s/oYYuqbelBfCCSLW4Qo4POA)

[A Theory for Emergence of Complex Skills in Language Models](https://arxiv.org/abs/2307.15936)：


![涌现](../assets/emergent%20ability.png)

LLM的3种典型涌现能力及其对应代表模型：

### 上下文学习(in-context learning)

GPT-3（[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)）提出，只要提供一个自然语言指令和/或几个任务演示，语言模型就能通过完成输入文本的词序列的方式来为测试实例生成预期输出，不用额外的梯度更新。

+ ICL能力小模型不具备：1750亿的GPT-3有ICL能力，但GPT-1和GPT-2无此能力。
+ ICL能力取决于具体下游任务：130亿的GPT-3能在算术任务上有ICL，但1750亿的GPT-3在波斯语QA上无能为力。


### 指令遵循(instruction following)

使用**自然语言描述的混合多任务数据集进行微调（指令微调）**，LLM在**未见过的以指令形式描述的任务**上表现出色，具有更好的泛化能力。例如[Multitask prompted training enables zero-shot task generalization](https://arxiv.org/pdf/2110.08207.pdf)、[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)、[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)。

在[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)的实验中，当模型大小达到680亿时，经过指定微调的LaMDA-PT开始在未见过的任务上显著优于未微调的模型，而80亿或更小的模型则没有这个现象。

在[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)的实验中，PaLM至少在620亿参数上才能在4个评估基准的各种任务上表现良好。

[精准0误差，输入价格打骨折！OpenAI官宣API支持结构化输出，JSON准确率100％](https://mp.weixin.qq.com/s/257SBcB2hr-xKPNYkUEErQ)


### 逐步推理(multi-step reasoning)

对于涉及多个推理步骤的复杂任务（如数学），可以使用**思维链（Chain-of-Thought, CoT）**提示策略（[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)），让LLM通过**利用中间推理步骤的提示机制**来解决这类任务。

[Chain of thought prompting elicits reasoning in large language models](https://arxiv.org/pdf/2201.11903.pdf)发现，CoT在模型大于600亿的PaLM和LaMBDA变体中能够提升在算术推理基准任务的效果，而当模型大于1000亿时，相比标准提示的优势更明显。

[How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

## LLM关键点

如何让LLM能够**通用**且**有能力**？

### 扩展

更大的模型、数据规模和更多的训练计算，但计算预算是有限的，可以用扩展法更高效地分配计算资源，如Chinchilla在**相同计算预算下增加训练token数**，优于更大模型规模的Gopher，同时需要数据清理。

### 训练

+ 分布式的训练框架：包括DeepSpeed（[Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters](https://dl.acm.org/doi/abs/10.1145/3394486.3406703)）和Megatron-LM（[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)和[Efficient large-scale language model training on GPU clusters using megatron-lm](https://arxiv.org/pdf/2104.04473.pdf)）
+ 优化技巧：有助于提升训练稳定性和模型性能，如**重新开始以克服训练损失激增**（[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)）和**混合精度训练**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

### 能力引导

当LLM执行某些特定任务时，可能不会显式地展示出其通用求解器的能力，**设计合适的任务指令或具体的ICL策略**可以**激发**这种能力，例如

+ 通过**包含中间推理步骤的CoT提示**
+ 使用**自然语言表达的任务描述**，对LLM进行**指令微调**

### 对齐微调

由于预训练语料库包括高质量和低质量的数据，LLM可能生成有毒、偏见甚至有害的内容，要让LLM和人类价值观保持一致，如**有用性、诚实性和无害性**。RLHF相关工作如[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)和[Deep reinforcement learning from human preferences](https://arxiv.org/pdf/1706.03741.pdf)能够产生高质量、无害的回答（例如拒绝回答侮辱性问题）。


### 工具操作

LLM本质是基于海量文本语料库进行文本生成训练的，对于不适合以文本形式表达的任务表现不佳（如数字计算），且其能力受限于预训练数据，无法获取最新信息。可以利用外部工具：

+ [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/pdf/2302.04761.pdf)能利用计算器进行准确计算
+ [Webgpt: Browser-assisted question-answering with human feed- back](https://arxiv.org/pdf/2112.09332.pdf)能利用搜索引擎检索未知信息

# LLM数据集

## 常用数据集

llm中文数据集：[https://juejin.cn/post/7238921093553438779](https://juejin.cn/post/7238921093553438779)

+ Books：
    + [BookCorpus](https://arxiv.org/pdf/1506.06724.pdf)：超过11000本电子书，用于GPT和GPT-2。
    + [Gutenberg](https://www.gutenberg.org/)：超过70000本文学作品，包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共领域，用于MT-NLG和LLaMA。
    + Books1和Books2：比BookCorpus大得多，但未公开，用于GPT-3。
+ CommonCrawl：最大的开源网络爬虫数据库之一，**百万亿字节**，有大量噪音和低质信息，需要过滤，有如下4个子集：
    + [C4](https://www.tensorflow.org/datasets/catalog/c4)：包括en（806G，训练T5、LaMDA、Gopher、UL2）、en.noclean（6T）、realnewslike（36G）、webtextlike（17G）、multilingual（38T，训练mT5）。
    + [CC-Stories](https://arxiv.org/pdf/1806.02847.pdf)：31G，内容以故事的形式展示
    + [CC-News](https://arxiv.org/pdf/1907.11692.pdf)：76G
    + [RealNews](https://arxiv.org/pdf/1905.12616.pdf)：120G
+ Reddit Links：Reddit上的帖子，高赞通常比较有用，可以拿来创建高质量数据集。
    + [WebText](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)：由Reddit上的高赞链接组成，未公开，对应的开源版是[OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/)。
    + [Pushshift.io](https://arxiv.org/pdf/2001.08435.pdf)：实时更新的数据集，包括Reddit自创建以来的历史数据，有数据存储，也有实用工具，供用户搜索、总结和统计分析。
+ Wikipedia：大部分文章使用写作风格，并支持引用，英语版本用于大多数LLM，如GPT-3、LaMDA、LLaMA，还有多语言版。
+ Code：包括开源许可证的公共代码库（如github）和与代码相关的问答平台（如StackOverflow）,Google公开了[BigQuery](https://cloud.google.com/bigquery?hl=zh-cn)数据集，CodeGen用的BIGQUERY是其的一个子集。
+ 其他：
    + [The Pile](https://arxiv.org/pdf/2101.00027.pdf)有800G，包括书籍、网站、代码、科学论文和社交媒体平台，有22个子集，用于GPT-J(6B)、CodeGen(16B)、Megatron-Turing NLG（530B）。
    + [ROOTS](https://arxiv.org/pdf/2303.03915.pdf)由各种小数据集组成，共1.6T，包括59种语言（自然语言和编程语言），用于BLOOM。


## 数据收集

### 数据获取

+ 通用文本数据：
    + 网页：例如CommonCrawl，同时需要过滤和处理以提高质量
    + 对话文本：公共对话数据如PushShift.io，对于在线社交媒体的对话数据，可以**转换成树形结构**，每句话与回应其的话相连。多方的对话树可以划分为预训练语料库中的多个子对话。过度引入对话数据可能会有潜在风险（[OPT: open pre-trained transformer language models](https://arxiv.org/pdf/2205.01068.pdf)）：陈述性指令和直接疑问句被错误地认为是对话的开始，导致指令的有效性下降。
    + 书籍：更正式的长文本，利于**学习语言知识**、**建模长期依赖关系**、**生成叙述性和连贯的文本**。
+ 专用文本数据：
    + 多语言文本：BLOOM的预训练语料中包括了46种语言，PaLM包含了122种
    + 科学文本：如arxiv论文、科学教材、数学 网页等，通常需要特定的标记化和预处理。
    + 代码：一是编程问答社区，二是开源代码仅为。对应长距离依赖和准确的执行逻辑，可能是复杂推理能力的来源。将推理任务格式化为代码形式还能帮LLM生成更准确的结果（如[Language models of code are few-shot commonsense learners](https://arxiv.org/pdf/2210.07128.pdf)和[Autoformalization with large language models](https://arxiv.org/pdf/2205.12615.pdf)）

### 数据预处理

+ 质量过滤：有一些基于分类器的方法，例如维基百科的数据为正样本，负采样其他数据训练二分类器，但这种方法会删除方言、口语和社会语言的高质量文本，可能导致有偏、减少多样性。还有启发式的方法，主要包括：
    + 基于语言的过滤：如果该llm主要用于某种语言，可以把其他语言删了
    + 基于度量的过滤：利用生成文本的评估度量（如**perplexity**）来检测和删除不自然的句子
    + 基于统计的过滤：如**标点符号分布**、**符号和单词比例**、**句子长度**等
    + 基于关键词的过滤：删除噪声或无用元素，如**HTML标签**、**超链接**、**模板**、**攻击性词语**等。
+ 去重：[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)中发现重复数据会降低多样性，可能导致训练不稳定。下面3个级的去重都很有用
    + 句子级：删掉包含重复单词和短语的句子，因为可能在语言建模中引入**重复模式**（[The curious case of neural text degeneration](https://arxiv.org/pdf/1904.09751.pdf)）(后面的章节会讲)
    + 文档级：通过文档间的表层特征（如n-gram或单词重合率）来删掉重复文档
    + 数据集级：训练集中删掉测试集可能出现的重复文本，防止训练集和评估集间的重叠
+ 隐私去除：删掉可识别个人信息（PII），如基于关键词（姓名、地址、电话号码）识别。另外，[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/pdf/2202.06539.pdf)发现LLM在隐私攻击下的脆弱性可能归因于预训练语料中存在**重复PII数据**。
+ 分词：可以直接利用已有分词器，也可以使用专门为预训练语料库设计的分词器，如SentencePiece，而且**BPE**(byte pair encoding)能**确保分词后的信息不会丢失**，但其中的如NFKC([Unicode normalization forms](https://unicode.org/reports/tr15/))的**归一化技术**可能会**降低分词的性能**。

### 预训练语料的重要性

+ 混合来源：不同领域和场景的数据能让LLM有更强大的泛化能力。需要**仔细设置数据分布**，Gopher对数据分布消融，发现增加书籍数据可以提升捕捉长期依赖的能力，增加c4数据集比例可以提升其在c4验证集上的效果，但单独训练过多的某个领域数据会影响LLM在其他领域的泛化能力。
+ 数据量：模型性能方面，**数据大小**也能看到与模型大小类似的**扩展法则**。LLaMA发现，用更多数据训练更长时间，较小的模型也能实现良好性能。
+ 数据质量：Gopher、GLaM和T5都发现，在清理后的数据上训练能提升llm效果。数据的重复可能导致『双下降现象』（[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)和[Deep double descent: Where bigger models and more data hurt](https://arxiv.org/pdf/1912.02292.pdf)），甚至会导致训练不稳定。此外，[Scaling laws and interpretability of learning from repeated data](https://arxiv.org/pdf/2205.10487.pdf)还发现，**重复数据会降低LLM从上下文复制的能力**，从而影响**ICL中的泛化能力**。

注：**双下降**指的是随着模型复杂性的增加，可能**loss先下降，然后再升高，最后又下降**：
+ 当模型的复杂性低于数据的复杂性时，增加模型的复杂性可以帮助减少训练误差。
+ 当模型的复杂性超过数据的复杂性时，增加模型的复杂性反而可能导致训练误差增加。这是因为模型开始过拟合数据，捕获数据中的噪声而非实际的模式。
+ 当模型的复杂性远大于数据的复杂性时，训练误差可能再次开始减少。这是因为模型有足够的能力来对数据的噪声进行平滑，同时仍然能够捕获数据的实际模式。

## benchmark

MMLU

[Measuring massive multitask language understanding](https://arxiv.org/pdf/2009.03300)

# LLM模型架构

## MoE原理

[MoE模型的前世今生](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

[MoE 系列论文解读：Gshard、FastMoE、Tutel、MegaBlocks 等](https://mp.weixin.qq.com/s/T5eJZWGH3yRpK9bxmvhhTA)

[CMU开源GRIFFIN：一种新颖的无需训练的MoE方法，提高大模型的生成效率！](https://mp.weixin.qq.com/s/33ISRxfXp4Z7OCN1dBKGyA)

[Prompt-prompted Mixture of Experts for Efficient LLM Generation](https://arxiv.org/pdf/2404.01365.pdf)

[From Sparse to Soft Mixtures of Experts](https://arxiv.org/pdf/2308.00951): softmoe

MoE的核心问题是一个**如何把token分配给哪个专家**的**离散优化**问题，有如下离散+稀疏的分配方法，都需要**辅助loss**来**平衡每个专家的负载**，以减少drop tokens：

+ 线性规划
+ 强化学习
+ 人为固定规则
+ 最优运输方法
+ 贪心topk token-choose-expert
+ 贪心topk expert-choose-token

![softmoe](../assets/softmoe.png)

在softmoe中，假设N个token，S个slot，E个expert

代码：

[https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97](https://github.com/google-research/vmoe/blob/main/vmoe/projects/soft_moe/router.py#L97)和[https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128](https://github.com/google-research/vmoe/blob/main/vmoe/moe.py#L128)

[算法、系统和应用，三个视角全面读懂混合专家（MoE）](https://mp.weixin.qq.com/s/3UEFwy87f8O4H1Tt4VVnig)


[A Survey on Mixture of Experts](https://arxiv.org/pdf/2407.06204)

![moe-gating](../assets/moe-gating.png)

[从ACL 2024录用论文看混合专家模型（MoE）最新研究进展](https://mp.weixin.qq.com/s/BCsRHvHnn3B8oOODjim-Kg)

### DeepSeekMOE

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066)

[https://github.com/deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)

动机：

+ 专家不够分化：以往的MoE模型专家数量很少，假如模型的知识涉及的领域很多，平均一个专家要包含很多领域的知识，即不够**专**
+ 专家有冗余：假设每个token只能选一个专家，又假设每个token都需要常识知识，结果就是不论选哪个专家，这个专家的参数里都有常识知识，因此有冗余了——最好的情况是有个专家专门负责提供常识知识，所有 token 都会用一下这个专家。

解法：

+ **增加专家数量**：专家是个FFN，假设两个矩阵是$$dh$$和$$hd$$，如果将它拆成2个专家，就是拆成2个$$dh/2$$和$$h/2*d$$，假设原来一个token选top-1专家，现在就是选top-2专家，拆前拆后的计算量和参数量没变，好处就是排列组合多样性更多了，选择也更灵活了。
+ **增设共享专家**：有的专家是必选的，除此以外，每个token按照自己的喜好，再来选top-k。比如有64个专家，那么第一个专家是所有token都要选的，除此以外，每个token还从剩下的63个里选择自己的top-1，其实就是top-2。

### Dynamic MoE

[Harder Tasks Need More Experts: Dynamic Routing in MoE Models](https://arxiv.org/pdf/2403.07652)

[https://github.com/ZhenweiAn/Dynamic_MoE](https://github.com/ZhenweiAn/Dynamic_MoE)



### XMoE

是上面两种方法的并集

[XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection](https://arxiv.org/pdf/2403.18926)

[https://github.com/ysngki/XMoE](https://github.com/ysngki/XMoE)

### HyperMoE

[HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/pdf/2402.12656)

[https://github.com/Bumble666/Hyper_MoE](https://github.com/Bumble666/Hyper_MoE)

### Expert Pruning

[Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models](https://arxiv.org/pdf/2402.14800)

[https://github.com/Lucky-Lance/Expert_Sparsity](https://github.com/Lucky-Lance/Expert_Sparsity)

### MixLoRA

[Multimodal Instruction Tuning with Conditional Mixture of LoRA](https://arxiv.org/pdf/2402.15896)

### ESFT

[Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv.org/pdf/2407.01906)



### 多模态MoE

[混合专家更有主见了，能感知多模态分情况行事，Meta提出模态感知型专家混合](https://mp.weixin.qq.com/s/1FNqu0CwPmMFuDTMhli7WA)

[Chameleon: Mixed-modal early-fusion foundation models](https://arxiv.org/pdf/2405.09818)单一Transformer架构，可以根据下一个token的预测目标，对由离散图像和文本token组成的混合模态序列进行建模，从而在不同模态之间进行无缝推理和生成。然而对于Chameleon这样各种模态会在模型训练的早期混合起来的模型，想要拓展它的能力，需要投入大量算力。

[MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://arxiv.org/pdf/2407.21770)使用路由式稀疏架构（routed sparse architecture）

### HMOE

(toread)

[优化传统MoE结构，腾讯混元团队提出专家差异化新思路](https://mp.weixin.qq.com/s/gOXGL_MReneAZCmO7lBIng)

[HMoE: Heterogeneous Mixture of Experts for Language Modeling](https://arxiv.org/pdf/2408.10681)

在 HMoE 中，**每个专家的大小不再相同**，从而赋予了每个专家不同的表达能力。这种差异化设计使得路由可以根据专家的实际能力动态分配不同难度的 token，有效解决了专家专业化程度不足的问题。 

### OLMoE

(toread)

[OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/pdf/2409.02060)

[https://huggingface.co/allenai/OLMoE-1B-7B-0924](https://huggingface.co/allenai/OLMoE-1B-7B-0924)

[https://github.com/allenai/OLMoE](https://github.com/allenai/OLMoE)

### 元象MOE

[中国最大开源MoE模型，255B参数无条件免费商用，元象发布](https://mp.weixin.qq.com/s/vX0gt2YfsbUbrCpvNBFHZw)

XVERSE-MoE-A36B，该模型总参数255B，激活参数36B，达到100B模型性能的「跨级」跃升。

[https://github.com/xverse-ai/XVERSE-MoE-A36B](https://github.com/xverse-ai/XVERSE-MoE-A36B)

### MoEUT

[Jurgen、曼宁等大佬新作：MoE重塑6年前的Universal Transformer，高效升级](https://mp.weixin.qq.com/s/CckHYrUpwCpft53-lZ4DwA)

[MoEUT: Mixture-of-Experts Universal Transformers](https://arxiv.org/pdf/2405.16039)

[https://github.com/robertcsordas/moeut](https://github.com/robertcsordas/moeut)

### MoA

[无问芯穹提出混合稀疏注意力方案MoA，加速长文本生成，实现最高8倍吞吐率提升](https://mp.weixin.qq.com/s/rjGAJfusY_CHSx3Q0SHVmg)

[https://github.com/thu-nics/MoA](https://github.com/thu-nics/MoA)

[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](https://arxiv.org/abs/2406.14909)

### GRIN

[专家模型不要专家并行！微软开源MoE新路径](https://mp.weixin.qq.com/s/pt-AlH_z4e3PNiKC9Iyz7A)

[GRIN: GRadient-INformed MoE](https://arxiv.org/abs/2409.12136)


## 主流框架

+ **编码器-解码器架构(encoder-decoder)**：标准Transformer，如T5、BART，**只有少数LLLM还用这种结构**，如Flan-T5
+ **因果解码器架构(causual decoder)**：也叫**decoder-only**，**单向注意力掩码**，输入和输出token通过解码器以相同方式进行处理，以GPT系列为代表，现有大部分LLM都是这种架构，如OPT、BLOOM、Gopher等。
+ **前缀解码器架构(prefix decoder)**：修正因果解码器的掩码机制，使其能**对前缀token执行双向注意力**，并且**仅对生成的token执行单向注意力**（和encoder-decoder类似），即[Unified language model pre-training for natural language understanding and generation](https://arxiv.org/pdf/1905.03197.pdf)提出的uni-lm。[What language model architecture and pretraining objective works best for zero-shot generalization?](https://arxiv.org/pdf/2204.05832.pdf)建议不从头开始预训练，而是**继续训练因果编码器，然后将其转换成前缀编码器以加速收敛**。例如U-PaLM从PaLM演化而来，还有GLM-130B也是这种架构。

![uni-lm](../assets/uni-lm.png)

[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)

对于这3种架构，都可以用**MoE**进行扩展，每个输入的**一小部分神经网络权重**被**稀疏激活**，如[Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)和GLaM。[Unified scaling laws for routed language models](https://arxiv.org/pdf/2202.01169.pdf)发现，通过**增加专家数量或总参数大小**，性能会有显著改进。

### 讨论：为什么现在的LLM都是Decoder only的架构？

&nbsp;

[https://www.zhihu.com/question/588325646/answer/2940298964](https://www.zhihu.com/question/588325646/answer/2940298964)

+ **泛化性能强**：ICML 22的[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf).在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在**各种下游任务上zero-shot泛化性能最好**；另外，ACL23的[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/pdf/2212.10559.pdf)等工作表明，decoder-only模型相当于基于给出的几个示例**隐式地进行梯度下降**，对应的in-context learning泛化能力更强，
+ **秩**的讨论：[Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)的讨论，$$n\times d$$和$$d\times n$$相乘后（$$n\gg d$$）再加上softmax后，秩不超过$$d$$，而decoder-only中有一个下三角矩阵的mask，所以输入的是一个下三角矩阵，而下三角矩阵的行列式是对角线之积，且有softmax，对角线肯定大于0，所以是满秩的(行列式不为0-->矩阵经过变换后不会有一行或者一列全为0-->当前矩阵满秩)
+ 预训练**任务难度**更大：相比encoder-decoder，decoder-only架构里**每个位置能接触到的信息更少**，故难度更高，当模型大小和数据量够的时候，上限更高
+ 隐式学习了**位置信息**：[Transformer Language Models without Positional Encodings Still Learn Positional Information](https://aclanthology.org/2022.findings-emnlp.99.pdf)，encoder里对语序的区分能力较弱，需要结合position encoding，而causual attention隐式地具备了这种建模位置的能力。
+ **工程效率**：支持**复用kv-cache**，对多轮对话更友好，**『DIN的FLOPS』**一节里有讲

[盛名一时的BERT哪去了？这个问题的答案昭示了LLM范式的转变](https://mp.weixin.qq.com/s/fKeorQYwRlmmepuG1_aJlQ)

[What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising)

#### 去噪目标

&nbsp;

**去噪目标**指的是span corruption任务的任意变体，即填充(infilling)或填空(fill in the blank)。表达方式有很多，比如span长度、随机性、sentinel token等。

+ BERT类的模型中，大部分是**in-place**的去噪目标，例如对mask tokens的分类head，
+ T5的做法则是通过encoder-decoder或decoder-only模型来处理数据变换，即把masked token move to the back给模型预测。

去噪目标的效果很好，可以作为常规语言建模的**补充目标**，但不足以单独作为目标，因为去噪有两个缺点：

+ **更少的loss exposure**：在去噪目标中，只有**少量token会被mask和学习**，而常规语言建模则接近100%，使得**每个FLOP的样本效率非常低**
+ 比常规语言建模更不自然：以一种奇怪的方式重新设定输入输出格式，不太适合少样本学习

#### PrefixLM vs decoder-only

&nbsp;

注：归纳偏置(inductive bias)指模型在预测未遇到的输入时，做的一些假设的集合，例如最小描述长度（奥卡姆剃刀）指的就是当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。

对语言模型来说，双向注意力是一种有趣的**归纳偏置**，相比于较小规模的场景，双向注意力**在规模较大时可能就没那么重要了**，或者可能对不同的任务或模态有不同的影响，例如PaliGemma就用的prefixLM（[PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/pdf/2407.07726)）

![paligemma](../assets/paligemma.png)

PrefixLM也存在缓存问题，是这类架构的一个固有缺陷

#### encoder-decoder的优缺点

&nbsp;

+ 相比decoder-only的优势：endocer不受causal的限制，可以激进地试各种pooling/linear attention，因此可以offload一些**不那么重要的context到encoder中**，也可以**让encoder更小**，例如[Charformer](https://arxiv.org/pdf/2106.12672)
+ 相比prefixLM的缺点：输入和目标必须分配固定的预算，例如输入预算是1024token，那么encoder就**必须pad**到1024，而这可能会浪费大量计算。相反，在 PrefixLM 中，输入和目标可以直接concat起来，从而可以缓解这个问题


## 组件配置

### 标准化（norm）

&nbsp;

LN(layer norm)能缓解LLM训练不稳定的问题，其位置很重要。

![pre-ln](../assets/pre-ln.jpeg)

+ 前置LN：最初Transformer使用后置LN，但大多数LLM采用前置LN以实现更稳定的训练，尽管会有一些性能损失([On layer normalization in the transformer architecture](https://arxiv.org/pdf/2002.04745.pdf))。[Sandwich-LN](https://arxiv.org/pdf/2105.13290.pdf)在残差连接前添加额外的LN，虽然能避免数值爆炸，但有时会无法稳定LLM的训练，可能导致训练崩溃（[GLM-130B: an open bilingual pre-trained model](https://arxiv.org/pdf/2210.02414.pdf)）
+ [RMS Norm](https://arxiv.org/pdf/1910.07467.pdf)：训练和性能都不错，在Gopher和Chinchilla里使用
+ [Deep Norm](https://arxiv.org/pdf/2203.00555.pdf)：比LN有更好的训练稳定性，和后标准化一起用在GLM-130B里

![deep-norm](../assets/deep%20norm.png)

此外，**在emb后直接加额外的LN**能提升训练稳定性，但会导致**显著的性能下降**([What language model to train if you have one million GPU hours?](https://arxiv.org/pdf/2210.15424.pdf))，在后来的LLM中**被移除**（[BLOOM: A 176b-parameter open-access multilingual language model](https://arxiv.org/pdf/2211.05100.pdf)）。

[神经网络可能不再需要激活函数？Layer Normalization也具有非线性表达！](https://mp.weixin.qq.com/s/YRAcAKouScQGt3lOxe8fBQ)

[On the Nonlinearity of Layer Normalization](https://arxiv.org/pdf/2406.01255)

### 激活函数

&nbsp;

FFN中的激活函数：

+ [GeLU](https://arxiv.org/pdf/1606.08415.pdf)：大部分都是这个
+ [GLU(gated linear units)的变体](https://arxiv.org/pdf/2002.05202.pdf)：应用在PaLM和LaMDA等模型中，如SwiGLU和GeGLU有更好的效果，但在FFN中的参数量比GeLU要大50%


原始Transformer中

XXX\operatorname{FFN}\left(x, W_1, W_2, b_1, b_2\right)=\max \left(0, x W_1+b_1\right) W_2+b_2XXX

T5中把bias干掉了

XXX\operatorname{FFN}_{\operatorname{ReLU}}\left(x, W_1, W_2\right)=\max \left(x W_1, 0\right) W_2XXX

然后，$$\operatorname{GELU}(x)=x \Phi(x)$$，同时$$\operatorname{Swish}_\beta(x)=x \sigma(\beta x)$$，接下来

XXX\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes(x V+c)XXX

XXX\operatorname{Bilinear}(x, W, V, b, c)=(x W+b) \otimes(x V+c)XXX

XXX\operatorname{ReGLU}(x, W, V, b, c)=\max (0, x W+b) \otimes(x V+c)XXX

XXX\operatorname{GEGLU}(x, W, V, b, c)=\operatorname{GELU}(x W+b) \otimes(x V+c)XXX

XXX\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)XXX

对应起来就是

XXX\operatorname{FFN}_{\mathrm{GLU}}\left(x, W, V, W_2\right)=(\sigma(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {Bilinear }}\left(x, W, V, W_2\right)=(x W \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\operatorname{ReGLU}}\left(x, W, V, W_2\right)=(\max (0, x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {GEGLU }}\left(x, W, V, W_2\right)=(\operatorname{GELU}(x W) \otimes x V) W_2XXX

XXX\operatorname{FFN}_{\text {SwiGLU }}\left(x, W, V, W_2\right)=\left(\operatorname{Swish}_1(x W) \otimes x V\right) W_2XXX


### 位置编码

&nbsp;

Transformer的self-attention有转换不变性，故要位置编码以引入绝对或相对位置信息来建模序列。

+ 绝对位置编码：
    + 正弦函数：原始Transformer中使用
    + 可学习的位置编码：LLM中常用
+ 相对位置编码：[Exploring the limits of transfer learning with a unified text-to-text transformer](https://arxiv.org/pdf/1910.10683.pdf)提出，其实是在[Self-attention with relative position representations](https://arxiv.org/pdf/1803.02155.pdf)一文提出的，根据**k和q之间的偏移量**生成emb
+ Alibi：[Train short, test long: Attention with linear biases enables input length extrapolation](https://arxiv.org/pdf/2108.12409.pdf)提出，使用**k和q之间距离的惩罚**来给注意力分数加bias，[What language model architecture and pretraining objective works best for zero-shot generalization](https://arxiv.org/pdf/2204.05832.pdf)发现其有更好的**零样本泛化能力**和更强的**外推能力**，能够在**比训练序列更长的序列**上表现良好。
+ RoPE：[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)提出，**k和q之间的分数用相对位置信息计算**，利于建模长序列，在PaLM、LLaMA、GLM-130B中都有应用。
  

[Transformer升级之路：RoPE的底数设计原则](https://mp.weixin.qq.com/s/YhpfIz0Pi1OMLwN3V3J1mQ)

[Base of RoPE Bounds Context Length](https://arxiv.org/pdf/2405.14591)

[Decoder-only的LLM为什么需要位置编码？](https://mp.weixin.qq.com/s/3sBYrKyEPP93nwigaOAAAA)

[HuggingFace工程师亲授：如何在Transformer中实现最好的位置编码](https://mp.weixin.qq.com/s/_vhyBrTM041FfHEohwLqMA)

(toread)

### 注意力机制和Bias

+ 稀疏注意力：[Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf))，**计算复杂度更低**，GPT-3用了
+ FlashAttention：[Flashattention: Fast and memory-efficient exact attention with IO-awareness](https://arxiv.org/pdf/2205.14135.pdf)，考虑显存访问
+ 其他attention：如[Random feature attention](https://arxiv.org/pdf/2103.02143.pdf)、[Big bird: Transformers for longer sequences](https://arxiv.org/pdf/2007.14062.pdf)
+ 移除bias：PaLM和Galactica中将bias删了，能够增加训练稳定性。


### 小结

#### 归一化位置

&nbsp;

sublayer表示FFN或self-attention模块

| 方法 | 公式 | 
|------|---------------|
| post Norm | $$\operatorname{Norm}(\mathbf{x}+\operatorname{Sulayerb}(\mathbf{x}))$$ |
| pre Norm | $$\mathbf{x}+\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x}))$$ |
| Sandwich Norm | $$\mathbf{x}+\operatorname{Norm}(\operatorname{Sublayer}(\operatorname{Norm}(\mathbf{x})))$$ |

#### 归一化方法

| 方法 | 公式 | 
|------|---------------|
|Layer Norm| $$\frac{\mathrm{x}-\mu}{\sqrt{\sigma}} \cdot \gamma+\beta, \quad \mu=\frac{1}{d} \sum_{i=1}^d x_i, \quad \sigma=\sqrt{\frac{1}{d} \sum_{i=1}^d(x_i-\mu)^2}$$ |
|RMSNorm| $$\frac{\mathrm{x}}{\operatorname{RMS}(\mathrm{x})} \cdot \gamma, \quad \operatorname{RMS}(\mathbf{x})=\sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}$$ |
|Deep Norm| $$LayerNorm (\alpha \cdot \mathbf{x}+\operatorname{Sublayer}(\mathbf{x}))$$ |

#### 激活函数

| 方法 | 公式 | 
|------|-----------------------|
|ReLU| $$\operatorname{ReLU}(\mathbf{x})=\max (\mathbf{x}, \mathbf{0})$$ |
| GeLU | $$\operatorname{GeLU}(\mathbf{x})=0.5 \mathrm{x} \otimes[1+\operatorname{erf}(\mathbf{x} / \sqrt{2})], \quad \operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$$ |
|Swish | $$\operatorname{Swish}(\mathbf{x})=\mathbf{x} \otimes \operatorname{sigmoid}(\mathbf{x})$$ |
|SwiGLU|$$\operatorname{SwiGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{Swish}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$ |
|GeGLU|$$\operatorname{GeGLU}\left(\mathbf{x}_1, \mathbf{x}_2\right)=\operatorname{GeLU}\left(\mathbf{x}_1\right) \otimes \mathbf{x}_2$$|

#### 位置嵌入

+ $$A_{ij}$$：**q和k之间**的**注意力分数**
+ $$r_{i-j}$$：基于**q和k之间偏移**的可学习标量
+ $$\mathbf{R}_{\theta, i-j}$$：旋转角度为$$t\cdot \theta$$的旋转矩阵

| 方法 | 公式 | 
|------|--------------|
|绝对位置编码| $$\mathbf{x}_i=\mathbf{x}_i+\mathbf{p}_i$$ |
|相对位置编码|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T+r_{i-j}$$|
|RoPE|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T$$|
|Alibi|$$A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{R}_{\theta, i-j} \mathbf{x}_j^T \mathbf{W}_k^T A_{i j}=\mathbf{W}_q \mathbf{x}_i \mathbf{x}_j^T \mathbf{W}_k^T-m(i-j)$$|


## 预训练任务

### 语言建模

&nbsp;

语言建模是**仅解码器LLM**的常见目标，给定token序列$$\mathbf{x}=\left\{x_1, \ldots, x_n\right\}$$，旨在基于序列中前面的token，自回归地预估目标token：

XXX
\mathcal{L}_{L M}(\mathbf{x})=\sum_{i=1}^n \log P\left(x_i \mid x_{<i}\right)
XXX


对应到代码里：

```python
hidden_states = outputs[0]
logits = self.lm_head(hidden_states)

# gemma系列模型会做soft-cap
cap = self.config.logits_soft_cap
logits = nn.functional.tanh(logits / cap) * cap

logits = logits.float()
loss = None
if labels is not None:
    # Shift so that tokens < n predict n
    ## 假设句子长度为4，词表有3个词，
    ## logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3], [0.6, 0.1, 0.3] ]
    ## labels: [0, 2, 1, 0]
    ## shift_logits: [ [0.1, 0.2, 0.7], [0.3, 0.4, 0.3], [0.5, 0.2, 0.3]
    shift_logits = logits[..., :-1, :].contiguous()
    ## shift_labels: [2, 1, 0]
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    # [bs, seq_len - 1, vocab_size] 变为 [(bs * (seq_len - 1)), vocab_size]
    shift_logits = shift_logits.view(-1, self.config.vocab_size)
    # [bs, seq_len - 1] 变为 [(bs * (seq_len - 1))]
    shift_labels = shift_labels.view(-1)
    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels)
```

**前缀解码器**架构使用的是前缀语言建模任务，其loss**不涉及对前缀内token的预测**，故预训练时**涉及的序列中token较少**，故当预训练token数相同时，前缀语言模型的**性能往往略低**于传统语言模型任务。

另外，自回归的loss：

+ 训练时：是可以并行的，因为每个位置的label是已知的，可以并行算，
+ 预测时：是串行的，因为得预测完了第t个词，才能去预测第t+1个词。

### 去噪自编码

&nbsp;

DAE是BERT待模型的常见任务，即MLM（masked language model），输入$$\mathbf{x}_{\backslash \tilde{\mathbf{x}}}$$是一些**有随机替换区间的损坏文本**，目标是恢复被替换的token $$\tilde{\mathbf{x}}$$：

XXX
\mathcal{L}_{D A E}(\mathbf{x})=\log P\left(\tilde{\mathbf{x}} \mid \mathbf{x}_{\backslash \tilde{\mathbf{x}}}\right)
XXX

在T5和GLM-130B中使用，**自回归地恢复替换区间**。

### 其他任务

multi-token prediction，一次预估未来的k个词

[Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/pdf/2404.19737)

![multi-token-prediction](../assets/multi-token-prediction.png)


# 新的模型结构

## 长上下文的问题

[长序列（Long Context）大模型笔记](https://mp.weixin.qq.com/s/nUX3MbKbyxw6b6mhgoL-Mw?poc_token=HK37E2ejmhfX-g6qnVG3pIxFJXZEVk-vPpwriYAj)

[LLM长上下文的问题](https://mp.weixin.qq.com/s/5e5HtxJrxNuhsVxrKsL2gA)

以中文为例，大部分模型每个token对应的中文字数都>1.5个字，所以200k的token就对应30w字的上下文

对长文本的几个要求：

+ 在文本比较长的时候，还能保证通顺，**ppl要足够低**
+ 能attention到前面提过的细节，**不能自我矛盾**

注意：如果训练时是2k长度的语料，而推理设定8k窗口，那么PPL会急剧上升，因为

+ RoPE不能很好地处理没有训练过的**位置编码**
+ 推理时**注意力机制**所处理的token数量远超训练时的数量，导致注意力机制的崩坏

### 两阶段训练方式

+ 直接输入连续长文本（如书籍）
+ 多个中等文本拼接，再通过attention mask来限制各段文本之间注意力，让它们可以在各自的位置上各训各的，互不干扰。甚至实际上即使不做attention mask，效果也挺好。

如果简单地增加长度，例如从4k变到32k，长度增加8倍，为了加速计算需要缓存中间结果（如QK的结果是$$s^2$$的空间复杂度），所以显存会扩大$$8^2=64$$倍。一般的做法是2阶段：

+ 第一阶段：用2k或者4k训练一个基础模型，让模型学好**文本内容**和**短位置关系**
+ 第二阶段：用**比第一阶段小的数据量优化**模型在**长上下文的效果**，具体做法见下节

### 针对位置编码的插值类方法

#### 线性插值

&nbsp;

[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf)

#### NTK-Aware Interpolation

&nbsp;

#### NTK-by-parts

&nbsp;

#### Dynamically NTK Scaled RoPE

&nbsp;

### 针对attention score的缩放方法

#### YaRN

&nbsp;

#### logn

&nbsp;

### lossless long context

[专访月之暗面杨植麟：lossless long context is everything](https://mp.weixin.qq.com/s/UMY0qZsCGh87KnW4wjfvoA)

## 外推问题

[语言模型窗口外推技术综述](https://mp.weixin.qq.com/s/5CqrlUZHrciMAFmeV2oYdw)

### longRoPE

[LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens](https://mp.weixin.qq.com/s/4ryyv59ofNOD--RCSdqktQ)

### CoPE

[解决Transformer根本缺陷，CoPE论文爆火：所有大模型都能获得巨大改进](https://mp.weixin.qq.com/s/JxB6JU6MxO3709mkg7penw)

[Contextual Position Encoding: Learning to Count What’s Important](https://arxiv.org/pdf/2405.18719)

### DAPE

[NeurIPS 2024 | Transformer长度外推，全新位置编码DAPE大幅提升模型性能](https://mp.weixin.qq.com/s/-7YsAMYYO92nItRJbqSrpw)

[DAPE: Data-Adaptive Positional Encoding for Length Extrapolation](https://arxiv.org/abs/2405.14722)

[https://github.com/chuanyang-Zheng/DAPE](https://github.com/chuanyang-Zheng/DAPE)


## retrieval head

[Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/pdf/2404.15574)


## SSM

[挑战Transformer的Mamba是什么来头？作者博士论文理清SSM进化路径](https://mp.weixin.qq.com/s/oXSwnL0sD96nnnqJyko7UA)

[Pretraining Without Attention](https://arxiv.org/pdf/2212.10544.pdf) 

[预训练无需注意力，扩展到4096个token不成问题，与BERT相当](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650864837&idx=2&sn=c57f0b8a7daf7d45093448c8ff5df3fc&chksm=84e538bbb392b1ad857f3ad2c2d6d6dbc9562ff492a7cda336dc07abe4d3912c19fa4c30d365&scene=21#wechat_redirect)

[Diffusion Models Without Attention](https://arxiv.org/pdf/2311.18257.pdf)

[丢掉注意力的扩散模型：Mamba带火的SSM被苹果、康奈尔盯上了](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650900014&idx=4&sn=315ca75ba700a93598b5c581ad2fb735&chksm=84e44250b393cb4619e7c24b9f6affe88e4c71ebb97d0c43b5301edd0a11154af6322f68cfd4&scene=21#wechat_redirect)

[一文看懂Mamba，Transformer最强竞争者](https://mp.weixin.qq.com/s/gwE_OVWigV71Qiup7F_9Cg)

[Long Range Language Modeling via Gated State Spaces](https://arxiv.org/abs/2206.13947) 认为 Transformer 和 SSM 完全可以互补。

选择性状态空间模型(selective state space model)是Mamba论文作者Albert Gu此前主导研发的S4架构(Structured State Spaces for Sequence Modeling)的一个简单泛化。

[https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf](https://stacks.stanford.edu/file/druid:mb976vf9362/gu_dissertation-augmented.pdf)

序列模型在训练和推理时的侧重点：

+ 训练时：在整个list上计算loss，需要优化forward的耗时
+ 推理时：一次输入一个时间步，需要高效地顺序处理

### SSM原理

输入$$u(t) \in \mathbb{R}$$，输出是$$y(t) \in \mathbb{R}$$，引入**状态**$$x(t) \in \mathbb{R}^N$$，目标是学习映射$$u(t) \mapsto y(t)$$，

XXX
\begin{aligned}
x^{\prime}(t) & =\boldsymbol{A} x(t)+\boldsymbol{B} u(t) \\
y(t) & =\boldsymbol{C} x(t)+\boldsymbol{D} u(t)
\end{aligned}
XXX

结合[维基百科](https://en.wikipedia.org/wiki/State-space_representation)的图，方便理解

![ssm](../assets/ssm.png)

各变量含义如下（看起来和RNN很像）：

+ $$\boldsymbol{A}\in \mathbb{R}^{N\times N}$$：状态矩阵
+ $$\boldsymbol{B}\in \mathbb{R}^{N\times N}$$：输入矩阵
+ $$\boldsymbol{C}\in \mathbb{R}^{N\times N}$$：输出矩阵
+ $$\boldsymbol{D}\in \mathbb{R}^{N\times N}$$：feedforward矩阵或者feedthrough矩阵
+ $$x^{\prime}(t):=\frac{d}{d t} \mathbf{x}(t)$$是$$x(t)$$关于$$t$$的导数，所以图里有一个**积分**$$\int$$操作把它变回$$x(t)$$

假设序列长度为$$L$$，那么

+ 时间复杂度：
    + SSM：$$O(N^2L)$$，因为$$\boldsymbol{A} x(t)$$是$$N\times N$$和$$N\times 1$$的矩阵乘法，复杂度是$$N^2$$
    + RNN：$$O(N^2L)$$，和SSM类似
    + CNN：$$O(kLM^2)$$，假设$$k$$个卷积核，每个卷积核$$M\times M$$，一般$$M \ll N$$
+ 空间复杂度：
    + SSM：$$O(N^2+NL)$$，因为中间状态$$x(t) \in \mathbb{R}^N$$是需要**额外存储**的
    + RNN：$$O(N^2)$$，所有时间步共享权重，不需要像SSM专门为每个时间步存一个状态$$x(t)$$
    + CNN：$$kM^2$$

所以传统SSM的时空复杂度都很高，作者提出了S4(structured state space model)

====>hippo的假设会让模型更关注宽波，对于很窄的毛刺信号效果不好，所以对大海捞针任务效果并不好

## Mamba

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)

[https://huggingface.co/state-spaces](https://huggingface.co/state-spaces)

[https://github.com/havenhq/mamba-chat](https://github.com/havenhq/mamba-chat)

[MODELING SEQUENCES WITH STRUCTURED STATE SPACES](https://stacks.stanford.edu/file/druid:mb976vf9362)

## Mamba2

[再战Transformer！原作者带队的Mamba 2来了，新架构训练效率大幅提升](https://mp.weixin.qq.com/s/31t6pJqcXrZDjT6XiJZC_g)

[Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/pdf/2405.21060)

[https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)

## Mamba+Transformer

[Mamba真比Transformer更优吗？Mamba原作者：两个都要！混合架构才是最优解](https://mp.weixin.qq.com/s/omImpaiddmSJ968bCZ8qmw)

[An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887)

## Block-State Transformer

[Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)

## Jamba

[Mamba做大做强！混合Transformer，打败Transformer](https://mp.weixin.qq.com/s/Lde0G_zysfrGcRUX5nOf8g)

[https://huggingface.co/ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1)

让mamba做短程头，让transformer做长程头

## Hawk & Griffin

[RNN效率媲美Transformer，谷歌新架构两连发：同等规模强于Mamba](https://mp.weixin.qq.com/s/RtAZiEzjRWgqQw3yu3lvcg)

[Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/pdf/2402.19427)

### 简介

global attention在infer阶段的效率和序列长度成二次关系，而且序列长度与KV cache呈线性增长关系。[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)提出的multi-query attention(MQA)可以通过一个**constant factor**来**减小cache size**，部分缓解这个问题，但cache还是与序列长度线性相关。

循环模型能够将整个序列压缩到一个**fixed-size的hidden state**，并通过迭代进行更新。但要想取代transformer，rnn不仅需要在效果上可比，还要相似的硬件效率，相关工作如下：

+ [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396)
+ [Long range language modeling via gated state spaces](https://arxiv.org/pdf/2206.13947)提出了GSS block
+ [Simplified state space layers for sequence modeling](https://arxiv.org/pdf/2208.04933)
+ [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ [Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)
+ [Hyena hierarchy: Towards larger convolutional language models](https://arxiv.org/pdf/2302.10866)
+ [Mamba: Linear-time sequence modeling with selective state spaces](https://arxiv.org/pdf/2312.00752)

本文提出了新的**RG-LRU层**，一种新的gated linear recurrent layer，并基于此提出了将MLP和RG-LRU结合的Hawk，还有将MLP和RG-LRU与local attention混合的Griffin。

![griffin-hawk](../assets/griffin-hawk.png)

+ 对于最多超过7B的模型，Hawk和Griffin发现了**held-out loss**和**训练FLOPS**间的**power law scaling**（上图左边）
+ Hawk-3B比Mamba-3B在下游任务上要好，而训练的token数只有mamba的一半；Griffin-7B和14B与Llama-2效果相当，而训练数据只有其1/7
+ 训练在TPU-v3上完成，用JAX中的Pallas实现了RG-LRU(Real-Gated Linear Recurrent Unit)层的内核([https://jax.readthedocs.io/en/latest/pallas/index.html](https://jax.readthedocs.io/en/latest/pallas/index.html))，减小内存transfer
+ infer阶段比MQA的**吞吐高很多**（上图右边），而且对长序列有**更低的latency**
+ 训练时在长序列上比transformer更好，而且能够高效地学习**复制和检索**的任务。但如果没有finetune，直接对比pretrain的效果，transformer会更好

### 网络结构

包括三大类组件：

+ residual block: 
+ MLP block：
+ temporal-mixing block


![griffin-arch](../assets/griffin-arch.png)

#### Residual Block

&nbsp;

受pre-norm transformer的启发，用的RMSNorm

#### MLP block

&nbsp;

参考[Language modeling with gated convolutional networks](https://arxiv.org/pdf/1612.08083)（类似star两个W直接element-wise product），采用了gated MLP，即如下两个linear的结果（输入维度$$D$$，输出维度都是$$MD,M=3$$）进行element-wise product(类似GeGLU（[Glu variants improve transformer]((https://arxiv.org/pdf/2002.05202.pdf))）)，再过一个linear

+ 直接过一个linear，但没有激活
+ linear后加一个GeLU激活（实际用的```nn.functional.gelu(input, approximate="tanh")```）

看recurrentGemma-2b的[config](https://huggingface.co/google/recurrentgemma-2b-it/blob/main/config.json)，发现总共有26个block，一个block依次包括：

+ rg-lru或者MQA，每个block里的选择方式是(rg,rg,att,rg,rg,att,rg,rg,att,...)
+ 再过一个gated MLP

```python
  "_block_types": [
    "recurrent",
    "recurrent",
    "attention"
  ],
  "num_hidden_layers": 26,
  "attention_window_size": 2048,

```

#### Temporal-mixing block

&nbsp;

##### global Multi-Query Attention(MQA)

&nbsp;

[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)中为了加速推理，采用了MQA的方法，本文固定了head的维度为$$D_{head}=128$$，head的个数$$H$$也固定，且$$HD_{head}=D$$，所以model的维度$$D$$需要是128的倍数。没用绝对位置编码，用了RoPE。

##### local(sliding-window) MQA

&nbsp;

[Longformer: The long-document transformer](https://arxiv.org/pdf/2004.05150)提出了可以用local attention，即滑动窗口attention。让每个位置只和前面的固定个tokens去算attentioin，可以

+ 降低计算的FLOPS
+ 让KV cache的size的上界变成了**window的size**，从而不是序列长度的二次关系

##### RG-LRU

&nbsp;

类似GSS block，也类似Mamba，输入$$D$$，分别过一个linear得到两个分支，均是$$D_{RNN}$$：

+ 分支1：过GeLU激活，同上，实际用的```nn.functional.gelu(input, approximate="tanh")```
+ 分支2：
  + 参考[Hungry hungry hippos: Towards language modeling with state space models](https://arxiv.org/pdf/2212.14052)的H3模型里的Shift-SSM，过一个Conv1D，其temporal filter dim是4
  + Conv1D的参数只有$$4D_{RNN}$$，因此再过一个RG—LRU模块

两个分支的输出element-wise product一下，再过一个linear得到$$d$$。

conv1d参考torch官方[doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)

+ 输入：$$\left(N, C_{i n}, L_{i n}\right)$$
+ 输出：$$\left(N, C_{\text {out }}, L_{\text {out }}\right)$$

其中，

XXX
L_{\text {out }}=\left\lfloor\frac{L_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel\_size }-1)-1}{\text { stride }}+1\right\rfloor
XXX

当```groups==in_channels and out_channels=K*in_channels```，也叫depthwise convolution，K是depthwise乘子。即**每个输入通道都有自己的卷积核，并且只作用于该通道**。

```python
        ## conv1d_width = 4, 
        ## 对图像卷积来讲，padding就是让输出的shape不小于输入shape
        ## 输入是hidden_size=2560，而lru_width也是2560
        self.conv_1d = nn.Conv1d(
                    config.lru_width, # in_channels
                    config.lru_width, # out_channels
                    kernel_size=config.conv1d_width,
                    groups=config.lru_width,
                    padding=config.conv1d_width - 1,
                )
        
        ## 输入[bs, seq_len, hidden_size]
        x_branch = self.linear_x(input_states)
        ## 变成[bs, hidden_size, seq_len]
        x_branch = x_branch.transpose(1, 2)
        if use_cache:
            if cache_position.shape[0] != 1:  # prefill
                self.conv1d_state = nn.functional.pad(x_branch, 
                    (self.conv1d_width - x_branch.shape[-1] - 1, 0))
                x_branch = self.conv_1d(x_branch)[..., :seq_len]
            else:  # decoding
                conv_state = torch.cat((self.conv1d_state, x_branch), -1)
                x_branch = torch.sum(conv_state * self.conv_1d.weight[:, 0, :], dim=-1) 
                    + self.conv_1d.bias
                x_branch = x_branch.unsqueeze(-1)
                self.conv1d_state = conv_state[:, :, 1:]
        else:
            # 前面维度不变，最后一维截断到seq_len
            x_branch = self.conv_1d(x_branch)[..., :seq_len]
```


参考[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)提出的LRU(Linear Recurrent Unit)，并参考传统LSTM和GRU引入了gate：

XXX
\begin{aligned}
r_t & =\sigma\left(W_a x_t+b_a\right), \quad \text { recurrence gate } \\
i_t & =\sigma\left(W_x x_t+b_x\right), \quad \text { input gate } \\
a_t & =a^{c r_t}, \\
h_t & =a_t \odot h_{t-1}+\sqrt{1-a_t^2} \odot\left(i_t \odot x_t\right) .
\end{aligned}
XXX

其中，recurrent weight $$a=\sigma(\Lambda)$$是一个对角矩阵，$$\Lambda$$是一个可学习的参数。$$c$$是一个常数8，为了**计算稳定**，在log-space计算$$a^{c r_t}$$，即先算出$$\log a_t$$，再取exp。

XXX
\log a_t=\log a^{c r_t}=\log \sigma(\Lambda)^{c r_t}=-\operatorname{csoftplus}(\Lambda) \odot r_t 
XXX

(这个公式可能有点问题，感觉应该是$$-\operatorname{csoftplus}(-\Lambda) \odot r_t$$，不过$$\Lambda$$是一个可学习的nn.Parameter，其实这个错误无所谓吧)

```python
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.block_width = config.lru_width // self.num_attention_heads

        self.recurrent_param = nn.Parameter(torch.empty([config.lru_width]))
        self.input_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.input_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))

        self.recurrent_gate_weight = nn.Parameter(
            torch.empty([self.num_attention_heads, self.block_width, self.block_width])
        )
        self.recurrent_gate_bias = nn.Parameter(torch.empty([self.num_attention_heads, self.block_width]))
        self.recurrent_states = None

    def forward(xxx):
        ## reshape成适合多头的情况
        reshape_act = activations.reshape(batch_size * seq_len, self.num_attention_heads, self.block_width)
        ## (num_attention_heads, batch_size * seq_len, block_width)
        reshape_act = reshape_act.permute(1, 0, 2)

        ## 批量矩阵乘法（baddbmm），在reshape_act和self.input_gate_weight之间进行，
        ## 并加上偏置self.input_gate_bias。这一步计算输入门的原始值。
        res = torch.baddbmm(self.input_gate_bias[:, None, :], reshape_act, self.input_gate_weight)
        input_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        ## 类似input_gate
        res = torch.baddbmm(self.recurrent_gate_bias[:, None, :], reshape_act, self.recurrent_gate_weight)
        recurrent_gate = torch.sigmoid(res.transpose(0, 1).reshape(batch_size, seq_len, lru_width))

        # Compute the parameter `A` of the recurrence.
        # 上面的公式
        log_recurrent_gate = -8.0 * recurrent_gate * nn.functional.softplus(self.recurrent_param)
        recurrent_gate = torch.exp(log_recurrent_gate)
```

还有如下几个特点：

+ $$W_a$$和$$W_x$$用LeCun init初始化
+ 初始化$$\Lambda$$，使得在训练开始时，$$a^c$$均匀分布在0.9和0.999之间，类似[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/pdf/2303.06349)
+ 不像大部分SSM（例如[Hippo: Recurrent memory with optimal polynomial projections](https://arxiv.org/pdf/2008.07669)）基于orthogonal polynomials（正交多项式）理论进行初始化。
+ 也不像[Efficiently modeling long sequences with structured state spaces](https://arxiv.org/pdf/2111.00396.pdf)在底层的连续系统上进行离散化定义
+ 不像原始的LRU用了复数，虽然[ On the universality of linear recurrences followed by nonlinear projections](https://arxiv.org/pdf/2307.11888v1)说复数的表达能力更强，但在实践中对语言模型并没有什么用（[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)）

##### gate特点

&nbsp;

先复习一下LSTM：

![lstm-peephole-lstm](../assets/lstm-peephole-lstm.png)

GRU：

+ 重置门（reset gate）：如果重置门关闭，会**忽略掉历史信息**，即历史不相干的信息不会影响未来的输出。
+ 更新门（update gate）：将LSTM的**输入门和遗忘门合并**，用于控制**历史信息对当前时刻隐层输出的影响**。如果更新门接近1，会把历史信息传递下去。

![gru](../assets/gru.png)

+ 两个gate只和$$x_t$$有关，**和$$h_{t-1}$$无关**。
+ input gate $$i_t$$和LSTM类似，直接对输入$$x_t$$进行filter或者scale down。
+ recurrent gate $$r_t$$和之前的gate机制不同：
    + mamba里的selection机制和GRU的**update gate**类似，在**之前的状态**和**当前输入$$x_t$$**之间进行插值（interpolate），功能类似LSTM的**forget gate**，能够**reset状态，并遗忘之前的信息**
    + 本文的recurrent gate则类似于在LRU的更新和之前的隐藏状态之间进行插值，能够**有效地丢弃输入**，并且**保持之前历史里的所有信息**。使得模型在处理不相关或重复输入（**uniformative inputs**）时，更能达到**超指数的记忆能力**，以更有效地保留有用信息(因为这个gate**和$$h_{t-1}$$无关**)。

## SAMBA

[长文本模型近期研究工作梳理](https://mp.weixin.qq.com/s/5u2w08twsJ6CgHZ2FpN8JA)

[SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://arxiv.org/pdf/2406.07522)

## Mamba in llama

(toread)

[Mamba作者新作：将Llama3蒸馏成混合线性 RNN](https://mp.weixin.qq.com/s/jZGcBxhVUfb-Qv1UkqYK4A)

[The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/pdf/2408.15237)

## feedback attention memory

[TransformerFAM: Feedback attention is working memory](https://arxiv.org/pdf/2404.09173.pdf)

## infini-attention

[Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143.pdf)

[【重磅】谷歌重塑Transformer：无限记忆力，无限长输入，LLM基础研究重大突破](https://mp.weixin.qq.com/s/bV2b9uJ4GFQPhhggHT3VIA)

将**压缩记忆**整合进标准的点积注意力机制，并在单个Transformer块内同时实现了**掩码局部注意力**和**长期线性注意力机制**

![infini-attention](../assets/infini-attention.png)

与transformer-xl对比：

![infini-attention-vs-transformer-xl](../assets/infini-attention-vs-transformer-xl.png)

[https://github.com/mustafaaljadery/gemma-2B-10M](https://github.com/mustafaaljadery/gemma-2B-10M)

[https://github.com/dingo-actual/infini-transformer](https://github.com/dingo-actual/infini-transformer)

## MEGA

[Mega: Moving average equipped gated attention](https://arxiv.org/pdf/2209.10655.pdf)

[https://github.com/facebookresearch/mega](https://github.com/facebookresearch/mega)

### 标准的self-attention公式

&nbsp;

对于序列长度=$$n$$的输入$$\boldsymbol{X}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\right\} \in \mathbb{R}^{n \times d}$$，输出$$\boldsymbol{Y}=\left\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_n\right\} \in \mathbb{R}^{n \times d}$$

XXX
\boldsymbol{Y}=\operatorname{Attn}(\boldsymbol{X})=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \boldsymbol{V}
XXX

XXX
\boldsymbol{Q}=\boldsymbol{X} W_q+b_q,\boldsymbol{K}=\boldsymbol{X} W_k+b_k,\boldsymbol{V}=\boldsymbol{X} W_v+b_v
XXX

其中，$$\text { Attn : } \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$$，$$W_q, W_k, W_v \in \mathbb{R}^{d \times d}$$，$$b_q, b_k, b_v \in \mathbb{R}^d$$，而且有两种定义方式：

+ $$f(\cdot)=f_{\text {softmax }}(\cdot)$$，同时$$\tau(\boldsymbol{X})=\sqrt{d}$$，这是经典操作[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)
+ $$f(\cdot)=f_{\mathrm{relu}^2}(\cdot)$$即$$relu(x)^2$$，同时$$\tau(\boldsymbol{X})=n$$，这是[Primer: Searching for efficient transformers for language modeling](https://arxiv.org/pdf/2109.08668.pdf)和[Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf)提出的

![gau](../assets/gau.png)

对于$$h$$个attention heads，计算$$\boldsymbol{A}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}\right) \in \mathbb{R}^{n \times n}$$需要的时间和空间复杂度都是$$O(hn^2)$$

### EMA

&nbsp;

对于输出的序列$$\boldsymbol{Y}$$采用Exponential Moving Average(指数滑动平均)如下

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha}) \odot \mathbf{y}_{t-1}
XXX

其中，$$\boldsymbol{\alpha} \in(0,1)^d$$表示权重衰减，$$\odot$$是element-wise product

$$\boldsymbol{\alpha}$$越大，$$1-\boldsymbol{\alpha}$$越小，对历史的衰减也越快：

![ema](../assets/ema.png)

EMA的计算看成$$n$$个**独立的卷积**，可以通过**FFT(快速傅立叶变换)**来加速计算。具体。。。再看看

**Damped EMA**：由于输入的x是d维向量，可以引入一个因子$$\boldsymbol{\delta} \in(0,1)^d$$让EMA更鲁棒：

XXX
\mathbf{y}_t=\boldsymbol{\alpha} \odot \mathbf{x}_t+(1-\boldsymbol{\alpha} \odot \boldsymbol{\delta}) \odot \mathbf{y}_{t-1}
XXX

### MEGA

&nbsp;

EMA可以看成是一种**与位置相关的归纳偏置（inductive bias）**，即假设当前位置与之前位置满足滑动平均的关系，而attention矩阵的计算其实并没有考虑位置信息，所以可以把二者结合一下。

![mega](../assets/mega.png)

其中的multi-dimensional damped EMA大致流程如下：

+ **先变成h维**：先把$$\boldsymbol{X} \in \mathbb{R}^{n \times d}$$通过矩阵$$\beta$$映射成$$\boldsymbol{U}\in \mathbb{R}^{n \times h}$$
+ **计算EMA**：然后通过EMA得到$$\boldsymbol{h}\in \mathbb{R}^{n \times h}$$（具体$$\mathbf{h}_t^{(j)}=\boldsymbol{\alpha}_j \odot \mathbf{u}_t^{(j)}+\left(1-\boldsymbol{\alpha}_j \odot \boldsymbol{\delta}_j\right) \odot \mathbf{h}_{t-1}^{(j)}$$）
+ **再变回d维**：再通过一个矩阵$$\eta$$变回$$\boldsymbol{Y}\in \mathbb{R}^{n \times h}$$

然后看整个流程：

+ 先计算EMA

XXX
\begin{aligned}
\boldsymbol{X}^{\prime} & =\operatorname{EMA}(\boldsymbol{X}) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Z} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_z+b_z\right) & & \in \mathbb{R}^{n \times z}
\end{aligned}
XXX

+ 再基于EMA的结果计算QK，基于原始的X计算V：

XXX
\begin{array}{ll}
\boldsymbol{Q}=\boldsymbol{\kappa}_q \odot \boldsymbol{Z}+\boldsymbol{\mu}_q & \in \mathbb{R}^{n \times z} \\
\boldsymbol{K}=\boldsymbol{\kappa}_k \odot \boldsymbol{Z}+\boldsymbol{\mu}_k & \in \mathbb{R}^{n \times z} \\
\boldsymbol{V}=\phi_{\text {silu }}\left(\boldsymbol{X} W_v+b_v\right) & \in \mathbb{R}^{n \times v}
\end{array}
XXX

+ 计算带位置bias的attention：

XXX
\boldsymbol{O}=f\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\tau(\boldsymbol{X})}+\boldsymbol{b}_{\mathrm{rel}}\right) \boldsymbol{V} \quad \in \mathbb{R}^{n \times v}
XXX

+ 通过reset gate $$\boldsymbol{\gamma}$$和update gate $$\boldsymbol{\varphi}$$计算输出

XXX
\begin{aligned}
\boldsymbol{\gamma} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_\gamma+b_\gamma\right) & & \in \mathbb{R}^{n \times v} \\
\boldsymbol{\varphi} & =\phi_{\text {sigmoid }}\left(\boldsymbol{X}^{\prime} W_{\varphi}+b_{\varphi}\right) & & \in \mathbb{R}^{n \times d} \\
\hat{\boldsymbol{H}} & =\phi_{\text {silu }}\left(\boldsymbol{X}^{\prime} W_h+(\boldsymbol{\gamma} \boldsymbol{O}) U_h+b_h\right) & & \in \mathbb{R}^{n \times d} \\
\boldsymbol{Y}&=\boldsymbol{\varphi} \odot \hat{\boldsymbol{H}}+(1-\boldsymbol{\varphi}) \odot \boldsymbol{X} \quad & & \in \mathbb{R}^{n \times d}
\end{aligned}
XXX


这里把attention里的softmax改成了如下的laplace函数：

XXX
f_{\text {laplace }}(x ; \mu, \sigma)=0.5 \times\left[1+\operatorname{erf}\left(\frac{x-\mu}{\sigma \sqrt{2}}\right)\right]
XXX

其中，$$\operatorname{erf}(x)=\frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} \mathrm{~d} t=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \mathrm{~d} t$$是[误差函数](https://zh.wikipedia.org/wiki/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0)，

为了让laplace逼近$$f_{\text {relu }^2}$$，对于$$x=\sqrt{2}$$这个点，求解如下方程

XXX
\begin{aligned}
& f_{\text {relu2 }}(\sqrt{2})=f_{\text {laplace }}(\sqrt{2}) \\
& f_{\text {relu2 }}^{\prime}(\sqrt{2})=f_{\text {laplace }}^{\prime}(\sqrt{2})
\end{aligned}
XXX

可以得到$$\mu=\sqrt{1 / 2}$$，$$\sigma=\sqrt{1 / 4 \pi}$$，对应的曲线和准确率如下：

![laplace-vs-relu2](../assets/laplace-vs-relu2.png)

### mega-chunk

![mega-chunk](../assets/mega-chunk.png)

将序列切分成长度固定为$$c$$的$$k=n/c$$个chunk，对**每个chunk独立计算**上面的attention，这样复杂度就变成了$$O(kc^2)=O(nc)$$，由于有EMA，所以这样做还是能够保持一定程度的长距离依赖。

## MEGALODON

[Meta无限长文本大模型来了：参数仅7B，已开源](https://mp.weixin.qq.com/s/VML5hExo5iPsyEavxzIZSA)

[革命新架构掀翻Transformer！无限上下文处理，2万亿token碾压Llama 2](https://mp.weixin.qq.com/s/xgP9P51gjqJ93FYSWfPeaA)

[MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

[https://github.com/XuezheMax/megalodon](https://github.com/XuezheMax/megalodon)

[Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)

基于MEGA进行改进，能够同时实现

+ 高效训练（减少通信和计算量）
+ 高效推理（保持恒定的KV缓存）


## MOD

[Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/pdf/2404.02258.pdf)


## multi-head moe

[微软让MoE长出多个头，大幅提升专家激活率](https://mp.weixin.qq.com/s/ZCRyb63M2DL4hOQh7uxxaw)

[Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)

[https://github.com/yushuiwx/MH-MoE](https://github.com/yushuiwx/MH-MoE)

## Lory

[150B token从头训练，普林斯顿Meta发布完全可微MoE架构Lory](https://mp.weixin.qq.com/s/UKIXGJTFzSeSZvoTe_c9CQ)

[Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training](https://arxiv.org/pdf/2405.03133)

## Perciever

[Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf)

## Aaren

[Bengio等人新作：注意力可被视为RNN，新模型媲美Transformer，但超级省内存](https://mp.weixin.qq.com/s/mRt2A1n1CmO7uqzuLQHxkw)

[Attention as an RNN](https://arxiv.org/pdf/2405.13956)

### many-to-one RNN

对query向量$$q$$的attention可以看成是一个函数，对输入的$$N$$个token $$x_{1: N}$$通过他们的key和value $$\left\{\left(k_i, v_i\right)\right\}_{i=1}^N$$ 变换成输出 $$\text { Attention }\left(q, k_{1: N}, v_{1: N}\right)$$，假设$$\bar{s}_i=\operatorname{dot}\left(q, k_i\right)$$，那么输出就是

XXX
o_N=\sum_{i=1}^N \operatorname{softmax}(s)_i v_i=\frac{\sum_{i=1}^N \exp \left(s_i\right) v_i}{\sum_{i=1}^N \exp \left(s_i\right)}=\frac{\hat{a}_N}{\hat{c}_N}
XXX

可以发现，分子和分母其实都可以写成递推形式$$\hat{a}_k=\hat{a}_{k-1}+\exp \left(s_k\right) v_k$$和$$\hat{c}_k=\hat{c}_{k-1}+\exp \left(s_k\right)$$，由于直接这么做可能会产生很大或者很小的值（例如算exp），所以可以通过如下方式进行缓解：计录到第k步的最大值$$m_k=\max _{i \in\{1, \ldots, k\}} s_i$$，然后减掉它，即$$a_k=\sum_{i=1}^k \exp \left(s_i-m_k\right) v_i$$和$$c_k=\sum_{i=1}^k \exp \left(s_i-m_k\right)$$，这样就可以改写为如下形式：

XXX
\begin{aligned}
a_k & =a_{k-1} \exp \left(m_{k-1}-m_k\right)+v_k \exp \left(s_k-m_k\right) \\
c_k & =c_{k-1} \exp \left(m_{k-1}-m_k\right)+\exp \left(s_k-m_k\right) \\
m_k & =\max \left(m_{k-1}, s_k\right)
\end{aligned}
XXX

其中第一行其实就是$$s_i-m_k=s_i-m_k+m_{k-1}-m_k$$，然后exp一下，$$exp(s_i-m_k)=exp(s_i-m_k)exp(m_{k-1}-m_k)$$，总结成如下图：

![attention-rnn-cell](../assets/attention-rnn-cell.png)

所以Attention的RNN cell就是输入$$\left(a_{k-1}, c_{k-1}, m_{k-1}, q\right)$$，输出$$\left(a_k, c_k, m_k, q\right)$$，初始的状态是$$\left(a_0, c_0, m_0, q\right)=(0,0,0,q)$$

然后就可以将之前的attention看成如下几类many-to-one的RNN了：

+ 传统attention只计算最后的一个输出
+ self-attention使用**输入token**作为初始状态
+ Perceiver的cross-attention使用**依赖input的隐变量**作为初始状态

![many-to-one-rnn](../assets/many-to-one-rnn.png)

对于新来的token而言：

+ 传统的RNN一般是流式地输入数据，因此只需要O(1)的内存和计算就行了
+ Transformer需要把这个新token当成一个初始状态加进来，所以需要把之前时间步的再重新算一次，需要O(N)的计算
+ Perceiver中的隐变量是依赖输入的，而且这个新token会改变value，因此初始状态也会变，所以需要从头算一遍，需要O(NL)的计算，L是隐变量个数（可以参考代码[perceiver_torch](https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_pytorch.py)和如下gpt4o的回答）

![perceiver](../assets/perceiver.png)

### many-to-many RNN

要计算$$\left\{o_i=\operatorname{Attention}\left(q, x_{1: i}\right)\right\}_{i=1}^N$$，通过如下的并行前缀扫描算法，对于$$N$$个序列数据的$$N$$个前缀，通过关联运算符$$\oplus$$并行计算，能够高效地通过$$\left\{x_k\right\}_{k=1}^N$$计算$$\left\{\bigoplus_{i=1}^k x_i\right\}_{k=1}^N$$

![parallel-prefix-scan](../assets/parallel-prefix-scan.png)

由于$$\operatorname{Attention}\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)=o_k=\frac{a_k}{c_k}$$，为了计算$$\left\{\text { Attention }\left(\mathrm{q}, \mathrm{x}_{1: \mathrm{k}}\right)\right\}_{k=1}^N$$，只需要先按这个并行扫描算法计算$$\left\{a_k\right\}_{k=1}^N$$、$$\left\{c_k\right\}_{k=1}^N$$和$$\left\{m_k\right\}_{k=1}^N$$，再把$$a_k$$和$$c_k$$结合起来就行。

接来来定义三元组$$\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right)$$，其中

+ $$A$$：一些下标的集合
+ $$\mathrm{m}_A=\max _{i \in A} s_i$$
+ $$\mathrm{u}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right)$$
+ $$\mathrm{w}_A=\sum_{i \in A} \exp \left(s_i-\mathrm{m}_A\right) v_i$$

所以并行扫描算法的输入是$$\left\{\left(\mathrm{m}_{\{i\}}, \mathrm{u}_{\{i\}}, \mathrm{W}_{\{i\}}\right)\right\}_{i=1}^N=\left\{\left(s_i, 1, v_i\right)\right\}_{i=1}^N$$，再来定义操作$$\oplus$$：

XXX
\left(\mathrm{m}_A, \mathrm{u}_A, \mathrm{w}_A\right) \oplus\left(\mathrm{m}_B, \mathrm{u}_B, \mathrm{w}_B\right)=\left(\mathrm{m}_{A \cup B}, \mathrm{u}_{A \cup B}, \mathrm{w}_{A \cup B}\right)
XXX

其中，

+ $$\mathrm{m}_{A \cup B}=\max \left(\mathrm{m}_A, \mathrm{~m}_B\right)$$
+ $$\mathrm{u}_{A \cup B}=\mathrm{u}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{u}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$
+ $$\mathrm{w} _{A \cup B}=\mathrm{w}_A \exp \left(\mathrm{m}_A-\mathrm{m}_{A \cup B}\right)+\mathrm{w}_B \exp \left(\mathrm{m}_B-\mathrm{m}_{A \cup B}\right)$$

这个并行扫描算法最终输出下式，即$$\left\{\left(m_k, c_k, a_k\right)\right\}_{k=1}^N$$：

XXX
\left\{\left(\mathrm{m}_{\{1, \ldots, k\}}, \mathrm{u}_{\{1, \ldots, k\}}, \mathrm{w}_{\{1, \ldots, k\}}\right)\right\}_{k=1}^N=\left\{\left(m_k, \sum_{i=1}^k \exp \left(s_i-m_k\right), \sum_{i=1}^k \exp \left(s_i-m_k\right) v_i\right)\right\}_{k=1}^N
XXX


![many-to-many-rnn](../assets/many-to-many-rnn.png)

### Aaren

Aaren(attention as a recurrent newral network)的结构如下：

XXX
\begin{aligned}
h_1^{(0)}, \ldots, h_N^{(0)} & \leftarrow x_1, \ldots, x_N \\
{\left[h_1^{(j+1)}, \ldots, h_N^{(j+1)}\right] } & \leftarrow \operatorname{Aaren}\left(q^{(j)},\left[h_1^{(j)}, \ldots, h_N^{(j)}\right]\right)
\end{aligned}
XXX

transformer的query是输入的token，而Aaren的query token $$q$$是在训练的过程中通过bp学习的。迭代地计算$$y_k$$只需要常数级的计算，因为它依赖$$h_{k-1}$$和$$x_k$$。

transformer：

+ 使用kv cache时需要线性的内存
+ 需要保存所有之前的tokens，包括在中间层的那些

aaren：

+ 只需要常数级的内存
+ 不需要保存之前的所有tokens

![stacking-aaren](../assets/stacking-aaren.png)

## Matmul-free

[从LLM中完全消除矩阵乘法，效果出奇得好，10亿参数跑在FPGA上接近大脑功耗](https://mp.weixin.qq.com/s/HUvGGug48nGBx067nCbkag)

## H2O attention

[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/pdf/2306.14048)

+ sparsity for small cache size：kv cache很稀疏，只要5%效果就可以了
+ Heavy-Hitters for low miss rate：一小部分的token贡献了大部分attention score
+ 贪心法选择需要干掉的token：假设总共需要保留k个token，对于第i个token，加进来后，遍历集合里的所有token，看干掉哪一个对attention score影响最小，就把它干掉

## TransNAR

[拯救Transformer推理能力！DeepMind新研究TransNAR：给模型嵌入「算法推理大脑」](https://mp.weixin.qq.com/s/YPICpkYHAC7zTLC_0M_XkQ)

[Transformers meet Neural Algorithmic Reasoners](https://arxiv.org/pdf/2406.09308)

基于：[Neural Algorithmic Reasoning](https://arxiv.org/pdf/2105.02761)

## softmax数学原理

[通向概率分布之路：盘点Softmax及其替代品](https://mp.weixin.qq.com/s/tA9kJqD279dHnivzPkvbjg)

把softmax的分母记为$$Z(x)$$，其对数是max的一个光滑近似：

XXX
\begin{array}{r}
\log Z(\boldsymbol{x})=\log \sum_{j=1}^n e^{x_j}=\operatorname{logsumexp}(\boldsymbol{x}) \\
\lim _{\tau \rightarrow 0^{+}} \tau \operatorname{logsumexp}(\boldsymbol{x} / \tau)=\max (\boldsymbol{x})
\end{array}
XXX

当$$\tau$$取1时，可以得到$$\operatorname{logsumexp}(\boldsymbol{x}) \approx \max (\boldsymbol{x})$$

## StreamingLLM

[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453)

[深度解析streamingLLM之无限长文本生成能力](https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&mid=2247484158&idx=1&sn=666bd5fed13815b5765124646206bac5&chksm=cea54ae8f9d2c3fea1f3d5dfc94fc3d48abbd02a1278584bfd76ff8fe52ce59ce8966ced9cd5&token=562866785&lang=zh_CN#rd)

attention sink：输入给LLM推理开头的几个intial tokens是非常特殊的，就像水龙头一样，出水那一瞬间吸引了人们大量的attention。而且intial tokens与生成token的绝对距离距离和语义信息都不重要，重要的是这第一个或者前面几个token。

softmax需要所有位置的值的总和为1，因此必须给某些位置权重，即使这个位置对输出没有任何贡献，可能导致在backward的过程中产生错误的权重更新，而这个错误在后续的过程中很难被纠正。因此，模型倾向于将不必要的注意力值转嫁给特定的token。其实在[https://www.evanmiller.org/attention-is-off-by-one.html](https://www.evanmiller.org/attention-is-off-by-one.html)就提出了，要给softmax的分母+1：$$(\operatorname{softmax_1}(\mathrm{x}))_i=\frac{\exp \left(x_i\right)}{\sum_j \exp \left(x_j\right)+1}$$

当每个token都趋向负无穷时，softmax的极限是1/k（k是token数），也就是会让每个token的概率都是1/k

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}(\operatorname{softmax}(x))_i=\frac{1}{k}>0
XXX

而$$$$softmax_1$$$$因为分母有个1（即$$exp(0)=1$$），相当于在最前面引入了一个或者多个的无意义的global_token。同样当每个token都趋向负无穷时，因为分母已经有一个1了，所以可以让其他正常token都趋于0

XXX
\lim _{x 1 \rightarrow-\infty} \ldots \lim _{x_k \rightarrow-\infty}\left(\operatorname{softmax}_1(x)\right)_i=0
XXX


[https://github.com/mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm)

集成到trt-llm里了：[https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#run-llama-with-streamingllm)

+ trtllm-build时，加上```--streamingllm enable```
+ infer时，加上```--sink_token_length```设置sink的token数，加上```--max_attention_window_size```设置sliding_window

## 是否还要RAG?

[谷歌重磅：告别RAG，长上下文的大语言模型无需检索增强](https://mp.weixin.qq.com/s/lOORnoyrA8lqOEXWKxhAQg)

## memory^3

[鄂维南院士领衔新作：大模型不止有RAG、参数存储，还有第3种记忆](https://mp.weixin.qq.com/s/_7mpswMvpg5sRrIKsF-Vvw)

[Memory3 : Language Modeling with Explicit Memory](https://arxiv.org/pdf/2407.01178)

## TSLLM

[没想到！AlphaZero式树搜索也能用来增强大语言模型推理与训练](https://mp.weixin.qq.com/s/k3HuSPGJFJ223thy316eCg)

[AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training](https://arxiv.org/pdf/2309.17179)

[https://github.com/waterhorse1/LLM_Tree_Search](https://github.com/waterhorse1/LLM_Tree_Search)

## PEER

[单一作者论文，谷歌提出百万专家Mixture，超越密集前馈、稀疏MoE](https://mp.weixin.qq.com/s/-oocmPNRT5ddvNwIvYxiQA)

[MoE也有Scaling Law，「百万专家」利用率近100%！DeepMind华人挑战MoE极限](https://mp.weixin.qq.com/s/tBe9DZvzB6NB8HhLYP31CQ)

[Mixture of A Million Experts](https://arxiv.org/pdf/2407.04153)

## TTT

[彻底改变语言模型：全新架构TTT超越Transformer，ML模型代替RNN隐藏状态](https://mp.weixin.qq.com/s/QSw9PKB_HhSxeO7agnzBgQ)

[Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/pdf/2407.04620)

[https://github.com/test-time-training/ttt-lm-jax](https://github.com/test-time-training/ttt-lm-jax)

[https://github.com/test-time-training/ttt-lm-pytorch](https://github.com/test-time-training/ttt-lm-pytorch)

![ttt](../assets/ttt.png)

[连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路](https://mp.weixin.qq.com/s/tfrG21mfteVAkjqYx5mDsQ)

[The Surprising Effectiveness of Test-Time Training for Abstract Reasoning](https://ekinakyurek.github.io/papers/ttt.pdf)

## Axiomatic Training

[6700万参数比肩万亿巨兽GPT-4！微软MIT等联手破解Transformer推理密码](https://mp.weixin.qq.com/s/ySRE3MaEH539vqrWDi6KBQ)

[公理训练让LLM学会因果推理：6700万参数模型比肩万亿参数级GPT-4](https://mp.weixin.qq.com/s/Yera281WG8RNAOrRUYjH6g)

[Teaching Transformers Causal Reasoning through Axiomatic Training](https://arxiv.org/pdf/2407.07612v1)

训练大模型新范式——**公理框架（Axiomatic Framework）**，作者从头开始训练了6700万参数的模型，仅使用了简单的因果链作为训练数据。在推断复杂图表中的因果关系时，**67M模型的表现超越了十亿级参数LLM**，甚至可以与GPT-4相媲美。

## PNN

[AI大模型有望再扩1000倍！剑桥耶鲁康奈尔：PNN是变革关键](https://mp.weixin.qq.com/s/XjU-r2rKGWaatObzdh5TGw)

[Training of Physical Neural Networks](https://arxiv.org/pdf/2406.03372)

## JRT

[小技巧大功效，「仅阅读两次提示」让循环语言模型超越Transformer++](https://mp.weixin.qq.com/s/zdPlK4IHeEiW0ikmQMPJUA)，注：这里的transformer++指的就是llama2的架构

[Just read twice: closing the recall gap for recurrent language models](https://arxiv.org/pdf/2407.05483)

[https://github.com/HazyResearch/prefix-linear-attention](https://github.com/HazyResearch/prefix-linear-attention)

![jrt](../assets/jrt.png)

### order的重要性

&nbsp;

线性attention的recurrent state很小，存储空间有限，往往很难选择要存储哪些state。

如果集合A和集合B有交集，且A元素比B多，先出现集合A的时候，需要存储整个A，而先出现集合B的时候，则只需要存储集合B

### JRT-Prompt

&nbsp;

上下文学习任务以$$(\mathcal{C}, \mathcal{Q}, \mathcal{Y})$$作为输入，$$\mathcal{C}$$为一些上下文来源（如文档或代码存储库），$$\mathcal{Q}$$为给定上下文时对模型的一些问题或请求，$$\mathcal{Y}$$为答案。

+ 使用自回归的标准上下文学习模型$$\mathcal{A}$$，输入$$\mathcal{C}$$和$$\mathcal{Q}$$，并根据正确的完成情况$$Y$$来评估生成的输出$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q})$$。
+ JRT-PROMPT：在提示模型输出答案之前会在上下文中重复提示中的信息（如问题和文档），例如$$\hat{\mathcal{Y}}=\mathcal{A}(\mathcal{C}, \mathcal{Q}, \mathcal{C}, \mathcal{Q})$$。在上下文第二次出现时，模型根据完整的上下文来决定存储哪些信息。

示例：

```shell
# 原来的prompt
## input: 
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年

# JRT的prompt
## input：
百度成立于哪一年？百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。
百度是一个xxx公司，1999年，xxxxx，2001年，xxx，2002年，xxxx。百度成立于
## output: 
2001年
```

### base的linear transformer

&nbsp;

通过$$\phi: \mathbb{R}^d \rightarrow \mathbb{R}^{\tilde{d}}$$，使得$$\phi\left(\boldsymbol{q}_i\right)^{\top} \phi\left(\boldsymbol{k}_j\right) \approx \exp \left(\boldsymbol{q}_i^{\top} \boldsymbol{k}_j / \sqrt{d}\right) .$$

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i\left(\phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j\right)}{\phi\left(\boldsymbol{q}_i\right) \sum_{j=1}^i \phi\left(\boldsymbol{k}_j\right)}
XXX

先计算k和v的乘法，时间和空间复杂度是$$\mathcal{O}(N d \tilde{d})$$，而softmax attention是$$O\left(N^2 d\right)$$

infer阶段包括两个phases：

+ prefill：并行处理prompt，得到两个state：
    + KV-state：$$\boldsymbol{s}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top} \boldsymbol{v}_j$$
    + K-state：$$\boldsymbol{z}_l=\sum_{j=1}^l \phi\left(\boldsymbol{k}_j\right)^{\top}$$
+ decoding：计算如下3步，其中$$\boldsymbol{s}_i \in \mathbb{R}^{d \times \tilde{d}}$$，$$\boldsymbol{z}_i \in \mathbb{R}^{\tilde{d}}$$，一个decode step有$$O(1)$$的时间和空间复杂度，而softmax attention加上kv-caching有$$O(N)$$
    + $$\boldsymbol{s}_i=\boldsymbol{s}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top} \boldsymbol{v}_i$$
    + $$\boldsymbol{z}_i=\boldsymbol{z}_{i-1}+\phi\left(\boldsymbol{k}_i\right)^{\top}$$
    + $$\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{s}_i}{\phi\left(\boldsymbol{q}_i\right) \boldsymbol{z}_i}$$

### JRT-RNN架构

&nbsp;

PLA（Prefix Linear Attention）受Prefix-LM启发，主要有2个特点：

+ prefix-LM在encoder和decoder的projection是共享的，而JRT-RNN的encoder用$$\boldsymbol{k}_e, \boldsymbol{v}_e$$，decoder用$$\boldsymbol{k}_d, \boldsymbol{v}_d$$
+ 编码器使用了non-causal的线性注意力，而解码器使用标准causal线性注意力。

prefill阶段，并行地对长度为$$l$$的prompt进行如下计算，如果长度$$l< M$$，进行left-pad到$$M$$长度

XXX
\boldsymbol{y}_i=\frac{\phi\left(\boldsymbol{q}_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}\right)}{\phi\left(\boldsymbol{v} q_i\right)\left(\sum_{j=1}^i \phi\left(\boldsymbol{k}_{d_j}\right)^{\top}+\sum_{j=1}^M \phi\left(\boldsymbol{k}_{e_j}\right)^{\top}\right)}
XXX

初始化如下：

XXX
\boldsymbol{s}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top} \boldsymbol{v}_{e_j}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top} \boldsymbol{v}_{d_j}\right),\ \boldsymbol{z}_M=\sum_{j=1}^M\left(\phi\left(\boldsymbol{k}_{e_j}\right)^{\top}+\phi\left(\boldsymbol{k}_{d_j}\right)^{\top}\right)
XXX

对于decoding阶段，输出$$y_i, i>M$$，和base的linear transformer一样，不需要修改

训练loss是ntp和mlm的混合，假设序列长度是$$N$$，前$$M$$个token算MLM，后面的$$N-M$$个token算NTP：

XXX
\mathcal{L}=\frac{w_1 \mathcal{L}_{\mathrm{NTP}}+w_2 \mathcal{L}_{\mathrm{MLM}}}{w_1+w_2}
XXX

### 效率提升

虽然Linear attention比softmax attention要快，但其实不如精心优化的softmax attention（如flash attention），[Simple linear attention language models balance the recall-throughput tradeof](https://arxiv.org/pdf/2402.18668)实现了一个io-aware的kernel（[https://github.com/HazyResearch/based/](https://github.com/HazyResearch/based/)），在prefill阶段精细地partitioning & storing暛的matrix-valued recurrent state across warp-registers，PLA参考这个实现了自己的加速版。

## Falcon Mamba

[非Transformer架构站起来了！首个纯无注意力大模型，超越开源巨头Llama 3.1](https://mp.weixin.qq.com/s/ET9gghK4asEr5ObuW2padw)

[Welcome FalconMamba: The first strong attention-free 7B model](https://huggingface.co/blog/falconmamba)

[https://huggingface.co/tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b)

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon_mamba/modeling_falcon_mamba.py)

核心是```FalconMambaMixer```这个类

无需增加内存存储，就可以处理任意长度的序列，并且能够在单个 24GB A10 GPU 上运行。

训练数据有5500GT ，主要由RefinedWeb数据集组成，并添加了来自公共源的高质量技术数据、代码数据和数学数据。

采用**多阶段训练策略**进行训练，上下文长度从2048增加到了8192。此外，受到**课程学习**概念的启发，整个训练阶段精心选择了混合数据，充分考虑了数据的**多样性和复杂性**。在最后的训练阶段，使用了一小部分高质量精选数据（即来自 Fineweb-edu 的样本），以进一步提升性能。

大部分训练是在**256个H100 80GB GPU**上完成的，采用了 3D 并行（TP=1、PP=1、DP=256）与 ZeRO 相结合的策略。

用adamW+WSD（预热 - 稳定 - 衰减）学习率schedule，在前50 GT的训练过程中，batch大小从b_min=128增加到了b_max=2048

## LM-steer

ACL 2024

[LM-Steer: Word Embeddings Are Steers for Language Models](https://arxiv.org/pdf/2305.12798)

[https://github.com/Glaciohound/LM-Steer](https://github.com/Glaciohound/LM-Steer)

发现词向量空间上的线性变换空间等价于对语言模型生成样式的调节，并以此设计了名为 LM-Steers 的语言模型调控方法。我们发现词向量的这种调节作用普遍存在于各种尺寸的语言模型中。它只需要学习原始模型 0.2% 的参数就可以引导各种风格。在语言模型去毒化和生成情感控制等任务上，LM-Steers 可以实现与最先进的受控生成方法相当或更好的性能，同时保持更好的生成质量平衡。学习到的 LM-Steer 还可以充当文本风格的解读器：它可以解释各种文本样式与词向量哪些维度相关，并且可以用于寻找最具代表性的文本片段。 LM-Steer 可通过显式计算来在不同语言模型之间转移，而不需要额外训练。我们还可以简单地通过缩放 LM-Steer 来实现风格的连续控制，或者实现多种生成控制的组合。

## razerattention

减少kv-cache的大小

[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://www.arxiv.org/pdf/2407.15891)

受anthropic的induction head([In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html))启发:

+ short head：对长文没有任何响应
+ long head(induction head)：对长文能直接找到对应位置，并且对那附近的词有很强的信号

## MLP-Mixer

[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)

## sigmoid attention

[Sigmoid注意力一样强，苹果开始重新审视注意力机制](https://mp.weixin.qq.com/s/4DvgsqkyNcj6HBrAHTygCA)

[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/pdf/2409.04431)

[https://github.com/apple/ml-sigmoid-attention](https://github.com/apple/ml-sigmoid-attention)

证明了从理论上讲，与 softmax 注意力相比，具有sigmoid注意力的Transformer是**通用函数逼近器**，并且受益于**改进的正则化**。

## lstm+transformer

[LSTM+Transformer王炸创新，荣登Nature](https://mp.weixin.qq.com/s/OTwbZEy-v93-fzvh1Z4m2Q)

## AnyGraph

[港大黄超团队推出AnyGraph, 首次揭秘图大模型的Scaling Law](https://mp.weixin.qq.com/s/iqQi4ZP7FdpHnMxZByOyoQ)

[AnyGraph: Graph Foundation Model in the Wild](https://arxiv.org/pdf/2408.10700)

[https://github.com/HKUDS/AnyGraph](https://github.com/HKUDS/AnyGraph)


## 液态神经网络

[给机器人装上「虫脑」？非Transformer液态神经网络终于来了！MIT CSAIL负责人创业成果](https://mp.weixin.qq.com/s/oowid3yCpFNTALCvgdSwXg)

Liquid Foundation Models（LFM），1B、3B和40B LFM在各个规模上均能实现SOTA性能，同时保持更小的内存占用和更高效的推理。2020年就有了[Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439)

## 差分transformer

[这篇论文非常火！差分Transformer竟能消除注意力噪声，犹如降噪耳机](https://mp.weixin.qq.com/s/hG_S85HkyAkTFAI2iQjl6g)

[Differential Transformer](https://arxiv.org/pdf/2410.05258)

Transformer往往会过度关注不相关的上下文，即注意力噪声（attention noise），而差分Transformer则能放大对答案范围的注意力并消除噪音，从而增强上下文建模的能力。

## SparseLLM

[NeurIPS 2024｜SparseLLM：突破性全局剪枝技术，大语言模型稀疏化革命](https://mp.weixin.qq.com/s/mfdkUFXCsB50iqKgxv7hQQ)

[SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://arxiv.org/abs/2402.17946)

[https://github.com/BaiTheBest/SparseLLM](https://github.com/BaiTheBest/SparseLLM)

## minLSTM+minGRU

[图灵奖得主Yoshua Bengio新作：Were RNNs All We Needed?](https://mp.weixin.qq.com/s/ueid-TAw-9OjtKFKA5lqSw)

[Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201v1)

## MixCon

[北大林宙辰团队全新混合序列建模架构MixCon：性能远超Mamba](https://mp.weixin.qq.com/s/FfCYq1Bx6d7NARKvDPwz2Q)

[MixCon: A Hybrid Architecture for Efficient and Adaptive Sequence Modeling](https://zhouchenlin.github.io/Publications/2024-ECAI-MixCon.pdf)

## self-lengthen

[阿里千问提出Self-Lengthen，大模型实现自迭代扩展输出长度](https://mp.weixin.qq.com/s/1m1hUkhs3altxjYP6IxUVw)

[Language Models Can Self-Lengthen to Generate Long Texts](https://arxiv.org/abs/2410.23933)

[https://github.com/QwenLM/Self-Lengthen](https://github.com/QwenLM/Self-Lengthen)

## BLT

[Tokenization不存在了？Meta最新研究，无需Tokenizer的架构来了](https://mp.weixin.qq.com/s/7ju-PjPZVPrBLQ1qFnFoKw)

[Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/pdf/2412.09871)


# 典型LLM简介

llm榜单：

[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

![llm-families](../assets/llm-families.png)


## GPT系列

### GPT3

&nbsp;

2020年的gpt3：[Language models are few-shot learners](https://arxiv.org/pdf/2005.14165.pdf)，175b（1750亿）参数，当参数量到达千亿时出现了『涌现』现象，发现可以in-context learning。

### CODEX

&nbsp;

[Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)

能够解析自然语言，并生成代码。CODEX是gpt3在github上收集的代码语料上进行finetune得到的，并且在微软的copilot中使用。

### WebGPT

&nbsp;

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[https://openai.com/blog/webgpt/](https://openai.com/blog/webgpt/)

为了回答开放性问题，使用基于文本的浏览器对gpt3进行finetune，包括如下3个步骤：

+ 学习使用人类示范（demonstration）数据来模仿人类的浏览行为
+ 学习一个reward函数来预测人类偏好
+ 用强化学习和拒绝采样来优化reward函数


注：[重要性采样和拒绝采样](https://zhuanlan.zhihu.com/p/664233442)

+ **重要性采样**的关键是**降低方差**：因为相同的样本量，用$$\pi(x)$$分布采样得到的结果方差较大(或者是$$\pi(x)$$不好采样)，而用$$p(x)$$采样的样本得到的结果方差较小，用来估计原分布$$\pi(x)$$
+ **拒绝采样**：引入易于采样的分布$$Q(x)$$，然后从中随机地筛掉某些样本(根据**接受概率**接受或者拒绝样本)，使得剩下的样本服从分布$$P(x)$$

拒绝采样的步骤：

+ 从辅助分布$$Q(x)$$中采样得到样本$$x_i$$
+ 计算接受概率$$A = P(x_i) / (M \times Q(x_i))$$，其中$$M$$是一个常数，满足$$P(x) \leq M \times Q(x)$$对于所有$$x$$成立
+ 以概率$$A$$接受样本$$x_i$$，即生成一个随机数$$u$$，如果$$u \leq A$$，则接受样本$$x_i$$；否则拒绝样本$$x_i$$。

重复上述步骤，直到获得足够数量的样本。


### InstructGPT

&nbsp;

[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)

sft+rm+rl，在最小性能降低的情况下，提升了生成结果的真实性，并降低了毒害性

### ChatGPT&GPT-4

&nbsp;

2022.11.30推出了ChatGPT，基于GPT3.5，即InstructGPT的兄弟

2023.3推出了GPT-4，多模态LLM，能输入图像和文本


## LLaMA系列

### LLaMA

&nbsp;

2023年2月发布，[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)，开源的LLaMA-13B比 GPT3 175B在很多任务上都更好

参考代码：
[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

[https://github.com/meta-llama/llama](https://github.com/meta-llama/llama)

之前的工作考虑的是在训练预算有限的前提下，如何提升模型性能（2022年deepmind的[Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)的Chinchilla）,llama考虑在预测时的预算。例如chinchilla是一个10b的模型在200b的token上训练，但其实一个7b的模型当用了1T的token后，性能仍在提升。LLama-13b比gpt3在大多数benchmark上好，但size只有1/10，在一个GPU上就能跑。

llama只用公开数据训练，而Chinchilla、PaLM、GPT-3都有自己的未公开数据集。其他的OPT、GPT-NeoX、BLOOM、GLM虽然也只用公开数据集，但打不过PaLM-62B或者Chinchilla

#### 预训练数据

&nbsp;

+ English CommonCrawl(67%)：使用CCNet pipeline，去重、用fasttext把非英文的页面删了，用n-gram把低质内容删了。此外，还训了一个线性模型，对页面进行分类：作为维基百科的引用 vs 随机采样的页面，最后把不属于引用这个类别的页面删了
+ C4(15%)：与CCNet类似，主要区别在质量过滤是基于启发式的规则，如标点符号的存在，或者词数和句子数
+ github(4.5%)：使用Google BigQuery里的公开github数据集，只用Apache、BSD和MIT证书的。低质判断是启发式规则，如字母数字占比、行的长度等，用正则删掉head等样式，最终以文件粒度进行去重。
+ wikipedia(4.5%)：2022年6-8月的数据，包括20种语言
+ Gutenberg and Books3(4.5%)：两个书籍数据集，对有90%以上内容重复的书籍做去重。
+ Arxiv(2.5%)：拿原始的tex文件，删掉first section之前的东西，还有一些注释、宏
+ Stack Exchange(2%)：高质量的问答网站，按答案的分数排序

![llama_data](../assets/llama_data.png)

tokenizer：BPE，使用sentencepiece的实现。将所有numbers切成单个数字，回退到字节去处理未知的utf8字符（fallback to bytes to decompose unknown UTF-8 characters）

总共有1.4T的token，对大部分训练数据，每个token在训练时只用了一次，除了维基和book大概用了两次。

附：gpt4说：当我们说"一个token只训练一次"，我们其实是在说在一个epoch（一个完整遍历训练集的过程）中，我们只遍历一次完整的数据集。如果一个特定的token在数据集中出现多次，那么在一个epoch中，这个token就会被用来训练模型多次。

![llama](../assets/llama_params.png)


![一些大模型](../assets/LLM/WechatIMG322.jpg)


#### 网络结构

&nbsp;

+ SwiGLU激活函数(PaLM)：取代ReLU，[Glu variants improve trans- former](https://arxiv.org/abs/2002.05202)，把PaLM里的$$4d$$改了$$2/34d$$

说白了就是输入$$x$$，SwiGLU激活完是$$swish(w_1(x)) * w_3(x)$$，其中swish又叫silu，是$$f(x)=x \cdot sigmoid(x)$$

然后再过一个$$w_2$$，得到$$w_2(swish(w_1(x)) * w_3(x))$$就是最终的ffn输出

以下是transformers里的实现：[https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act] ## 默认是silu，即swish

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) 
                for i in range(self.config.pretraining_tp)], dim=-1)

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) 
                    for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

这个是llama官方代码的实现：[https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345](https://github.com/meta-llama/llama/blob/ef351e9cd9496c579bf9f2bb036ef11bdc5ca3d2/llama/model.py#L337-L345)

```python
class FeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        hidden_dim: int,
        multiple_of: int,
        ffn_dim_multiplier: Optional[float],
    ):
        """
        Initialize the FeedForward module.

        Args:
            dim (int): Input dimension.
            hidden_dim (int): Hidden dimension of the feedforward layer.
            multiple_of (int): Value to ensure hidden dimension is 
                a multiple of this value.
            ffn_dim_multiplier (float, optional): 
                Custom multiplier for hidden dimension. Defaults to None.

        Attributes:
            w1 (ColumnParallelLinear): Linear transformation for the first layer.
            w2 (RowParallelLinear): Linear transformation for the second layer.
            w3 (ColumnParallelLinear): Linear transformation for the third layer.

        """
        super().__init__()
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if ffn_dim_multiplier is not None:
            hidden_dim = int(ffn_dim_multiplier * hidden_dim)
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )
        self.w2 = RowParallelLinear(
            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x
        )
        self.w3 = ColumnParallelLinear(
            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x
        )

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
```


+ Rotary embeddings(GPTNeo)：删掉原来的绝对位置编码，加上rotary positional embedding(RoPE)，网络的每一层都加，参考[Roformer: Enhanced transformer with rotary position embedding](https://arxiv.org/pdf/2104.09864.pdf)
+ pre-normalization(gpt3)：提升训练**稳定性**，对每个子层的输入做norm，而非输出。此外，使用的是RMSNorm函数([Root mean square layer normalization](https://arxiv.org/abs/1910.07467))取代标准的layer-norm
    + layernorm计算单个样本在单层中所有激活的均值和标准差，并使用这些统计数据来归一化该层的激活。
    + RMSnorm只计算激活的平方根均值（RMS），而不是标准差。这样做的一个好处是计算上更简单，因为它省去了计算均值的步骤，只关注激活的规模（scale）而非其准确的分布。$$\operatorname{RMSNorm}\left(x_i\right)=\frac{x_i}{\sqrt{\frac{1}{H} \sum_{j=1}^H x_j^2}+\epsilon}$$，其中$$H$$是该层的神经元个数，而且也不用求均值

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        """
        Initialize the RMSNorm normalization layer.

        Args:
            dim (int): The dimension of the input tensor.
            eps (float, optional): A small value added to the denominator 
                for numerical stability. Default is 1e-6.

        Attributes:
            eps (float): A small value added to the denominator 
                for numerical stability.
            weight (nn.Parameter): Learnable scaling parameter.

        """
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        Apply the RMSNorm normalization to the input tensor.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The normalized tensor.

        """
        # rsqrt(x)= 1/ sqrt(x)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        Forward pass through the RMSNorm layer.

        Args:
            x (torch.Tensor): The input tensor.

        Returns:
            torch.Tensor: The output tensor after applying RMSNorm.

        """
        output = self._norm(x.float()).type_as(x)
        return output * self.weight
```

优化器：AdamW，cosine学习率schedule，最终学习率是最大学习率的10%。0.1的weight decay和1.0的gradient cliping，使用2000steps的warmup

#### 训练加速

&nbsp;

+ 对causal multi-head attention加速：实现在[http://github.com/facebookresearch/xformers](http://github.com/facebookresearch/xformers)中，降低内存使用和运行时间，参考[self-attention does not need $$o(n^2)$$ memory](https://arxiv.org/pdf/2112.05682.pdf)，以及[Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)。思想是
    + 不存储attention weights
    + 不计算被mask的key/query得分
+ 减少xxx：


### LLaMA2

&nbsp;

2023年7月，[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)

+ 基于公开数据集预自监督地训练一个llama-2
+ llama-2-chat模型:
    + sft后得到初始版本
    + 使用RLHF迭代地更新（拒绝采样+ppo） 

![llama-2-chat](../assets/llama-2-chat.png)

[https://zhuanlan.zhihu.com/p/636784644](https://zhuanlan.zhihu.com/p/636784644)

使用了GQA（grouped query attention）(参考[Gqa: Training generalized multi-query transformer models from multi-head checkpoints](https://arxiv.org/pdf/2305.13245.pdf))，在注意力机制中对**K/V进行参数共享**的方案，可以在**推理**过程中**减小KV缓存**。

![gqa](../assets/gqa.png)


### LLaMA3

#### 原始LLaMa3

&nbsp;

2024年4月

[开源大模型Llama 3王者归来！最大底牌4000亿参数，性能直逼GPT-4](https://mp.weixin.qq.com/s/KCyL8WTzXutPQ_k0Vl9Vwg)

[Llama 3超大杯有何惊喜？Meta会一直开源吗？当初为何笃信元宇宙？扎克伯格新访谈回应一切](https://mp.weixin.qq.com/s/e2n4ttcT8raDU877t53GPQ)

[Llama 3细节公布！AI产品总监站台讲解：Llama系列超庞大生态系统](https://mp.weixin.qq.com/s/iDAlop_LNv9evZtfPMPyUg)

[OpenAI 前创始成员、特斯拉自动驾驶前负责人 Andrej Karpathy 发表 Meta Llama 3 笔记](https://mp.weixin.qq.com/s/701PSyi954QHz_mt6Ddn-Q)

[Karpathy称赞，从零实现LLaMa3项目爆火，半天1.5k star](https://mp.weixin.qq.com/s/1poG0tEjmym1456mmR66nQ)

[Karpathy点赞，这份报告教你如何用 LLaMa 3创建高质量网络数据集](https://mp.weixin.qq.com/s/luZGMG1RRUT4X_ckt8hsCQ)

[https://github.com/naklecha/llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)

三个版本：8B 和 70B 参数的模型，还有一个 405B 参数的密集模型（还在训练之中，但已经在逼近GPT-4的领域，例如84.8 MMLU vs. 86.5 4Turbo），8B版本基本上与Llama-2的最大版本一样强大。

Llama 3的主要亮点：

+ 训练语料：基于超过15T token训练，比Llama 2数据集(2T)的7倍还多：
    + **scaling law的新发现**：对于一个8B模型，Chinchilla的**计算最优点**将是训练约200B词汇(前面提到了10B模型，大概要205B的token来训练，以此类推)，所以这里**超出了75倍**，而且**还未收敛**。可见，我们经常使用的LLMs在训练上显著不足，可能是100-1000倍或更多，远未达到它们的收敛点。
+ tokenizer：词汇数量从Llama 2的32K增加到Llama 3的128K，增加了4倍，拥有更多的词汇可以在长度上**更有效地压缩序列**。
+ 上下文窗口：从Llama 2的4096和Llama 1的2048增加到了**8192**，相比GPT4的128k还差得很远
+ 训练效率：比 Llama 2 高 3 倍，做了很多工程优化；
+ 模型结构：在Llama 2中，只在更大的模型使用了**分组查询注意力（GQA）**，但llama3的所有模型都使用了，包括最小的8B模型。
+ 新能力范畴：Llama-2 只能使用非常特定的工具，而 Llama-3 能使用好得多的工具，无需人工编程就能让其使用谷歌执行搜索，类似的功能还有编程和运行代码等。

[https://github.com/meta-llama/](https://github.com/meta-llama/)

[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

#### 多模态llama3

&nbsp;

[多模态 Llama-3 它来了 ！！[全网首发微调教程]](https://mp.weixin.qq.com/s/IFVIkVvMqeAL0lZF9OvRSA)

[https://github.com/InternLM/XTuner](https://github.com/InternLM/XTuner)

#### 中文llama3

&nbsp;

[首批中文版Llama3模型来了，解释成语、答弱智吧问题](https://mp.weixin.qq.com/s/ny0gBOxf4-tJiwjgp3o9HQ)

[https://github.com/CrazyBoyM/llama3-Chinese-chat](https://github.com/CrazyBoyM/llama3-Chinese-chat)

[https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)


### LLama3.1

[最强模型Llama 3.1 405B正式发布，扎克伯格：开源引领新时代](https://mp.weixin.qq.com/s/QUWumWsTF_Qq77tdlyCHdg)

[微调大模型，AMD MI300X就够了！跟着这篇博客微调Llama 3.1 405B，效果媲美H100](https://mp.weixin.qq.com/s/eJwg4GwH--9IVFedum_BnA)

[The Llama 3 Herd of Models](https://github.com/daiwk/collections/blob/master/assets/LLM/llama3.1.pdf)

[Llama3 92页技术报告中文全文详解](https://mp.weixin.qq.com/s/9Mnd1ass0XYUGW_UqDF8bw)

还开放了一个生态系统：

+ model：[https://github.com/meta-llama/llama-models/tree/main/models/llama3_1](https://github.com/meta-llama/llama-models/tree/main/models/llama3_1)
+ tool-chain：[https://github.com/meta-llama/llama-toolchain](https://github.com/meta-llama/llama-toolchain)
+ agent-system：[https://github.com/meta-llama/llama-agentic-system](https://github.com/meta-llama/llama-agentic-system)


#### 整体架构

&nbsp;

![multimodal-llama3.1](../assets/multimodal-llama3.1.png)

有2个主要的stage：

+ language model pre-training：用多语言的语料进行next token prediction
    + 使用超过15万亿(15T)个token训练Llama 3.1 405B，context window是8k tokens
    + 又加上一个continued pretraining stage，将context windows增加到128k tokens
+ language model post-training：sft+dpo+新的能力（如工具使用，能提升代码和推理能力）+safety

(toread)

这里有一些总结：[关于post-training和一些思考](https://mp.weixin.qq.com/s/Bpd6_zq9kmTTeHxZ9WJJpw)

为了支持多模态，还加入了如下3个stage：

+ 多模态encoder pre-training：
    + image encoder：在大量的img-text pair上训练，让模型能理解图片+描述
    + speech encoder：自监督，mask掉部分的speech输入，通过离散token表示来重建被mask的部分
+ vision adapter training：
    + 训练一个adapter将pretrained的image encoder融入到pretrained language model里，adapter包括一系列的cross-attention层将image-encoder表示输入language model。在image-text pair上训练，对齐图文表示。训练时**更新image-encoder的参数**，但**不更新language model的参数。**
    + 基于这个image-encoder来训练一个video-adapter，用的是video-text数据，让模型能聚合多帧间的信息。视频侧的temporal aggregator是有一个perceiver的resampler
+ speech adapter training：将speech encodings转成token表示，直接输入finetuned language model。在sft阶段，**adaper和encoder的参数联合更新**，但**不更新language model的参数**，还集成进了一个text-to-speech的系统。


#### 预训练

&nbsp;

在超过16,000个H100（80G HBM3）上训练，训练平台是[Meta open compute project, grand teton ai platform](https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/)，基于[MAST: Global scheduling of ml training across geo-distributed datacenters at hyperscale](https://www.usenix.org/system/files/osdi24-choudhury.pdf)进行schedule

选择了decoder only transformer with minor adaptations，而**不是MOE**，以最大限度地**提高训练稳定性**。

+ 使用GQA with 8 key-value heads，提升推理速度，在decoding时降低k-v cache的大小
+ 使用attention mask，**让同一序列里的不同documents不计算self-attention**。在标准的pretraining中影响不大，但**对于超长序列的continued pre-training非常重要**
+ 128K tokens的vocab，100k的tiktoken tokenizer+28k的额外token，更好地支持非英语的语言。相比llama2，每个token能压缩的字符3.17变成3.94，**压缩率更高了**，即同样计算量能读更多文本，同时也能提升下游任务效果。
+ 把RoPE的base frequency超参**加大到500,000**，能更好地**支持更长contexts**，[Effective Long-Context Scaling of Foundation Models](https://aclanthology.org/2024.naacl-long.260.pdf)说这个值对32768长度的context很有效。


参数量：

|                          | 8B          | 70B         | 405B        |
|--------------------------|-------------|-------------|-------------|
| **Layers**               | 32          | 80          | 126         |
| **Model Dimension**      | 4,096       | 8192        | 16,384      |
| **FFN Dimension**        | 6,144       | 12,288      | 20,480      |
| **Attention Heads**      | 32          | 64          | 128         |
| **Key/Value Heads**      | 8           | 8           | 8           |
| **Peak Learning Rate**   | 3e-4    | 1.5e-4  | 8e-5    |
| **Activation Function**  | SwiGLU      | SwiGLU      | SwiGLU      |
| **Vocabulary Size**      | 128,000     | 128,000     | 128,000     |
| **Positional Embeddings**| RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) | RoPE ($$\theta$$ = 500,000) |


#### 后训练

&nbsp;

采用iterative post-training procedure，即预训练后进行多轮对齐，每轮都使用sft、RS（拒绝采样）、直接偏好优化(DPO, Direct Preference Optimization)，能够为**每轮**创建**最高质量的合成数据**，并提高每项能力（capability）的性能。

+ 使用**合成数据生成(synthetic data generation)**来产生绝大多数SFT示例，并**多次迭代**以在所有能力生成越来越高质量的合成数据。
+ 采用了多种数据处理技术来过滤这些合成数据，达到最高质量，并可以**跨能力**来**扩展微调数据量**。

#### 推理

&nbsp;

从bf16量化为fp8

### LLama-3.1-Minitron

[英伟达玩转剪枝、蒸馏：把Llama 3.1 8B参数减半，性能同尺寸更强](https://mp.weixin.qq.com/s/zxW9EagxGJX-rS5loNLKXw)

[Compact Language Models via Pruning and Knowledge Distillation](https://www.arxiv.org/pdf/2407.14679)

NVIDIA用TensorRT-LLM优化了Llama 3.1 8B和Llama-3.1-Minitron 4B模型。

### LLama-3.1-Nemotron

[英伟达开源最新大模型Nemotron 70B后，只有OpenAI o1一个对手了](https://mp.weixin.qq.com/s/ebJkBkGAn8QS-_xVK__MMw)

### Llama-3.2

[刚刚，Llama 3.2 来了！支持图像推理，还有可在手机上运行的版本](https://mp.weixin.qq.com/s/3JP9UgfXNMlI5jaYHyekYA)

Llama 3.2 

+ 最大的两个模型11B和90B：都支持图像推理，包括文档级的图表理解、图像描述和视觉定位任务，比如直接根据自然语言描述定位图像中的事物。
+ 轻量级的1B和3B：都是纯文本模型，但也具备多语言文本生成和工具调用能力。

[Sebastian Raschka最新博客：从头开始，用Llama 2构建Llama 3.2](https://mp.weixin.qq.com/s/RuTRkJPeEP1hqWevxn9h6Q)

### Llama-3.3

[新版Llama 3 70B反超405B！Meta开卷后训练，谷歌马斯克都来抢镜](https://mp.weixin.qq.com/s/6Iv4VzMlYrkmSsAo_IRGTg)

Llama 3.3能用70B实现405B的效果，主要是“运用了后训练技术的最新进展”，其中包括在线偏好优化（online preference optimization）。

[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)

[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)


### Alpaca

[Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html?trk=cndc-detail)

Stanford的羊驼（Alpaca）模型，有70亿（7b）参数，**没有使用RLHF**，而是使用**监督学习**的方法，参考[Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)（代码[https://github.com/yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)）

数据集是通过查询基于GPT-3的text-davinci-003模型的结果，得到的52k的指令-输出对（instruction-output pairs）。

因此，Alpaca本质上使用的是一种弱监督（weakly supervised）或以知识蒸馏（knowledge-distillation-flavored）为主的微调，即“**用 LLM 来训练 LLM**”。

![Alpaca](../assets/alpaca.jpeg)

[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

### Vicuna

通过ShareGPT收集的用户对话数据，对llama进行finetune得到的13B模型。效果接近chatgpt的92%，而且训练消耗比较低，大概只要300美元。

### Guanaco

&nbsp;

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)也是对llama进行微调，使用了QLoRA，能够在一台48G的GPU上微调65B的模型。只需要在单台GPU上finetune 24小时就能达到99.3%的chatgpt的效果。

[https://github.com/artidoro/qlora](https://github.com/artidoro/qlora)

[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

QLoRA将一个固定的4 bit量化的预训练权重转成low Rank Adapters来更新梯度

### Koala

&nbsp;

[Koala: A dialogue model for academic research](https://bair.berkeley.edu/blog/2023/04/03/koala/)

使用用户与闭源大模型交互的用户输入和模型返回数据进行训练

### Mistral

&nbsp;

[Mistral 7b](https://arxiv.org/pdf/2310.06825.pdf)

7B参数比最好的13B模型（llama-2-13B）要更好，而且比llama-34B在reasoning、数学、代码生成都更好。

+ 使用**grouped-query attention**来做更快的infer
+ 使用**滑动窗口attention**来用更低的infer消耗来高效地处理**任意长度的序列**。

[https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/) 提出了moe的8x7b

[Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf)

[Mixtral 8x7B(Mistral MoE) 模型解析](https://mp.weixin.qq.com/s/-5yp0KU6_vkpWmY9wROWbg)

[Mistral开源8X22B大模型，OpenAI更新GPT-4 Turbo视觉，都在欺负谷歌](https://mp.weixin.qq.com/s/hf4uq3yrHxTGhQuzl61Imw)

[https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1)

[Mistral AI两连发：7B数学推理专用、Mamba2架构代码大模型](https://mp.weixin.qq.com/s/fFB0A0vv_2Deb0rWd4tagw)

mathtral：[https://huggingface.co/mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)

codestral-mamba: [https://huggingface.co/mistralai/mamba-codestral-7B-v0.1](https://huggingface.co/mistralai/mamba-codestral-7B-v0.1)，基于mamba2，可以直接用trt-llm启动

[准狙击Llama 3.1？Mistral AI开源Large 2，123B媲美Llama 405B](https://mp.weixin.qq.com/s/6_d_e6DlQpyONdWxd8EZvA)

Mistral AI 基于此前Codestral 22B和Codestral Mamba的经验，在很大一部分代码上训练了 Mistral Large 2。其表现远远优于上一代的Mistral Large，并且与GPT-4o、Claude 3 Opus和Llama 3 405B等顶尖模型相当。

[https://huggingface.co/mistralai/Mistral-Large-Instruct-2407](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)

[​Mistral首个多模态模型Pixtral 12B来了！还是直接放出24GB磁力链接](https://mp.weixin.qq.com/s/c_fmpkuimmEtY6pD9f_bZQ)

[https://huggingface.co/mistral-community/pixtral-12b-240910](https://huggingface.co/mistral-community/pixtral-12b-240910)

[发力了，Mistral对标ChatGPT全面升级le Chat，还祭出超大杯多模态模型](https://mp.weixin.qq.com/s/sABAaxKNGM-odULaM0-_Zg)

### phi-3

[微软发布Phi-3，性能超Llama-3，可手机端运行](https://mp.weixin.qq.com/s/kb_gfaYkXiW_cR22K2bX9g)

[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/pdf/2404.14219)

## PaLM系列

### PaLM

&nbsp;

2022年4月提出了第一个PaLM：[Palm: Scaling language modeling with pathways](https://arxiv.org/pdf/2204.02311.pdf)，直到2023年3月还是private的。是一个540B(5400亿)参数的模型，在包含了780B的tokens的高质量数据集上预训练。使用Pathways的6144块TPU v4进行训练。

### U-PaLM

&nbsp;

[Transcending scaling laws with 0.1% extra compute](https://arxiv.org/pdf/2210.11399.pdf)提出了8B，62B和540B的U-PaLM模型，用**UL2R**来对PaLM进行继续训练，用的是UL2的**mixture-of-denoiser objective**([UL2: Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131.pdf))

### Flan-PaLM

&nbsp;

Flan-PaLM（[Scaling instruction-finetuned language models](https://arxiv.org/pdf/2210.11416.pdf)）是instrunction-finetuned版本的U-PaLM。使用了更多的任务、更大的模型，以及CoT数据。使用了473个数据集，146类的task，总共1836个task。

![flan-palm](../assets/flan-palm.png)

[https://huggingface.co/google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)


### PaLM-2

&nbsp;

[PaLM 2 Technical Report](https://arxiv.org/pdf/2305.10403.pdf)是一个更计算高效型的LLM，有更好的多语种和reasoning能力，在很多任务上都比PaLM好，并且在infer上比PaLM要更快更高效。

### Med-PaLM

&nbsp;

Nature的[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)提出了Med-PaLM，在PaLM上用parameter-efficient方法进行instruction prompt tuning，使用少量的典型范例（exemplars）让LLM对齐到新领域。

Med-PaLM2([Towards expert- level medical question answering with large language models](https://arxiv.org/pdf/2305.09617.pdf))通过med-domain的finetuning和ensemble refinement prompting，效果比Med-PaLM要好。


## 其他LLM

### FLAN

&nbsp;

[Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)

通过instruction tuning，大模型能很好地提升在没见过任务上的zero-shot能力。对一个137B的预训练模型在60个NLP数据集上使用instruction template进行instruction tuning。

![flan](../assets/flan.png)

一些instruction template：

![flan-instruction-template](../assets/flan-instruction-template.png)

[https://github.com/google-research/flan](https://github.com/google-research/flan)

### Gopher

&nbsp;

[Scaling language models: Methods, analysis & insights from training gopher](https://arxiv.org/pdf/2112.11446.pdf)基于152个多样的任务，对比了从44M到208B（Gopher）的不同transformer，发现Gopher在大多数任务上均达到sota：

| Model         | Layers | Number Heads | Key/Value Size | d_model | Max LR     | Batch Size |
|---------------|--------|--------------|----------------|---------------|------------|------------|
| 44M           | 8      | 16           | 32             | 512               | $$6 \times 10^{-4}$$  | 0.25M      |
| 117M          | 12     | 12           | 64             | 768               | $$6 \times 10^{-4}$$  | 0.25M      |
| 417M          | 12     | 12           | 128            | 1,536             | $$2 \times 10^{-4}$$  | 0.25M      |
| 1.4B          | 24     | 16           | 128            | 2,048             | $$2 \times 10^{-4}$$  | 0.25M      |
| 7.1B          | 32     | 32           | 128            | 4,096             | $$1.2 \times 10^{-4}$$  | 2M        |
| Gopher 280B | 80     | 128          | 128            | 16,384            | $$4 \times 10^{-5}$$  | 3M -> 6M    |

### T0

&nbsp;

[Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207)设计了一个系统，将任意nlp任务映射成一个人类可读的prompt格式。训练了一个encoder-decoder的T0模型，输入文本，输出文本，在混合数据集上进行多任务学习。

### ERNIE 3.0

&nbsp;

[ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/pdf/2107.02137)结合自回归网络和autoencoding网络，让模型能够同时做语言理解和语言生成任务，可以支持zero-shot、few-shot和finetuning。有10B参数，在4TB的文本和图谱数据上训练。

![ernie3.0](../assets/ernie3.0.png)

### RETRO

&nbsp;

[参数量仅为4%，性能媲美GPT-3：开发者图解DeepMind的RETRO](https://baijiahao.baidu.com/s?id=1721015293574115195&wfr=spider&for=pc)

[http://jalammar.github.io/illustrated-retrieval-transformer/](http://jalammar.github.io/illustrated-retrieval-transformer/)

[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)

Retrieval Enhanced Transformer(Retro)基于与preceding(前述) tokens的相似度，从大的语料库里检索出文档段（document chunks）作为条件，增强了自回归模型的能力。通过少25%的参数，在Pile数据集上达到了与gpt3和Jurassic-1相当的效果

![retro](../assets/retro.png)

+ 左图：
  + 输入一个长度为12的序列，每4个token一个chunk，切成3个chunk
  + 每个chunk通过freezed的bert去语料库中拿2个相似neighbors出来，每个neighbor过bert得到向量
  + 邻居作为k和v，原来的3个chunk作为k，做attention(CCA, chunked cross attention)
+ 右图：CCA的结构
  + 保证了因果性，即chunk1的邻居只对chunk1的last token以及chunk2的所有token有影响


### GLaM

&nbsp;

[Glam: Efficient scaling of language models with mixture-of-experts](https://arxiv.org/pdf/2112.06905)提出了Generalist Language Model(GLaM)，用稀疏激活的MOE架构，同时能scale模型容量，也能相比dense更可观地降低训练成本。最大的模型有1.2T参数，是gpt3的7倍。但只需要GPT3的1/3的能量来训练，而且在infer时也只有一半的flops，且在29个NLP任务上都有更好的0/1/few shot效果。

![Glam](../assets/glam.png)

如图，2 in 64的结构，每个token只会取64个experts里的top 2相关的expert，对这两个的输出加权平均输入给后面的层


### LaMDA

&nbsp;

[Lamda: Language models for dialog applications](https://arxiv.org/pdf/2201.08239)，有137B的参数，在1.5T的公开对话数据和互联网文本上pretrain，用标注数据微调，以及让模型能够咨询（consult）外部知识源能够让模型在安全性和factual grounding上有很好的改进。

### OPT

&nbsp;

[OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068)提出了从125M到175B的预训练decoder-only模型

### Chinchilla

&nbsp;

### Galactica

&nbsp;

### CodeGen

&nbsp;

### AlexaTM

&nbsp;

### Sparrow

&nbsp;

### MoD

&nbsp;

### BLOOM

&nbsp;

### GLM

&nbsp;

ACL22 [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)

iclr23 [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)


### GLM-4

[GLM-4开源版本终于来了：超越Llama3，多模态比肩GPT4V，MaaS平台也大升级](https://mp.weixin.qq.com/s/MqxiXeYs8dg_lynsUIR0Tg)

[https://github.com/THUDM/GLM-4](https://github.com/THUDM/GLM-4)

### Pythia

&nbsp;

### Orca

&nbsp;

### StarCoder

&nbsp;

### KOSMOS

&nbsp;

### Gemini

&nbsp;

#### Gemini 1.0

[Gemini: a family of highly capable multimodal models](https://arxiv.org/pdf/2312.11805.pdf)

#### Gemini 1.5

[谷歌Gemini 1.5深夜爆炸上线，史诗级多模态硬刚GPT-5！最强MoE首破100万极限上下文纪录](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652444347&idx=1&sn=51ae7e3e100e24fd49b0f75924e74695&chksm=f093b48285369da37b6148803e41fb272c51c013bc31ac1ba6b09672ff1efd38269af1192b53&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)

[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)

+ 更新的 Gemini 1.5 Pro，其大部分功能和基准都超过了 2 月份的版本
+ Gemini 1.5 Flash，一种更轻量级的变体，专为提高效率而设计，并且在性能方面的减益很小。

此前的SOTA模型能处理**20万(200K)**的token，Gemini 1.5能稳定处理**100万(1M)**的token（极限为**1000万(10M)**的token），能够处理11小时的音频、1小时的视频、超过3w行的代码库、超过70w个单词

#### gemini 2.0

[OpenAI深夜被狙，谷歌Gemini 2.0掀翻牌桌！最强智能体组团击毙o1](https://mp.weixin.qq.com/s/RfTGAlYVlH1RlTzuzugrKQ)

[谷歌“狙击”OpenAI，发布新一代大模型！主打Agent+多模态](https://mp.weixin.qq.com/s/c39MbjULBW5M_8vdO-VUXQ)

[我扒出了Gemini 2.0超实时多模态幕后的黑科技，第六代TPU芯片Trillium！](https://mp.weixin.qq.com/s/BZpL9PwvCWmR4R6h2502Kw)

[谷歌逆风翻盘暴击OpenAI，90天王者归来！44页报告押注25年三大技术前沿](https://mp.weixin.qq.com/s/b0yqPre6wMlP59cH6Qrbdw)

[data and ai trends report 2024](https://services.google.com/fh/files/misc/data_ai_trends_report.pdf)

### gemma

[https://blog.google/technology/developers/gemma-open-models/](https://blog.google/technology/developers/gemma-open-models/)

[Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)

代码：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py)


#### 架构&训练方法

+ moe：
    + 


### codegemma

&nbsp;

+ 专门处理代码补全和代码生成任务的 7B 预训练变体
+ 用于代码聊天和指令跟随的 7B 指令调优变体
+ 在本地计算机上运行快速代码补全的 2B 预训练变体

[CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)

![codegemma](../assets/codegemma.png)

### RecurrentGemma

&nbsp;

[RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf)

在infer阶段，需要检索**KV cache**，并加载到内存中，而KV cache会随着序列长度线性增长。[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)通过**local attention**来降低cache大小，但模型效果会变差。recurrent gemma将输入序列**压缩到一个固定大小的state中**，从而不会降低效果，对长序列能降低内存占用并高效infer。

基于Griffin架构，将**门控线性递归**与**本地滑动窗口注意力**混合在一起，在生成长序列时实现快速推理，相比gemma：

+ 减少内存用量：内存要求越低，就越能在内存有限的设备（例如单个 GPU 或 CPU）上生成较长的样本。
+ 吞吐量较高：能够以明显较高的batch_size执行推理，这意味着每秒可以生成更多tokens，尤其是在生成长序列时。

相比原始griffin：

+ 对输入emb**乘以一个常数**（等于model width(下表中的2560)的平方根，如下代码所示）。输入和输出的emb是tied的，这个常数**没有乘到output上去**。gemma里也有一个类似的因子
+ 对**recurrent layers（RG-LRU）**在训练时并**没有进行weight decay(本质是L2正则化，参考[https://zhuanlan.zhihu.com/p/607909453](https://zhuanlan.zhihu.com/p/607909453))**，当bp到**开方操作**时，为了训练稳定会加最大值为1000的**梯度clip**

```python
def __init__(self, ...):
    #...
    self.register_buffer(
        "normalizer", torch.tensor(self.config.hidden_size**0.5, 
            dtype=torch.bfloat16), persistent=False
    )
def forward(self, ...):
    #...
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)

    hidden_states = inputs_embeds

    if use_cache and inputs_embeds.shape[1] != 1:  
        # TODO let's maybe only call in the `generate`?
        self._setup_cache(self.config, hidden_states.shape[0], 
        hidden_states.device, hidden_states.dtype)

    if cache_position is None:
        cache_position = torch.arange(hidden_states.shape[1], 
        device=hidden_states.device)
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    causal_mask = self._update_causal_mask(attention_mask, 
        inputs_embeds, cache_position)

    ## 这里就是那个因子
    hidden_states = hidden_states * self.normalizer.type(hidden_states.dtype)
```

具体参数：

| 变量 | 大小 |
|----------|-------------|
| 总参数量 | 2.7b |
| 非emb参数量 | 2.0b |
| emb参数量 | 0.7b |
| vocab size | 256k |
| model width | 2560 |
| rnn width | 2560 |
| MLP expansion factor | 3，即intermediate_size=2560*3=7680 |
| Depth | 26 |
| Attention heads | 10 |
| local attention window size | 2048 |

训练：

[https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py)

在线推理的C++实现(cpu版本)

[https://github.com/google/gemma.cpp](https://github.com/google/gemma.cpp)

### gemma-2

&nbsp;

[单张A100全精度推理！谷歌明星开源模型Gemma 2上新9B/27B，挑战3140亿Grok-1](https://mp.weixin.qq.com/s/z3h1eExDgItDf38Xar6yPg)

[谷歌「诚意之作」，开源9B、27B版Gemma2，主打高效、经济！](https://mp.weixin.qq.com/s/0Iy3gOlWRLKRCMnrXQavaA)

[https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)

[https://blog.google/technology/developers/google-gemma-2/](https://blog.google/technology/developers/google-gemma-2/)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/pdf/2408.00118)

用了RoPE和GeGLU，网络结构上：

+ **局部滑动窗口**和**全局注意力**：在每隔一层中交替使用局部滑动窗口注意力([Longformer: The long-document transformer]((https://arxiv.org/pdf/2004.05150)))和全局注意力（[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)），局部注意力层的滑动窗口大小设置为4096个token，而全局注意力层的跨度设置为8192个token。
+ **Logit软封顶**：根据**Gemini 1.5的方法**，研究团队在每个注意力层和最终层限制logit，使得logit的值保持在$$−soft\_cap,+soft\_cap$$之间。对于9B和27B模型，注意力对数封顶设置为50.0，最终对数封顶设置为30.0。截至本文发表时，注意力logit软封顶与常见的FlashAttention实现不兼容，因此他们已从使用FlashAttention的库中移除了此功能。对模型生成进行了有无注意力logit软封顶的消融实验，发现大多数预训练和后期评估中，生成质量几乎不受影响。本文中的所有评估均使用包含注意力logit软封顶的完整模型架构。然而，某些下游性能可能仍会受到此移除的轻微影响。

XXX
\text { logits } \leftarrow \text { soft\_cap } * \text { tanh(logits } / \text { soft\_cap) }
XXX

+ **RMSNorm**进行post-norm和pre-norm。为了稳定训练，用RMSNorm对每个变换子层、注意力层和前馈层的输入和输出进行归一化。 
+ **gqa**：27B和9B模型均使用GQA，num_groups = 2，基于消融实验表明在保持下游性能的同时提高了推理速度。

[https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py](https://github.com/huggingface/transformers/blob/v4.42.3/src/transformers/models/gemma2/modeling_gemma2.py)


#### gemma-scope

&nbsp;

[Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[https://huggingface.co/google/gemma-scope](https://huggingface.co/google/gemma-scope)

### datagemma

[整合海量公共数据，谷歌开源AI统计学专家DataGemma](https://mp.weixin.qq.com/s/Fr8I9VwiyHcMnhrWMcgDeQ)

[Knowing When to Ask - Bridging Large Language Models and Data](https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf)

提出了一种名为**检索交错生成（Retrieval Interleaved Generation，RIG）**的新方法，可以可靠地将Data Commons中的公共统计数据整合到LLM的响应中。RIG是一种受工具启发的方法，可以将统计数据标记与适合从Data Commons检索的自然语言问题交错。

为了获得这种能力，他们利用Gemini 1.5的帮助生成了一个指令-响应数据集，并在此基础上对LLM进行了微调。RIG方法将事实准确性从5-7%提高到了约58%。

### Claude

&nbsp;

[全球最强大模型一夜易主，GPT-4时代终结！Claude 3提前狙击GPT-5，3秒读懂万字论文理解力接近人类](https://mp.weixin.qq.com/s/WqQWS-hiQ1i1Ve6IPH3djw)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)

+ 对齐：使用Constitutional AI([Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf))， explicitly specifying rules and principles based on sources like the UN Declaration of Human Rights.
+ 基于Collective Constitutional AI([Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input))，added an additional principle to Claude’s constitution to encourage respect for disability rights
+ 公开了部分RHLF数据：[https://huggingface.co/datasets/Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)，对应的论文是[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)

[力压GPT-4o！新王Claude 3.5 Sonnet来了，直接免费可用](https://mp.weixin.qq.com/s/HnQ7D4iDVgWteZZdTJoadg)

[大模型代肝，自动刷《崩铁》升级材料，Claude操纵计算机还能这么用！](https://mp.weixin.qq.com/s/F8CX6_VWebWLy26NeRNQsw)

[The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use](https://arxiv.org/pdf/2411.10323)

[https://github.com/showlab/computer_use_ootb](https://github.com/showlab/computer_use_ootb)

### grok

&nbsp;

[马斯克开源Grok-1：3140亿参数迄今最大，权重架构全开放，磁力下载](https://mp.weixin.qq.com/s/hvt5zwoazDx26KOaKuTs_w)

[https://github.com/xai-org/grok-1](https://github.com/xai-org/grok-1)

[马斯克发布Grok 1.5！编码和数学能力大幅提升](https://mp.weixin.qq.com/s/QwM1uk61o1DMpe5EPq9giQ)

+ 上下文窗口提升16倍，达到128k
+ 不使用通用的Python语言+Pytorch框架，采用分布式训练架构，使用Rust、JAX+Kubernetes构建。
+ 提出了自定义训练协调器，可自动检测到有问题的节点，然后剔除。
+ 优化了checkpointing、数据加载和训练重启等流程，最大限度地减少故障停机时间。

[马斯克的首款多模态大模型来了，GPT-4V又被超越了一次](https://mp.weixin.qq.com/s/2GDjZS6ctayAF8e8eFb3CQ)

Grok-1.5V: [https://x.ai/blog/grok-1.5v](https://x.ai/blog/grok-1.5v)

grok-2：[Grok-2来了，能生图识图、性能比肩GPT-4o，马斯克：发展猛如火箭](https://mp.weixin.qq.com/s/nBaY2srcMSzvEoecOyh1Cg)

### Gecko

[谷歌DeepMind发布Gecko：专攻检索，与大7倍模型相抗衡](https://mp.weixin.qq.com/s/5e_Py_Xm0RsmP1YMcikpaQ)

[Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/pdf/2403.20327.pdf)

![gecko](../assets/gecko.png)

主要包括两阶段：

+ pre-finetuning：类似[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)，自监督
+ finetuning：2-step llm distillation，提出了一个FRet数据集（Few-shot Prompted Retrieval dataset）

#### Pre-finetuning

&nbsp;

2个数据集：

+ 大型社区qa数据集：[Large dual encoders are generalizable retrievers](https://arxiv.org/pdf/2112.07899)中的来自网上论坛和qa网站的问答pair对
+ 去各种网站上爬了title-body的pair对，因为[Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/pdf/2212.03533)发现这种自然出现的pair对对于pre-finetuning embedding模型很有用

pre-finetuning的目标是让模型看到大量的多样性语料，对于一个预训练的语言模型$$\mathcal{M}$$，长度为$$n$$的句子对应的contextual向量$$\mathbf{W} \in \mathbb{R}^{n \times d}$$，对于任务特征$$t$$，数据集$$\mathcal{D}_{\text {pre }}=\left\{\left(q_i, p_i\right)\right\}_{i=1}^N$$，得到的向量如下：

XXX
\begin{aligned}
\mathbf{q}_i & =\text { mean\_pool }_{|t|+\left|q_i\right|}\left[\mathcal{M}\left(t \oplus q_i\right) \in \mathbb{R}^{\left(|t|+\left|q_i\right|\right) \times d}\right] \in \mathbb{R}^d \\
\mathbf{p}_i & =\text { mean\_pool }_{\left|p_i\right|}\left[\mathcal{M}\left(p_i\right) \in \mathbb{R}^{\left|p_i\right| \times d}\right] \in \mathbb{R}^d
\end{aligned}
XXX

对于batchsize为$$B$$的样本来说，inbatch负例，$$\operatorname{sim}(\mathbf{x}, \mathbf{y})=\frac{\mathbf{x}^{\top} \mathbf{y}}{\|\mathbf{x}\| \cdot \cdot\|\mathbf{y}\|}$$，其loss如下：

XXX
\mathcal{L}_{\text {pre }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_i\right) / \tau}}{\sum_{j=1}^B e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{p}_j\right) / \tau}}\right]
XXX

在这个阶段没有用hard负例，用了能适配设备的最大batchsize，这是[Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/pdf/2308.03281)和[Text embeddings by weakly-supervised contrastive pre-training](https://arxiv.org/pdf/2212.03533)里的经验。


#### FRet

![fret](../assets/fret.png)

##### LLM-based Diverse Query Generation 

&nbsp;

从web语料$$\text { C }$$中抽出一个段落$$p_{\text {seed }}$$，$$\mathbb{P}_{\mathrm{QG}}$$是一个固定的few-shot的prompt，让LLM生成任务描述和这个task的query

XXX
\operatorname{LLM}\left(\mathbb{P}_{\mathrm{QG}}, p_{\text {seed }}\right) \rightarrow(t, q)
XXX

生成的任务t例如：

+ question answering：```Given a query, find a passage that has the answer to the query```
+ fact checking：```Given a query, find a passage that allows you to check whether the query is true or not```

为了保证多样性：

+ 网页库本身就有很多的topic和很多的写作风格
+ 在prompt里加多样的任务描述，来让模型生成的query保证多样性。

##### LLM-based Positive and Negative Mining

&nbsp;

对于某个query $$q$$，之前的工作一般会直接把输入的段落$$p_{\text {seed }}$$当成正样本，但实践中发现$$p_{\text {seed }}$$一般比较长，而生成的query一般只关注其中一小部分，所以可能在整个语料库中有比$$p_{\text {seed }}$$更准确的答案。因此，通过如下方法构造了一个FRet数据集：

+ 先把$$p_{\text {seed }}$$当成正样本，in-batch负例训一个embedding模型。
+ 用这个模型从文档库中检索出top N的相似段落$$P=\left\{p^{(1)}, \ldots, p^{(N)}\right\}$$
+ 用生成query的LLM给这N个文档排序，有两种排序方法：
    + query likelihood：参考[Improving passage retrieval with zero-shot question generation](https://arxiv.org/pdf/2204.07496)，给定段落$$p$$，衡量query $$q$$的likelihood，$$\mathrm{QL}(q, p)=\operatorname{LLM}\left(q \mid p, \mathbb{P}_{\mathrm{QL}}\right)$$，其中的prompt参考[PaRaDe: Passage Ranking using Demonstrations with Large Language Models](https://arxiv.org/pdf/2310.14408)包括了判断query likelihood的指令，以及一些相关query+段落的few-shot。基本思想是：**如果q和p高度相关，那么从p生成q的概率应该很高**。
    + relevance classification：参考[Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels](https://arxiv.org/pdf/2310.14122)，给定query $$q$$和段落$$p$$后，衡量特定相关性label的log likelihood：$$\operatorname{RC}(q, p)=\operatorname{LLM}\left(\text { label } \mid q, p, \mathbb{P}_{\mathrm{RC}}\right)$$。基本思想是：**让LLM直接判断q和p的相关程度,并输出一个相关性标签**。
+ 通过标准的Reciprocal Rank Fusion（RRF，倒数融合，[Reciprocal rank fusion outperforms condorcet and individual rank learning methods](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)）方法得到rank函数，对上面两个方法的排序结果merge：$$R(q, p)=1 / r_{\mathrm{QL}}(q, p)+1 / r_{\mathrm{RC}}(q, p)$$...其实就是rankindex
+ 基于得分筛选样本：
    + 正样本：$$p^{+}=\underset{p \in P}{\arg \max } R(q, p)=p_1$$
    + hard负例：排名第$$N$$位的样本$$p_N$$，也可以从除了第1个外的N-1个里随机sample

#### Unified Fine-tuning Mixture

&nbsp;

除了FRet，还融合了多个公开数据集：

+ Natural Questions：[Natural Questions: A Benchmark for Question Answering Research](https://aclanthology.org/Q19-1026.pdf)
+ HotpotQA：[Hotpotqa: A dataset for diverse, explainable multi-hop question answering](https://arxiv.org/pdf/1809.09600)
+ FEVER：[Fever: a large-scale dataset for fact extraction and verification](https://arxiv.org/pdf/1803.05355)
+ MedMCQA：[Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering](https://arxiv.org/pdf/2203.14371)
+ SNLI：[A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326)
+ MNLI：[A broad-coverage challenge corpus for sentence understanding through inference](https://arxiv.org/pdf/1704.05426)
+ MIRACL：多语言的数据集[Miracl: A multilingual retrieval dataset covering 18 diverse languages](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering)
+ huggingface上的一些分类数据集

将这些数据集处理成一个统一的格式，发现不同的任务对prompt的格式敏感程度不同，，非对称任务（如BEIR）对格式更敏感，而对称任务的性能相对稳定。

+ 对称格式（Symmetric Formatting）：输入和目标使用相同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：task: {task} | query: {target}
+ 非对称格式（Asymmetric Formatting）：输入和目标使用不同的格式。
    + 输入：task: {task} | query: {input}
    + 目标：title: {title} | text: {target}

参考[One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://aclanthology.org/2023.findings-acl.71.pdf)，对于每个文本$$x$$，找到另一个同样label是$$y$$的样本当成正样本$$x^{+}$$，随机找一个label不是$$y$$的当作负样本$$x^{-}$$。

实践中，在一个batch中，同一个$$x^+$$可能出现overlap，会造成in-batch negatives中的false negative问题。参考[poe的回复](https://poe.com/s/XHJyRAAOc9ZAuQu5yz2p)，即同一个mini-batch中假设有(x1, x1+, x1-) 和 (x2, x2+, x2-)，可能出现x1+ 与 x2 或 x2+ 相同或非常相似的情况。那么：

+ 对于x1来说，x1+是正例。
+ 但是如果x1+ 与 x2 相同或非常相似，模型可能会错误地认为x1+应该与x2区分开来，因为x2是另一个样本的输入。
+ 或者如果x1+ 与 x2+ 相同或非常相似，模型可能会混淆应该如何处理这个重叠的样本。

因为在理想情况下，x1+应该只是x1的正例，而不应该被视为任何其他样本的负例。但由于重叠，模型可能错误地将x1+视为x2或其他样本的负例。解决方法，给每个三元组分配唯一的id，让模型专注于在给定x的情况下，区分x+和x-。

有$$M$$个数据集$$\left[\mathcal{D}^{(1)}, \ldots, \mathcal{D}^{(M)}\right]$$，每个数据集是$$\mathcal{D}^{(m)}=\left\{\left(t_i, q_i, p_i^{+}, p_i^{-}\right)\right\}_{i=1}^N$$，$$t$$是任务描述，给定一个batch_size=B的batch，同batch里的其他query可以看成[SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives](https://arxiv.org/pdf/2306.02516)中说的same-tower negative（对同模态的效果比较好，例如这边都是query）：

XXX
\mathcal{L}_{\text {main }}=\frac{1}{B} \sum_{i=1}^B\left[-\log \frac{e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_i^{+}\right) / \tau}}{\sum_{j=1}^B\left(e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{p}_j^{+}\right) / \tau}+\mathbb{1}_{[j \neq i]} e^{\operatorname{sim}\left(\mathbf{q}_i, \mathbf{q}_j\right) / \tau}\right)+e^{\operatorname{sim}\left(\mathbf{q}_i \mathbf{P}_i^{-}\right) / \tau}}\right]
XXX

同时还参考[Matryoshka representation learning](https://arxiv.org/pdf/2205.13147)的俄罗斯套娃加了一个MRL的loss，让模型适配不同的dim，gecko是768和256的dim


### Octopus

[超越GPT-4，斯坦福团队手机可跑的大模型火了，一夜下载量超2k](https://mp.weixin.qq.com/s/qnFZOPLpdRxW42_cLUcImA)

[Octopus v2: On-device language model for super agent](https://arxiv.org/pdf/2404.01744.pdf)

[https://huggingface.co/NexaAIDev/Octopus-v2](https://huggingface.co/NexaAIDev/Octopus-v2)

[参数量不到10亿的OctopusV3，如何媲美GPT-4V和GPT-4？](https://mp.weixin.qq.com/s/mUpX-nvo221WVii-gnjUmQ)

[Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent](https://arxiv.org/pdf/2404.11459.pdf)

[Octopus v4: Graph of language models](https://arxiv.org/pdf/2404.19296)

### Cohere Command R+

[开源模型打败GPT-4！LLM竞技场最新战报，Cohere Command R+上线](https://mp.weixin.qq.com/s/uUeGQFWel5NLfFRFFF8w9g)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

[https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit](https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit)

1040亿的参数量，相比于Grok-1（3140亿）还差了一些，但Command R+并非Grok那种MoE架构，所以这1040亿参数是实打实的完全用于推理，而Grok-1的活跃参数为860亿。相比commend R：

+ 高级检索增强生成（RAG）与引用以减少幻觉
+ 10种主要语言的多语言覆盖，支持全球业务运营
+ 工具的运用以自动化复杂的业务流程

[明确了：文本数据中加点代码，训练出的大模型更强、更通用](https://mp.weixin.qq.com/s/3Ks72bIGZrNNUukqaHLSQg)

[To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/abs/2408.10914)


### CT-LLM：以中文为中心的LLM

[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/pdf/2404.04167.pdf)

当前，绝大多数LLM基本上都是以英文语料库训练得到的，然后经过SFT来匹配不同的语种。本文作者考虑以中文为基础的预训练模型是否可以激活对其它语言的能力。

作者从头开始训练中文大模型，在训练过程中「主要纳入中文文本数据」，最终作者得到了一个2B规模的中文Tiny LLM（CT-LLM）。结果表明，该模型在中文任务上表现出色，且通过SFT也能很好的支持英文。

### OpenELM

[苹果卷开源大模型，公开代码、权重、数据集、训练全过程，OpenELM亮相](https://mp.weixin.qq.com/s/uwDoKG2Q9-w37ogewBJTrQ)

[OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619.pdf)

[https://github.com/apple/corenet](https://github.com/apple/corenet)

[超强Siri即将到来！苹果10篇重磅AI研究全总结，iOS 18关键一瞥](https://mp.weixin.qq.com/s/zQu0BtDYG-XAAX87i4MGIA)

[苹果智能背后模型公布：3B模型优于Gemma-7B，服务器模型媲美GPT-3.5-Turbo](https://mp.weixin.qq.com/s/xkZrD_8zsmVfYYohH7ucUw)

### Arctic

[仅需Llama3 1/17的训练成本，Snowflake开源128x3B MoE模型](https://mp.weixin.qq.com/s/0mqx1xkyhOXDGpbu42d_5g)

[全球最大开源模型再刷爆纪录！4800亿参数MoE击败Llama 3、Mixtral](https://mp.weixin.qq.com/s/Wbs30QvvtWtYB6mp47Z8NA)

[https://huggingface.co/Snowflake/snowflake-arctic-instruct](https://huggingface.co/Snowflake/snowflake-arctic-instruct)

### Qwen

[Qwen1.5-110B：首个国产千亿参数开源大模型](https://mp.weixin.qq.com/s/Lb08elR2r23UN5kWnhF-BA)

4月26日，Qwen开源了其第一个千亿参数大模型Qwen1.5-110B，这应该也是国内第一个千亿规模的开源大模型。其包含1100亿参数，更重要的是这是一个Dense模型，而非MoE模型。从各项评测来看，Qwen1.5-110B足以与Llama3-70B相抗衡，部分指标也取得了更高的水平。

[阿里云Qwen2.5发布！再登开源大模型王座，Qwen-Max性能逼近GPT-4o](https://mp.weixin.qq.com/s/D2D9hga06bg2AMuGnizN5w)

所有 Qwen2.5 系列模型都在 18 万亿（18T）tokens 的数据上进行了预训练。在语言模型方面，Qwen2.5开源了7个尺寸：0.5B、1.5B、3B、7B、14B、32B、72B，每个都在同等参数赛道创造了业界最佳成绩。这些型号的设定充分考虑了下游场景的不同需求：

+ 3B：适配手机等端侧设备的黄金尺寸；
+ 32B：最受开发者期待的「性价比之王」，可在性能和功耗之间获得最佳平衡
+ Qwen2.5-32B的整体表现甚至超越了Qwen2-72B。

### DeepSeek-V2

[一块钱100万token，超强MoE模型开源，性能直逼GPT-4-Turbo](https://mp.weixin.qq.com/s/tAA8XUbU__9FgvEvXxsykw)

[https://github.com/deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)

[从MHA到MLA看Attention优化：谈谈DeepSeek拼多多级的推理价格](https://mp.weixin.qq.com/s/l2uUXGQ-8Rj_nI3JG5lZ_g)

[缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://spaces.ac.cn/archives/10091)

[](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

#### DeepSeek-Prover-V1.5

[DeepSeek开源数学大模型，高中、大学定理证明新SOTA](https://mp.weixin.qq.com/s/q-CZWNrcka6ttLuDW_1itg)

[DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/pdf/2408.08152)

[https://github.com/deepseek-ai/DeepSeek-Prover-V1.5](https://github.com/deepseek-ai/DeepSeek-Prover-V1.5)

#### Deepseek-v3

[DeepSeek-V3 正式发布](https://mp.weixin.qq.com/s/iFZOQsUNkpkXPDvOkE99wQ)

[论文](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)

### FALCON 2

[时隔一年Falcon回归！110亿参数5.5万亿token，性能超越Llama 3](https://mp.weixin.qq.com/s/CZNVl_GmYm_aPidMJrRHUg)

### MiniCPM

#### MiniCPM-Llama3-V

[登顶Top2！MiniCPM-V 8B新版本：GPT-4V水准小钢炮，8G显存，4070轻松推理！](https://mp.weixin.qq.com/s/TQVHJlZDExD3nMPRsqa_5w)

[可信度超越GPT-4V，清华&面壁揭秘「小钢炮」模型背后的高效对齐技术](https://mp.weixin.qq.com/s/7otafJLrrj4jlZIltQxcjQ)

[RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/pdf/2405.17220)

[https://github.com/RLHF-V/RLAIF-V](https://github.com/RLHF-V/RLAIF-V)

#### MiniCPM 3.0

[小模型杀疯了！仅4B参数性能超GPT-3.5！无限长文本性能超Kimi](https://mp.weixin.qq.com/s/qJM9OTDHS3pJB9ozFuRP1g)

[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/pdf/2404.06395)

### DCFormer

[彩云科技DCFormer大模型发布，效率是Transformer的两倍！](https://zhuanlan.zhihu.com/p/699582521)

[Improving Transformers with Dynamically Composable Multi-Head Attention](https://arxiv.org/pdf/2405.08553)

[https://github.com/Caiyun-AI/DCFormer](https://github.com/Caiyun-AI/DCFormer)

### hunyuan

[腾讯混元又来开源，一出手就是最大MoE大模型](https://mp.weixin.qq.com/s/GVFhlelNgrIDnzIIyW3yXg)

[Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265)

[https://github.com/Tencent/Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large)

[https://llm.hunyuan.tencent.com/](https://llm.hunyuan.tencent.com/)

### 阶跃星辰

[在「最难LLM评测榜单」上，阶跃万亿参数模型拿下中国第一](https://mp.weixin.qq.com/s/rGv2r_-owZ3jIx3IPk6XfQ)

### Tülu 3(基于llama3.1)

(toread)

[这才是真・开源模型！公开「后训练」一切，性能超越Llama 3.1 Instruct](https://mp.weixin.qq.com/s/sTtBkVkqy0CQtpzcR6SN-A)

[TÜLU 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/pdf/2411.15124)

[https://github.com/allenai/open-instruct](https://github.com/allenai/open-instruct)

[https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)

## post training的一些总结

[工业界主流大语言模型后训练(Post-Training)技术总结](https://mp.weixin.qq.com/s/sm2HEKJaeIUy9yXZCj0L-g)

### baichuan

[百川新模型超GPT-4o近20%，首创自约束训练方案突破瓶颈，主打「领域增强」](https://mp.weixin.qq.com/s/7oupuln3BQHPa7zpfGdMXQ)

Baichuan4-Finance

评测benchmark：[https://github.com/FLAME-ruc/FLAME/tree/main](https://github.com/FLAME-ruc/FLAME/tree/main)

## 小模型

[权重、代码、数据集全开源，性能超越Mistral-7B，苹果小模型来了](https://mp.weixin.qq.com/s/M58y8F5WeOH6i5nNKhtHbw)

[DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/pdf/2406.11794)

[https://huggingface.co/apple/DCLM-7B](https://huggingface.co/apple/DCLM-7B)

[小模型卷起来了：Mistral联合英伟达开源12B小模型，128k上下文](https://mp.weixin.qq.com/s/7oSxdFyqJ7MUpbfuNB_n5Q)

Mistral NeMo 使用基于 Tiktoken 的新分词器 Tekken，该分词器经过 100 多种语言的训练，能比以前 Mistral 模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。在压缩源代码、中文、意大利文、法文、德文、西班牙文和俄文时，它的效率要高出约 30%。在压缩韩文和阿拉伯文时，它的效率是原来的 2 倍和 3 倍。事实证明，与 Llama 3 分词器相比，Tekken 在压缩所有语言中约 85% 的文本方面更胜一筹。

Mistral NeMO 经历了高级微调和对齐阶段。与 Mistral 7B 相比，它在遵循精确指令、推理、处理多轮对话和生成代码方面的能力大大提升。

+ 基础模型：[https://huggingface.co/mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)
+ 指令微调模型：[https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)

[大模型已过时，小模型SLM才是未来？苹果正在研究这个](https://mp.weixin.qq.com/s/vAa1Tmse-Sn_nhaceWC1lg)

[Computational Bottlenecks of Training Small-scale Large Language Models](https://arxiv.org/pdf/2410.19456)

[研究大模型门槛太高？不妨看看小模型SLM，知识点都在这](https://mp.weixin.qq.com/s/sg7HveGDjMEj-ZcHS3YizA)

[A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/abs/2411.03350)

[https://github.com/FairyFali/SLMs-Survey](https://github.com/FairyFali/SLMs-Survey)

### BitNet

[微软开源爆火1.58bit大模型推理框架！千亿参数模型量化后单CPU可跑，速度每秒5-7个token](https://mp.weixin.qq.com/s/gerCRxj4eULOut9PtMlNog)

[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453)

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)

[https://github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)

### Zamba 2

[7B新王登基！Zamba 2完胜同级模型，推理效率比Llama 3提升20%，内存用量更少](https://mp.weixin.qq.com/s/6_dQod3hS1IJ_xU09x_jrg)

用了mamba 2

[Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/pdf/2405.16712)

+ hf版本：[https://huggingface.co/Zyphra/Zamba2-2.7B](https://huggingface.co/Zyphra/Zamba2-2.7B)
+ 纯pytorch版本：[https://github.com/Zyphra/Zamba2](https://github.com/Zyphra/Zamba2)



## 小结

[开源模型进展盘点：最新Mixtral、Llama 3、Phi-3、OpenELM到底有多好？](https://mp.weixin.qq.com/s/bgdDYkGHbPZMMSJPIutFSQ)

