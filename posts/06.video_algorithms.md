## 视频算法

### 视频数据集

#### VTAB

[The Visual Task Adaptation Benchmark](https://arxiv.org/abs/1910.04867)

谷歌 AI 推出了「视觉任务适应性基准」（Visual Task Adaptation Benchmark，VTAB）。这是一个多样性的、真实的和具有挑战性的表征基准。这一基准基于以下原则：在所需领域内数据有限的情况下，更好的表征应当能够在未见任务上实现更佳的性能。受启发于推动其他机器学习领域进展的一些基准，如用于自然图像分类的 ImageNet、自然语言处理的 GLUE 和强化学习的 Atari，VTAB 遵循相似的准则：（i）对解决方案施加最小约束，以鼓励创造性；（ii）注重实际；（iii）借助挑战性任务进行评估。

### 视频检索

[用语言直接检索百万视频，这是阿里TRECVID 视频检索冠军算法](https://mp.weixin.qq.com/s/wQRut3_QO0WCTzGklE07DA)

### 视频编码相关

[TIP 2019开源论文：基于深度学习的HEVC多帧环路滤波方法](https://mp.weixin.qq.com/s/OkywKX4XygM8VqkL8A1fcA)

### 视频显著区域检测

[AAAI 2020 \| 速度提升200倍，爱奇艺&北航等提出基于耦合知识蒸馏的视频显著区域检测算法](https://mp.weixin.qq.com/s/VbvCTEYC2FSMAff6bkjAjQ)

[Ultrafast Video Attention Prediction with Coupled Knowledge Distillation](https://arxiv.org/pdf/1904.04449.pdf)

### 视频理解

#### pyslowfast

[视频识别SOTA模型都在这了—PySlowFast! Facebook AI Research开源视频理解前沿算法代码库](https://mp.weixin.qq.com/s/kRUa4fL64BbxqQ6Y-kuQ1g)

[https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast)

### 视频插帧相关

#### DAIN

[Depth-Aware Video Frame Interpolation](https://arxiv.org/pdf/1904.00830)

[https://github.com/baowenbo/DAIN](https://github.com/baowenbo/DAIN)

视频帧合成是信号处理领域的一个有趣的分支。通常，这都是关于在现有视频中合成视频帧的。如果在视频帧之间完成操作，则称为内插（interpolation）；而在视频帧之后进行此操作，则称为外推（extrapolation）。视频帧内插是一个长期存在的课题，并且已经在文献中进行了广泛的研究。这是一篇利用了深度学习技术的有趣论文。通常，由于较大的物体运动或遮挡，插值的质量会降低。在本文中，作者使用深度学习通过探索深度信息来检测遮挡。

他们创建了称为“深度感知视频帧内插”（Depth-Aware video frame INterpolation，DAIN）的架构。该模型利用深度图、局部插值核和上下文特征来生成视频帧。本质上，DAIN是基于光流和局部插值核，通过融合输入帧、深度图和上下文特征来构造输出帧。
在这些文章中，我们有机会看到一些有趣的论文和在深度学习领域取得的进步。这一领域在不断发展，我们预计2020年会更有趣。

#### Quadratic Video Interpolation

[NeurIPS 2019 Spotlight \| 超清还不够，商汤插帧算法让视频顺滑如丝](https://mp.weixin.qq.com/s/KUM5Qygxa7EuEYoR-UA_bA)

这个方法的论文被 NeurIPS 2019 接收为 Spotlight 论文，该方法还在 ICCV AIM 2019 VideoTemporal Super-Resolution Challenge 比赛中获得了冠军。

[Quadratic Video Interpolation](https://papers.nips.cc/paper/8442-quadratic-video-interpolation.pdf)

### TVN

[单CPU处理1s视频仅需37ms、GPU仅需10ms，谷歌提出TVN视频架构](https://mp.weixin.qq.com/s/Ev2vBSIPyLpFa9pU4ybcTA)

[Tiny Video Networks](https://arxiv.org/abs/1910.06961v1)

### MvsGCN：多视频摘要

[MvsGCN: A Novel Graph Convolutional Network for Multi-video Summarization](https://dl.acm.org/citation.cfm?doid=3343031.3350938)

试图为视频集合生成单个摘要的多视频摘要，是处理不断增长的视频数据的重要任务。在本文中，我们第一个提出用于多视频摘要的图卷积网络。这个新颖的网络衡量了每个视频在其自己的视频以及整个视频集中的重要性和相关性。提出了一种重要的节点采样方法，以强调有效的特征，这些特征更有可能被选择作为最终的视频摘要。为了解决视频摘要任务中固有的类不平衡问题，提出了两种策略集成到网络中。针对多样性的损失正则化用于鼓励生成多样化的摘要。通过大量的实验，与传统的和最新的图模型以及最新的视频摘要方法进行了比较，我们提出的模型可有效地生成具有良好多样性的多个视频的代表性摘要。它还在两个标准视频摘要数据集上达到了最先进的性能。

### A-GANet

[Deep Adversarial Graph Attention Convolution Network for Text-Based Person Search](https://dl.acm.org/citation.cfm?id=3350991)

新出现的基于文本的行人搜索任务旨在通过对自然语言的查询以及对行人的详细描述来检索目标行人。与基于图像/视频的人搜索（即人重新识别）相比，它实际上更适用，而不需要对行人进行图像/视频查询。在这项工作中，我们提出了一种新颖的深度对抗图注意力卷积网络（A-GANet），用于基于文本的行人搜索。A-GANet利用文本和视觉场景图，包括对象属性和关系，从文本查询和行人画廊图像到学习信息丰富的文本和视觉表示。它以对抗性学习的方式学习有效的文本-视觉联合潜在特征空间，弥合模态差距并促进行人匹配。具体来说，A-GANet由图像图注意力网络，文本图注意力网络和对抗学习模块组成。图像和文本图形注意网络设计了一个新的图注意卷积层，可以在学习文本和视觉特征时有效利用图形结构，从而实现精确而有区别的表示。开发了具有特征转换器和模态鉴别器的对抗学习模块，以学习用于跨模态匹配的联合文本-视觉特征空间。在两个具有挑战性的基准（即CUHK-PEDES和Flickr30k数据集）上的大量实验结果证明了该方法的有效性。

### VRD-GCN

[Video Relation Detection with Spatio-Temporal Graph](https://dl.acm.org/citation.cfm?doid=3343031.3351058)

我们从视觉内容中看到的不仅是对象的集合，还包括它们之间的相互作用。用三元组<subject，predicate，object>表示的视觉关系可以传达大量信息，以供视觉理解。与静态图像不同，由于附加的时间通道，视频中的动态关系通常在空间和时间维度上都相关，这使得视频中的关系检测变得更加复杂和具有挑战性。在本文中，我们将视频抽象为完全连接的时空图。我们使用图卷积网络使用新颖的VidVRD模型在这些3D图中传递消息并进行推理。我们的模型可以利用时空上下文提示来更好地预测对象及其动态关系。此外，提出了一种使用暹罗网络的在线关联方法来进行精确的关系实例关联。通过将我们的模型（VRD-GCN）与所提出的关联方法相结合，我们的视频关系检测框架在最新基准测试中获得了最佳性能。我们在基准ImageNet-VidVRD数据集上验证了我们的方法。实验结果表明，我们的框架在很大程度上领先于最新技术，一系列的消去研究证明了我们方法的有效性。

### video caption

[AAAI 2020 \| 北理工&阿里文娱：结合常识与推理，更好地理解视频并生成描述](https://mp.weixin.qq.com/s/zkf5_vsgdgDgk0OiTTI-IA)

[Joint Commonsense and Relation Reasoning for Image and Video Captioning](https://wuxinxiao.github.io/assets/papers/2020/C-R_reasoning.pdf)

北京理工大学和阿里合作的一篇关于利用对象之间的关系进行图像和视频描述 (image caption/video caption) 的论文。大多数现有方法严重依赖于预训练的对象及其关系的检测器，因此在面临诸如遮挡，微小物体和长尾类别等检测挑战时可能效果不佳。

在本文中，研究者提出了一种联合常识和关系推理的方法 (C-R Reasoning)，该方法利用先验知识进行图像和视频描述，而无需依赖任何目标检测器。先验知识提供对象之间的语义关系和约束，作为指导以建立概括对象关系的语义图，其中一些对象之间的关系是不能直接从图像或视频中获得。特别是，本文的方法是通过常识推理和关系推理的迭代学习算法交替实现的，常识推理将视觉区域嵌入语义空间以构建语义图，关系推理用于编码语义图以生成句子。作者在几个基准数据集上的实验验证了该方法的有效性。

这篇论文并不是聚焦于常识知识和常识推理本身，而是联合常识和关系推理使得图像和视频描述中那些「难以捉摸」，「并非直接可见」的物体或关系现形，使得描述更加精准。

### 小视频推荐

#### MMGCN

[MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video](https://dl.acm.org/citation.cfm?id=3351034)

个性化推荐在许多在线内容共享平台中起着核心作用。为了提供优质的微视频推荐服务，重要的是考虑用户与项目（即短视频）之间的交互以及来自各种模态（例如视觉，听觉和文本）的项目内容。现有的多媒体推荐作品在很大程度上利用多模态内容来丰富项目表示，而为利用用户和项目之间的信息交换来增强用户表示并进一步捕获用户对不同模式的细粒度偏好所做的工作却较少。在本文中，我们建议利用用户-项目交互来指导每种模式中的表示学习，并进一步个性化微视频推荐。我们基于图神经网络的消息传递思想设计了一个多模态图卷积网络（MMGCN）框架，该框架可以生成用户和微视频的特定模态表示，以更好地捕获用户的偏好。具体来说，我们在每个模态中构造一个user-item二部图，并用其邻居的拓扑结构和特征丰富每个节点的表示。通过在三个公开可用的数据集Tiktok，Kwai和MovieLens上进行的大量实验，我们证明了我们提出的模型能够明显优于目前最新的多模态推荐方法。

#### ALPINE

[Routing Micro-videos via A Temporal Graph-guided Recommendation System](https://dl.acm.org/citation.cfm?id=3350950)

在过去的几年中，短视频已成为社交媒体时代的主流趋势。同时，随着短视频数量的增加，用户经常被他们不感兴趣的视频所淹没。尽管现有的针对各种社区的推荐系统已经取得了成功，但由于短视频平台中的用户具有其独特的特征：多样化的动态兴趣，多层次的兴趣以及负样本，因此它们无法应用于短视频的一种好的方式。为了解决这些问题，我们提出了一个时间图指导的推荐系统。特别是，我们首先设计了一个新颖的基于图的顺序网络，以同时对用户的动态兴趣和多样化兴趣进行建模。同样，可以从用户的真实负样本中捕获不感兴趣的信息。除此之外，我们通过用户矩阵将用户的多层次兴趣引入推荐模型，该矩阵能够学习用户兴趣的增强表示。最后，系统可以通过考虑上述特征做出准确的推荐。在两个公共数据集上的实验结果证明了我们提出的模型的有效性。

### 长视频剪辑

[让UP主不再为剪视频发愁，百度等提出用AI自动截取故事片段](https://mp.weixin.qq.com/s/yZ1lTEPVK1KaLr9__NC51Q)

[TruNet: Short Videos Generation from Long Videos via Story-Preserving Truncation](https://arxiv.org/pdf/1910.05899v1.pdf)

### AutoFlip

[不想横屏看视频？谷歌开源框架AutoFlip一键截出最精彩竖版视频](https://mp.weixin.qq.com/s/Jtf7ZsploJ40-WninCPuVg)

在使用过程中，只需要将一段视频和目标维度（如截取的长宽比类型）作为输入，AutoFlip 会分析视频内容并提出一个优化路径和裁剪策略，最后输出一段视频。

[https://github.com/google/mediapipe](https://github.com/google/mediapipe)

[https://github.com/google/mediapipe/blob/master/mediapipe/docs/autoflip.md](https://github.com/google/mediapipe/blob/master/mediapipe/docs/autoflip.md)

### 快手视频相关工作

[同为工业界最大的推荐业务场景，快手短视频推荐与淘宝推荐有何不同？](https://mp.weixin.qq.com/s/AsJDF-JmbYlv8dYFeYXVKw)

[AI碰撞短视频，从推荐到直播，快手探索了这些ML新思路](https://mp.weixin.qq.com/s/Wn-5VD2-YWwVUWCMEy-lvw)

视频推荐、内容分发优化、视频码率优化这三方面探索提升快手视频体验的新方案。

#### EIUM：讲究根源的快手短视频推荐

[Explainable Interaction-driven User Modeling over Knowledge Graph for Sequential Recommendation](https://dl.acm.org/citation.cfm?id=3350893)

#### Comyco：基于质量感知的码率自适应策略

[Comyco: Quality-aware Adaptive Video Streaming via Imitation Learning](https://dl.acm.org/citation.cfm?id=3351014)

#### Livesmart：智能CDN调度

[Livesmart: a QoS-Guaranteed Cost-Minimum Framework of Viewer Scheduling for Crowdsourced Live Streaming](https://dl.acm.org/citation.cfm?id=3351013)

### 抖音视频相关工作

[图解抖音推荐算法](https://mp.weixin.qq.com/s/oP6I5S7MVkfafmRBL-3WqA)

### google视频相关工作

[通过未标记视频进行跨模态时间表征学习](https://mp.weixin.qq.com/s/5qC70NoTBQ95vjI4cGl66g)

两篇：

[VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)，VideoBert模型。

[Contrastive Bidirectional Transformer for Temporal Representation Learning](https://arxiv.org/abs/1906.05743)，CBT模型。

### 阿里短视频推荐相关工作

[淘宝如何拥抱短视频时代？视频推荐算法实战](https://mp.weixin.qq.com/s/8N09Argm9sNJRYipq3Mipw)

