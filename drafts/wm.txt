
VLA(Vision-Language-Action Model)是一种统一的多模态模型,旨在将视觉感知、语言理解和动作执行融合在一个端到端的框架内。它通常采用Transformer架构,能够接收图像/视频和自然语言指令作为输入,并自回归地生成一系列动作指令(Token)。

典型的VLA模型(如RT-2,OpenVLA)遵循两阶段训练:
大规模预训练:在海量的互联网图文、视频数据上进行学习,以获得通用的视觉和语言表征能力。监督微调(SFT):在由人类专家操作机器人录制的高质量"状态-对力作"轨迹数据上进行行为克隆(Behavioral Cloning),教会模型执行具体任务。
然而,如前文所述,纯SFT的VLA模型本质上是"模仿学习",它严重依赖于演示数据的覆盖范围,当遇到新场景或新指令时,表现脆弱,泛化能力不足。

机器人技术的核心挑战在于如何让智能体在复杂、多变且充满不确定性的开放世界中,像人一样具备强大的泛化能力和自主学习能力。当前研究主要面临以下瓶颈:
模型泛化能力差:传统的机器人大多在特定、封闭的实验室环境中工作,一旦进入全新的家庭环境,面对新物体、新布局和新光照,便会"失灵"。这暴露了其严重依赖训练分布后,缺乏"开放世界泛化"能力的根本问题
监督学习(SFT)的局限性:当前主流的视觉-语言-动作(VLA)模型通常采用"预训练+监督微调(SFT)"的范式。SFT依赖于大量高质量的专家演示数据,但这不仅收集成本高昂、过程繁琐(数据稀缺性),更关键的是,模型仅仅在"模仿"专家,无法通过试错来探索更优或更鲁棒的约策略,导致泛化能力差(Poor Generalization)。
强化学习(RL)的效率问题:无模型(Model-Free)强化学习虽然能通过探索学习,但直接应用于高维视觉输入时,样本效率极低,需要数亿次的交互才能学会简单任务,,在真实机器人上几乎不可行。
仿真到现实(Sim-to-Real)的鸿沟:虽然可以在模拟器中低成本地训练,但模拟器与真实世界之间始终存在动力学差异,导致在模拟器里表现优异的策略,迁移到真实机器人上时性能会大幅下降
为了解决这些问题,研究者们开始探索将世界模型(WM)作为环不境的内部模拟器以提升学习效率,并利用强化学习(RL)赋予VLA模型超越模仿的探索和泛化能力,最终走向一个统一、高效且泛化的智能机器人框架。


世界模型(World Model,WM)是智能体内部构建的关于环境如何运作的动态模型。其核心思想是,让智能体先学习一个环境的"模拟器",然后可以在这个内部的、可微分的"梦境"或"想象"中进行高效、快速的规划和策略学习,从而大幅减少与真实世界的昂贵交互。世界模型主要扮演三大核心角色:

动力学模型(Dynamic Model for MBRL):这是最经典的角色,用于模型基强化学习(MBRL)。模型在紧凑的潜在空间(LatentSpace)中预测未来的状态和奖励,智能体在此基础上进行策略优化。经典代表是Dreamer系列,它基于循环状态空间模型(RSSM)在潜在空间中进行长时序的想象(rollout),并完全在想原中训练Actor-Critic策略,极大地提升了样本效率。
神经模拟器(Neural Simulator):此类世界模型专注于生成高保真、可控的感官数据(如视频),作为传统物理引擎的替代品。它可以用来生成训练数据、合成罕见或危险的场景、或进行闭环测试。代表模型如Cosmos、GAIA-1等,通常基于扩散模型(Diffusion Model)或自回归Transformer,能够生成高质量、时空一致的视频
奖励模型(Reward Model):在奖励稀疏或难以设计的任务中,世界模型可以用来生成稠密的奖励信号。例如,VIPER利用预训练的视频测测模型,将"智能体轨迹与专家演示视频的相似度"作为奖励,从而在没有显式奖励函数的情况下驱动策略学习。
从架构上看,世界模型经历了从早期的递归模型(如RSSM)到能够捕捉长程依赖的Transformer模型,再到生成质量更高的扩散模型和自回归模型的演进。

vla+wm
该方向旨在将VLA的语义理解能力与WM的动态预测/规划能力深度融合,构建一个既能"听懂指令",又能"预见未来"的统一智能体。典型实现方案:
统一的自回归框架:WorldVLA将"动作模型(VLA)"和"世界模型(WM)"统一到一个共享的自回归Transformer框架和词表中。通过不同的提示(Prompt),模型既可以以生成动作(VLA任务:"机器人下一步该做什么?"),也可以预测世界的下一顿视觉画面(WM任务:"接下来会发生什么?")。这种设计带来了双向增益:WM学到的物理因果关系能提升动作的合理性;而VLA对动作的理解则能让WM的视频预测更符合物理规律。
基于世界表征的策略学习:DreamVLA提出了一种更解耦的思路。它首先用一个世界模型从输入中提取并预测一个抽象的worldembedding,这个embedding代表了"未来世界状态的批抽象表示"。然后,VLA的动作策略不再直接观察原始图像,而是基于这个紧凑且蕴含未来的world embedding来决策。这种"看未来的因果结果,再决定动作"的方式,比传统的"模式匹配像素"更为高效和鲁棒。
世界模型作为动态后训练:在一些框架中,WM被用作一个独立的训练阶段。例如,在策略微调之前,先让模型仅用视频和指令来学习预测后续视频帧(世界模型后训练),,这能让模型扎实地学到"语义+时序动力学",为后续生成更精准、更具前瞻性的动作策略打下坚实基础。
通过VLA与WM的结合,模型不仅能理解任务目标,还能在内部进行"推演",评估不同动作可能带来的后果,从而做出更智能的决策
